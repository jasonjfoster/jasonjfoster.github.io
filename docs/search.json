[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jasonjfoster.github.io",
    "section": "",
    "text": "Eigen\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCloud\n\n\n\ncomputing\n\n\npython\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware\n\n\n\ndevelopment\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nalgorithms\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 7, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/optim-r/index.html",
    "href": "posts/optim-r/index.html",
    "title": "Optimization",
    "section": "",
    "text": "library(quantmod)\nlibrary(roll)\nlibrary(data.table)\nfactors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\nfactors &lt;- c(factors_r, factors_d)\nwidth &lt;- 252\nscale &lt;- list(\"periods\" = 252, \"overlap\" = 5)\ngetSymbols(factors, src = \"FRED\")\nlevels_xts &lt;- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))\nreturns_xts &lt;- do.call(merge, lapply(factors, function(i) {\n    if (i %in% factors_r) {\n        diff(log((levels_xts[ , i])))\n    } else if (i %in% factors_d) {\n        -diff(levels_xts[ , i]) / 100\n    }    \n}))\noverlap_xts &lt;- roll_mean(returns_xts, scale[[\"overlap\"]], min_obs = 1, na_restore = TRUE)\nlibrary(CVXR)\ntickers &lt;- \"BAICX\" # fund inception date is \"2011-11-28\" \ninvisible(getSymbols(tickers, src = \"tiingo\", api.key = Sys.getenv(\"TIINGO_API_KEY\"), adjust = TRUE))\nprices_xts &lt;- do.call(merge, c(lapply(tickers, function(i) Cl(get(i))), all = TRUE))\ncolnames(prices_xts) &lt;- tickers\nindex(prices_xts) &lt;- as.Date(index(prices_xts))\nreturns_xts &lt;- merge(returns_xts, diff(log(prices_xts)))\noverlap_xts &lt;- merge(overlap_xts, roll_mean(returns_xts[ , tickers], scale[[\"overlap\"]], min_obs = 1))\n# weights &lt;- 0.9 ^ ((width - 1):0)\nweights &lt;- rep(1, width)\noverlap_xts &lt;- na.omit(overlap_xts)\noverlap_x_xts &lt;- tail(overlap_xts[ , factors], width) # same dimension as `weights`\noverlap_y_xts &lt;- tail(overlap_xts[ , tickers], width)"
  },
  {
    "objectID": "posts/optim-r/index.html#random-turnover",
    "href": "posts/optim-r/index.html#random-turnover",
    "title": "Optimization",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\nrand_turnover1 &lt;- function(n_sim, n_assets, lower, upper, target) {\n    \n    rng &lt;- upper - lower\n    \n    result &lt;- rand_weights2b(n_sim, n_assets) * rng\n    result &lt;- result - rng / n_assets\n    \n    return(result)\n    \n}\n\n\nlower &lt;- -0.05\nupper &lt;- 0.05\ntarget &lt;- 0\n\n\napproach1 &lt;- rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\nrand_iterative &lt;- function(n_assets, lower, upper, target) {\n    \n    result &lt;- runif(n_assets - 1, min = lower, max = upper)\n    temp &lt;- target - sum(result)\n    \n    while (!((temp &lt;= upper) && (temp &gt;= lower))) {\n        \n        result &lt;- runif(n_assets - 1, min = lower, max = upper)\n        temp &lt;- target - sum(result)\n        \n    }\n    \n    result &lt;- append(result, temp)\n    \n    return(result)\n    \n}\n\n\nrand_turnover2 &lt;- function(n_sim, n_assets, lower, upper, target) {\n  \n    result_ls &lt;- list()\n    \n    for (i in 1:n_sim) {\n        \n        result_sim &lt;- rand_iterative(n_assets, lower, upper, target)\n        result_ls &lt;- append(result_ls, list(result_sim))\n        \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    \n    return(result)\n    \n}\n\n\napproach2 &lt;- rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-means",
    "href": "posts/optim-r/index.html#maximize-means",
    "title": "Optimization",
    "section": "Maximize means",
    "text": "Maximize means\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ntarget &lt;- 0.06\n\n\nmax_mean_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Maximize(t(params) %*% mu)\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0,\n                 quad_form(params, sigma) &lt;= target ^ 2)\n    \n    prob &lt;- Problem(obj, cons)\n    \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams1 &lt;- t(max_mean_optim(mu, sigma, target))\nparams1\n\n          [,1]      [,2]         [,3]         [,4]\n[1,] 0.4684635 0.5315365 8.714296e-09 5.696857e-09\n\n\n\nparams1 %*% mu\n\n           [,1]\n[1,] 0.04154086\n\n\n\nsqrt(params1 %*% sigma %*% t(params1))\n\n     [,1]\n[1,] 0.06\n\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\") # roll (&gt;= 1.1.7)\n# library(rolloptim)\n# \n# mu &lt;- roll_mean(returns_x_xts, 5)\n# sigma &lt;- roll_cov(returns_x_xts, width = 5)\n# \n# xx &lt;- roll_crossprod(returns_x_xts, returns_x_xts, 5)\n# xy &lt;- roll_crossprod(returns_x_xts, y, 5) # TO DO\n# \n# roll_max_mean(mu)"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-variance",
    "href": "posts/optim-r/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ntarget &lt;- 0.03\n\n\nmin_var_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Minimize(quad_form(params, sigma))\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0,\n                 sum(mu * params) &gt;= target)\n    \n    prob &lt;- Problem(obj, cons)\n    \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams2 &lt;- t(min_var_optim(mu, sigma, target))\nparams2\n\n          [,1]      [,2]      [,3]         [,4]\n[1,] 0.3155993 0.5167696 0.1676311 2.388936e-21\n\n\n\nparams2 %*% mu\n\n     [,1]\n[1,] 0.03\n\n\n\nsqrt(params2 %*% sigma %*% t(params2))\n\n           [,1]\n[1,] 0.04225288\n\n\n\n# roll_min_var(sigma)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-utility",
    "href": "posts/optim-r/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nir &lt;- 0.5\ntarget &lt;- ir / 0.06 # ir / std (see Black-Litterman)\n\n\nmax_utility_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Minimize(0.5 * target * quad_form(params, sigma) - t(mu) %*% params)\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0)\n    \n    prob &lt;- Problem(obj, cons)\n        \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams3 &lt;- t(max_utility_optim(mu, sigma, target))\nparams3\n\n          [,1]      [,2]         [,3]         [,4]\n[1,] 0.4783768 0.5216232 1.483917e-22 2.206656e-22\n\n\n\nparams3 %*% mu \n\n           [,1]\n[1,] 0.04214324\n\n\n\nsqrt(params3 %*% sigma %*% t(params3))\n\n           [,1]\n[1,] 0.06116808\n\n\n\n# roll_max_utility(mu, sigma)"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nhttps://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html\n\n\n# roll_min_rss(xx, xy)\n\n\ndata.frame(\"max_pnl\" = t(params1),\n           \"min_risk\" = t(params2),\n           \"max_ratio\" = t(params3))\n\n       max_pnl     min_risk    max_ratio\n1 4.684635e-01 3.155993e-01 4.783768e-01\n2 5.315365e-01 5.167696e-01 5.216232e-01\n3 8.714296e-09 1.676311e-01 1.483917e-22\n4 5.696857e-09 2.388936e-21 2.206656e-22"
  },
  {
    "objectID": "posts/optim-py/index.html",
    "href": "posts/optim-py/index.html",
    "title": "Optimization",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\nimport os\nfrom scipy.optimize import minimize\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\nprices_df = pdr.get_data_tiingo(tickers, start = \"1900-01-01\", api_key = os.getenv(\"TIINGO_API_KEY\"))\n\nC:\\Users\\jason\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas_datareader\\tiingo.py:234: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  return pd.concat(dfs, self._concat_axis)\n\nprices_df = prices_df.pivot_table(index = \"date\", columns = \"symbol\", values = \"adjClose\") \\\n    .tz_localize(None)\nreturns_cols = list(zip([\"returns\"], tickers))\noverlap_cols = list(zip([\"overlap\"], tickers))\nreturns_df[returns_cols] = np.log(prices_df).diff()\nreturns_df[overlap_cols] = returns_df[returns_cols].rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df.sort_index(axis = 1, inplace = True)\n# weights = np.array([0.9 ** i for i in range(width - 1, -1, -1)]).reshape((width, 1))\nweights = np.array([1] * width).reshape((width, 1))\noverlap_df = returns_df.dropna()[\"overlap\"]\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors][-width:] # same dimension as `weights`\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers][-width:]"
  },
  {
    "objectID": "posts/optim-py/index.html#random-turnover",
    "href": "posts/optim-py/index.html#random-turnover",
    "title": "Optimization",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights2b(n_sim, n_assets) * rng\n    result = result - rng / n_assets\n    \n    return result\n\n\nlower = -0.05\nupper = 0.05\ntarget = 0\n\n\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n    temp = target - sum(result)\n    \n    while not ((temp &lt;= upper) and (temp &gt;= lower)):\n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n    result = np.append(result, temp)\n    \n    return result\n\n\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n  \n    result_ls = []\n    \n    for i in range(n_sim):\n      \n      result_sim = rand_iterative(n_assets, lower, upper, target)\n      result_ls.append(result_sim)\n      \n    result = pd.DataFrame(result_ls)\n    \n    return result\n\n\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-mean",
    "href": "posts/optim-py/index.html#maximize-mean",
    "title": "Optimization",
    "section": "Maximize mean",
    "text": "Maximize mean\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ntarget = 0.06\nstart = np.array([1] * len(factors))\nbnds = [(np.finfo(float).eps, 1) for i in range(len(factors))]\n\n\ndef max_mean_cons(params, sigma, target):\n    \n    var = np.dot(params, np.dot(sigma, params))\n    \n    result = target ** 2 - var\n    \n    return result\n\ndef max_mean_obj(params, mu):\n    \n    result = -np.dot(mu, params)\n    \n    return result\n\ndef max_mean_optim(params, mu, sigma, target):\n  \n    cons = [{\"type\": \"ineq\", \"fun\": max_mean_cons, \"args\": (sigma, target)},\n           {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n    \n    result = minimize(max_mean_obj, params, args = (mu), bounds = bnds, constraints = cons)\n    \n    return result.x\n\n\nparams1 = max_mean_optim(start, mu, sigma, target)\nparams1\n\narray([4.68463527e-01, 5.31536473e-01, 2.22044605e-16, 2.22044605e-16])\n\n\n\nnp.dot(mu, params1)\n\n0.04154085933516425\n\n\n\nnp.sqrt(np.dot(params1, np.dot(sigma, params1)))\n\n0.06000000196240091"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-variance",
    "href": "posts/optim-py/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ntarget = 0.03\n\n\ndef min_var_cons(params, mu, target):\n    \n    result = np.dot(mu, params) - target\n    \n    return result\n\ndef min_var_obj(params, sigma):\n    \n    result = np.dot(params, np.dot(sigma, params))\n    \n    return result\n\ndef min_var_optim(params, mu, sigma, target):\n  \n    cons = [{\"type\": \"ineq\", \"fun\": min_var_cons, \"args\": (mu, target)},\n            {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n    \n    result = minimize(min_var_obj, params, args = (sigma), bounds = bnds, constraints = cons)\n    \n    return result.x\n\n\nparams2 = min_var_optim(start, mu, sigma, target)\nparams2\n\narray([0.30995599, 0.54503374, 0.11204235, 0.03296792])\n\n\n\nnp.dot(mu, params2)\n\n0.02999999998928274\n\n\n\nnp.sqrt(np.dot(params2, np.dot(sigma, params2))) \n\n0.042587027204221015"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-utility",
    "href": "posts/optim-py/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\n\n\ndef max_utility_obj(params, mu, sigma, target):\n    \n    result = 0.5 * target * (np.dot(params, np.dot(sigma, params))) - np.dot(mu, params)\n    \n    return result\n\ndef max_utility_optim(params, mu, sigma, target):\n  \n    cons = [{\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n    \n    result = minimize(max_utility_obj, params, args = (mu, sigma, target), bounds = bnds,\n                      constraints = cons) \n    \n    return result.x\n\n\nparams3 = max_utility_optim(start, mu, sigma, target)\nparams3\n\narray([4.77313935e-01, 5.22686065e-01, 2.77989437e-16, 2.68882139e-16])\n\n\n\nnp.dot(mu, params3)\n\n0.04207865418228583\n\n\n\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n\n0.061041525547569746"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nhttps://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html\n\n\npd.DataFrame({\n  \"max_pnl\": params1,\n  \"min_risk\": params2,\n  \"max_ratio\": params3\n})\n\n        max_pnl  min_risk     max_ratio\n0  4.684635e-01  0.309956  4.773139e-01\n1  5.315365e-01  0.545034  5.226861e-01\n2  2.220446e-16  0.112042  2.779894e-16\n3  2.220446e-16  0.032968  2.688821e-16"
  },
  {
    "objectID": "posts/eigen-r/index.html",
    "href": "posts/eigen-r/index.html",
    "title": "Eigen",
    "section": "",
    "text": "library(quantmod)\nlibrary(roll)\nlibrary(data.table)\n\n\nfactors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\nfactors &lt;- c(factors_r, factors_d)\nwidth &lt;- 252\nscale &lt;- list(\"periods\" = 252, \"overlap\" = 5)\n\n\ngetSymbols(factors, src = \"FRED\")\nlevels_xts &lt;- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))\n\n\nreturns_xts &lt;- do.call(merge, lapply(factors, function(i) {\n    if (i %in% factors_r) {\n        diff(log((levels_xts[ , i])))\n    } else if (i %in% factors_d) {\n        -diff(levels_xts[ , i]) / 100\n    }    \n}))\noverlap_xts &lt;- roll_mean(returns_xts, scale[[\"overlap\"]], min_obs = 1, na_restore = TRUE)\n\n\noverlap_xts &lt;- na.omit(overlap_xts)\n\n\nDecomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\neigen_decomp &lt;- function(x, comps) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values[1:comps]\n    V &lt;- LV$vectors[ , 1:comps]\n    \n    result &lt;- V %*% sweep(t(V), 1, L, \"*\")\n    \n    return(result)\n    \n}\n\n\ncomps &lt;- 1\n\n\neigen_decomp(overlap_xts, comps) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  0.0300775965 -3.659506e-03 -1.572091e-04  2.578798e-03\n[2,] -0.0036595055  4.452477e-04  1.912744e-05 -3.137594e-04\n[3,] -0.0001572091  1.912744e-05  8.216978e-07 -1.347882e-05\n[4,]  0.0025787985 -3.137594e-04 -1.347882e-05  2.211015e-04\n\n\n\n# cov(overlap_xts) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\nvariance_explained &lt;- function(x) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values\n    \n    result &lt;- cumsum(L) / sum(L)\n    \n    return(result)\n    \n}\n\n\nvariance_explained(overlap_xts)\n\n[1] 0.8733494 0.9922194 0.9982806 1.0000000\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n\\]\n\nroll_eigen1 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        evec &lt;- eigen(cov(x[idx, ]))[[\"vectors\"]][ , comp]\n        result_ls &lt;- append(result_ls, list(evec))\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\ncomp &lt;- 1\n\n\nraw_df &lt;- roll_eigen1(overlap_xts, width, comp)\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jjf234/rolleigen\") # roll (&gt;= 1.1.7)\n# library(rolleigen)\n# raw_df &lt;- roll_eigen(overlap_xts, width)[[\"vectors\"]][ , comp, ]\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\nroll_eigen2 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        evecs &lt;- eigen(cov(x[idx, ]))[[\"vectors\"]]\n                \n        if (i &gt; width) {\n          \n            similarity &lt;- crossprod(evecs, result_ls[[length(result_ls)]])\n            order &lt;- which.max(abs(similarity))\n            evec &lt;- sign(similarity)[order] * evecs[ , order]\n            \n            result_ls &lt;- append(result_ls, list(evec))\n            \n        } else {\n            result_ls &lt;- append(result_ls, list(evecs[ , comp]))\n        }\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\nclean_df &lt;- roll_eigen2(overlap_xts, width, comp)"
  },
  {
    "objectID": "posts/eigen-py/index.html",
    "href": "posts/eigen-py/index.html",
    "title": "Eigen",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\n\n\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\n\n\nhttps://pandas-datareader.readthedocs.io/en/latest/remote_data.html\n\n\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\n\n\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\n\n\noverlap_df = returns_df.dropna()[\"overlap\"]\n\n\ndef eigen(x):\n  \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    V = V[:, idx]\n    \n    result = {\n        \"values\": L,\n        \"vectors\": V\n    }\n    \n    return result\n\n\nDecomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\ndef eigen_decomp(x, comps):\n    \n    LV = eigen(x)\n    L = LV[\"values\"][:comps]\n    V = LV[\"vectors\"][:, :comps]\n    \n    result = np.dot(V, np.multiply(L, V.T))\n    \n    return result\n\n\ncomps = 1\n\n\neigen_decomp(overlap_df, comps) * scale[\"periods\"] * scale[\"overlap\"]\n\narray([[ 3.00775965e-02, -3.65950552e-03, -1.57209079e-04,\n         2.57879845e-03],\n       [-3.65950552e-03,  4.45247699e-04,  1.91274423e-05,\n        -3.13759352e-04],\n       [-1.57209079e-04,  1.91274423e-05,  8.21697786e-07,\n        -1.34788207e-05],\n       [ 2.57879845e-03, -3.13759352e-04, -1.34788207e-05,\n         2.21101492e-04]])\n\n\n\n# np.cov(overlap_df.T) * scale[\"periods\"] * scale[\"overlap\"]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\ndef variance_explained(x):\n    \n    LV = eigen(x)\n    L = LV[\"values\"]\n    \n    result = L.cumsum() / L.sum()\n    \n    return result\n\n\nvariance_explained(overlap_df)\n\narray([0.87334938, 0.99221936, 0.99828063, 1.        ])\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n\\]\n\ndef roll_eigen1(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evec = eigen(x.iloc[idx])[\"vectors\"][:, comp - 1]\n        result_ls.append(evec)\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df  \n\n\ncomp = 1\n\n\nraw_df = roll_eigen1(overlap_df, width, comp)\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\ndef roll_eigen2(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evecs = eigen(x.iloc[idx])[\"vectors\"]\n        \n        if i &gt; width - 1:\n            \n            similarity = np.dot(evecs.T, result_ls[-1])\n            order = np.argmax(np.abs(similarity))\n            evec = np.multiply(np.sign(similarity[order]), evecs[:, order])\n            \n            result_ls.append(evec)\n            \n        else:\n            result_ls.append(evecs[:, comp - 1])\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df\n\n\nclean_df = roll_eigen2(overlap_df, width, comp)"
  }
]