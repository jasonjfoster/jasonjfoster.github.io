[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jasonjfoster.github.io",
    "section": "",
    "text": "Risk\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 1, 2025\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nalgorithms\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 23, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/risk-r/index.html",
    "href": "posts/risk-r/index.html",
    "title": "Risk",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\ntickers &lt;- \"BAICX\" # fund inception date is \"2011-11-28\"\nintercept &lt;- TRUE"
  },
  {
    "objectID": "posts/risk-r/index.html#ordinary-least-squares",
    "href": "posts/risk-r/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\nlm_coef &lt;- function(x, y, weights, intercept) {\n  \n  # cbind.xts() changes names, e.g., \"(Intercept)\" =&gt; \"X.Intercept.\"\n  if (intercept) x &lt;- cbind(\"(Intercept)\" = 1, as.matrix(x))\n  \n  result &lt;- solve(crossprod(x, diag(weights)) %*% x) %*% crossprod(x, diag(weights) %*% y)\n  \n  return(result)\n  \n}\n\n\nt(lm_coef(overlap_x_xts, overlap_y_xts, weights, intercept))\n\n       (Intercept)     SP500  DTWEXAFEGS    DGS10 BAMLH0A0HYM2\nBAICX 0.0001652841 0.1826507 -0.09116029 2.982376     1.138883\n\n\n\nif (intercept) {\n  form &lt;- reformulate(termlabels = factors, response = tickers)\n} else {\n  form &lt;- reformulate(termlabels = factors, response = tickers, intercept = FALSE)\n}\n\nfit &lt;- lm(form, data = overlap_xts, weights = weights)\n    \ncoef(fit)\n\n  (Intercept)         SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 0.0001652841  0.1826506943 -0.0911602864  2.9823757010  1.1388834588 \n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\nlm_rsq &lt;- function(x, y, weights, intercept) {\n        \n  coef &lt;- lm_coef(x, y, weights, intercept)\n  \n  if (intercept) {\n    \n    x &lt;- cbind(1, x)\n    x &lt;- sweep(x, 2, apply(x, 2, weighted.mean, w = weights), \"-\")\n    y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n      \n  }\n  \n  result &lt;- (t(coef) %*% (crossprod(x, diag(weights)) %*% x) %*% coef) / (crossprod(y, diag(weights)) %*% y)\n  colnames(result) &lt;- \"R-squared\"\n  \n  return(result)\n    \n}\n\n\nlm_rsq(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n      R-squared\nBAICX  0.723516\n\n\n\nsummary(fit)$r.squared\n\n[1] 0.723516\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\nlm_se &lt;- function(x, y, weights, intercept) {\n    \n  n_rows &lt;- nrow(x)\n  n_cols &lt;- ncol(x)\n  \n  rsq &lt;- lm_rsq(x, y, weights, intercept)\n  \n  if (intercept) {\n    \n    # cbind.xts() changes names, e.g., \"(Intercept)\" =&gt; \"X.Intercept.\"\n    x &lt;- cbind(\"(Intercept)\" = 1, as.matrix(x))\n    y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n    \n    df_resid &lt;- n_rows - n_cols - 1\n      \n  } else {\n    df_resid &lt;- n_rows - n_cols\n  }\n  \n  var_y &lt;- crossprod(y, diag(weights)) %*% y\n  var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n  \n  result &lt;- sqrt(var_resid * diag(solve(crossprod(x, diag(weights)) %*% x)))\n  \n  return(result)\n    \n}\n\n\nlm_se(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n (Intercept)        SP500   DTWEXAFEGS        DGS10 BAMLH0A0HYM2 \n6.419903e-05 1.923799e-02 3.379825e-02 2.653411e-01 2.241477e-01 \n\n\n\ncoef(summary(fit))[ , \"Std. Error\"]\n\n (Intercept)        SP500   DTWEXAFEGS        DGS10 BAMLH0A0HYM2 \n6.419903e-05 1.923799e-02 3.379825e-02 2.653411e-01 2.241477e-01 \n\n\n\n\nShapley values\n\\[\nR^{2}_{i}=\\sum_{S\\subseteq N\\setminus\\{i\\}}{\\frac{|S|!\\;(n-|S|-1)!}{n!}}(R^{2}(S\\cup\\{i\\})-R^{2}(S))\n\\]\n\nhttps://real-statistics.com/multiple-regression/shapley-owen-decomposition/\n\n\nlm_shap &lt;- function(x, y, weights, intercept) {\n  \n  n_rows &lt;- nrow(x)\n  n_cols &lt;- ncol(x)\n  n_combn &lt;- 2 ^ n_cols\n  n_vec &lt;- array(0, n_combn)\n  ix_mat &lt;- matrix(0, nrow = n_cols, ncol = n_combn)\n  rsq &lt;- array(0, n_combn)\n  result &lt;- array(0, n_cols)\n  \n  # number of binary combinations\n  for (k in 1:n_combn) {\n    \n    n &lt;- 0\n    n_size &lt;- k - 1\n    \n    # find the binary combination\n    for (j in 1:n_cols) {\n      \n      if (n_size %% 2 == 0) {\n        \n        n &lt;- n + 1\n        \n        ix_mat[j, k] = j - 1 + 1\n        \n      }\n      \n      n_size &lt;- n_size %/% 2\n      \n    }\n    \n    n_vec[k] &lt;- n\n    \n    if (n &gt; 0) {\n      \n      ix_subset&lt;- which(ix_mat[ , k] != 0)\n      x_subset &lt;- x[ , ix_subset]\n      \n      rsq[k] &lt;- lm_rsq(x_subset, y, weights, intercept)\n\n    }\n    \n  }\n\n  # calculate the exact Shapley value for r-squared\n  for (j in 1:n_cols) {\n\n    ix_pos &lt;- which(ix_mat[j, ] != 0)\n    ix_neg &lt;- which(ix_mat[j, ] == 0)\n    ix_n &lt;- n_vec[ix_neg]\n    rsq_diff &lt;- rsq[ix_pos] - rsq[ix_neg]\n\n    for (k in 1:(n_combn / 2)) {\n\n      s &lt;- ix_n[k]\n      weight &lt;- factorial(s) * factorial(n_cols - s - 1) / factorial(n_cols)\n      result[j] &lt;- result[j] + weight * rsq_diff[k]\n\n    }\n    \n  }\n\n  return(result)\n  \n}\n\n\nlm_shap(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n[1] 0.33361553 0.01130286 0.12853382 0.25006378"
  },
  {
    "objectID": "posts/risk-r/index.html#principal-component-regression",
    "href": "posts/risk-r/index.html#principal-component-regression",
    "title": "Risk",
    "section": "Principal component regression",
    "text": "Principal component regression\n\nlibrary(pls) # \"Error in mvrValstats(object = fit, estimate = 'train'): could not find function 'mvrValstats'\"\n\n\ncomps &lt;- 1\n\n\nCoefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\npcr_coef &lt;- function(x, y, comps) {\n  \n  x &lt;- sweep(x, 2, colMeans(x), \"-\")\n  LV &lt;- eigen(cov(x))\n  V &lt;- LV[[\"vectors\"]]\n  \n  W &lt;- x %*% V\n  gamma &lt;- solve(crossprod(W)) %*% (crossprod(W, y))\n  \n  result &lt;- V[ , 1:comps] %*% as.matrix(gamma[1:comps])\n  \n  return(result)\n  \n}\n\n\nscale_x_xts &lt;- scale(overlap_x_xts)\n\n\nt(pcr_coef(scale_x_xts, overlap_y_xts, comps))\n\n             [,1]         [,2]          [,3]         [,4]\n[1,] 0.0006304978 3.882602e-05 -0.0001630078 0.0006354041\n\n\n\nt(pcr_coef(overlap_x_xts, overlap_y_xts, comps))\n\n          [,1]        [,2]          [,3]       [,4]\n[1,] 0.2499547 0.002610802 -0.0008341955 0.01743854\n\n\n\nfit &lt;- pls::pcr(reformulate(termlabels = \".\", response = tickers),\n                data = merge(scale_x_xts, overlap_y_xts), ncomp = comps)\ncoef(fit)[ , , 1]\n\n        SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 6.304978e-04  3.882602e-05 -1.630078e-04  6.354041e-04 \n\n\n\n\nR-squared\n\npcr_rsq &lt;- function(x, y, comps) {\n  \n  coef &lt;- pcr_coef(x, y, comps)\n  \n  x &lt;- sweep(x, 2, colMeans(x), \"-\")\n  y &lt;- sweep(y, 2, colMeans(y), \"-\")\n  \n  result &lt;- (t(coef) %*% crossprod(x) %*% coef) / crossprod(y)\n  colnames(result) &lt;- \"R-squared\"\n  \n  return(result)\n  \n}\n\n\npcr_rsq(scale_x_xts, overlap_y_xts, comps)\n\n      R-squared\nBAICX 0.4632559\n\n\n\npcr_rsq(overlap_x_xts, overlap_y_xts, comps)\n\n      R-squared\nBAICX 0.5481285\n\n\n\npls::R2(fit)$val[comps + 1]\n\n[1] 0.4632559\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\npcr_se &lt;- function(x, y, comps) {\n  \n  n_rows &lt;- nrow(x)\n  n_cols &lt;- ncol(x)\n  \n  rsq &lt;- pcr_rsq(x, y, comps)\n  \n  y &lt;- sweep(y, 2, colMeans(y), \"-\")\n  \n  df_resid &lt;- n_rows - n_cols - 1\n  \n  var_y &lt;- crossprod(y)\n  var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n  \n  LV &lt;- eigen(cov(x))\n  L &lt;- LV$values[1:comps] * (n_rows - 1)\n  V &lt;- LV$vectors[ , 1:comps]\n  \n  result &lt;- sqrt(var_resid * diag(V %*% sweep(t(V), 1, 1 / L, \"*\")))\n  \n  return(result)\n  \n}\n\n\npcr_se(scale_x_xts, overlap_y_xts, comps)\n\n[1] 4.318255e-05 2.659179e-06 1.116434e-05 4.351858e-05\n\n\n\npcr_se(overlap_x_xts, overlap_y_xts, comps)\n\n[1] 1.444040e-02 1.508314e-04 4.819319e-05 1.007460e-03"
  },
  {
    "objectID": "posts/risk-r/index.html#partial-least-squares",
    "href": "posts/risk-r/index.html#partial-least-squares",
    "title": "Risk",
    "section": "Partial least squares",
    "text": "Partial least squares"
  },
  {
    "objectID": "posts/risk-r/index.html#standalone-risk",
    "href": "posts/risk-r/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\nlm_sar &lt;- function(x, y, weights, intercept) {\n  \n  coef &lt;- lm_coef(x, y, weights, intercept)\n  rsq &lt;- lm_rsq(x, y, weights, intercept)\n  \n  if (intercept) x &lt;- cbind(1, x)\n  \n  sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n  sar &lt;- coef ^ 2 * diag(sigma[-ncol(sigma), -ncol(sigma)])\n  sar_eps &lt;- (1 - rsq) * sigma[ncol(sigma), ncol(sigma)]\n  \n  result &lt;- sqrt(c(sigma[ncol(sigma), ncol(sigma)],\n                   sar,\n                   sar_eps))\n  \n  return(result)\n  \n}\n\n\nlm_sar(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])\n\n[1] 0.064313451 0.000000000 0.034621630 0.005843891 0.024572245 0.018637466\n[7] 0.033817135"
  },
  {
    "objectID": "posts/risk-r/index.html#risk-contribution",
    "href": "posts/risk-r/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttps://bookdown.org/compfinezbook/introcompfinr/Portfolio-risk-reports.html\n\n\nlm_mcr &lt;- function(x, y, weights, intercept) {\n  \n  coef &lt;- lm_coef(x, y, weights, intercept)\n  rsq &lt;- lm_rsq(x, y, weights, intercept)\n  \n  if (intercept) x &lt;- cbind(1, x)\n  \n  sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n  mcr &lt;- coef * sigma[-ncol(sigma), -ncol(sigma)] %*% coef / sqrt(sigma[ncol(sigma), ncol(sigma)])\n  mcr_eps &lt;- sqrt(sigma[ncol(sigma), ncol(sigma)]) - sum(mcr)\n  \n  result &lt;- c(sqrt(sigma[ncol(sigma), ncol(sigma)]),\n              mcr,\n              mcr_eps)\n  \n  return(result)\n  \n}\n\n\nlm_mcr(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])\n\n[1] 0.0643134508 0.0000000000 0.0256352014 0.0006764419 0.0076700287\n[6] 0.0125501386 0.0177816402"
  },
  {
    "objectID": "posts/risk-r/index.html#implied-shocks",
    "href": "posts/risk-r/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\nimplied_shocks &lt;- function(shocks, x, z, weights) {\n  \n  beta &lt;- solve(crossprod(z, diag(weights) %*% z)) %*% crossprod(z, diag(weights) %*% x)\n  \n  result &lt;- shocks %*% beta\n  \n  return(result)\n  \n}\n\n\nshocks &lt;- c(-0.1, 0.1)\noverlap_z_xts &lt;- overlap_x_xts[ , 1:2]\n\n\nimplied_shocks(shocks, overlap_x_xts, overlap_z_xts, weights)\n\n     SP500 DTWEXAFEGS        DGS10 BAMLH0A0HYM2\n[1,]  -0.1        0.1 -0.001446173 -0.006859041"
  },
  {
    "objectID": "posts/risk-r/index.html#stress-pl",
    "href": "posts/risk-r/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\npnl_stress &lt;- function(shocks, x, y, z, weights, intercept) {\n  \n  coef &lt;- lm_coef(x, y, weights, intercept)\n  \n  # cbind.xts() changes names, e.g., \"(Intercept)\" =&gt; \"X.Intercept.\"\n  if (intercept) x &lt;- cbind(\"(Intercept)\" = 1, as.matrix(x))\n  \n  result &lt;- t(coef) * implied_shocks(shocks, x, z, weights)\n  \n  return(result)    \n  \n}\n\n\npnl_stress(shocks, overlap_x_xts, overlap_y_xts, overlap_z_xts, weights, intercept)\n\n       (Intercept)       SP500   DTWEXAFEGS       DGS10 BAMLH0A0HYM2\nBAICX -0.002273131 -0.01826507 -0.009116029 -0.00431303 -0.007811649"
  },
  {
    "objectID": "posts/securities-r/index.html",
    "href": "posts/securities-r/index.html",
    "title": "Securities",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/securities-r/index.html#value",
    "href": "posts/securities-r/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_value &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n\n  r_df &lt;- exp(-r * tau)\n  q_df &lt;- exp(-q * tau)\n  \n  call_value &lt;- S * q_df * Phi(d1) - r_df * K * Phi(d2)\n  put_value &lt;- r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n  result &lt;- ifelse(type == \"call\", call_value, put_value)\n  \n  return(result)\n  \n} \n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\nbs_d1 &lt;- function(S, K, r, q, tau, sigma) {\n  \n  result &lt;- (log(S / K) + (r - q + sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n  \n  return(result)\n  \n}\n\nbs_d2 &lt;- function(S, K, r, q, tau, sigma) {\n  \n  result &lt;- (log(S / K) + (r - q - sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n  \n  return(result)\n  \n}\n\nphi &lt;- function(x) {\n  \n  result &lt;- dnorm(x)\n  \n  return(result)\n  \n}\n\nPhi &lt;- function(x) {\n  \n  result &lt;- pnorm(x)\n  \n  return(result)\n  \n}\n\n\ngreeks_dt[ , d1 := bs_d1(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , d2 := bs_d2(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , value := bs_value(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order",
    "href": "posts/securities-r/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_delta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n  q_df &lt;- exp(-q * tau)\n\n  call_value &lt;- q_df * Phi(d1)\n  put_value &lt;- -q_df * Phi(-d1)\n  result &lt;- ifelse(type == \"call\", call_value, put_value)\n  \n  return(result)\n  \n} \n\n\ngreeks_dt[ , delta := bs_delta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nDelta-beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\nbs_delta_diff &lt;- function(type, S, K, r, q, tau, sigma, delta0) {\n  \n  d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n  d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n  delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n  \n  call_value &lt;- delta - delta0\n  put_value &lt;- delta0 - delta\n  \n  result &lt;- ifelse(type == \"call\", call_value, put_value)\n      \n  return(result)\n  \n}\n\n\nbeta &lt;- 0.35\ntype &lt;- \"call\"\nn &lt;- 1\nmultiple &lt;- 100\ntotal &lt;- 1000000\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\nsec &lt;- list(\n  \"n\" = n,\n  \"multiple\" = multiple,\n  \"S\" = S,\n  \"delta\" = bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n  \"beta\" = 1\n)\n\n\nbeta_dt &lt;- data.table::CJ(type = type, shock = shocks)\nbeta_dt[ , spot := level_shock(shock, S, tau, sigma), by = c(\"type\", \"shock\")]\nbeta_dt[ , static := beta]\nbeta_dt[ , diff := bs_delta_diff(type, spot, K, r, q, tau, sigma, sec[[\"delta\"]])]\nbeta_dt[ , dynamic := beta + sec[[\"n\"]] * sec[[\"multiple\"]] * sec[[\"S\"]] * sec[[\"beta\"]] * diff / total, by = c(\"type\", \"shock\")]\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\nbs_vega &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n  q_df &lt;- exp(-q * tau)\n  \n  result &lt;- S * q_df * phi(d1) * sqrt(tau)\n  \n  return(result)\n  \n}\n\n\ngreeks_dt[ , vega := bs_vega(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_theta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n  r_df &lt;- exp(r * tau)\n  q_df &lt;- exp(q * tau)\n\n  call_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) -\n    r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n  \n  put_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) +\n    r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n      \n  result &lt;- ifelse(type == \"call\", call_value, put_value)\n  \n  return(result)\n  \n}\n\n\ngreeks_dt[ , theta := bs_theta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order",
    "href": "posts/securities-r/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\nbs_gamma &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n\n  q_df &lt;- exp(-q * tau)\n  \n  result &lt;- q_df * phi(d1) / (S * sigma * sqrt(tau))\n  \n  return(result)\n  \n}\n\n\ngreeks_dt[ , gamma := bs_gamma(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order-1",
    "href": "posts/securities-r/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\n\npnl_bond &lt;- function(duration, convexity, dy) {\n  \n  duration_pnl &lt;- -duration * dy\n  convexity_pnl &lt;- (convexity / 2) * dy ^ 2\n  income_pnl &lt;- dy\n  \n  result &lt;- list(\n    \"total\" = duration_pnl + convexity_pnl + income_pnl,\n    \"duration\" = duration_pnl,\n    \"convexity\" = convexity_pnl,\n    \"income\" = income_pnl\n  )\n  \n  return(result)\n  \n} \n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor &lt;- \"DGS10\"\nduration &lt;- 6.5\nconvexity &lt;- 0.65\ny &lt;- zoo::coredata(tail(zoo::na.locf(levels_xts[ , factor]), width)[1])\n\n\nbonds_dt &lt;- data.table::data.table(index = zoo::index(tail(levels_xts, width)),\n                                   duration = duration, convexity = convexity,\n                                   dy = zoo::na.locf(tail(levels_xts[ , factor], width)))\ndata.table::setnames(bonds_dt, c(\"index\", \"duration\", \"convexity\", \"dy\"))\nbonds_dt[ , dy := (dy - y) / 100, by = index]\n\n\nattrib_dt &lt;- bonds_dt[ , as.list(unlist(pnl_bond(duration, convexity, dy))), by = index]\n\n\n\n\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=\\frac{\\partial D}{\\partial y}=\\frac{\\partial}{\\partial y}\\left(-\\frac{1}{P}\\frac{\\partial D}{\\partial y}\\right)\\\\\n&=-\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}+\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=-C+D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(-C\\times 100+D^{2}\\right)=-C+\\frac{D^{2}}{100}\\). For example, if convexity and yield are percent then \\(\\text{Drift}=\\left(-0.65+\\frac{6.5^{2}}{100}\\right)\\partial y\\times100\\) or basis points then \\(\\text{Drift}=\\left(-65+6.5^{2}\\right)\\partial y\\).\n\nyield_shock &lt;- function(shock, tau, sigma) {\n  \n  result &lt;- shock * sigma * sqrt(tau)\n  \n  return(result)\n  \n}\n\n\nduration_drift &lt;- function(duration, convexity, dy) {\n  \n  drift &lt;- -convexity + duration ^ 2 / 100\n  change &lt;- drift * dy * 100\n  \n  result &lt;- list(\n    \"drift\" = drift,\n    \"change\" = change\n  )\n  \n  return(result)\n  \n}\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor &lt;- \"DGS10\"\nsigma &lt;- zoo::coredata(sd_xts[nrow(sd_xts), factor])\n\n\nduration_dt &lt;- data.table::CJ(shock = shocks)\nduration_dt[ , spot := yield_shock(shock, tau, sigma), by = \"shock\"]\nduration_dt[ , static := duration]\nduration_dt[ , dynamic := duration + duration_drift(duration, convexity, spot)[[\"change\"]], by = \"shock\"]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order-1",
    "href": "posts/securities-r/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlack’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\npnl_option &lt;- function(type, S, K, r, q, tau, sigma, dS, dt, dsigma) {\n  \n  d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n  d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n  value &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n  delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n  vega &lt;- bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n  theta &lt;- bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n  gamma &lt;- bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n  \n  delta_pnl &lt;- delta * dS / value\n  gamma_pnl &lt;- gamma / 2 * dS ^ 2 / value\n  vega_pnl &lt;- vega * dsigma / value\n  theta_pnl &lt;- theta * dt / value\n  \n  result &lt;- list(\n    \"total\" = delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n    \"delta\" = delta_pnl,\n    \"gamma\" = gamma_pnl,\n    \"vega\" = vega_pnl,\n    \"theta\" = theta_pnl\n  )\n  \n  return(result)    \n  \n}\n\n\nfactor &lt;- \"SP500\"\ntype &lt;- \"call\"\nS &lt;- zoo::coredata(tail(zoo::na.locf(levels_xts[ , factor]), width)[1])\nK &lt;- S # * (1 + 0.05)\ntau &lt;- 1 # = 252 / 252\nsigma &lt;- zoo::coredata(tail(sd_xts[ , factor], width)[1])\n\n\noptions_dt &lt;- data.table::data.table(index = zoo::index(tail(levels_xts, width)),\n                                     spot = zoo::na.locf(tail(levels_xts[ , factor], width)),\n                                     sigma = tail(sd_xts[ , factor], width))\ndata.table::setnames(options_dt, c(\"index\", \"spot\", \"sigma\"))\noptions_dt[ , dS := spot - S, by = index]\noptions_dt[ , dt_diff := as.numeric(index - index[1])]\noptions_dt[ , dt := dt_diff / tail(dt_diff, 1)]\noptions_dt[ , dsigma := sigma - ..sigma, by = index]\n\n\nattrib_dt &lt;- options_dt[ , as.list(unlist(pnl_option(type, S, K, r, q, tau, ..sigma,\n                                                     dS, dt, dsigma))), by = index]\n\n\n\n\n\n\n\n\n\n\n\n\nIto’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\nsim_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n  \n  result &lt;- S * exp(cumsum(sigma * sqrt(dt) * rnorm(n_sim)) +\n                    (mu - 0.5 * sigma ^ 2) * dt)\n  \n  return(result)\n  \n}\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\nsim_multi_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n  \n  n_cols &lt;- ncol(sigma)\n  \n  Z &lt;- matrix(rnorm(n_sim * n_cols), nrow = n_sim, ncol = n_cols)\n  X &lt;- sweep(sqrt(dt) * (Z %*% chol(sigma)), 2, (mu - 0.5 * diag(sigma)) * dt, \"+\")\n  \n  result &lt;- sweep(apply(X, 2, function(x) exp(cumsum(x))), 2, S, \"*\")\n  \n  return(result)\n  \n}\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS &lt;- rep(1, length(factors))\nsigma &lt;- cov(returns_xts, use = \"complete.obs\") * scale[[\"periods\"]]\nmu &lt;- colMeans(na.omit(returns_xts)) * scale[[\"periods\"]]\nmu &lt;- mu + diag(sigma) / 2 # drift\ndt &lt;- 1 / scale[[\"periods\"]]\n\n\nmu_ls &lt;- list()\nsigma_ls &lt;- list()\n\n\nfor (i in 1:1e4) {\n  \n  # assumes stock prices\n  levels_sim &lt;- sim_multi_gbm(width + 1, S, mu, sigma, dt)\n  returns_sim &lt;- diff(log(levels_sim))\n\n  mu_sim &lt;- colMeans(returns_sim) * scale[[\"periods\"]]\n  sigma_sim &lt;- apply(returns_sim, 2, sd) * sqrt(scale[[\"periods\"]])\n  \n  mu_ls &lt;- append(mu_ls, list(mu_sim))\n  sigma_ls &lt;- append(sigma_ls, list(sigma_sim))\n  \n}\n\n\ndata.frame(\n  \"empirical\" = colMeans(na.omit(returns_xts)) * scale[[\"periods\"]],\n  \"theoretical\" = colMeans(do.call(rbind, mu_ls)\n))\n\n                 empirical   theoretical\nSP500         0.1182866969  0.1192013486\nDTWEXAFEGS   -0.0054335166 -0.0049268087\nDGS10        -0.0005403087 -0.0005663181\nBAMLH0A0HYM2  0.0040523156  0.0039961740\n\n\n\ndata.frame(\n  \"empirical\" = sqrt(diag(sigma)),\n  \"theoretical\" = colMeans(do.call(rbind, sigma_ls))\n)\n\n               empirical theoretical\nSP500        0.183914917 0.183748752\nDTWEXAFEGS   0.061168097 0.061124763\nDGS10        0.008470421 0.008464328\nBAMLH0A0HYM2 0.016957011 0.016941305\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility",
    "href": "posts/securities-r/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\nimplied_vol_newton &lt;- function(params, type, S, K, r, q, tau) {\n  \n  target0 &lt;- 0\n  sigma &lt;- params[[\"sigma\"]]\n  sigma0 &lt;- sigma\n  \n  while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma0)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma0)\n    \n    target0 &lt;- bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n    d_target0 &lt;- bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n    \n    sigma &lt;- sigma0 - (target0 - params[[\"target\"]]) / d_target0\n    sigma0 &lt;- sigma\n    \n  }\n  \n  return(sigma)\n  \n}\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS &lt;- zoo::coredata(zoo::na.locf(levels_xts)[nrow(levels_xts), factor])\nK &lt;- S # * (1 + 0.05)\nsigma &lt;- zoo::coredata(sd_xts[nrow(sd_xts), factor]) # overrides matrix\nstart1 &lt;- 0.2\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\ntarget1 &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams1 &lt;- list(\n  \"target\" = target1,\n  \"sigma\" = start1,\n  \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nimplied_vol_newton(params1, type, S, K, r, q, tau)\n\n         SP500\n[1,] 0.1874257"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity",
    "href": "posts/securities-r/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\nyld_newton &lt;- function(params, cash_flows) {\n  \n  target0 &lt;- 0\n  yld &lt;- params[[\"cpn\"]]\n  yld0 &lt;- yld\n  \n  while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n    \n  target0 &lt;- 0\n  d_target0 &lt;- 0\n  dd_target0 &lt;- 0\n  \n  for (i in 1:length(cash_flows)) {\n    \n    t &lt;- i\n    \n    # present value of cash flows\n    target0 &lt;- target0 + cash_flows[i] / (1 + yld0) ^ t\n    \n    # first derivative of present value of cash flows\n    d_target0 &lt;- d_target0 - t * cash_flows[i] / (1 + yld0) ^ (t + 1) # use t for Macaulay duration\n    \n    # second derivative of present value of cash flows\n    dd_target0 &lt;- dd_target0 - t * (t + 1) * cash_flows[i] / (1 + yld0) ^ (t + 2)\n    \n  }\n  \n  yld &lt;- yld0 - (target0 - params[[\"target\"]]) / d_target0\n  yld0 &lt;- yld\n    \n  }\n  \n  result &lt;- list(\n    \"price\" = target0,\n    \"yield\" = yld * params[[\"freq\"]],\n    \"duration\" = -d_target0 / params[[\"target\"]] / params[[\"freq\"]],\n    \"convexity\" = -dd_target0 / params[[\"target\"]] / params[[\"freq\"]] ^ 2\n  )\n  \n  return(result)\n  \n}\n\n\nhttps://www.bloomberg.com/markets/rates-bonds/government-bonds/us\nhttps://quant.stackexchange.com/a/61025\nhttps://pages.stern.nyu.edu/~igiddy/spreadsheets/duration-convexity.xls\n\n\ntarget2 &lt;- 0.9928 * 1000 # present value\nstart2 &lt;- 0.0438 # coupon\ncash_flows &lt;- rep(start2 * 1000 / 2, 10 * 2)\ncash_flows[10 * 2] &lt;- cash_flows[10 * 2] + 1000\n\n\nparams2 &lt;- list(\n  \"target\" = target2,\n  \"cpn\" = start2,\n  \"freq\" = 2,\n  \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nt(yld_newton(params2, cash_flows))\n\n     price yield      duration convexity\n[1,] 992.8 0.04470076 8.016596 76.6811"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility-1",
    "href": "posts/securities-r/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\nimplied_vol_obj &lt;- function(param, type, S, K, r, q, tau, target) {\n  \n  d1 &lt;- bs_d1(S, K, r, q, tau, param)\n  d2 &lt;- bs_d2(S, K, r, q, tau, param)\n  target0 &lt;- bs_value(type, S, K, r, q, tau, param, d1, d2)\n  \n  result &lt;- abs(target0 - target)\n  \n  return(result)\n    \n}\n\nimplied_vol_optim &lt;- function(param, type, S, K, r, q, tau, target) {\n  \n  result &lt;- optim(param, implied_vol_obj, type = type, S = S, K = K, r = r, q = q,\n                  tau = tau, target = target, method = \"Brent\", lower = 0, upper = 1)\n  \n  return(result$par)\n    \n}\n\n\nimplied_vol_optim(start1, type, S, K, r, q, tau, target1)\n\n[1] 0.1874257"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity-1",
    "href": "posts/securities-r/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\nyld_obj &lt;- function(param, cash_flows, target) {\n  \n  target0 &lt;- 0\n      \n  for (i in 1:length(cash_flows)) {\n    target0 &lt;- target0 + cash_flows[i] / (1 + param) ^ i\n  }\n\n  result &lt;- abs(target0 - target)\n  \n  return(result)\n    \n}\n\nyld_optim &lt;- function(params, cash_flows, target) {\n  \n  result &lt;- optim(params[[\"cpn\"]], yld_obj, target = target, cash_flows = cash_flows,\n                  method = \"Brent\", lower = 0, upper = 1)\n  \n  return(result$par * params[[\"freq\"]])\n    \n}\n\n\nyld_optim(params2, cash_flows, target2)\n\n[1] 0.04470077"
  },
  {
    "objectID": "posts/securities-py/index.html",
    "href": "posts/securities-py/index.html",
    "title": "Securities",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/securities-py/index.html#value",
    "href": "posts/securities-py/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n  \n  if (type == \"call\"):\n    result =  S * np.exp(-q * tau) * Phi(d1) - np.exp(-r * tau) * K * Phi(d2)\n  elif (type == \"put\"):\n    result = np.exp(-r * tau) * K * Phi(-d2) - S * np.exp(-q * tau) * Phi(-d1)\n      \n  return result\n\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n  \n  r_df = np.exp(-r * tau)\n  q_df = np.exp(-q * tau)\n  \n  call_value = S * q_df * Phi(d1) - r_df * K * Phi(d2)\n  put_value = r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n  result = np.where(type == \"call\", call_value, put_value)\n  \n  return result\n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\ndef bs_d1(S, K, r, q, tau, sigma):\n    \n  result = (np.log(S / K) + (r - q + sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n  \n  return result\n\ndef bs_d2(S, K, r, q, tau, sigma):\n  \n  result = (np.log(S / K) + (r - q - sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n  \n  return result\n    \ndef phi(x):\n  \n  result = norm.pdf(x)\n  \n  return result\n\ndef Phi(x):\n  \n  result = norm.cdf(x)\n  \n  return result\n\n\ngreeks_df[\"d1\"] = bs_d1(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"d2\"] = bs_d2(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"value\"] = bs_value(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order",
    "href": "posts/securities-py/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_delta(type, S, K, r, q, tau, sigma, d1, d2):\n\n  q_df = np.exp(-q * tau)\n  \n  call_value = q_df * Phi(d1)\n  put_value = -q_df * Phi(-d1)\n  result = np.where(type == \"call\", call_value, put_value)\n  \n  return result\n\n\ngreeks_df[\"delta\"] = bs_delta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\nDelta-beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\ndef bs_delta_diff(type, S, K, r, q, tau, sigma, delta0):\n  \n  d1 = bs_d1(S, K, r, q, tau, sigma)\n  d2 = bs_d2(S, K, r, q, tau, sigma)\n  delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n  \n  call_value = delta - delta0\n  put_value = delta0 - delta\n  \n  result = np.where(type == \"call\", call_value, put_value)\n      \n  return result\n\n\nbeta = 0.35\ntype = \"call\"\nn = 1\nmultiple = 100\ntotal = 1000000\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\nsec = {\n  \"n\": n,\n  \"multiple\": multiple,\n  \"S\": S,\n  \"delta\": bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n  \"beta\": 1\n}\n\n\nbeta_df = pd.DataFrame([(x, y) for x in types for y in shocks], \n  columns = [\"type\", \"shock\"])\nbeta_df[\"spot\"] = level_shock(beta_df[\"shock\"], S, tau, sigma)\nbeta_df[\"static\"] = beta\nbeta_df[\"diff\"] = bs_delta_diff(type, beta_df[\"spot\"], K, r, q, tau, sigma, sec[\"delta\"])\nbeta_df[\"dynamic\"] = beta + sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * sec[\"beta\"] * beta_df[\"diff\"] / total\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\ndef bs_vega(type, S, K, r, q, tau, sigma, d1, d2):\n  \n  q_df = np.exp(-q * tau)\n  result = S * q_df * phi(d1) * np.sqrt(tau)\n  \n  return result\n\n\ngreeks_df[\"vega\"] = bs_vega(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                            greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_theta(type, S, K, r, q, tau, sigma, d1, d2):\n  \n  r_df = np.exp(-r * tau)\n  q_df = np.exp(-q * tau)\n  \n  call_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) - \\\n    r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n      \n  put_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) + \\\n    r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n      \n  result = np.where(type == \"call\", call_value, put_value)\n  \n  return result\n\n\ngreeks_df[\"theta\"] = bs_theta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order",
    "href": "posts/securities-py/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\ndef bs_gamma(type, S, K, r, q, tau, sigma, d1, d2):\n\n  q_df = np.exp(-q * tau)\n  \n  result = q_df * phi(d1) / (S * sigma * np.sqrt(tau))\n  \n  return result\n\n\ngreeks_df[\"gamma\"] = bs_gamma(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order-1",
    "href": "posts/securities-py/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\n\ndef pnl_bond(duration, convexity, dy):\n  \n  duration_pnl = -duration * dy\n  convexity_pnl = (convexity / 2) * dy ** 2\n  income_pnl = dy\n  \n  result = pd.DataFrame({\n    \"total\": duration_pnl + convexity_pnl + income_pnl,\n    \"duration\": duration_pnl,\n    \"convexity\": convexity_pnl,\n    \"income\": income_pnl\n  })\n  \n  return result\n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor = \"DGS10\"\nduration = 6.5\nconvexity = 0.65\ny = levels_df.ffill()[factor].iloc[-width]\n\n\nbond_df = pd.DataFrame({\n  \"duration\": duration,\n  \"convexity\": convexity,\n  \"dy\": (levels_df.ffill()[factor].iloc[-width:] - y) / 100\n})\n\n\nattrib_df = pnl_bond(bond_df[\"duration\"], bond_df[\"convexity\"], bond_df[\"dy\"])\n\n\n\n\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=\\frac{\\partial D}{\\partial y}=\\frac{\\partial}{\\partial y}\\left(-\\frac{1}{P}\\frac{\\partial D}{\\partial y}\\right)\\\\\n&=-\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}+\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=-C+D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(-C\\times 100+D^{2}\\right)=-C+\\frac{D^{2}}{100}\\). For example, if convexity and yield are percent then \\(\\text{Drift}=\\left(-0.65+\\frac{6.5^{2}}{100}\\right)\\partial y\\times100\\) or basis points then \\(\\text{Drift}=\\left(-65+6.5^{2}\\right)\\partial y\\).\n\ndef yield_shock(shock, tau, sigma):\n  \n  result = shock * sigma * np.sqrt(tau)\n  \n  return result\n\n\ndef duration_drift(duration, convexity, dy):\n  \n  drift = -convexity + duration ** 2 / 100\n  change = drift * dy * 100\n  \n  result = {\n    \"drift\": drift,\n    \"change\": change\n  }\n  \n  return result\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor = \"DGS10\"\nsigma = sd_df[factor].iloc[-1]\n\n\nduration_df = pd.DataFrame(shocks).rename(columns = {0: \"shock\"})\nduration_df[\"spot\"] = yield_shock(duration_df[\"shock\"], tau, sigma)\nduration_df[\"static\"] = duration\nduration_df[\"dynamic\"] = duration + \\\n  duration_drift(duration, convexity, duration_df[\"spot\"])[\"change\"]"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order-1",
    "href": "posts/securities-py/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlack’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\ndef pnl_option(type, S, K, r, q, tau, sigma, dS, dt, dsigma):\n  \n  d1 = bs_d1(S, K, r, q, tau, sigma)\n  d2 = bs_d2(S, K, r, q, tau, sigma)\n  value = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n  delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n  vega = bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n  theta = bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n  gamma = bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n  \n  delta_pnl = delta * dS / value\n  gamma_pnl = gamma / 2 * dS ** 2 / value\n  vega_pnl = vega * dsigma / value\n  theta_pnl = theta * dt / value\n  \n  result = pd.DataFrame({\n    \"total\": delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n    \"delta\": delta_pnl,\n    \"gamma\": gamma_pnl,\n    \"vega\": vega_pnl,\n    \"theta\": theta_pnl\n  })\n  \n  return result\n\n\nfactor = \"SP500\"\ntype = \"call\"\nS = levels_df.ffill()[factor].iloc[-width]\nK = S # * (1 + 0.05)\ntau = 1 # = 252 / 252\nsigma = sd_df[factor].iloc[-width]\n\n\noptions_df = pd.DataFrame({\n  \"spot\": levels_df.ffill()[factor].iloc[-width:],\n  \"sigma\": sd_df[factor].iloc[-width:]\n})\noptions_df[\"dS\"] = options_df[\"spot\"] - S\noptions_df[\"dt_diff\"] = (options_df.index - options_df.index[0]).days\noptions_df[\"dt\"] = options_df[\"dt_diff\"] / options_df[\"dt_diff\"].iloc[-1]\noptions_df[\"dsigma\"] = options_df[\"sigma\"] - sigma\n\n\nattrib_df = pnl_option(type, S, K, r, q, tau, sigma,\n                       options_df[\"dS\"], options_df[\"dt\"], options_df[\"dsigma\"])\n\n\n\n\n\n\n\n\n\n\n\n\nIto’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\ndef sim_gbm(n_sim, S, mu, sigma, dt):\n  \n  result = S * np.exp(np.cumsum(sigma * np.sqrt(dt) * np.random.normal(size = n_sim)) + \\\n                      (mu - 0.5 * sigma ** 2) * dt)\n  \n  return result\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\ndef sim_multi_gbm(n_sim, S, mu, sigma, dt):\n  \n  n_cols = sigma.shape[1]\n  \n  Z = np.random.normal(size = n_sim * n_cols).reshape((n_sim, n_cols))\n  X = np.sqrt(dt) * Z @ np.linalg.cholesky(sigma).T + (mu - 0.5 * np.diag(sigma)) * dt\n  \n  result = S * np.exp(X.cumsum(axis = 0))\n  \n  return result\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS = [1] * len(factors)\nsigma = np.cov(returns_df.dropna().T, ddof = 1) * scale[\"periods\"]\nmu = np.array(returns_df.dropna().mean()) * scale[\"periods\"]\nmu = mu + np.diag(sigma) / 2 # drift\ndt = 1 / scale[\"periods\"]\n\n\nmu_ls = []\nsigma_ls = []\n\n\nfor i in range(10000): # \"TypeError: 'float' object cannot be interpreted as an integer\"\n\n  # assumes underlying stock price follows geometric Brownian motion with constant volatility\n  levels_sim = pd.DataFrame(sim_multi_gbm(width + 1, S, mu, sigma, dt))\n  returns_sim = np.log(levels_sim).diff()\n\n  mu_sim = returns_sim.mean() * scale[\"periods\"]\n  sigma_sim = returns_sim.std() * np.sqrt(scale[\"periods\"])\n\n  mu_ls.append(mu_sim)\n  sigma_ls.append(sigma_sim)\n\n\nmu_df = pd.DataFrame(mu_ls)\nsigma_df = pd.DataFrame(sigma_ls)\n\n\npd.DataFrame({\n  \"empirical\": np.array(returns_df.dropna().mean()) * scale[\"periods\"],\n  \"theoretical\": mu_df.mean()\n})\n\n   empirical  theoretical\n0   0.122062     0.121060\n1  -0.006109    -0.005529\n2  -0.000486    -0.000410\n3   0.004276     0.004100\n\n\n\npd.DataFrame({\n  \"empirical\": np.sqrt(np.diag(sigma)),\n  \"theoretical\": sigma_df.mean()\n})\n\n   empirical  theoretical\n0   0.183927     0.183697\n1   0.061157     0.061101\n2   0.008468     0.008457\n3   0.016954     0.016943\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility",
    "href": "posts/securities-py/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\ndef implied_vol_newton(params, type, S, K, r, q, tau):\n  \n  target0 = 0\n  sigma = params[\"sigma\"]\n  sigma0 = sigma\n  \n  while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n      \n    d1 = bs_d1(S, K, r, q, tau, sigma0)\n    d2 = bs_d2(S, K, r, q, tau, sigma0)\n    \n    target0 = bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n    d_target0 = bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n    \n    sigma = sigma0 - (target0 - params[\"target\"]) / d_target0\n    sigma0 = sigma\n      \n  return sigma\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS = levels_df.ffill()[factor].iloc[-1]\nK = S # * (1 + 0.05)\nsigma = sd_df[factor].iloc[-1] # overrides matrix\nstart1 = 0.2\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\ntarget1 = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams1 = {\n  \"target\": target1,\n  \"sigma\": start1,\n  \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nimplied_vol_newton(params1, type, S, K, r, q, tau) \n\nnp.float64(0.19094186709138591)"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity",
    "href": "posts/securities-py/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\ndef yld_newton(params, cash_flows):\n  \n  target0 = 0\n  yld0 = params[\"cpn\"] / params[\"freq\"]\n  yld = yld0 # assignment to `yield` variable is not possible\n  \n  while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n      \n    target0 = 0\n    d_target0 = 0\n    dd_target0 = 0\n    \n    for i in range(len(cash_flows)):\n      \n      t = i + 1\n    \n      # present value of cash flows\n      target0 += cash_flows[i] / (1 + yld0) ** t\n      \n      # first derivative of present value of cash flows\n      d_target0 -= t * cash_flows[i] / (1 + yld0) ** (t + 1) # use t for Macaulay duration\n      \n      # second derivative of present value of cash flows\n      dd_target0 -= t * (t + 1) * cash_flows[i] / (1 + yld0) ** (t + 2)\n    \n    yld = yld0 - (target0 - params[\"target\"]) / d_target0\n    yld0 = yld\n      \n  result = {\n    \"price\": target0,\n    \"yield\": yld * params[\"freq\"],\n    \"duration\": -d_target0 / params[\"target\"] / params[\"freq\"],\n    \"convexity\": -dd_target0 / params[\"target\"] / params[\"freq\"] ** 2\n  }\n      \n  return result\n\n\nhttps://www.bloomberg.com/markets/rates-bonds/government-bonds/us\nhttps://quant.stackexchange.com/a/61025\nhttps://pages.stern.nyu.edu/~igiddy/spreadsheets/duration-convexity.xls\n\n\ntarget2 = 0.9928 * 1000 # present value\nstart2 = 0.0438 # coupon\ncash_flows = [start2 * 1000 / 2] * 10 * 2\ncash_flows[-1] += 1000\n\n\nparams2 = {\n  \"target\": target2,\n  \"cpn\": start2,\n  \"freq\": 2,\n  \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nyld_newton(params2, cash_flows)\n\n{'price': 992.8000005704454, 'yield': 0.044700757710159994, 'duration': 8.0165959605043, 'convexity': 76.68109754287481}"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility-1",
    "href": "posts/securities-py/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\ndef implied_vol_obj(param, type, S, K, r, q, tau, target):\n  \n  d1 = bs_d1(S, K, r, q, tau, param)\n  d2 = bs_d2(S, K, r, q, tau, param)\n  target0 = bs_value(type, S, K, r, q, tau, param, d1, d2)\n  \n  result = abs(target0 - target)\n  \n  return result\n\ndef implied_vol_optim(param, type, S, K, r, q, tau, target):\n  \n  result = minimize(implied_vol_obj, param, args = (type, S, K, r, q, tau, target))\n  \n  return np.float64(result.x.item())\n\n\nimplied_vol_optim(start1, type, S, K, r, q, tau, target1)\n\nnp.float64(0.1909418609673873)"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity-1",
    "href": "posts/securities-py/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\ndef yld_obj(param, cash_flows, target):\n  \n  target0 = 0\n  \n  for i in range(len(cash_flows)):\n      \n    target0 += cash_flows[i] / (1 + param) ** (i + 1)\n  \n  target0 = abs(target0 - target)\n  \n  return target0\n  \ndef yld_optim(params, cash_flows, target):\n  \n  result = minimize(yld_obj, params[\"cpn\"], args = (cash_flows, target))\n  \n  return np.float64(result.x.item()) * params[\"freq\"]\n\n\nyld_optim(params2, cash_flows, target2)\n\nnp.float64(0.04470075001603727)"
  },
  {
    "objectID": "posts/statistics/index.html",
    "href": "posts/statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Usage\n\noptions(microbenchmark.unit = \"us\")\n\n\nn_vars &lt;- 10\nn_obs &lt;- 1000\nweights &lt;- 0.9 ^ (n_obs:1)\n\nx &lt;- matrix(rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)\ny &lt;- matrix(rnorm(n_obs), nrow = n_obs, ncol = 1)\nx_lgl &lt;- x &lt; 0\n\n\n\nRolling any\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_any(x_lgl, width = 125, min_obs = 1),\n  \"250\" = roll::roll_any(x_lgl, width = 250, min_obs = 1),\n  \"500\" = roll::roll_any(x_lgl, width = 500, min_obs = 1),\n  \"1000\" = roll::roll_any(x_lgl, width = 1000, min_obs = 1)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq     max neval\n  125 122.1 139.45 150.118 143.70 156.30   322.3   100\n  250 131.0 138.35 885.063 148.20 162.15 73523.7   100\n  500 128.6 135.25 149.147 143.65 160.35   191.1   100\n 1000 120.3 127.70 143.784 134.30 156.20   367.0   100\n\n\n\n\nRolling all\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_all(x_lgl, width = 125, min_obs = 1),\n  \"250\" = roll::roll_all(x_lgl, width = 250, min_obs = 1),\n  \"500\" = roll::roll_all(x_lgl, width = 500, min_obs = 1),\n  \"1000\" = roll::roll_all(x_lgl, width = 1000, min_obs = 1)\n)\nprint(result)\n\nUnit: microseconds\n expr  min    lq    mean median     uq   max neval\n  125 82.4 90.15 114.051  94.70 134.00 228.1   100\n  250 80.6 88.95 110.066  94.95 134.35 216.0   100\n  500 75.7 85.60 108.501  89.40 135.15 292.7   100\n 1000 72.8 78.70  94.912  82.30 106.70 181.8   100\n\n\n\n\nRolling sums\n\\[\n\\begin{aligned}\n&\\text{Expanding window} \\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_sum(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_sum(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_sum(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_sum(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr  min     lq    mean median     uq   max neval\n  125 95.5 107.60 127.335 114.25 130.00 278.5   100\n  250 94.2 106.95 130.752 113.75 136.90 328.1   100\n  500 95.3 106.80 132.233 117.80 148.15 296.6   100\n 1000 91.6 101.80 127.463 109.20 138.10 291.1   100\n\n\n\n\nRolling products\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{prod}_{w}\\leftarrow\\text{prod}_{w}\\times\\text{w}_{new}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}/\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_prod(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_prod(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_prod(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_prod(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min    lq    mean median     uq   max neval\n  125 207.2 219.7 236.949 228.60 241.55 374.9   100\n  250 207.8 224.7 240.577 236.35 247.40 350.6   100\n  500 136.6 154.1 175.491 165.55 180.55 388.8   100\n 1000 135.6 155.0 178.438 165.30 176.85 557.2   100\n\n\n\n\nRolling means\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times \\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_mean(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_mean(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_mean(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_mean(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125  99.3 107.25 133.575 113.50 147.65 305.0   100\n  250 101.8 109.90 134.233 115.55 149.90 268.7   100\n  500  99.2 106.25 128.771 112.30 143.80 246.4   100\n 1000  96.5 102.70 129.033 115.45 143.80 217.0   100\n\n\n\n\nRolling minimums\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_min(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_min(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_min(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_min(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min    lq    mean median     uq   max neval\n  125 131.5 147.0 167.555 154.40 180.05 268.4   100\n  250 117.8 145.8 163.208 154.75 167.00 304.5   100\n  500 139.9 149.8 173.730 159.40 184.25 366.6   100\n 1000 134.7 147.1 181.030 162.50 194.20 911.0   100\n\n\n\n\nRolling maximums\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_max(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_max(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_max(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_max(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 102.6 115.70 131.797 123.20 134.20 233.4   100\n  250 101.7 114.05 136.393 122.15 137.10 256.8   100\n  500 105.0 113.95 133.193 121.95 135.75 332.2   100\n 1000 101.4 111.05 130.468 117.40 136.45 261.7   100\n\n\n\n\nRolling index of minimums\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_idxmin(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_idxmin(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_idxmin(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_idxmin(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 101.0 116.00 132.410 130.55 142.85 301.5   100\n  250 106.5 120.05 138.459 132.95 148.00 320.5   100\n  500  98.8 119.50 136.189 133.15 144.40 333.5   100\n 1000  97.2 116.05 131.241 128.35 141.80 194.9   100\n\n\n\n\nRolling index of maximums\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_idxmax(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_idxmax(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_idxmax(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_idxmax(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 103.2 111.45 129.493 118.15 137.95 241.2   100\n  250 100.1 111.30 130.007 116.80 132.60 274.4   100\n  500 103.8 111.30 130.981 117.00 134.80 346.0   100\n 1000  97.0 110.70 133.250 117.05 148.35 274.4   100\n\n\n\n\nRolling medians\n\n# \"'online' is only supported for equal 'weights'\"\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_median(x, width = 125, min_obs = 1),\n  \"250\" = roll::roll_median(x, width = 250, min_obs = 1),\n  \"500\" = roll::roll_median(x, width = 500, min_obs = 1),\n  \"1000\" = roll::roll_median(x, width = 1000, min_obs = 1)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 746.3 807.45 910.488 832.10 866.85 7271.2   100\n  250 740.6 810.15 851.863 834.15 924.65  963.7   100\n  500 679.8 735.50 800.554 761.80 790.10 3382.7   100\n 1000 518.2 566.25 598.112 589.10 610.20 1407.7   100\n\n\n\n\nRolling quantiles\n\n# \"'online' is only supported for equal 'weights'\"\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_quantile(x, width = 125, min_obs = 1),\n  \"250\" = roll::roll_quantile(x, width = 250, min_obs = 1),\n  \"500\" = roll::roll_quantile(x, width = 500, min_obs = 1),\n  \"1000\" = roll::roll_quantile(x, width = 1000, min_obs = 1)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 788.7 820.50 849.449 834.00 845.80 1378.2   100\n  250 779.8 806.10 829.049 821.20 837.90 1052.1   100\n  500 728.5 751.65 774.702 766.60 783.90 1007.6   100\n 1000 552.5 583.80 604.054 598.05 612.75  783.1   100\n\n\n\n\nRolling variances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{x}_{old}-\\text{mean}_{prev_x})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_var(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_var(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_var(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_var(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 118.9 137.50 154.914 147.30 156.65 290.7   100\n  250 117.5 135.75 151.086 148.65 155.30 298.6   100\n  500 120.0 132.40 150.285 143.05 151.80 373.9   100\n 1000 108.1 125.20 138.725 134.70 143.15 275.0   100\n\n\n\n\nRolling standard deviations\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_sd(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_sd(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_sd(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_sd(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 122.1 142.80 161.970 153.45 164.25 326.4   100\n  250 132.7 141.90 160.125 151.60 165.45 296.8   100\n  500 129.9 137.45 152.858 146.75 158.05 313.0   100\n 1000 109.0 132.35 156.075 142.70 159.25 406.1   100\n\n\n\n\nRolling scaling and centering\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_scale(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_scale(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_scale(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_scale(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median    uq   max neval\n  125 145.1 152.90 178.613 164.30 191.1 569.6   100\n  250 145.1 150.90 176.003 159.40 178.7 642.6   100\n  500 141.7 148.50 173.040 162.25 190.2 351.5   100\n 1000 135.0 141.05 161.846 154.95 168.9 280.1   100\n\n\n\n\nRolling covariances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{y}_{old}-\\text{mean}_{prev_y})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_cov(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_cov(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_cov(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_cov(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq     mean median      uq    max neval\n  125 742.8 911.80 1038.197 972.50 1036.05 7535.8   100\n  250 672.6 855.65 1017.200 935.00 1020.50 6087.4   100\n  500 653.2 836.15  956.936 905.35  969.30 6041.5   100\n 1000 564.3 732.95  823.702 762.20  813.55 6017.8   100\n\n\n\n\nRolling correlations\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_cor(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_cor(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_cor(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_cor(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min      lq     mean  median      uq    max neval\n  125 861.5 1029.60 1078.781 1077.90 1126.75 1490.8   100\n  250 774.2  980.30 1126.301 1064.40 1147.30 4535.5   100\n  500 739.7  895.50 1000.387  974.70 1042.45 4175.1   100\n 1000 641.4  772.45  909.451  828.05  865.25 4145.3   100\n\n\n\n\nRolling crossproducts\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_crossprod(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_crossprod(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_crossprod(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_crossprod(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min    lq    mean median     uq    max neval\n  125 551.4 624.5 678.036 675.05 711.45 1168.1   100\n  250 547.9 605.9 755.236 659.45 717.90 3993.3   100\n  500 531.5 579.8 737.054 635.25 680.05 3927.9   100\n 1000 501.6 537.8 623.640 562.30 618.70 3822.3   100\n\n\n\n\nRolling linear models\n\\[\n\\begin{aligned}\n&\\text{coef}=\\text{cov}_{xx}^{-1}\\times\\text{cov}_{xy}\\\\\n&\\text{intercept}=\\text{mean}_{y}-\\text{coef}\\times\\text{mean}_{x}\\\\\n&\\text{rsq}=\\frac{\\text{coef}^{T}\\times\\text{cov}_{xx}\\times\\text{coef}}{\\text{var}_{y}}\\\\\n&\\text{var}_{resid}=\\frac{(1-\\text{rsq})(\\text{var}_{y})(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})}{\\text{n}_{rows}-\\text{n}_{cols}}\\\\\n&\\text{xx}=\\text{cov}_{xx}\\times(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})\\\\\n&\\text{se}_{coef}=\\sqrt{\\text{var}_{resid}\\times\\text{diag}(\\text{xx}^{-1})}\\\\\n&\\text{se}_{intercept}=\\sqrt{\\text{var}_{resid}\\left(1/\\text{sum}_{w}+\\text{mean}_{x}^{T}\\text{xx}^{-1}\\text{mean}_{x}\\right)}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_lm(x, y, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_lm(x, y, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_lm(x, y, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_lm(x, y, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq    max neval\n  125 2140.9 2342.15 2510.482 2432.95 2549.65 4350.2   100\n  250 2078.6 2328.70 2521.667 2442.40 2631.50 3821.7   100\n  500 2092.6 2276.50 2448.099 2384.40 2523.95 3547.1   100\n 1000 2030.9 2235.15 2496.523 2331.20 2457.30 6381.6   100\n\n\n\n\nReferences\n\nWeights: https://stackoverflow.com/a/9933794\nIndex: https://stackoverflow.com/a/11316626\nIndex: https://stackoverflow.com/a/34363187\nIndex: https://stackoverflow.com/a/243342\nQuantile (comparator): https://stackoverflow.com/a/51992954\nQuantile (comparator): https://stackoverflow.com/a/25921772\nQuantile (comparator): https://stackoverflow.com/a/40416506\nMedian: https://stackoverflow.com/a/5970314\nMedian: https://stackoverflow.com/a/5971248\nMedian: https://gist.github.com/ashelly/5665911\nStandard errors: https://stats.stackexchange.com/a/64217"
  },
  {
    "objectID": "posts/crowds-r/index.html",
    "href": "posts/crowds-r/index.html",
    "title": "Crowds",
    "section": "",
    "text": "# factors_r &lt;- c(\"SP500\") # \"SP500\" does not contain dividends\n# factors_d &lt;- c(\"DTB3\")\n\n\nParse web\n\nfilters &lt;- list(\"eq\", list(\"categoryname\", \"Tactical Allocation\"))\nquery &lt;- yfscreen::create_query(filters)\npayload &lt;- yfscreen::create_payload(\"mutualfund\", query, 250)\ndata &lt;- yfscreen::get_data(payload)\n\n\nsorted_df &lt;- data[order(\n  -data[[\"netAssets.raw\"]],\n  data[[\"netExpenseRatio.raw\"]],\n  data[[\"firstTradeDateMilliseconds\"]],\n  data[[\"longName\"]],\n  data[[\"symbol\"]]\n), ]\ntickers &lt;- sorted_df[!duplicated(sorted_df[[\"netAssets.raw\"]]), \"symbol\"]\n\n\nallocations &lt;- c(\"IVV\", \"IDEV\", \"IUSB\", \"IEMG\", \"IJH\", \"IAGG\", \"IJR\")\ntickers &lt;- c(tickers, allocations)\n\n\n\nOptimization\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\")\n\n\n# library(CVXR)\n\n\n# min_rss_optim &lt;- function(x, y) {\n# \n#   params &lt;- Variable(ncol(x))\n# \n#   obj &lt;- Minimize(sum_squares(y - x %*% params))\n# \n#   cons &lt;- list(sum(params) == 1, params &gt;= 0, params &lt;= 1)\n# \n#   prob &lt;- Problem(obj, cons)\n# \n#   result &lt;- solve(prob)$getValue(params)\n# \n#   return(result)\n# \n# }\n\n\nperformance_xts &lt;- roll::roll_prod(1 + returns_xts, width, min_obs = 1) - 1\n\n\nn_rows &lt;- nrow(overlap_xts)\nresult_ls &lt;- list()\nindex_ls &lt;- list()\n\n# for (i in width:n_rows) {\nfor (i in n_rows) {\n    \n  idx &lt;- max(i - width + 1, 1):i\n  x_subset &lt;- zoo::coredata(overlap_x_xts[idx, ])\n  y_subset &lt;- zoo::coredata(overlap_y_xts[idx, ])\n  params_ls &lt;- list()\n  tickers_ls &lt;- list()\n  performance_ls &lt;- list()\n  \n  for (j in tickers[!tickers %in% allocations]) {\n    \n    idx &lt;- complete.cases(x_subset, y_subset[ , j])\n    x_complete &lt;- x_subset[idx, , drop = FALSE]\n    y_complete &lt;- y_subset[idx, j]\n    \n    if ((nrow(x_complete) &gt; 0) && (length(y_complete) &gt; 0)) {\n      \n      xx &lt;- roll::roll_crossprod(x_complete, x_complete, width = nrow(x_complete))\n      xy &lt;- roll::roll_crossprod(x_complete, y_complete, width = nrow(x_complete))\n      \n      # params &lt;- t(min_rss_optim(x_complete, y_complete))\n      params &lt;- rolloptim::roll_min_rss(xx, xy, total = 1, lower = 0, upper = 1)[nrow(x_complete), ]\n      params_ls &lt;- append(params_ls, list(params))\n      \n      tickers_ls &lt;- append(tickers_ls, list(j))\n      \n      performance_ls &lt;- append(performance_ls, list(performance_xts[i, j]))\n        \n    }\n      \n  }\n  \n  if (length(params_ls) &gt; 0) {\n      \n    result &lt;- do.call(rbind, params_ls)\n    rownames(result) &lt;- unlist(tickers_ls)\n    \n    result &lt;- cbind(result, performance = unlist(performance_ls))\n    \n    result_ls &lt;- append(result_ls, list(result))\n    index_ls &lt;- append(index_ls, list(zoo::index(overlap_xts)[i]))\n      \n  }\n  \n}\n\n\n# save(result_ls, file = \"result_ls.rda\")\n# save(index_ls, file = \"index_ls.rda\")\n\n\n\nPerformance\n\n# load(\"result_ls.rda\")\n# load(\"index_ls.rda\")\n\n\nquantile_cut &lt;- function(x) {\n\n  result &lt;- cut(\n    -x,\n    breaks = quantile(-x, probs = c(0, 0.25, 0.5, 0.75, 1),\n                      na.rm = TRUE),\n    labels = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"),\n    include.lowest = TRUE\n  )\n\n  return(result)\n\n}\n\n\nn_rows &lt;- length(result_ls)\nnumeric_cols &lt;- c(allocations, \"performance\")\nscore_ls &lt;- list()\n\nfor (i in 1:n_rows) {\n  \n  score_df &lt;- data.frame(result_ls[[i]])\n  colnames(score_df) &lt;- numeric_cols\n  \n  score_df[[\"date\"]] &lt;- index_ls[[i]]\n  score_df[[\"quantile\"]] &lt;- quantile_cut(score_df[[\"performance\"]])\n  \n  score_form &lt;- as.formula(paste0(\"cbind(\", paste(numeric_cols, collapse = \",\"), \") ~ date + quantile\"))\n  overall_form &lt;- as.formula(paste0(\"cbind(\", paste(numeric_cols, collapse = \",\"), \") ~ date\"))\n  \n  score &lt;- aggregate(score_form, data = score_df, FUN = function(x) mean(x, na.rm = TRUE))\n  overall &lt;- aggregate(overall_form, data = score_df, FUN = function(x) mean(x, na.rm = TRUE))\n  overall[[\"quantile\"]] &lt;- \"Overall\"\n  overall &lt;- overall[ , c(\"date\", \"quantile\", numeric_cols)]\n  \n  score &lt;- rbind(score, overall)\n  \n  score_ls &lt;- append(score_ls, list(score))\n  \n}\n\n\nscore_df &lt;- do.call(rbind, score_ls)\nprint(score_df)\n\n        date quantile       IVV       IDEV       IUSB       IEMG        IJH\n1 2025-12-12       Q1 0.4001408 0.13460402 0.09609413 0.04542707 0.08407417\n2 2025-12-12       Q2 0.3168338 0.10468081 0.18037184 0.05657325 0.07019589\n3 2025-12-12       Q3 0.2590739 0.08056269 0.20637871 0.05120757 0.02588854\n4 2025-12-12       Q4 0.1429645 0.09564201 0.21303531 0.08019240 0.01441308\n5 2025-12-12  Overall 0.2812395 0.10425178 0.17300857 0.05819053 0.04908034\n       IAGG        IJR performance\n1 0.2045606 0.03509923  0.24338289\n2 0.2073041 0.06404029  0.14466333\n3 0.3068264 0.07006221  0.10146988\n4 0.3688457 0.08490697  0.03991415\n5 0.2710530 0.06317622  0.13372825\n\n\n\n# save(score_df, file = \"score_df.rda\") # see test-crowds-r\n# score_xts &lt;- xts(score_df[score_df[[\"quantile\"]] == \"Q1\", \"weight\"],\n#                  score_df[score_df[[\"quantile\"]] == \"Q1\", \"date\"])\n# plot(score_xts)"
  },
  {
    "objectID": "posts/crowds-py/index.html",
    "href": "posts/crowds-py/index.html",
    "title": "Crowds",
    "section": "",
    "text": "# factors_r = [\"SP500\"] # \"SP500\" does not contain dividends\n# factors_d = [\"DTB3\"]\n\n\nParse web\n\nimport yfscreen as yfs\n\n\nfilters = [\"eq\", [\"categoryname\", \"Tactical Allocation\"]]\nquery = yfs.create_query(filters)\npayload = yfs.create_payload(\"mutualfund\", query, 250)\ndata = yfs.get_data(payload)\n\n\nsorted_df = data.sort_values(\n  by = [\n    \"netAssets.raw\",\n    \"netExpenseRatio.raw\",\n    \"firstTradeDateMilliseconds\",\n    \"longName\",\n    \"symbol\"\n  ],\n  ascending = [False, True, True, True, True],\n  kind = \"stable\"\n)\ntickers = sorted_df.loc[~sorted_df[\"netAssets.raw\"].duplicated(), \"symbol\"].tolist()\n\n\nallocations = [\"IVV\", \"IDEV\", \"IUSB\", \"IEMG\", \"IJH\", \"IAGG\", \"IJR\"]\ntickers = tickers + allocations\n\n\n\nOptimization\n\nimport json\nimport cvxpy as cp\n\n\ndef min_rss_optim(x, y):\n    \n  w = cp.Variable(x.shape[1])\n    \n  objective = cp.Minimize(cp.sum_squares(y - x @ w))\n    \n  constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= 1]\n    \n  problem = cp.Problem(objective, constraints)\n  problem.solve()\n    \n  return w.value\n\n\ndef pnl(x):\n  return np.nanprod(1 + x) - 1\n\n\nperformance_df = returns_df.rolling(width, min_periods = 1).apply(pnl, raw = False)\n\n\nn_rows = overlap_df.shape[0]\nresult_ls = []\nindex_ls = []\n\n# for i in range(width - 1, n_rows):\nfor i in range(n_rows - 1, n_rows):\n  \n  idx = range(max(i - width + 1, 0), i + 1)\n  x_subset = overlap_x_df.iloc[idx]\n  y_subset = overlap_y_df.iloc[idx]\n  params_ls = []\n  tickers_ls = []\n  performance_ls = []\n  \n  for j in [ticker for ticker in tickers if ticker not in allocations]:\n    \n    idx = ~x_subset.isna().any(axis = 1) & ~y_subset[j].isna()\n    x_complete = x_subset.loc[idx]\n    y_complete = y_subset.loc[idx, j]\n    \n    if (x_complete.shape[0] &gt; 0) and (y_complete.size &gt; 0):\n        \n      params = min_rss_optim(x_complete.values, y_complete.values)\n      params_ls.append(params)\n      \n      tickers_ls.append(j)\n      \n      performance_ls.append(performance_df[j].iloc[i])\n\n  if params_ls:\n    \n    result = pd.DataFrame(params_ls, index = tickers_ls)\n    result[\"performance\"] = performance_ls\n    \n    result_ls.append(result)\n    index_ls.append(overlap_x_df.index[i])\n\n\n# json.dump([x.to_dict() for x in result_ls], open(\"result_ls.json\", \"w\"))\n# json.dump([x.isoformat() for x in index_ls], open(\"index_ls.json\", \"w\"))\n\n\n\nPerformance\n\n# result_ls = [pd.DataFrame(x) for x in json.load(open(\"result_ls.json\", \"r\"))]\n# index_ls = [pd.Timestamp(x) for x in json.load(open(\"index_ls.json\", \"r\"))]\n\n\ndef quantile_cut(x):\n  \n  result = pd.cut(\n    -x,\n    bins = np.nanquantile(-x, [0, 0.25, 0.5, 0.75, 1]),\n    labels = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n    include_lowest = True\n  )\n  \n  return result\n\n\nn_rows = len(result_ls)\nnumeric_cols = allocations + [\"performance\"]\nscore_ls = []\n\nfor i in range(n_rows):\n  \n  score_df = pd.DataFrame(result_ls[i])\n  score_df.columns = numeric_cols\n  \n  score_df[\"date\"] = index_ls[i]\n  score_df[\"quantile\"] = quantile_cut(score_df[\"performance\"])\n\n  score = score_df.groupby([\"date\", \"quantile\"], observed = True)[numeric_cols] \\\n    .mean() \\\n    .reset_index()\n\n  overall_means = score_df[numeric_cols].mean()\n  \n  overall = pd.DataFrame({\n    \"date\": [index_ls[i]],\n    \"quantile\": [\"Overall\"],\n    **{col: [overall_means[col]] for col in numeric_cols}\n  })\n  \n  score = pd.concat([score, overall], ignore_index = True)\n  \n  score_ls.append(score)\n\n\nscore_df = pd.concat(score_ls, ignore_index = True)\nprint(score_df)\n\n        date quantile       IVV      IDEV      IUSB      IEMG       IJH  \\\n0 2025-12-12       Q1  0.400167  0.134559  0.096774  0.045438  0.084075   \n1 2025-12-12       Q2  0.316834  0.104681  0.180371  0.056573  0.070196   \n2 2025-12-12       Q3  0.259074  0.080563  0.206378  0.051208  0.025888   \n3 2025-12-12       Q4  0.142944  0.095665  0.212472  0.080258  0.014413   \n4 2025-12-12  Overall  0.281241  0.104246  0.173045  0.058210  0.049080   \n\n       IAGG       IJR  performance  \n0  0.203887  0.035100     0.243383  \n1  0.207304  0.064041     0.144663  \n2  0.306827  0.070062     0.101470  \n3  0.369341  0.084907     0.039914  \n4  0.271001  0.063177     0.133728  \n\n\n\n# score_df.to_json(\"score_df.json\", date_format = \"iso\")"
  }
]