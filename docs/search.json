[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jasonjfoster.github.io",
    "section": "",
    "text": "Eigen\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nalgorithms\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 23, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/optim-r/index.html",
    "href": "posts/optim-r/index.html",
    "title": "Optimization",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/optim-r/index.html#random-turnover",
    "href": "posts/optim-r/index.html#random-turnover",
    "title": "Optimization",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\nrand_turnover1 &lt;- function(n_sim, n_assets, lower, upper, target) {\n    \n  rng &lt;- upper - lower\n  \n  result &lt;- rand_weights2b(n_sim, n_assets) * rng\n  result &lt;- result - rng / n_assets\n  \n  return(result)\n    \n}\n\n\nlower &lt;- -0.05\nupper &lt;- 0.05\ntarget &lt;- 0\n\n\napproach1 &lt;- rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\n\n\n\n\nrand_iterative &lt;- function(n_assets, lower, upper, target) {\n  \n  result &lt;- runif(n_assets - 1, min = lower, max = upper)\n  temp &lt;- target - sum(result)\n  \n  while (!((temp &lt;= upper) && (temp &gt;= lower))) {\n      \n    result &lt;- runif(n_assets - 1, min = lower, max = upper)\n    temp &lt;- target - sum(result)\n      \n  }\n  \n  result &lt;- append(result, temp)\n  \n  return(result)\n  \n}\n\n\nrand_turnover2 &lt;- function(n_sim, n_assets, lower, upper, target) {\n  \n  result_ls &lt;- list()\n  \n  for (i in 1:n_sim) {\n      \n    result_sim &lt;- rand_iterative(n_assets, lower, upper, target)\n    result_ls &lt;- append(result_ls, list(result_sim))\n      \n  }\n  \n  result &lt;- do.call(rbind, result_ls)\n  \n  return(result)\n  \n}\n\n\napproach2 &lt;- rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-means",
    "href": "posts/optim-r/index.html#maximize-means",
    "title": "Optimization",
    "section": "Maximize means",
    "text": "Maximize means\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmax_mean_optim &lt;- function(mu, sigma, target) {\n  \n  params &lt;- CVXR::Variable(length(mu))\n  \n  obj &lt;- CVXR::Maximize(t(params) %*% mu)\n  \n  cons &lt;- list(sum(params) == 1, params &gt;= 0,\n               CVXR::quad_form(params, sigma) &lt;= target ^ 2)\n  \n  prob &lt;- CVXR::Problem(obj, cons)\n  \n  result &lt;- CVXR::solve(prob)$getValue(params)\n  \n  return(result)\n\n}\n\n\ntarget &lt;- 0.06\n\n\nparams1 &lt;- t(max_mean_optim(mu, sigma, target))\nparams1\n\n          [,1]      [,2]      [,3]         [,4]\n[1,] 0.3473083 0.1198965 0.5327951 3.014309e-08\n\n\n\nparams1 %*% mu\n\n           [,1]\n[1,] 0.03741358\n\n\n\nsqrt(params1 %*% sigma %*% t(params1))\n\n     [,1]\n[1,] 0.06\n\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\") # roll (&gt;= 1.1.7)\n# library(rolloptim)\n# \n# mu &lt;- roll_mean(returns_x_xts, 5)\n# sigma &lt;- roll_cov(returns_x_xts, width = 5)\n# \n# xx &lt;- roll_crossprod(returns_x_xts, returns_x_xts, 5)\n# xy &lt;- roll_crossprod(returns_x_xts, returns_y_xts, 5)\n# \n# roll_max_mean(mu)"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-variance",
    "href": "posts/optim-r/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmin_var_optim &lt;- function(mu, sigma, target) {\n  \n  params &lt;- CVXR::Variable(length(mu))\n  \n  obj &lt;- CVXR::Minimize(CVXR::quad_form(params, sigma))\n  \n  cons &lt;- list(sum(params) == 1, params &gt;= 0,\n               sum(mu * params) &gt;= target)\n  \n  prob &lt;- CVXR::Problem(obj, cons)\n  \n  result &lt;- CVXR::solve(prob)$getValue(params)\n  \n  return(result)\n\n}\n\n\ntarget &lt;- 0.03\n\n\nparams2 &lt;- t(min_var_optim(mu, sigma, target))\nparams2\n\n          [,1]      [,2]      [,3]          [,4]\n[1,] 0.2860781 0.1003453 0.6135765 -1.376977e-20\n\n\n\nparams2 %*% mu\n\n     [,1]\n[1,] 0.03\n\n\n\nsqrt(params2 %*% sigma %*% t(params2))\n\n           [,1]\n[1,] 0.04941567\n\n\n\n# roll_min_var(sigma)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-utility",
    "href": "posts/optim-r/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmax_utility_optim &lt;- function(mu, sigma, target) {\n  \n  params &lt;- CVXR::Variable(length(mu))\n  \n  obj &lt;- CVXR::Minimize(0.5 * target * CVXR::quad_form(params, sigma) - t(mu) %*% params)\n  \n  cons &lt;- list(sum(params) == 1, params &gt;= 0)\n  \n  prob &lt;- CVXR::Problem(obj, cons)\n      \n  result &lt;- CVXR::solve(prob)$getValue(params)\n  \n  return(result)\n\n}\n\n\nir &lt;- 0.5\ntarget &lt;- ir / 0.06 # ir / std (see Black-Litterman)\n\n\nparams3 &lt;- t(max_utility_optim(mu, sigma, target))\nparams3\n\n          [,1]      [,2]      [,3]          [,4]\n[1,] 0.4258788 0.1214689 0.4526522 -1.137482e-23\n\n\n\nparams3 %*% mu \n\n           [,1]\n[1,] 0.04519259\n\n\n\nsqrt(params3 %*% sigma %*% t(params3))\n\n           [,1]\n[1,] 0.07398514\n\n\n\n# roll_max_utility(mu, sigma)"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nhttps://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html\n\n\nmin_rss_optim1 &lt;- function(mu, sigma) {\n    \n  params &lt;- CVXR::Variable(length(mu))\n  \n  obj &lt;- CVXR::Minimize(0.5 * CVXR::quad_form(params, sigma) - t(mu) %*% params)\n  \n  cons &lt;- list(sum(params) == 1, params &gt;= 0)\n  \n  prob &lt;- CVXR::Problem(obj, cons)\n      \n  result &lt;- CVXR::solve(prob)$getValue(params)\n  \n  return(result)\n\n}\n\n\nparams4 &lt;- t(min_rss_optim1(crossprod(overlap_x_xts, overlap_y_xts), crossprod(overlap_x_xts)))\nparams4\n\n          [,1]          [,2]      [,3]      [,4]\n[1,] 0.2777951 -2.233303e-23 0.1089221 0.6132828\n\n\n\nparams4 %*% mu \n\n           [,1]\n[1,] 0.03274093\n\n\n\nsqrt(params4 %*% sigma %*% t(params4))\n\n           [,1]\n[1,] 0.05915986\n\n\n\n# roll_min_rss(xx, xy)\n\n\nmin_rss_optim2 &lt;- function(x, y) {\n  \n  params &lt;- CVXR::Variable(ncol(x))\n  \n  obj &lt;- CVXR::Minimize(CVXR::sum_squares(y - x %*% params))\n  \n  cons &lt;- list(sum(params) == 1, params &gt;= 0)\n  \n  prob &lt;- CVXR::Problem(obj, cons)\n      \n  result &lt;- CVXR::solve(prob)$getValue(params)\n  \n  return(result)\n\n}\n\n\nparams5 &lt;- t(min_rss_optim2(zoo::coredata(overlap_x_xts), zoo::coredata(overlap_y_xts)))\nparams5\n\n          [,1]          [,2]      [,3]      [,4]\n[1,] 0.2777951 -1.054614e-20 0.1089221 0.6132828\n\n\n\nparams5 %*% mu \n\n           [,1]\n[1,] 0.03274093\n\n\n\nsqrt(params5 %*% sigma %*% t(params5))\n\n           [,1]\n[1,] 0.05915986\n\n\n\nround(data.frame(\"max_pnl\" = t(params1) * 100,\n                 \"min_risk\" = t(params2) * 100,\n                 \"max_utility\" = t(params3) * 100,\n                 \"min_rss1\" = t(params4) * 100,\n                 \"min_rss2\" = t(params5) * 100), 2)\n\n  max_pnl min_risk max_utility min_rss1 min_rss2\n1   34.64    28.61       42.59    27.78    27.78\n2   10.95    10.03       12.15     0.00     0.00\n3   54.41    61.36       45.27    10.89    10.89\n4    0.00     0.00        0.00    61.33    61.33"
  },
  {
    "objectID": "posts/optim-py/index.html",
    "href": "posts/optim-py/index.html",
    "title": "Optimization",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/optim-py/index.html#random-turnover",
    "href": "posts/optim-py/index.html#random-turnover",
    "title": "Optimization",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n  \n  rng = upper - lower\n  \n  result = rand_weights2b(n_sim, n_assets) * rng\n  result = result - rng / n_assets\n  \n  return result\n\n\nlower = -0.05\nupper = 0.05\ntarget = 0\n\n\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\n\n\n\n\ndef rand_iterative(n_assets, lower, upper, target):\n  \n  result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n  temp = target - sum(result)\n  \n  while not ((temp &lt;= upper) and (temp &gt;= lower)):\n    \n    result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n    temp = target - sum(result)\n      \n  result = np.append(result, temp)\n  \n  return result\n\n\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n\n  result_ls = []\n  \n  for i in range(n_sim):\n    \n    result_sim = rand_iterative(n_assets, lower, upper, target)\n    result_ls.append(result_sim)\n    \n  result = pd.DataFrame(result_ls)\n  \n  return result\n\n\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-mean",
    "href": "posts/optim-py/index.html#maximize-mean",
    "title": "Optimization",
    "section": "Maximize mean",
    "text": "Maximize mean\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ndef max_mean_optim(mu, sigma, target):\n  \n  params = cp.Variable(len(mu))\n  \n  obj = cp.Maximize(params.T @ mu)\n  \n  cons = [cp.sum(params) == 1, params &gt;= 0,\n          cp.quad_form(params, sigma) &lt;= target ** 2]\n  \n  prob = cp.Problem(obj, cons)\n  \n  prob.solve()\n  \n  return params.value\n\n\ntarget = 0.06\n\n\nparams1 = max_mean_optim(mu, sigma, target)\nparams1\n\narray([3.46425413e-01, 1.09438165e-01, 5.44136350e-01, 7.18343869e-08])\n\n\n\nnp.dot(mu, params1)\n\nnp.float64(0.03655833047420029)\n\n\n\nnp.sqrt(np.dot(params1, np.dot(sigma, params1)))\n\nnp.float64(0.05999999950216702)"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-variance",
    "href": "posts/optim-py/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ndef min_var_optim(mu, sigma, target):\n  \n  params = cp.Variable(len(mu))\n  \n  obj = cp.Minimize(cp.quad_form(params, sigma))\n  \n  cons = [cp.sum(params) == 1, params &gt;= 0,\n          params.T @ mu &gt;= target]\n  \n  prob = cp.Problem(obj, cons)\n  \n  prob.solve()\n  \n  return params.value\n\n\ntarget = 0.03\n\n\nparams2 = min_var_optim(mu, sigma, target)\nparams2\n\narray([ 2.86078128e-01,  1.00345327e-01,  6.13576545e-01, -1.37703742e-20])\n\n\n\nnp.dot(mu, params2)\n\nnp.float64(0.02999999999999999)\n\n\n\nnp.sqrt(np.dot(params2, np.dot(sigma, params2))) \n\nnp.float64(0.04941567115042831)"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-utility",
    "href": "posts/optim-py/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ndef max_utility_optim(mu, sigma, target):\n  \n  params = cp.Variable(len(mu))\n  \n  obj = cp.Minimize(0.5 * target * cp.quad_form(params, sigma) - params.T @ mu)\n  \n  cons = [cp.sum(params) == 1, params &gt;= 0]\n  \n  prob = cp.Problem(obj, cons)\n  \n  prob.solve()\n  \n  return params.value\n\n\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\n\n\nparams3 = max_utility_optim(mu, sigma, target)\nparams3\n\narray([ 4.25878844e-01,  1.21468938e-01,  4.52652217e-01, -1.28246908e-23])\n\n\n\nnp.dot(mu, params3)\n\nnp.float64(0.0451925858693699)\n\n\n\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n\nnp.float64(0.07398513592887458)"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nhttps://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html\n\n\ndef min_rss_optim1(mu, sigma):\n  \n  params = cp.Variable(len(mu))\n  \n  obj = cp.Minimize(0.5 * cp.quad_form(params, sigma) - params.T @ mu)\n  \n  cons = [cp.sum(params) == 1, params &gt;= 0]\n  \n  prob = cp.Problem(obj, cons)\n  \n  prob.solve()\n  \n  return params.value\n\n\nparams4 = min_rss_optim1(np.dot(overlap_x_df.T.values, overlap_y_df.values),\n                         np.dot(overlap_x_df.T.values, overlap_x_df.values))\nparams4\n\narray([2.77795410e-01, 1.02408874e-22, 1.08916141e-01, 6.13288448e-01])\n\n\n\nnp.dot(mu, params4)\n\nnp.float64(0.03274099242807619)\n\n\n\nnp.sqrt(np.matmul(np.transpose(params4), np.matmul(sigma, params4)))\n\nnp.float64(0.05916001486350478)\n\n\n\ndef min_rss_optim2(x, y):\n  \n  params = cp.Variable(x.shape[1])\n  \n  obj = cp.Minimize(cp.sum_squares(y - x @ params))\n  \n  cons = [cp.sum(params) == 1, params &gt;= 0]\n  \n  prob = cp.Problem(obj, cons)\n  \n  prob.solve()\n  \n  return params.value\n\n\nparams5 = min_rss_optim2(overlap_x_df.values, overlap_y_df.iloc[:, 0].values)\nparams5\n\narray([ 2.77795410e-01, -1.05502931e-20,  1.08916141e-01,  6.13288448e-01])\n\n\n\nnp.dot(mu, params5)\n\nnp.float64(0.03274099242808064)\n\n\n\nnp.sqrt(np.matmul(np.transpose(params5), np.matmul(sigma, params5)))\n\nnp.float64(0.05916001486350223)\n\n\n\npd.DataFrame({\n  \"max_pnl\": params1 * 100,\n  \"min_risk\": params2 * 100,\n  \"max_utility\": params3 * 100,\n  \"min_rss1\": params4 * 100,\n  \"min_rss2\": params5 * 100\n}).round(2)\n\n   max_pnl  min_risk  max_utility  min_rss1  min_rss2\n0    34.64     28.61        42.59     27.78     27.78\n1    10.94     10.03        12.15      0.00     -0.00\n2    54.41     61.36        45.27     10.89     10.89\n3     0.00     -0.00        -0.00     61.33     61.33"
  },
  {
    "objectID": "posts/eigen-r/index.html",
    "href": "posts/eigen-r/index.html",
    "title": "Eigen",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\n\n\nDecomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\neigen_decomp &lt;- function(x, comps) {\n  \n  LV &lt;- eigen(cov(x))\n  L &lt;- LV[[\"values\"]][1:comps]\n  V &lt;- LV[[\"vectors\"]][ , 1:comps]\n  \n  result &lt;- V %*% sweep(t(V), 1, L, \"*\")\n  \n  return(result)\n    \n}\n\n\ncomps &lt;- 1\n\n\neigen_decomp(overlap_xts, comps) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  3.112361e-02 -3.893592e-03 -8.078094e-05  2.637884e-03\n[2,] -3.893592e-03  4.870920e-04  1.010577e-05 -3.300017e-04\n[3,] -8.078094e-05  1.010577e-05  2.096660e-07 -6.846596e-06\n[4,]  2.637884e-03 -3.300017e-04 -6.846596e-06  2.235741e-04\n\n\n\n# cov(overlap_xts) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\nvariance_explained &lt;- function(x) {\n    \n  LV &lt;- eigen(cov(x))\n  L &lt;- LV[[\"values\"]]\n  \n  result &lt;- cumsum(L) / sum(L)\n  \n  return(result)\n    \n}\n\n\nvariance_explained(overlap_xts)\n\n[1] 0.8879254 0.9925357 0.9982268 1.0000000\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n\\]\n\nsimilarity &lt;- function(V, V0) {\n    \n  n_cols_v &lt;- ncol(V)\n  n_cols_v0 &lt;- ncol(V0)\n  result &lt;- matrix(0, nrow = n_cols_v, ncol = n_cols_v0)\n  \n  for (i in 1:n_cols_v) {\n    for (j in 1:n_cols_v0) {\n      result[i, j] &lt;- crossprod(V[ , i], V0[ , j]) /\n        sqrt(crossprod(V[ , i]) * crossprod(V0[ , j]))\n    }\n  }\n  \n  return(result)\n    \n}\n\n\nroll_eigen1 &lt;- function(x, width, comp) {\n    \n  n_rows &lt;- nrow(x)\n  result_ls &lt;- list()\n  \n  for (i in width:n_rows) {\n    \n    idx &lt;- max(i - width + 1, 1):i\n    \n    LV &lt;- eigen(cov(x[idx, ]))\n    V &lt;- LV[[\"vectors\"]]\n    \n    result_ls &lt;- append(result_ls, list(V[ , comp]))\n    \n  }\n  \n  result &lt;- do.call(rbind, result_ls)\n  result &lt;- xts::xts(result, zoo::index(x)[width:n_rows])\n  colnames(result) &lt;- colnames(x)\n  \n  return(result)\n    \n}\n\n\ncomp &lt;- 1\n\n\nraw_df &lt;- roll_eigen1(overlap_xts, width, comp)\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolleigen\") # roll (&gt;= 1.1.7)\n# raw_df &lt;- rolleigen::roll_eigen(overlap_xts, width, order = TRUE)[[\"vectors\"]][ , comp, ]\n# raw_df &lt;- xts::xts(t(raw_df), zoo::index(overlap_xts))\n# colnames(raw_df) &lt;- colnames(overlap_xts)\n\n\n\n\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\nroll_eigen2 &lt;- function(x, width, comp) {\n    \n  n_rows &lt;- nrow(x)\n  V_ls &lt;- list()\n  result_ls &lt;- list()\n  \n  for (i in width:n_rows) {\n    \n    idx &lt;- max(i - width + 1, 1):i\n    \n    LV &lt;- eigen(cov(x[idx, ]))\n    V &lt;- LV[[\"vectors\"]]\n    \n    if (i &gt; width) {\n      \n      # cosine &lt;- crossprod(V, V_ls[[length(V_ls)]])\n      cosine &lt;- similarity(V, V_ls[[length(V_ls)]])\n      order &lt;- apply(abs(cosine), 1, which.max)\n      V &lt;- t(sign(diag(cosine[ , order])) * t(V[ , order]))\n      \n    }\n    \n    V_ls &lt;- append(V_ls, list(V))\n    result_ls &lt;- append(result_ls, list(V[ , comp]))\n    \n  }\n  \n  result &lt;- do.call(rbind, result_ls)\n  result &lt;- xts::xts(result, zoo::index(x)[width:n_rows])\n  colnames(result) &lt;- colnames(x)\n  \n  return(result)\n    \n}\n\n\nclean_df &lt;- roll_eigen2(overlap_xts, width, comp)\n\n\n\n\n\n\n\n\n\n\n\n\nImplied shocks\nProduct of the \\(n\\)th eigenvector and square root of the \\(n\\)th eigenvalue:\n\nroll_shocks &lt;- function(x, width, comp) {\n  \n  n_rows &lt;- nrow(x)\n  V_ls &lt;- list()\n  result_ls &lt;- list()\n  \n  for (i in width:n_rows) {\n    \n    idx &lt;- max(i - width + 1, 1):i\n    \n    LV &lt;- eigen(cov(x[idx, ]))\n    L &lt;- LV[[\"values\"]]\n    V &lt;- LV[[\"vectors\"]]\n    \n    if (length(V_ls) &gt; 1) {\n      \n      # cosine &lt;- crossprod(V, V_ls[[length(V_ls)]])\n      cosine &lt;- similarity(V, V_ls[[length(V_ls)]])\n      order &lt;- apply(abs(cosine), 1, which.max)\n      L &lt;- L[order]\n      V &lt;- t(sign(diag(cosine[ , order])) * t(V[ , order]))\n      \n    }\n    \n    shocks &lt;- sqrt(L[comp]) * V[ , comp]\n    V_ls &lt;- append(V_ls, list(V))\n    result_ls &lt;- append(result_ls, list(shocks))\n    \n  }\n  \n  result &lt;- do.call(rbind, result_ls)\n  result &lt;- xts::xts(result, zoo::index(x)[width:n_rows])\n  colnames(result) &lt;- colnames(x)\n  \n  return(result)\n    \n}\n\n\nshocks_xts &lt;- roll_shocks(overlap_xts, width, comp) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])"
  },
  {
    "objectID": "posts/eigen-py/index.html",
    "href": "posts/eigen-py/index.html",
    "title": "Eigen",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\n\n\nDecomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\ndef eigen(x):\n  \n  L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n  idx = L.argsort()[::-1]\n  L = L[idx]\n  V = V[:, idx]\n  \n  result = {\n    \"values\": L,\n    \"vectors\": V\n  }\n    \n  return result\n\n\ndef eigen_decomp(x, comps):\n  \n  LV = eigen(x)\n  L = LV[\"values\"][:comps]\n  V = LV[\"vectors\"][:, :comps]\n  \n  result = np.dot(V, np.multiply(L, V.T))\n  \n  return np.ravel(result)\n\n\ncomps = 1\n\n\neigen_decomp(overlap_df, comps) * scale[\"periods\"] * scale[\"overlap\"]\n\narray([ 3.19294657e-02, -3.79444587e-03, -8.04947508e-05,  2.63187781e-03,\n       -3.79444587e-03,  4.50925786e-04,  9.56586550e-06, -3.12768086e-04,\n       -8.04947508e-05,  9.56586550e-06,  2.02928698e-07, -6.63501077e-06,\n        2.63187781e-03, -3.12768086e-04, -6.63501077e-06,  2.16940079e-04])\n\n\n\n# np.cov(overlap_df.T) * scale[\"periods\"] * scale[\"overlap\"]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\ndef variance_explained(x):\n  \n  LV = eigen(x)\n  L = LV[\"values\"]\n  \n  result = L.cumsum() / L.sum()\n  \n  return result\n\n\nvariance_explained(overlap_df)\n\narray([0.8856599 , 0.99267755, 0.99824402, 1.        ])\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n\\]\n\ndef similarity(V, V0):\n  \n  n_cols_v = V.shape[1]\n  n_cols_v0 = V0.shape[1]\n  result = np.zeros((n_cols_v, n_cols_v0))\n  \n  for i in range(n_cols_v):\n    for j in range(n_cols_v0):\n      result[i, j] = np.dot(V[:, i], V0[:, j]) / \\\n      np.sqrt(np.dot(V[:, i], V[:, i]) * np.dot(V0[:, j], V0[:, j]))\n      \n  return result\n\n\ndef roll_eigen1(x, width, comp):\n    \n  n_rows = len(x)\n  result_ls = []\n  \n  for i in range(width - 1, n_rows):\n      \n    idx = range(max(i - width + 1, 0), i + 1)\n    \n    LV = eigen(x.iloc[idx])\n    V = LV[\"vectors\"]\n    \n    result_ls.append(V[:, comp - 1])\n  \n  result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                           columns = x.columns)\n  \n  return result_df  \n\n\ncomp = 1\n\n\nraw_df = roll_eigen1(overlap_df, width, comp)\n\n\n\n\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\ndef roll_eigen2(x, width, comp):\n    \n  n_rows = len(x)\n  V_ls = []\n  result_ls = []\n  \n  for i in range(width - 1, n_rows):\n      \n    idx = range(max(i - width + 1, 0), i + 1)\n    \n    LV = eigen(x.iloc[idx])\n    V = LV[\"vectors\"]\n    \n    if i &gt; width - 1:\n        \n      # cosine = np.dot(V.T, V_ls[-1])\n      cosine = similarity(V.T, V_ls[-1])\n      order = np.argmax(np.abs(cosine), axis = 1)\n      V = np.sign(np.diag(cosine[:, order])) * V[:, order]\n        \n    V_ls.append(V)\n    result_ls.append(V[:, comp - 1])\n  \n  result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                           columns = x.columns)\n  \n  return result_df\n\n\nclean_df = roll_eigen2(overlap_df, width, comp)\n\n\n\n\n\n\n\n\n\n\n\n\nImplied shocks\nProduct of the \\(n\\)th eigenvector and square root of the \\(n\\)th eigenvalue:\n\ndef roll_shocks(x, width, comp):\n    \n  n_rows = len(x)\n  V_ls = []\n  result_ls = []\n  \n  for i in range(width - 1, n_rows):\n    \n    idx = range(max(i - width + 1, 0), i + 1)\n    \n    LV = eigen(x.iloc[idx])\n    L = LV[\"values\"]\n    V = LV[\"vectors\"]\n    \n    if len(V_ls) &gt; 1:\n        \n      # cosine = np.dot(V.T, V_ls[-1])\n      cosine = similarity(V.T, V_ls[-1])\n      order = np.argmax(np.abs(cosine), axis = 1)\n      L = L[order]\n      V = np.sign(np.diag(cosine[:, order])) * V[:, order]\n    \n    shocks = np.sqrt(L[comp - 1]) * V[:, comp - 1]\n    V_ls.append(V)\n    result_ls.append(shocks)\n    \n  result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                           columns = x.columns)\n  \n  return result_df\n\n\nshocks_df = roll_shocks(overlap_df, width, comp) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])"
  },
  {
    "objectID": "posts/markets-py/index.html",
    "href": "posts/markets-py/index.html",
    "title": "Markets",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/markets-py/index.html#expected-value",
    "href": "posts/markets-py/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-py/index.html#variance",
    "href": "posts/markets-py/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\ndef sd(x):\n    \n  n_rows = sum(~np.isnan(x))\n      \n  if n_rows &gt; 1:\n    result = np.sqrt(np.nansum(x ** 2) / (n_rows - 1))\n  else:\n    result = np.nan\n      \n  return result\n\n\n# volatility scale only\nscore_df = (momentum_df / momentum_df.rolling(width, min_periods = 1).apply(sd, raw = False)).dropna()\n\n\n# overall_df = score_df.mean(axis = 1)\n# overall_df = overall_df / overall_df.rolling(width, min_periods = 1).apply(risk, raw = False)\n\n\n# score_df.insert(loc = 0, column = \"Overall\", value = overall_df)\n# score_df = score_df.dropna()"
  },
  {
    "objectID": "posts/markets-py/index.html#interquartile-range",
    "href": "posts/markets-py/index.html#interquartile-range",
    "title": "Markets",
    "section": "Interquartile range",
    "text": "Interquartile range\nOutliers are defined as the regression residuals that fall below \\(Q_{1}−1.5\\times IQR\\) or above \\(Q_{3}+1.5\\times IQR\\):\n\nhttps://stats.stackexchange.com/a/1153\nhttps://stats.stackexchange.com/a/108951\nhttps://robjhyndman.com/hyndsight/tsoutliers/\n\n\ndef outliers(z):\n  \n  n_cols = z.shape[1]\n  result_ls = []\n\n  for j in range(n_cols):\n    \n    y = z.iloc[:, j]\n\n    if (n_cols == 0):\n      x = sm.add_constant(range(len(y)))\n    else:\n      x = sm.add_constant(z.drop(z.columns[j], axis = 1))\n\n    coef = sm.WLS(y, x).fit().params\n    predict = coef.iloc[0] + np.dot(x.iloc[:, 1:], coef[1:])\n    resid = y - predict\n\n    lower = resid.quantile(0.25)\n    upper = resid.quantile(0.75)\n    iqr = upper - lower\n\n    total = y[(resid &lt; lower - 1.5 * iqr) | (resid &gt; upper + 1.5 * iqr)]\n    \n    total = pd.DataFrame({\"date\": total.index, \"symbol\": total.name, \"values\": total})\n    result_ls.append(total)\n\n  result = pd.concat(result_ls, ignore_index = True)\n  result = result.pivot_table(index = \"date\", columns = \"symbol\", values = \"values\")\n\n  return result\n\n\noutliers_df = outliers(score_df)"
  },
  {
    "objectID": "posts/markets-py/index.html#contour-ellipsoid",
    "href": "posts/markets-py/index.html#contour-ellipsoid",
    "title": "Markets",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid"
  },
  {
    "objectID": "posts/risk-py/index.html",
    "href": "posts/risk-py/index.html",
    "title": "Risk",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\nintercept = True"
  },
  {
    "objectID": "posts/risk-py/index.html#ordinary-least-squares",
    "href": "posts/risk-py/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\ndef lm_coef(x, y, weights, intercept):\n  \n  if (intercept): x = sm.add_constant(x)\n      \n  result = np.dot(np.linalg.inv(np.dot(x.T, np.multiply(weights, x))),\n                  np.dot(x.T, np.multiply(weights, y)))\n  \n  return np.ravel(result)\n\n\nlm_coef(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([ 1.26517152e-04,  1.83289818e-01, -1.01982452e-01,  3.11705519e+00,\n        1.14645113e+00])\n\n\n\nif (intercept): overlap_x_df = sm.add_constant(overlap_x_df)\n    \nfit = sm.WLS(overlap_y_df, overlap_x_df, weights = weights).fit()\n\nif (intercept): overlap_x_df = overlap_x_df.iloc[:, 1:]\n\nnp.array(fit.params)\n\narray([ 1.26517152e-04,  1.83289818e-01, -1.01982452e-01,  3.11705519e+00,\n        1.14645113e+00])\n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\ndef lm_rsq(x, y, weights, intercept):\n          \n  coef = np.matrix(lm_coef(x, y, weights, intercept))\n  \n  if (intercept):\n    \n    x = sm.add_constant(x)\n    x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n    y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n      \n  result = np.dot(coef, np.dot(np.dot(x.T, np.multiply(weights, x)), coef.T)) / \\\n    np.dot(y.T, np.multiply(weights, y))\n  \n  return np.float64(result.item())\n\n\nlm_rsq(overlap_x_df, overlap_y_df, weights, intercept)\n\nnp.float64(0.7368866722990308)\n\n\n\nfit.rsquared\n\nnp.float64(0.7368866722990306)\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\ndef lm_se(x, y, weights, intercept):\n  \n  n_rows = x.shape[0]\n  n_cols = x.shape[1]\n  \n  rsq = lm_rsq(x, y, weights, intercept)\n  \n  if (intercept):\n    \n    x = sm.add_constant(x)\n    y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n    \n    df_resid = n_rows - n_cols - 1 \n    \n  else:\n    df_resid = n_rows - n_cols        \n  \n  var_y = np.dot(y.T, np.multiply(weights, y))\n  var_resid = (1 - rsq) * var_y / df_resid\n  \n  result = np.sqrt(var_resid * np.linalg.inv(np.dot(x.T, np.multiply(weights, x))).diagonal())\n  \n  return np.ravel(result)\n\n\nlm_se(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([6.24702982e-05, 1.93411170e-02, 3.34838428e-02, 2.52682278e-01,\n       2.23655552e-01])\n\n\n\nnp.array(fit.bse)\n\narray([6.24702982e-05, 1.93411170e-02, 3.34838428e-02, 2.52682278e-01,\n       2.23655552e-01])\n\n\n\n\nShapley values\n\\[\nR^{2}_{i}=\\sum_{S\\subseteq N\\setminus\\{i\\}}{\\frac{|S|!\\;(n-|S|-1)!}{n!}}(R^{2}(S\\cup\\{i\\})-R^{2}(S))\n\\]\n\nhttps://real-statistics.com/multiple-regression/shapley-owen-decomposition/\n\n\ndef lm_shap(x, y, weights, intercept):\n\n  n_rows = x.shape[0]\n  n_cols = x.shape[1]\n  n_combn = 2 ** n_cols\n  n_vec = np.zeros(n_combn)\n  ix_mat = np.zeros((n_cols, n_combn))\n  rsq = np.zeros(n_combn)\n  result = np.zeros(n_cols)\n  \n  # number of binary combinations\n  for k in range(n_combn):\n    \n    n = 0\n    n_size = k\n    \n    # find the binary combination\n    for j in range(n_cols):\n      \n      if n_size % 2 == 0:\n        \n        n += 1\n        \n        ix_mat[j, k] = j + 1\n          \n      n_size //= 2\n    \n    n_vec[k] = n\n    \n    if n &gt; 0:\n      \n      ix_subset = np.where(ix_mat[:, k] != 0)[0]\n      x_subset = x.iloc[:, ix_subset]\n      \n      rsq[k] = lm_rsq(x_subset, y, weights, intercept)\n\n  # calculate the exact Shapley value for r-squared\n  for j in range(n_cols):\n    \n    ix_pos = np.where(ix_mat[j, :] != 0)[0]\n    ix_neg = np.where(ix_mat[j, :] == 0)[0]\n    ix_n = n_vec[ix_neg]\n    rsq_diff = rsq[ix_pos] - rsq[ix_neg]\n\n    for k in range(int(n_combn / 2)):\n      \n      s = int(ix_n[k])\n      weight = math.factorial(s) * math.factorial(n_cols - s - 1) \\\n        / math.factorial(n_cols)\n      result[j] += weight * rsq_diff[k]\n\n  return result\n\n\nlm_shap(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([0.32139057, 0.01698864, 0.15811682, 0.24039065])"
  },
  {
    "objectID": "posts/risk-py/index.html#principal-component-regression",
    "href": "posts/risk-py/index.html#principal-component-regression",
    "title": "Risk",
    "section": "Principal component regression",
    "text": "Principal component regression\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\n\n\ncomps = 1\n\n\nCoefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\ndef pcr_coef(x, y, comps):\n  \n  x = x - np.average(x, axis = 0)\n  L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n  idx = L.argsort()[::-1]\n  V = V[:, idx]\n  \n  W = np.dot(x, V)\n  gamma = np.dot(np.dot(np.linalg.inv(np.dot(W.T, W)), W.T), y)\n  \n  result = np.dot(V[:, :comps], gamma[:comps])\n  \n  return np.ravel(result)\n\n\nscale_x_df = (overlap_x_df - np.average(overlap_x_df, axis = 0)) \\\n  / np.std(overlap_x_df, axis = 0, ddof = 1)\n\n\npcr_coef(scale_x_df, overlap_y_df, comps)\n\narray([ 6.47330935e-04,  4.55509438e-05, -1.31420379e-04,  6.51119479e-04])\n\n\n\npcr_coef(overlap_x_df, overlap_y_df, comps)\n\narray([ 0.25401205,  0.00302391, -0.00057864,  0.01779625])\n\n\n\npca = PCA(n_components = len(factors))\npca_x_df = pca.fit_transform(scale_x_df)\n\nfit = LinearRegression(fit_intercept = False).fit(pca_x_df, overlap_y_df)\n\ngamma = fit.coef_\nnp.dot(pca.components_.T[:, :comps], gamma.T[:comps]).ravel()\n\narray([ 6.47330935e-04,  4.55509438e-05, -1.31420379e-04,  6.51119479e-04])\n\n\n\n\nR-squared\n\ndef pcr_rsq(x, y, comps):\n  \n  coef = np.matrix(pcr_coef(x, y, comps))\n  \n  x = x - np.average(x, axis = 0)\n  y = y - np.average(y, axis = 0)\n  \n  result = np.dot(np.dot(coef, np.dot(x.T, x)), coef.T) / np.dot(y.T, y)\n  \n  return np.float64(result.item())\n\n\npcr_rsq(scale_x_df, overlap_y_df, comps)\n\nnp.float64(0.4565055362604753)\n\n\n\npcr_rsq(overlap_x_df, overlap_y_df, comps)\n\nnp.float64(0.530608586512265)\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\ndef pcr_se(x, y, comps):\n  \n  n_rows = x.shape[0]\n  n_cols = x.shape[1]\n  \n  rsq = pcr_rsq(x, y, comps)\n  \n  y = y - np.average(y, axis = 0)\n  \n  df_resid = n_rows - n_cols - 1\n  \n  var_y = np.dot(y.T, y)   \n  var_resid = (1 - rsq) * var_y / df_resid\n  \n  # uses statsmodels for illustrative purposes\n  pca = sm.multivariate.PCA(x, standardize = False, demean = True)\n  L = pca.eigenvals[:comps]\n  V = pca.eigenvecs.iloc[:, :comps]\n  \n  result = np.sqrt(var_resid * np.dot(V, np.dot(np.diag(1 / L), V.T)).diagonal())\n  \n  return np.ravel(result)\n\n\npcr_se(scale_x_df, overlap_y_df, comps)\n\narray([4.49420029e-05, 3.16244835e-06, 9.12407353e-06, 4.52050287e-05])\n\n\n\npcr_se(overlap_x_df, overlap_y_df, comps)\n\narray([1.52014939e-02, 1.80967702e-04, 3.46291531e-05, 1.06502643e-03])"
  },
  {
    "objectID": "posts/risk-py/index.html#partial-least-squares",
    "href": "posts/risk-py/index.html#partial-least-squares",
    "title": "Risk",
    "section": "Partial least squares",
    "text": "Partial least squares"
  },
  {
    "objectID": "posts/risk-py/index.html#standalone-risk",
    "href": "posts/risk-py/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\ndef cov_wt(x, weights, center):\n  \n  sum_w = sum(weights)\n  sumsq_w = sum(np.power(weights, 2))\n  \n  if (center):\n  \n    x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n  \n  result = np.dot(x.T, np.multiply(weights, x)) / (sum_w - sumsq_w / sum_w)\n  \n  return result\n\n\ndef lm_sar(x, y, weights, intercept):\n    \n  coef = lm_coef(x, y, weights, intercept)\n  rsq = lm_rsq(x, y, weights, intercept)\n  \n  if (intercept): x = sm.add_constant(x)\n  \n  # sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n  #                aweights = weights.reshape(-1))\n  sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n  sar = np.multiply(np.power(coef, 2).T, sigma[:-1, :-1].diagonal())\n  sar_eps = (1 - rsq) * sigma[-1, -1]\n  \n  result = np.sqrt(np.concatenate((np.matrix(sigma[-1, -1]),\n                                   np.matrix(sar),\n                                   np.matrix(sar_eps)), axis = 1))\n  \n  return np.ravel(result)\n\n\nlm_sar(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])\n\narray([0.06583626, 0.        , 0.03443123, 0.00664231, 0.02707   ,\n       0.01870893, 0.03377043])"
  },
  {
    "objectID": "posts/risk-py/index.html#risk-contribution",
    "href": "posts/risk-py/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttps://bookdown.org/compfinezbook/introcompfinr/Portfolio-risk-reports.html\n\n\ndef lm_mcr(x, y, weights, intercept):\n    \n  coef = np.matrix(lm_coef(x, y, weights, intercept)).T\n  rsq = lm_rsq(x, y, weights, intercept)\n      \n  if (intercept): x = sm.add_constant(x)\n  \n#     sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n#                    aweights = weights.reshape(-1))\n  sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n  mcr = np.multiply(coef, np.dot(sigma[:-1, :-1], coef)) / np.sqrt(sigma[-1, -1])\n  mcr_eps = np.sqrt(sigma[-1, -1]) - sum(mcr)\n  \n  result = np.concatenate((np.sqrt(np.matrix(sigma[-1, -1])),\n                           np.matrix(mcr).T,\n                           np.matrix(mcr_eps)), axis = 1)\n  \n  return np.ravel(result)\n\n\nlm_mcr(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])\n\narray([0.06583626, 0.        , 0.02509125, 0.00099811, 0.01005174,\n       0.01237277, 0.0173224 ])"
  },
  {
    "objectID": "posts/risk-py/index.html#implied-shocks",
    "href": "posts/risk-py/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\ndef implied_shocks(shocks, x, z, weights):\n\n  beta = np.linalg.lstsq(np.multiply(weights, z), np.multiply(weights, x), rcond = None)[0]\n                   \n  result = np.dot(shocks, beta)\n  \n  return result\n\n\nshocks = np.array([-0.1, 0.1])\noverlap_z_df = overlap_x_df.iloc[:, [0, 1]]\n\n\nimplied_shocks(shocks, overlap_x_df, overlap_z_df, weights)\n\narray([-0.1       ,  0.1       , -0.00201402, -0.00724209])"
  },
  {
    "objectID": "posts/risk-py/index.html#stress-pl",
    "href": "posts/risk-py/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\ndef pnl_stress(shocks, x, y, z, weights, intercept):\n  \n  coef = lm_coef(x, y, weights, intercept)\n  \n  if (intercept): x = sm.add_constant(x)\n  \n  result = np.multiply(coef.T, implied_shocks(shocks, x, z, weights))\n  \n  return result\n\n\npnl_stress(shocks, overlap_x_df, overlap_y_df, overlap_z_df, weights, intercept)\n\narray([-0.00115613, -0.01832898, -0.01019825, -0.0062778 , -0.0083027 ])"
  },
  {
    "objectID": "posts/markets-r/index.html",
    "href": "posts/markets-r/index.html",
    "title": "Markets",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/markets-r/index.html#expected-value",
    "href": "posts/markets-r/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-r/index.html#variance",
    "href": "posts/markets-r/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\n# volatility scale only\nscore_xts &lt;- na.omit(momentum_xts / roll::roll_sd(momentum_xts, width, center = FALSE, min_obs = 1))\n\n\n# overall_xts &lt;- xts::xts(rowMeans(score_xts), zoo::index(score_xts))\n# overall_xts &lt;- overall_xts / roll::roll_sd(overall_xts, width, center = FALSE, min_obs = 1)\n# colnames(overall_xts) &lt;- \"Overall\"\n\n\n# score_xts &lt;- na.omit(merge(overall_xts, score_xts))"
  },
  {
    "objectID": "posts/markets-r/index.html#interquartile-range",
    "href": "posts/markets-r/index.html#interquartile-range",
    "title": "Markets",
    "section": "Interquartile range",
    "text": "Interquartile range\nOutliers are defined as the regression residuals that fall below \\(Q_{1}−1.5\\times IQR\\) or above \\(Q_{3}+1.5\\times IQR\\):\n\nhttps://stats.stackexchange.com/a/1153\nhttps://stats.stackexchange.com/a/108951\nhttps://robjhyndman.com/hyndsight/tsoutliers/\n\n\noutliers &lt;- function(z) {\n  \n  n_cols &lt;- ncol(z)\n  result_ls &lt;- list()\n  \n  for (j in 1:n_cols) {\n    \n    y &lt;- z[ , j]\n    \n    if (n_cols == 1) {\n      x &lt;- 1:length(y)\n    } else {\n      x &lt;- cbind(1:length(y), z[ , -j])\n    }\n    \n    coef &lt;- coef(lm(y ~ x))\n    predict &lt;- coef[1] + x %*% as.matrix(coef[-1])\n    resid &lt;- y - predict\n    \n    lower &lt;- quantile(resid, prob = 0.25)\n    upper &lt;- quantile(resid, prob = 0.75)\n    iqr &lt;- upper - lower\n    \n    total &lt;- y[(resid &lt; lower - 1.5 * iqr) | (resid &gt; upper + 1.5 * iqr)]\n    \n    result_ls &lt;- append(result_ls, list(total))\n    \n  }\n  \n  result &lt;- do.call(merge, result_ls)\n  \n  return(result)\n  \n}\n\n\noutliers_xts &lt;- outliers(score_xts)"
  },
  {
    "objectID": "posts/markets-r/index.html#contour-ellipsoid",
    "href": "posts/markets-r/index.html#contour-ellipsoid",
    "title": "Markets",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid"
  },
  {
    "objectID": "posts/risk-r/index.html",
    "href": "posts/risk-r/index.html",
    "title": "Risk",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\ntickers &lt;- \"BAICX\" # fund inception date is \"2011-11-28\"\nintercept &lt;- TRUE"
  },
  {
    "objectID": "posts/risk-r/index.html#ordinary-least-squares",
    "href": "posts/risk-r/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\nlm_coef &lt;- function(x, y, weights, intercept) {\n  \n  if (intercept) x &lt;- model.matrix(~ x)\n  \n  result &lt;- solve(crossprod(x, diag(weights)) %*% x) %*% crossprod(x, diag(weights) %*% y)\n  \n  return(result)\n  \n}\n\n\nt(lm_coef(overlap_x_xts, overlap_y_xts, weights, intercept))\n\n          (Intercept)    xSP500 xDTWEXAFEGS   xDGS10 xBAMLH0A0HYM2\nadjclose 0.0001265385 0.1832117   -0.101991 3.116936       1.14714\n\n\n\nif (intercept) {\n  fit &lt;- lm(overlap_y_xts ~ overlap_x_xts, weights = weights)\n} else {\n  fit &lt;- lm(overlap_y_xts ~ overlap_x_xts - 1, weights = weights)\n}\n    \ncoef(fit)\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n             0.0001265385              0.1832116810             -0.1019909882 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             3.1169362388              1.1471402395 \n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\nlm_rsq &lt;- function(x, y, weights, intercept) {\n        \n  coef &lt;- lm_coef(x, y, weights, intercept)\n  \n  if (intercept) {\n      \n    x &lt;- model.matrix(~ x)\n    x &lt;- sweep(x, 2, apply(x, 2, weighted.mean, w = weights), \"-\")\n    y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n      \n  }\n  \n  result &lt;- (t(coef) %*% (crossprod(x, diag(weights)) %*% x) %*% coef) / (crossprod(y, diag(weights)) %*% y)\n  \n  return(result)\n    \n}\n\n\nlm_rsq(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n          adjclose\nadjclose 0.7367781\n\n\n\nsummary(fit)$r.squared\n\n[1] 0.7367781\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\nlm_se &lt;- function(x, y, weights, intercept) {\n    \n  n_rows &lt;- nrow(x)\n  n_cols &lt;- ncol(x)\n  \n  rsq &lt;- lm_rsq(x, y, weights, intercept)\n  \n  if (intercept) {\n      \n    x &lt;- model.matrix(~ x)\n    y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n    \n    df_resid &lt;- n_rows - n_cols - 1\n      \n  } else {\n    df_resid &lt;- n_rows - n_cols\n  }\n  \n  var_y &lt;- crossprod(y, diag(weights)) %*% y\n  var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n  \n  result &lt;- sqrt(var_resid * diag(solve(crossprod(x, diag(weights)) %*% x)))\n  \n  return(result)\n    \n}\n\n\nlm_se(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n  (Intercept)        xSP500   xDTWEXAFEGS        xDGS10 xBAMLH0A0HYM2 \n 6.246275e-05  1.934827e-02  3.348208e-02  2.526642e-01  2.236746e-01 \n\n\n\ncoef(summary(fit))[ , \"Std. Error\"]\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n             6.246275e-05              1.934827e-02              3.348208e-02 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             2.526642e-01              2.236746e-01 \n\n\n\n\nShapley values\n\\[\nR^{2}_{i}=\\sum_{S\\subseteq N\\setminus\\{i\\}}{\\frac{|S|!\\;(n-|S|-1)!}{n!}}(R^{2}(S\\cup\\{i\\})-R^{2}(S))\n\\]\n\nhttps://real-statistics.com/multiple-regression/shapley-owen-decomposition/\n\n\nlm_shap &lt;- function(x, y, weights, intercept) {\n  \n  n_rows &lt;- nrow(x)\n  n_cols &lt;- ncol(x)\n  n_combn &lt;- 2 ^ n_cols\n  n_vec &lt;- array(0, n_combn)\n  ix_mat &lt;- matrix(0, nrow = n_cols, ncol = n_combn)\n  rsq &lt;- array(0, n_combn)\n  result &lt;- array(0, n_cols)\n  \n  # number of binary combinations\n  for (k in 1:n_combn) {\n    \n    n &lt;- 0\n    n_size &lt;- k - 1\n    \n    # find the binary combination\n    for (j in 1:n_cols) {\n      \n      if (n_size %% 2 == 0) {\n        \n        n &lt;- n + 1\n        \n        ix_mat[j, k] = j - 1 + 1\n        \n      }\n      \n      n_size &lt;- n_size %/% 2\n      \n    }\n    \n    n_vec[k] &lt;- n\n    \n    if (n &gt; 0) {\n      \n      ix_subset&lt;- which(ix_mat[ , k] != 0)\n      x_subset &lt;- x[ , ix_subset]\n      \n      rsq[k] &lt;- lm_rsq(x_subset, y, weights, intercept)\n\n    }\n    \n  }\n\n  # calculate the exact Shapley value for r-squared\n  for (j in 1:n_cols) {\n\n    ix_pos &lt;- which(ix_mat[j, ] != 0)\n    ix_neg &lt;- which(ix_mat[j, ] == 0)\n    ix_n &lt;- n_vec[ix_neg]\n    rsq_diff &lt;- rsq[ix_pos] - rsq[ix_neg]\n\n    for (k in 1:(n_combn / 2)) {\n\n      s &lt;- ix_n[k]\n      weight &lt;- factorial(s) * factorial(n_cols - s - 1) / factorial(n_cols)\n      result[j] &lt;- result[j] + weight * rsq_diff[k]\n\n    }\n    \n  }\n\n  return(result)\n  \n}\n\n\nlm_shap(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n[1] 0.32120082 0.01701392 0.15810406 0.24045932"
  },
  {
    "objectID": "posts/risk-r/index.html#principal-component-regression",
    "href": "posts/risk-r/index.html#principal-component-regression",
    "title": "Risk",
    "section": "Principal component regression",
    "text": "Principal component regression\n\nlibrary(pls) # \"Error in mvrValstats(object = fit, estimate = 'train'): could not find function 'mvrValstats'\"\n\n\ncomps &lt;- 1\n\n\nCoefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\npcr_coef &lt;- function(x, y, comps) {\n  \n  x &lt;- sweep(x, 2, colMeans(x), \"-\")\n  LV &lt;- eigen(cov(x))\n  V &lt;- LV[[\"vectors\"]]\n  \n  W &lt;- x %*% V\n  gamma &lt;- solve(crossprod(W)) %*% (crossprod(W, y))\n  \n  result &lt;- V[ , 1:comps] %*% as.matrix(gamma[1:comps])\n  \n  return(result)\n  \n}\n\n\nscale_x_xts &lt;- scale(overlap_x_xts)\n\n\nt(pcr_coef(scale_x_xts, overlap_y_xts, comps))\n\n             [,1]         [,2]          [,3]         [,4]\n[1,] 0.0006469949 4.537575e-05 -0.0001316164 0.0006507679\n\n\n\nt(pcr_coef(overlap_x_xts, overlap_y_xts, comps))\n\n          [,1]        [,2]          [,3]       [,4]\n[1,] 0.2539628 0.002999905 -0.0005833619 0.01779956\n\n\n\nfit &lt;- pls::pcr(reformulate(termlabels = \".\", response = tickers),\n                data = merge(scale_x_xts, overlap_y_xts), ncomp = comps)\ncoef(fit)[ , , 1]\n\n        SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 6.469949e-04  4.537575e-05 -1.316164e-04  6.507679e-04 \n\n\n\n\nR-squared\n\npcr_rsq &lt;- function(x, y, comps) {\n  \n  coef &lt;- pcr_coef(x, y, comps)\n  \n  x &lt;- sweep(x, 2, colMeans(x), \"-\")\n  y &lt;- sweep(y, 2, colMeans(y), \"-\")\n  \n  result &lt;- (t(coef) %*% crossprod(x) %*% coef) / crossprod(y)\n  \n  return(result)\n  \n}\n\n\npcr_rsq(scale_x_xts, overlap_y_xts, comps)\n\n          adjclose\nadjclose 0.4562923\n\n\n\npcr_rsq(overlap_x_xts, overlap_y_xts, comps)\n\n          adjclose\nadjclose 0.5303531\n\n\n\npls::R2(fit)$val[comps + 1]\n\n[1] 0.4562923\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\npcr_se &lt;- function(x, y, comps) {\n  \n  n_rows &lt;- nrow(x)\n  n_cols &lt;- ncol(x)\n  \n  rsq &lt;- pcr_rsq(x, y, comps)\n  \n  y &lt;- sweep(y, 2, colMeans(y), \"-\")\n  \n  df_resid &lt;- n_rows - n_cols - 1\n  \n  var_y &lt;- crossprod(y)\n  var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n  \n  LV &lt;- eigen(cov(x))\n  L &lt;- LV$values[1:comps] * (n_rows - 1)\n  V &lt;- LV$vectors[ , 1:comps]\n  \n  result &lt;- sqrt(var_resid * diag(V %*% sweep(t(V), 1, 1 / L, \"*\")))\n  \n  return(result)\n  \n}\n\n\npcr_se(scale_x_xts, overlap_y_xts, comps)\n\n[1] 4.493798e-05 3.151640e-06 9.141608e-06 4.520004e-05\n\n\n\npcr_se(overlap_x_xts, overlap_y_xts, comps)\n\n[1] 1.520635e-02 1.796231e-04 3.492953e-05 1.065771e-03"
  },
  {
    "objectID": "posts/risk-r/index.html#standalone-risk",
    "href": "posts/risk-r/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\nlm_sar &lt;- function(x, y, weights, intercept) {\n  \n  coef &lt;- lm_coef(x, y, weights, intercept)\n  rsq &lt;- lm_rsq(x, y, weights, intercept)\n  \n  if (intercept) x &lt;- model.matrix(~ x)\n  \n  sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n  sar &lt;- coef ^ 2 * diag(sigma[-ncol(sigma), -ncol(sigma)])\n  sar_eps &lt;- (1 - rsq) * sigma[ncol(sigma), ncol(sigma)]\n  \n  result &lt;- sqrt(c(sigma[ncol(sigma), ncol(sigma)],\n                   sar,\n                   sar_eps))\n  \n  return(result)\n  \n}\n\n\nlm_sar(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])\n\n[1] 0.065821538 0.000000000 0.034407178 0.006642862 0.027068968 0.018720178\n[7] 0.033769841"
  },
  {
    "objectID": "posts/risk-r/index.html#risk-contribution",
    "href": "posts/risk-r/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttps://bookdown.org/compfinezbook/introcompfinr/Portfolio-risk-reports.html\n\n\nlm_mcr &lt;- function(x, y, weights, intercept) {\n  \n  coef &lt;- lm_coef(x, y, weights, intercept)\n  rsq &lt;- lm_rsq(x, y, weights, intercept)\n  \n  if (intercept) x &lt;- model.matrix(~ x)\n  \n  sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n  mcr &lt;- coef * sigma[-ncol(sigma), -ncol(sigma)] %*% coef / sqrt(sigma[ncol(sigma), ncol(sigma)])\n  mcr_eps &lt;- sqrt(sigma[ncol(sigma), ncol(sigma)]) - sum(mcr)\n  \n  result &lt;- c(sqrt(sigma[ncol(sigma), ncol(sigma)]),\n              mcr,\n              mcr_eps)\n  \n  return(result)\n  \n}\n\n\nlm_mcr(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])\n\n[1] 0.06582154 0.00000000 0.02506747 0.00099939 0.01004748 0.01238152 0.01732567"
  },
  {
    "objectID": "posts/risk-r/index.html#implied-shocks",
    "href": "posts/risk-r/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\nimplied_shocks &lt;- function(shocks, x, z, weights) {\n  \n  beta &lt;- solve(crossprod(z, diag(weights) %*% z)) %*% crossprod(z, diag(weights) %*% x)\n  \n  result &lt;- shocks %*% beta\n  \n  return(result)\n  \n}\n\n\nshocks &lt;- c(-0.1, 0.1)\noverlap_z_xts &lt;- overlap_x_xts[ , 1:2]\n\n\nimplied_shocks(shocks, overlap_x_xts, overlap_z_xts, weights)\n\n     SP500 DTWEXAFEGS        DGS10 BAMLH0A0HYM2\n[1,]  -0.1        0.1 -0.002011851 -0.007241182"
  },
  {
    "objectID": "posts/risk-r/index.html#stress-pl",
    "href": "posts/risk-r/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\npnl_stress &lt;- function(shocks, x, y, z, weights, intercept) {\n  \n  coef &lt;- lm_coef(x, y, weights, intercept)\n  \n  if (intercept) x &lt;- model.matrix(~ x)\n  \n  result &lt;- t(coef) * implied_shocks(shocks, x, z, weights)\n  \n  return(result)    \n  \n}\n\n\npnl_stress(shocks, overlap_x_xts, overlap_y_xts, overlap_z_xts, weights, intercept)\n\n          (Intercept)      xSP500 xDTWEXAFEGS      xDGS10 xBAMLH0A0HYM2\nadjclose -0.001155425 -0.01832117  -0.0101991 -0.00627081  -0.008306651"
  },
  {
    "objectID": "posts/securities-r/index.html",
    "href": "posts/securities-r/index.html",
    "title": "Securities",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/securities-r/index.html#value",
    "href": "posts/securities-r/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_value &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n\n  r_df &lt;- exp(-r * tau)\n  q_df &lt;- exp(-q * tau)\n  \n  call_value &lt;- S * q_df * Phi(d1) - r_df * K * Phi(d2)\n  put_value &lt;- r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n  result &lt;- ifelse(type == \"call\", call_value, put_value)\n  \n  return(result)\n  \n} \n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\nbs_d1 &lt;- function(S, K, r, q, tau, sigma) {\n  \n  result &lt;- (log(S / K) + (r - q + sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n  \n  return(result)\n  \n}\n\nbs_d2 &lt;- function(S, K, r, q, tau, sigma) {\n  \n  result &lt;- (log(S / K) + (r - q - sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n  \n  return(result)\n  \n}\n\nphi &lt;- function(x) {\n  \n  result &lt;- dnorm(x)\n  \n  return(result)\n  \n}\n\nPhi &lt;- function(x) {\n  \n  result &lt;- pnorm(x)\n  \n  return(result)\n  \n}\n\n\ngreeks_dt[ , d1 := bs_d1(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , d2 := bs_d2(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , value := bs_value(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order",
    "href": "posts/securities-r/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_delta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n  q_df &lt;- exp(-q * tau)\n\n  call_value &lt;- q_df * Phi(d1)\n  put_value &lt;- -q_df * Phi(-d1)\n  result &lt;- ifelse(type == \"call\", call_value, put_value)\n  \n  return(result)\n  \n} \n\n\ngreeks_dt[ , delta := bs_delta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nDelta-beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\nbs_delta_diff &lt;- function(type, S, K, r, q, tau, sigma, delta0) {\n  \n  d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n  d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n  delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n  \n  call_value &lt;- delta - delta0\n  put_value &lt;- delta0 - delta\n  \n  result &lt;- ifelse(type == \"call\", call_value, put_value)\n      \n  return(result)\n  \n}\n\n\nbeta &lt;- 0.35\ntype &lt;- \"call\"\nn &lt;- 1\nmultiple &lt;- 100\ntotal &lt;- 1000000\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\nsec &lt;- list(\n  \"n\" = n,\n  \"multiple\" = multiple,\n  \"S\" = S,\n  \"delta\" = bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n  \"beta\" = 1\n)\n\n\nbeta_dt &lt;- data.table::CJ(type = type, shock = shocks)\nbeta_dt[ , spot := level_shock(shock, S, tau, sigma), by = c(\"type\", \"shock\")]\nbeta_dt[ , static := beta]\nbeta_dt[ , diff := bs_delta_diff(type, spot, K, r, q, tau, sigma, sec[[\"delta\"]])]\nbeta_dt[ , dynamic := beta + sec[[\"n\"]] * sec[[\"multiple\"]] * sec[[\"S\"]] * sec[[\"beta\"]] * diff / total, by = c(\"type\", \"shock\")]\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\nbs_vega &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n  q_df &lt;- exp(-q * tau)\n  \n  result &lt;- S * q_df * phi(d1) * sqrt(tau)\n  \n  return(result)\n  \n}\n\n\ngreeks_dt[ , vega := bs_vega(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_theta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n  r_df &lt;- exp(r * tau)\n  q_df &lt;- exp(q * tau)\n\n  call_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) -\n    r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n  \n  put_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) +\n    r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n      \n  result &lt;- ifelse(type == \"call\", call_value, put_value)\n  \n  return(result)\n  \n}\n\n\ngreeks_dt[ , theta := bs_theta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order",
    "href": "posts/securities-r/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\nbs_gamma &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n\n  q_df &lt;- exp(-q * tau)\n  \n  result &lt;- q_df * phi(d1) / (S * sigma * sqrt(tau))\n  \n  return(result)\n  \n}\n\n\ngreeks_dt[ , gamma := bs_gamma(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order-1",
    "href": "posts/securities-r/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\n\npnl_bond &lt;- function(duration, convexity, dy) {\n  \n  duration_pnl &lt;- -duration * dy\n  convexity_pnl &lt;- (convexity / 2) * dy ^ 2\n  income_pnl &lt;- dy\n  \n  result &lt;- list(\n    \"total\" = duration_pnl + convexity_pnl + income_pnl,\n    \"duration\" = duration_pnl,\n    \"convexity\" = convexity_pnl,\n    \"income\" = income_pnl\n  )\n  \n  return(result)\n  \n} \n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor &lt;- \"DGS10\"\nduration &lt;- 6.5\nconvexity &lt;- 0.65\ny &lt;- zoo::coredata(tail(zoo::na.locf(levels_xts[ , factor]), width)[1])\n\n\nbonds_dt &lt;- data.table::data.table(index = zoo::index(tail(levels_xts, width)),\n                                   duration = duration, convexity = convexity,\n                                   dy = zoo::na.locf(tail(levels_xts[ , factor], width)))\ndata.table::setnames(bonds_dt, c(\"index\", \"duration\", \"convexity\", \"dy\"))\nbonds_dt[ , dy := (dy - y) / 100, by = index]\n\n\nattrib_dt &lt;- bonds_dt[ , as.list(unlist(pnl_bond(duration, convexity, dy))), by = index]\n\n\n\n\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=\\frac{\\partial D}{\\partial y}=\\frac{\\partial}{\\partial y}\\left(-\\frac{1}{P}\\frac{\\partial D}{\\partial y}\\right)\\\\\n&=-\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}+\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=-C+D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(-C\\times 100+D^{2}\\right)=-C+\\frac{D^{2}}{100}\\). For example, if convexity and yield are percent then \\(\\text{Drift}=\\left(-0.65+\\frac{6.5^{2}}{100}\\right)\\partial y\\times100\\) or basis points then \\(\\text{Drift}=\\left(-65+6.5^{2}\\right)\\partial y\\).\n\nyield_shock &lt;- function(shock, tau, sigma) {\n  \n  result &lt;- shock * sigma * sqrt(tau)\n  \n  return(result)\n  \n}\n\n\nduration_drift &lt;- function(duration, convexity, dy) {\n  \n  drift &lt;- -convexity + duration ^ 2 / 100\n  change &lt;- drift * dy * 100\n  \n  result &lt;- list(\n    \"drift\" = drift,\n    \"change\" = change\n  )\n  \n  return(result)\n  \n}\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor &lt;- \"DGS10\"\nsigma &lt;- zoo::coredata(sd_xts[nrow(sd_xts), factor])\n\n\nduration_dt &lt;- data.table::CJ(shock = shocks)\nduration_dt[ , spot := yield_shock(shock, tau, sigma), by = \"shock\"]\nduration_dt[ , static := duration]\nduration_dt[ , dynamic := duration + duration_drift(duration, convexity, spot)[[\"change\"]], by = \"shock\"]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order-1",
    "href": "posts/securities-r/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlack’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\npnl_option &lt;- function(type, S, K, r, q, tau, sigma, dS, dt, dsigma) {\n  \n  d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n  d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n  value &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n  delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n  vega &lt;- bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n  theta &lt;- bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n  gamma &lt;- bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n  \n  delta_pnl &lt;- delta * dS / value\n  gamma_pnl &lt;- gamma / 2 * dS ^ 2 / value\n  vega_pnl &lt;- vega * dsigma / value\n  theta_pnl &lt;- theta * dt / value\n  \n  result &lt;- list(\n    \"total\" = delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n    \"delta\" = delta_pnl,\n    \"gamma\" = gamma_pnl,\n    \"vega\" = vega_pnl,\n    \"theta\" = theta_pnl\n  )\n  \n  return(result)    \n  \n}\n\n\nfactor &lt;- \"SP500\"\ntype &lt;- \"call\"\nS &lt;- zoo::coredata(tail(zoo::na.locf(levels_xts[ , factor]), width)[1])\nK &lt;- S # * (1 + 0.05)\ntau &lt;- 1 # = 252 / 252\nsigma &lt;- zoo::coredata(tail(sd_xts[ , factor], width)[1])\n\n\noptions_dt &lt;- data.table::data.table(index = zoo::index(tail(levels_xts, width)),\n                                     spot = zoo::na.locf(tail(levels_xts[ , factor], width)),\n                                     sigma = tail(sd_xts[ , factor], width))\ndata.table::setnames(options_dt, c(\"index\", \"spot\", \"sigma\"))\noptions_dt[ , dS := spot - S, by = index]\noptions_dt[ , dt_diff := as.numeric(index - index[1])]\noptions_dt[ , dt := dt_diff / tail(dt_diff, 1)]\noptions_dt[ , dsigma := sigma - ..sigma, by = index]\n\n\nattrib_dt &lt;- options_dt[ , as.list(unlist(pnl_option(type, S, K, r, q, tau, ..sigma,\n                                                     dS, dt, dsigma))), by = index]\n\n\n\n\n\n\n\n\n\n\n\n\nIto’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\nsim_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n  \n  result &lt;- S * exp(cumsum(sigma * sqrt(dt) * rnorm(n_sim)) +\n                    (mu - 0.5 * sigma ^ 2) * dt)\n  \n  return(result)\n  \n}\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\nsim_multi_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n  \n  n_cols &lt;- ncol(sigma)\n  \n  Z &lt;- matrix(rnorm(n_sim * n_cols), nrow = n_sim, ncol = n_cols)\n  X &lt;- sweep(sqrt(dt) * (Z %*% chol(sigma)), 2, (mu - 0.5 * diag(sigma)) * dt, \"+\")\n  \n  result &lt;- sweep(apply(X, 2, function(x) exp(cumsum(x))), 2, S, \"*\")\n  \n  return(result)\n  \n}\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS &lt;- rep(1, length(factors))\nsigma &lt;- cov(returns_xts, use = \"complete.obs\") * scale[[\"periods\"]]\nmu &lt;- colMeans(na.omit(returns_xts)) * scale[[\"periods\"]]\nmu &lt;- mu + diag(sigma) / 2 # drift\ndt &lt;- 1 / scale[[\"periods\"]]\n\n\nmu_ls &lt;- list()\nsigma_ls &lt;- list()\n\n\nfor (i in 1:1e4) {\n  \n  # assumes stock prices\n  levels_sim &lt;- sim_multi_gbm(width + 1, S, mu, sigma, dt)\n  returns_sim &lt;- diff(log(levels_sim))\n\n  mu_sim &lt;- colMeans(returns_sim) * scale[[\"periods\"]]\n  sigma_sim &lt;- apply(returns_sim, 2, sd) * sqrt(scale[[\"periods\"]])\n  \n  mu_ls &lt;- append(mu_ls, list(mu_sim))\n  sigma_ls &lt;- append(sigma_ls, list(sigma_sim))\n  \n}\n\n\ndata.frame(\n  \"empirical\" = colMeans(na.omit(returns_xts)) * scale[[\"periods\"]],\n  \"theoretical\" = colMeans(do.call(rbind, mu_ls)\n))\n\n                 empirical   theoretical\nSP500         0.1182866969  0.1183045038\nDTWEXAFEGS   -0.0054335166 -0.0060763457\nDGS10        -0.0005403087 -0.0005830431\nBAMLH0A0HYM2  0.0040523156  0.0041926321\n\n\n\ndata.frame(\n  \"empirical\" = sqrt(diag(sigma)),\n  \"theoretical\" = colMeans(do.call(rbind, sigma_ls))\n)\n\n               empirical theoretical\nSP500        0.183914917 0.183793252\nDTWEXAFEGS   0.061168097 0.061136205\nDGS10        0.008470421 0.008461933\nBAMLH0A0HYM2 0.016957011 0.016944259\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility",
    "href": "posts/securities-r/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\nimplied_vol_newton &lt;- function(params, type, S, K, r, q, tau) {\n  \n  target0 &lt;- 0\n  sigma &lt;- params[[\"sigma\"]]\n  sigma0 &lt;- sigma\n  \n  while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma0)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma0)\n    \n    target0 &lt;- bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n    d_target0 &lt;- bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n    \n    sigma &lt;- sigma0 - (target0 - params[[\"target\"]]) / d_target0\n    sigma0 &lt;- sigma\n    \n  }\n  \n  return(sigma)\n  \n}\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS &lt;- zoo::coredata(zoo::na.locf(levels_xts)[nrow(levels_xts), factor])\nK &lt;- S # * (1 + 0.05)\nsigma &lt;- zoo::coredata(sd_xts[nrow(sd_xts), factor]) # overrides matrix\nstart1 &lt;- 0.2\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\ntarget1 &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams1 &lt;- list(\n  \"target\" = target1,\n  \"sigma\" = start1,\n  \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nimplied_vol_newton(params1, type, S, K, r, q, tau)\n\n         SP500\n[1,] 0.1871511"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity",
    "href": "posts/securities-r/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\nyld_newton &lt;- function(params, cash_flows) {\n  \n  target0 &lt;- 0\n  yld &lt;- params[[\"cpn\"]]\n  yld0 &lt;- yld\n  \n  while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n    \n  target0 &lt;- 0\n  d_target0 &lt;- 0\n  dd_target0 &lt;- 0\n  \n  for (i in 1:length(cash_flows)) {\n    \n    t &lt;- i\n    \n    # present value of cash flows\n    target0 &lt;- target0 + cash_flows[i] / (1 + yld0) ^ t\n    \n    # first derivative of present value of cash flows\n    d_target0 &lt;- d_target0 - t * cash_flows[i] / (1 + yld0) ^ (t + 1) # use t for Macaulay duration\n    \n    # second derivative of present value of cash flows\n    dd_target0 &lt;- dd_target0 - t * (t + 1) * cash_flows[i] / (1 + yld0) ^ (t + 2)\n    \n  }\n  \n  yld &lt;- yld0 - (target0 - params[[\"target\"]]) / d_target0\n  yld0 &lt;- yld\n    \n  }\n  \n  result &lt;- list(\n    \"price\" = target0,\n    \"yield\" = yld * params[[\"freq\"]],\n    \"duration\" = -d_target0 / params[[\"target\"]] / params[[\"freq\"]],\n    \"convexity\" = -dd_target0 / params[[\"target\"]] / params[[\"freq\"]] ^ 2\n  )\n  \n  return(result)\n  \n}\n\n\nhttps://www.bloomberg.com/markets/rates-bonds/government-bonds/us\nhttps://quant.stackexchange.com/a/61025\nhttps://pages.stern.nyu.edu/~igiddy/spreadsheets/duration-convexity.xls\n\n\ntarget2 &lt;- 0.9928 * 1000 # present value\nstart2 &lt;- 0.0438 # coupon\ncash_flows &lt;- rep(start2 * 1000 / 2, 10 * 2)\ncash_flows[10 * 2] &lt;- cash_flows[10 * 2] + 1000\n\n\nparams2 &lt;- list(\n  \"target\" = target2,\n  \"cpn\" = start2,\n  \"freq\" = 2,\n  \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nt(yld_newton(params2, cash_flows))\n\n     price yield      duration convexity\n[1,] 992.8 0.04470076 8.016596 76.6811"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility-1",
    "href": "posts/securities-r/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\nimplied_vol_obj &lt;- function(param, type, S, K, r, q, tau, target) {\n  \n  d1 &lt;- bs_d1(S, K, r, q, tau, param)\n  d2 &lt;- bs_d2(S, K, r, q, tau, param)\n  target0 &lt;- bs_value(type, S, K, r, q, tau, param, d1, d2)\n  \n  result &lt;- abs(target0 - target)\n  \n  return(result)\n    \n}\n\nimplied_vol_optim &lt;- function(param, type, S, K, r, q, tau, target) {\n  \n  result &lt;- optim(param, implied_vol_obj, type = type, S = S, K = K, r = r, q = q,\n                  tau = tau, target = target, method = \"Brent\", lower = 0, upper = 1)\n  \n  return(result$par)\n    \n}\n\n\nimplied_vol_optim(start1, type, S, K, r, q, tau, target1)\n\n[1] 0.1871511"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity-1",
    "href": "posts/securities-r/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\nyld_obj &lt;- function(param, cash_flows, target) {\n  \n  target0 &lt;- 0\n      \n  for (i in 1:length(cash_flows)) {\n    target0 &lt;- target0 + cash_flows[i] / (1 + param) ^ i\n  }\n\n  result &lt;- abs(target0 - target)\n  \n  return(result)\n    \n}\n\nyld_optim &lt;- function(params, cash_flows, target) {\n  \n  result &lt;- optim(params[[\"cpn\"]], yld_obj, target = target, cash_flows = cash_flows,\n                  method = \"Brent\", lower = 0, upper = 1)\n  \n  return(result$par * params[[\"freq\"]])\n    \n}\n\n\nyld_optim(params2, cash_flows, target2)\n\n[1] 0.04470077"
  },
  {
    "objectID": "posts/securities-py/index.html",
    "href": "posts/securities-py/index.html",
    "title": "Securities",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/securities-py/index.html#value",
    "href": "posts/securities-py/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n  \n  if (type == \"call\"):\n    result =  S * np.exp(-q * tau) * Phi(d1) - np.exp(-r * tau) * K * Phi(d2)\n  elif (type == \"put\"):\n    result = np.exp(-r * tau) * K * Phi(-d2) - S * np.exp(-q * tau) * Phi(-d1)\n      \n  return result\n\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n  \n  r_df = np.exp(-r * tau)\n  q_df = np.exp(-q * tau)\n  \n  call_value = S * q_df * Phi(d1) - r_df * K * Phi(d2)\n  put_value = r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n  result = np.where(type == \"call\", call_value, put_value)\n  \n  return result\n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\ndef bs_d1(S, K, r, q, tau, sigma):\n    \n  result = (np.log(S / K) + (r - q + sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n  \n  return result\n\ndef bs_d2(S, K, r, q, tau, sigma):\n  \n  result = (np.log(S / K) + (r - q - sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n  \n  return result\n    \ndef phi(x):\n  \n  result = norm.pdf(x)\n  \n  return result\n\ndef Phi(x):\n  \n  result = norm.cdf(x)\n  \n  return result\n\n\ngreeks_df[\"d1\"] = bs_d1(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"d2\"] = bs_d2(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"value\"] = bs_value(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order",
    "href": "posts/securities-py/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_delta(type, S, K, r, q, tau, sigma, d1, d2):\n\n  q_df = np.exp(-q * tau)\n  \n  call_value = q_df * Phi(d1)\n  put_value = -q_df * Phi(-d1)\n  result = np.where(type == \"call\", call_value, put_value)\n  \n  return result\n\n\ngreeks_df[\"delta\"] = bs_delta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\nDelta-beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\ndef bs_delta_diff(type, S, K, r, q, tau, sigma, delta0):\n  \n  d1 = bs_d1(S, K, r, q, tau, sigma)\n  d2 = bs_d2(S, K, r, q, tau, sigma)\n  delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n  \n  call_value = delta - delta0\n  put_value = delta0 - delta\n  \n  result = np.where(type == \"call\", call_value, put_value)\n      \n  return result\n\n\nbeta = 0.35\ntype = \"call\"\nn = 1\nmultiple = 100\ntotal = 1000000\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\nsec = {\n  \"n\": n,\n  \"multiple\": multiple,\n  \"S\": S,\n  \"delta\": bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n  \"beta\": 1\n}\n\n\nbeta_df = pd.DataFrame([(x, y) for x in types for y in shocks], \n  columns = [\"type\", \"shock\"])\nbeta_df[\"spot\"] = level_shock(beta_df[\"shock\"], S, tau, sigma)\nbeta_df[\"static\"] = beta\nbeta_df[\"diff\"] = bs_delta_diff(type, beta_df[\"spot\"], K, r, q, tau, sigma, sec[\"delta\"])\nbeta_df[\"dynamic\"] = beta + sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * sec[\"beta\"] * beta_df[\"diff\"] / total\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\ndef bs_vega(type, S, K, r, q, tau, sigma, d1, d2):\n  \n  q_df = np.exp(-q * tau)\n  result = S * q_df * phi(d1) * np.sqrt(tau)\n  \n  return result\n\n\ngreeks_df[\"vega\"] = bs_vega(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                            greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_theta(type, S, K, r, q, tau, sigma, d1, d2):\n  \n  r_df = np.exp(-r * tau)\n  q_df = np.exp(-q * tau)\n  \n  call_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) - \\\n    r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n      \n  put_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) + \\\n    r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n      \n  result = np.where(type == \"call\", call_value, put_value)\n  \n  return result\n\n\ngreeks_df[\"theta\"] = bs_theta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order",
    "href": "posts/securities-py/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\ndef bs_gamma(type, S, K, r, q, tau, sigma, d1, d2):\n\n  q_df = np.exp(-q * tau)\n  \n  result = q_df * phi(d1) / (S * sigma * np.sqrt(tau))\n  \n  return result\n\n\ngreeks_df[\"gamma\"] = bs_gamma(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order-1",
    "href": "posts/securities-py/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\n\ndef pnl_bond(duration, convexity, dy):\n  \n  duration_pnl = -duration * dy\n  convexity_pnl = (convexity / 2) * dy ** 2\n  income_pnl = dy\n  \n  result = pd.DataFrame({\n    \"total\": duration_pnl + convexity_pnl + income_pnl,\n    \"duration\": duration_pnl,\n    \"convexity\": convexity_pnl,\n    \"income\": income_pnl\n  })\n  \n  return result\n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor = \"DGS10\"\nduration = 6.5\nconvexity = 0.65\ny = levels_df.ffill()[factor].iloc[-width]\n\n\nbond_df = pd.DataFrame({\n  \"duration\": duration,\n  \"convexity\": convexity,\n  \"dy\": (levels_df.ffill()[factor].iloc[-width:] - y) / 100\n})\n\n\nattrib_df = pnl_bond(bond_df[\"duration\"], bond_df[\"convexity\"], bond_df[\"dy\"])\n\n\n\n\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=\\frac{\\partial D}{\\partial y}=\\frac{\\partial}{\\partial y}\\left(-\\frac{1}{P}\\frac{\\partial D}{\\partial y}\\right)\\\\\n&=-\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}+\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=-C+D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(-C\\times 100+D^{2}\\right)=-C+\\frac{D^{2}}{100}\\). For example, if convexity and yield are percent then \\(\\text{Drift}=\\left(-0.65+\\frac{6.5^{2}}{100}\\right)\\partial y\\times100\\) or basis points then \\(\\text{Drift}=\\left(-65+6.5^{2}\\right)\\partial y\\).\n\ndef yield_shock(shock, tau, sigma):\n  \n  result = shock * sigma * np.sqrt(tau)\n  \n  return result\n\n\ndef duration_drift(duration, convexity, dy):\n  \n  drift = -convexity + duration ** 2 / 100\n  change = drift * dy * 100\n  \n  result = {\n    \"drift\": drift,\n    \"change\": change\n  }\n  \n  return result\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor = \"DGS10\"\nsigma = sd_df[factor].iloc[-1]\n\n\nduration_df = pd.DataFrame(shocks).rename(columns = {0: \"shock\"})\nduration_df[\"spot\"] = yield_shock(duration_df[\"shock\"], tau, sigma)\nduration_df[\"static\"] = duration\nduration_df[\"dynamic\"] = duration + \\\n  duration_drift(duration, convexity, duration_df[\"spot\"])[\"change\"]"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order-1",
    "href": "posts/securities-py/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlack’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\ndef pnl_option(type, S, K, r, q, tau, sigma, dS, dt, dsigma):\n  \n  d1 = bs_d1(S, K, r, q, tau, sigma)\n  d2 = bs_d2(S, K, r, q, tau, sigma)\n  value = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n  delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n  vega = bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n  theta = bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n  gamma = bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n  \n  delta_pnl = delta * dS / value\n  gamma_pnl = gamma / 2 * dS ** 2 / value\n  vega_pnl = vega * dsigma / value\n  theta_pnl = theta * dt / value\n  \n  result = pd.DataFrame({\n    \"total\": delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n    \"delta\": delta_pnl,\n    \"gamma\": gamma_pnl,\n    \"vega\": vega_pnl,\n    \"theta\": theta_pnl\n  })\n  \n  return result\n\n\nfactor = \"SP500\"\ntype = \"call\"\nS = levels_df.ffill()[factor].iloc[-width]\nK = S # * (1 + 0.05)\ntau = 1 # = 252 / 252\nsigma = sd_df[factor].iloc[-width]\n\n\noptions_df = pd.DataFrame({\n  \"spot\": levels_df.ffill()[factor].iloc[-width:],\n  \"sigma\": sd_df[factor].iloc[-width:]\n})\noptions_df[\"dS\"] = options_df[\"spot\"] - S\noptions_df[\"dt_diff\"] = (options_df.index - options_df.index[0]).days\noptions_df[\"dt\"] = options_df[\"dt_diff\"] / options_df[\"dt_diff\"].iloc[-1]\noptions_df[\"dsigma\"] = options_df[\"sigma\"] - sigma\n\n\nattrib_df = pnl_option(type, S, K, r, q, tau, sigma,\n                       options_df[\"dS\"], options_df[\"dt\"], options_df[\"dsigma\"])\n\n\n\n\n\n\n\n\n\n\n\n\nIto’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\ndef sim_gbm(n_sim, S, mu, sigma, dt):\n  \n  result = S * np.exp(np.cumsum(sigma * np.sqrt(dt) * np.random.normal(size = n_sim)) + \\\n                      (mu - 0.5 * sigma ** 2) * dt)\n  \n  return result\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\ndef sim_multi_gbm(n_sim, S, mu, sigma, dt):\n  \n  n_cols = sigma.shape[1]\n  \n  Z = np.random.normal(size = n_sim * n_cols).reshape((n_sim, n_cols))\n  X = np.sqrt(dt) * Z @ np.linalg.cholesky(sigma).T + (mu - 0.5 * np.diag(sigma)) * dt\n  \n  result = S * np.exp(X.cumsum(axis = 0))\n  \n  return result\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS = [1] * len(factors)\nsigma = np.cov(returns_df.dropna().T, ddof = 1) * scale[\"periods\"]\nmu = np.array(returns_df.dropna().mean()) * scale[\"periods\"]\nmu = mu + np.diag(sigma) / 2 # drift\ndt = 1 / scale[\"periods\"]\n\n\nmu_ls = []\nsigma_ls = []\n\n\nfor i in range(10000): # \"TypeError: 'float' object cannot be interpreted as an integer\"\n\n  # assumes underlying stock price follows geometric Brownian motion with constant volatility\n  levels_sim = pd.DataFrame(sim_multi_gbm(width + 1, S, mu, sigma, dt))\n  returns_sim = np.log(levels_sim).diff()\n\n  mu_sim = returns_sim.mean() * scale[\"periods\"]\n  sigma_sim = returns_sim.std() * np.sqrt(scale[\"periods\"])\n\n  mu_ls.append(mu_sim)\n  sigma_ls.append(sigma_sim)\n\n\nmu_df = pd.DataFrame(mu_ls)\nsigma_df = pd.DataFrame(sigma_ls)\n\n\npd.DataFrame({\n  \"empirical\": np.array(returns_df.dropna().mean()) * scale[\"periods\"],\n  \"theoretical\": mu_df.mean()\n})\n\n   empirical  theoretical\n0   0.118287     0.116745\n1  -0.005434    -0.005230\n2  -0.000540    -0.000346\n3   0.004052     0.003869\n\n\n\npd.DataFrame({\n  \"empirical\": np.sqrt(np.diag(sigma)),\n  \"theoretical\": sigma_df.mean()\n})\n\n   empirical  theoretical\n0   0.183915     0.183676\n1   0.061168     0.061122\n2   0.008470     0.008465\n3   0.016957     0.016938\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility",
    "href": "posts/securities-py/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\ndef implied_vol_newton(params, type, S, K, r, q, tau):\n  \n  target0 = 0\n  sigma = params[\"sigma\"]\n  sigma0 = sigma\n  \n  while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n      \n    d1 = bs_d1(S, K, r, q, tau, sigma0)\n    d2 = bs_d2(S, K, r, q, tau, sigma0)\n    \n    target0 = bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n    d_target0 = bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n    \n    sigma = sigma0 - (target0 - params[\"target\"]) / d_target0\n    sigma0 = sigma\n      \n  return sigma\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS = levels_df.ffill()[factor].iloc[-1]\nK = S # * (1 + 0.05)\nsigma = sd_df[factor].iloc[-1] # overrides matrix\nstart1 = 0.2\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\ntarget1 = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams1 = {\n  \"target\": target1,\n  \"sigma\": start1,\n  \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nimplied_vol_newton(params1, type, S, K, r, q, tau) \n\nnp.float64(0.18759233714468238)"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity",
    "href": "posts/securities-py/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\ndef yld_newton(params, cash_flows):\n  \n  target0 = 0\n  yld0 = params[\"cpn\"] / params[\"freq\"]\n  yld = yld0 # assignment to `yield` variable is not possible\n  \n  while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n      \n    target0 = 0\n    d_target0 = 0\n    dd_target0 = 0\n    \n    for i in range(len(cash_flows)):\n      \n      t = i + 1\n    \n      # present value of cash flows\n      target0 += cash_flows[i] / (1 + yld0) ** t\n      \n      # first derivative of present value of cash flows\n      d_target0 -= t * cash_flows[i] / (1 + yld0) ** (t + 1) # use t for Macaulay duration\n      \n      # second derivative of present value of cash flows\n      dd_target0 -= t * (t + 1) * cash_flows[i] / (1 + yld0) ** (t + 2)\n    \n    yld = yld0 - (target0 - params[\"target\"]) / d_target0\n    yld0 = yld\n      \n  result = {\n    \"price\": target0,\n    \"yield\": yld * params[\"freq\"],\n    \"duration\": -d_target0 / params[\"target\"] / params[\"freq\"],\n    \"convexity\": -dd_target0 / params[\"target\"] / params[\"freq\"] ** 2\n  }\n      \n  return result\n\n\nhttps://www.bloomberg.com/markets/rates-bonds/government-bonds/us\nhttps://quant.stackexchange.com/a/61025\nhttps://pages.stern.nyu.edu/~igiddy/spreadsheets/duration-convexity.xls\n\n\ntarget2 = 0.9928 * 1000 # present value\nstart2 = 0.0438 # coupon\ncash_flows = [start2 * 1000 / 2] * 10 * 2\ncash_flows[-1] += 1000\n\n\nparams2 = {\n  \"target\": target2,\n  \"cpn\": start2,\n  \"freq\": 2,\n  \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nyld_newton(params2, cash_flows)\n\n{'price': 992.8000005704454, 'yield': 0.044700757710159994, 'duration': 8.0165959605043, 'convexity': 76.68109754287481}"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility-1",
    "href": "posts/securities-py/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\ndef implied_vol_obj(param, type, S, K, r, q, tau, target):\n  \n  d1 = bs_d1(S, K, r, q, tau, param)\n  d2 = bs_d2(S, K, r, q, tau, param)\n  target0 = bs_value(type, S, K, r, q, tau, param, d1, d2)\n  \n  result = abs(target0 - target)\n  \n  return result\n\ndef implied_vol_optim(param, type, S, K, r, q, tau, target):\n  \n  result = minimize(implied_vol_obj, param, args = (type, S, K, r, q, tau, target))\n  \n  return result.x\n\n\nimplied_vol_optim(start1, type, S, K, r, q, tau, target1)\n\narray([0.18759233])"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity-1",
    "href": "posts/securities-py/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\ndef yld_obj(param, cash_flows, target):\n  \n  target0 = 0\n  \n  for i in range(len(cash_flows)):\n      \n    target0 += cash_flows[i] / (1 + param) ** (i + 1)\n  \n  target0 = abs(target0 - target)\n  \n  return target0\n  \ndef yld_optim(params, cash_flows, target):\n  \n  result = minimize(yld_obj, params[\"cpn\"], args = (cash_flows, target))\n  \n  return result.x * params[\"freq\"]\n\n\nyld_optim(params2, cash_flows, target2)\n\narray([0.04470075])"
  },
  {
    "objectID": "posts/statistics/index.html",
    "href": "posts/statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Usage\n\nlibrary(roll)\nlibrary(microbenchmark)\noptions(microbenchmark.unit = \"us\")\n\n\nn_vars &lt;- 10\nn_obs &lt;- 1000\nweights &lt;- 0.9 ^ (n_obs:1)\n\nx &lt;- matrix(rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)\ny &lt;- matrix(rnorm(n_obs), nrow = n_obs, ncol = 1)\nx_lgl &lt;- x &lt; 0\n\n\n\nRolling any\n\nresult &lt;- microbenchmark(\"125\" = roll_any(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_any(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_any(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_any(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125  89.9 143.60 158.607 155.50 172.30  218.2   100\n  250 117.5 139.65 153.598 149.80 162.70  210.3   100\n  500 105.7 141.20 164.751 154.10 167.25 1000.0   100\n 1000  83.0 126.95 142.486 135.75 155.55  219.8   100\n\n\n\n\nRolling all\n\nresult &lt;- microbenchmark(\"125\" = roll_all(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_all(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_all(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_all(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr  min    lq    mean median     uq   max neval\n  125 83.3 92.50 105.384  95.85 101.65 181.5   100\n  250 84.0 89.60 105.699  93.75 106.25 200.2   100\n  500 78.0 87.20 104.185  90.90 109.25 284.4   100\n 1000 74.3 80.35  89.024  82.40  88.40 177.2   100\n\n\n\n\nRolling sums\n\\[\n\\begin{aligned}\n&\\text{Expanding window} \\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_sum(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sum(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sum(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sum(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr  min     lq    mean median     uq   max neval\n  125 92.3 105.80 130.447 116.80 144.75 253.3   100\n  250 95.0 104.85 126.861 113.35 137.15 273.7   100\n  500 91.7 102.65 123.558 111.90 130.20 286.9   100\n 1000 88.5  99.10 123.163 107.45 140.50 227.8   100\n\n\n\n\nRolling products\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{prod}_{w}\\leftarrow\\text{prod}_{w}\\times\\text{w}_{new}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}/\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_prod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_prod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_prod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_prod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 204.7 219.35 241.780 234.20 246.50 387.8   100\n  250 207.3 221.75 253.678 234.35 259.70 610.5   100\n  500 139.8 152.10 173.136 167.15 185.20 266.8   100\n 1000 135.7 148.45 166.439 162.10 174.35 271.4   100\n\n\n\n\nRolling means\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times \\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_mean(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_mean(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_mean(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_mean(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125  93.0 109.80 129.285 116.10 152.35 216.7   100\n  250 101.5 110.35 132.540 118.95 151.65 355.6   100\n  500  96.0 108.40 130.634 115.35 153.00 232.8   100\n 1000  92.0 102.25 124.633 112.05 147.90 232.1   100\n\n\n\n\nRolling minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_min(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_min(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_min(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_min(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 106.2 116.80 139.655 124.25 145.55 231.4   100\n  250 101.6 115.55 139.389 123.45 157.75 240.4   100\n  500 102.6 113.20 136.138 124.00 133.10 301.6   100\n 1000 103.2 116.55 137.804 128.15 154.25 227.3   100\n\n\n\n\nRolling maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_max(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_max(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_max(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_max(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 104.9 123.35 155.302 140.35 180.35 287.7   100\n  250 106.4 123.60 157.260 137.30 188.45 309.5   100\n  500 108.3 123.25 151.723 131.10 170.30 415.7   100\n 1000 107.7 122.50 160.164 143.10 190.95 341.3   100\n\n\n\n\nRolling index of minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmin(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmin(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmin(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmin(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 103.0 133.35 164.849 158.10 189.80 375.4   100\n  250 104.4 132.20 160.987 160.90 181.05 306.5   100\n  500 104.3 125.25 159.586 151.00 180.95 348.4   100\n 1000  94.6 131.20 158.884 156.75 180.40 365.6   100\n\n\n\n\nRolling index of maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmax(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmax(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmax(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmax(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 108.1 128.95 158.456 149.75 180.05 310.0   100\n  250 102.0 130.25 154.937 151.95 170.40 278.5   100\n  500 106.6 121.95 149.305 141.75 174.35 243.6   100\n 1000 102.9 119.20 144.368 138.60 159.90 244.5   100\n\n\n\n\nRolling medians\n\n# \"'online' is only supported for equal 'weights'\"\nresult &lt;- microbenchmark(\"125\" = roll_median(x, width = 125, min_obs = 1),\n                         \"250\" = roll_median(x, width = 250, min_obs = 1),\n                         \"500\" = roll_median(x, width = 500, min_obs = 1),\n                         \"1000\" = roll_median(x, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 772.5 807.95 833.559 831.10 845.40 1283.9   100\n  250 749.9 803.20 817.830 817.35 833.55  871.3   100\n  500 719.1 746.15 864.045 766.05 784.65 5572.8   100\n 1000 544.9 567.45 615.572 584.50 603.45 2154.4   100\n\n\n\n\nRolling quantiles\n\n# \"'online' is only supported for equal 'weights'\"\nresult &lt;- microbenchmark(\"125\" = roll_quantile(x, width = 125, min_obs = 1),\n                         \"250\" = roll_quantile(x, width = 250, min_obs = 1),\n                         \"500\" = roll_quantile(x, width = 500, min_obs = 1),\n                         \"1000\" = roll_quantile(x, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 879.4 896.85 924.384 915.75 944.15 1217.5   100\n  250 863.5 880.90 912.054 907.50 930.65 1039.5   100\n  500 799.9 817.65 847.926 844.05 865.90  993.8   100\n 1000 614.1 628.35 657.674 657.60 682.85  718.3   100\n\n\n\n\nRolling variances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{x}_{old}-\\text{mean}_{prev_x})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_var(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_var(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_var(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_var(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 118.9 138.75 147.525 146.55 153.25 239.9   100\n  250 114.7 136.15 152.035 145.30 157.50 310.6   100\n  500 113.0 135.45 145.732 142.10 149.70 396.2   100\n 1000 106.5 123.95 139.719 130.85 137.90 641.3   100\n\n\n\n\nRolling standard deviations\n\nresult &lt;- microbenchmark(\"125\" = roll_sd(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sd(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sd(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sd(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median    uq   max neval\n  125 121.3 142.10 152.207 147.20 156.0 282.4   100\n  250 119.7 139.50 153.363 145.30 156.8 395.7   100\n  500 128.2 136.00 148.320 141.70 149.6 270.2   100\n 1000 109.7 130.95 140.115 136.65 143.7 287.5   100\n\n\n\n\nRolling scaling and centering\n\nresult &lt;- microbenchmark(\"125\" = roll_scale(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_scale(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_scale(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_scale(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 147.0 151.90 162.812 156.35 162.00  330.4   100\n  250 142.5 152.75 208.041 159.55 171.80 3517.7   100\n  500 140.3 149.00 164.837 156.00 165.75  321.4   100\n 1000 135.9 140.60 151.609 146.15 151.80  267.4   100\n\n\n\n\nRolling covariances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{y}_{old}-\\text{mean}_{prev_y})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_cov(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cov(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cov(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cov(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq     mean median      uq    max neval\n  125 724.4 906.60  968.837 983.15 1018.15 1164.9   100\n  250 724.5 881.15 1045.220 942.70  990.00 6748.3   100\n  500 677.5 820.30  960.156 867.15  929.85 6220.2   100\n 1000 613.7 739.35  828.754 770.95  834.35 5979.7   100\n\n\n\n\nRolling correlations\n\nresult &lt;- microbenchmark(\"125\" = roll_cor(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cor(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cor(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cor(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min      lq     mean  median      uq    max neval\n  125 862.8 1014.30 1093.189 1096.25 1165.60 1298.5   100\n  250 841.3  968.30 1121.754 1063.50 1115.20 4588.4   100\n  500 788.5  906.80 1027.385  986.30 1083.00 4503.5   100\n 1000 688.6  801.25  915.180  846.25  896.05 4321.1   100\n\n\n\n\nRolling crossproducts\n\nresult &lt;- microbenchmark(\"125\" = roll_crossprod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_crossprod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_crossprod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_crossprod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min    lq    mean median     uq    max neval\n  125 595.0 651.5 713.679 710.20 767.80  875.7   100\n  250 592.1 637.3 727.521 690.80 748.55 4091.1   100\n  500 574.7 624.7 759.912 715.65 739.85 4138.6   100\n 1000 529.9 564.7 684.105 589.80 674.30 4107.4   100\n\n\n\n\nRolling linear models\n\\[\n\\begin{aligned}\n&\\text{coef}=\\text{cov}_{xx}^{-1}\\times\\text{cov}_{xy}\\\\\n&\\text{intercept}=\\text{mean}_{y}-\\text{coef}\\times\\text{mean}_{x}\\\\\n&\\text{rsq}=\\frac{\\text{coef}^{T}\\times\\text{cov}_{xx}\\times\\text{coef}}{\\text{var}_{y}}\\\\\n&\\text{var}_{resid}=\\frac{(1-\\text{rsq})(\\text{var}_{y})(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})}{\\text{n}_{rows}-\\text{n}_{cols}}\\\\\n&\\text{xx}=\\text{cov}_{xx}\\times(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})\\\\\n&\\text{se}_{coef}=\\sqrt{\\text{var}_{resid}\\times\\text{diag}(\\text{xx}^{-1})}\\\\\n&\\text{se}_{intercept}=\\sqrt{\\text{var}_{resid}\\left(1/\\text{sum}_{w}+\\text{mean}_{x}^{T}\\text{xx}^{-1}\\text{mean}_{x}\\right)}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_lm(x, y, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_lm(x, y, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_lm(x, y, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_lm(x, y, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq    max neval\n  125 2190.5 2401.40 2549.411 2498.90 2593.55 3943.7   100\n  250 2193.5 2334.05 2565.596 2493.55 2627.20 6566.8   100\n  500 2111.9 2338.05 2567.908 2428.40 2577.60 6740.5   100\n 1000 2037.4 2267.00 2533.283 2398.95 2546.65 6696.1   100\n\n\n\n\nReferences\n\nWeights: https://stackoverflow.com/a/9933794\nIndex: https://stackoverflow.com/a/11316626\nIndex: https://stackoverflow.com/a/34363187\nIndex: https://stackoverflow.com/a/243342\nQuantile (comparator): https://stackoverflow.com/a/51992954\nQuantile (comparator): https://stackoverflow.com/a/25921772\nQuantile (comparator): https://stackoverflow.com/a/40416506\nMedian: https://stackoverflow.com/a/5970314\nMedian: https://stackoverflow.com/a/5971248\nMedian: https://gist.github.com/ashelly/5665911\nStandard errors: https://stats.stackexchange.com/a/64217"
  },
  {
    "objectID": "posts/crowds-py/crowds-py.html",
    "href": "posts/crowds-py/crowds-py.html",
    "title": "Crowds",
    "section": "",
    "text": "import requests\nfrom lxml import html\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\n\n\nfactors_r = [\"SP500\"] # \"SP500\" does not contain dividends\nfactors_d = [\"SOFR\"]\nfactors = factors_r + factors_d\nwidth = 20 * 3\nscale = {\"periods\": 252, \"overlap\": 5}\n\n\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\n\n\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\n\n\nimport os\nimport cvxpy as cp\n\n\ndef get_nth(x, n, offset = 0):\n    \n    result = x[offset::n]\n    \n    return result\n\n\ndef get_text(x, n = 0):\n    \n    result_ls = []\n    \n    for i in x:\n      \n        if (len(i) == 0):\n            result_ls.append(i.text_content()) # types\n        else:\n            result_ls.append(i[n].text_content()) # names and tickers\n    \n    return result_ls\n\n\ndef get_mstar():\n    \n    i = 0\n    status = True\n    names_ls = []\n    tickers_ls = []\n    types_ls = []\n\n    while status:\n\n        i += 1\n\n        url = \"https://www.morningstar.com/asset-allocation-funds?page=\" + str(i)\n        response = requests.get(url)\n        tree = html.fromstring(response.content)\n\n        table = tree.xpath(\"//div[@class='topic__table-container']\")\n\n        if (len(table) == 0):\n            status = False\n        else:\n\n            names_tickers = tree.xpath(\"//a[@class='mdc-link mds-link mds-link--data-table mdc-link--no-visited']\")\n            types = tree.xpath(\"//span[@class='mdc-data-point mdc-data-point--string mdc-string']\")\n            \n        names_ls.extend(get_text(get_nth(names_tickers, 2)))\n        tickers_ls.extend(get_text(get_nth(names_tickers, 2, 1)))\n        types_ls.extend(get_text(get_nth(types, 5, 2)))\n\n    result = pd.DataFrame({\n      \"name\": names_ls,\n      \"ticker\": tickers_ls,\n      \"type\": types_ls\n    })\n    \n    return result\n\n\nmstar_df = get_mstar()\n\n\ntickers = mstar_df.loc[mstar_df[\"type\"] == \"Tactical Allocation\", \"ticker\"].tolist()\nprices_df = pdr.get_data_tiingo(tickers, start = \"1900-01-01\", api_key = os.getenv(\"TIINGO_API_KEY\"))\nprices_df = prices_df.pivot_table(index = \"date\", columns = \"symbol\", values = \"adjClose\") \\\n    .tz_localize(None)\nprices_df.sort_index(axis = 0, inplace = True)\ntickers = prices_df.columns\n\n\nreturns_cols = [(\"returns\", i) for i in tickers]\noverlap_cols = [(\"overlap\", i) for i in tickers]\nreturns_df[returns_cols] = np.log(prices_df).diff()\nreturns_df[overlap_cols] = returns_df[returns_cols].rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df.sort_index(axis = 1, inplace = True)\n\n\noverlap_df = returns_df.dropna()[\"overlap\"]\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors]\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers]\n\n\ndef pnl(x):\n    return np.nanprod(1 + x) - 1\n\n\nOptimization\n\ndef min_rss_optim(x, y):\n    \n    w = cp.Variable(x.shape[1])\n    \n    objective = cp.Minimize(cp.sum_squares(y - x @ w))\n    \n    constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= 1]\n    \n    problem = cp.Problem(objective, constraints)\n    problem.solve()\n    \n    return w.value\n\n\nn_rows = overlap_df.shape[0]\nresult_ls = []\n\nfor i in range(width - 1, n_rows):\n  \n  idx = range(max(i - width + 1, 0), i + 1)\n  x_subset = overlap_x_df.iloc[idx]\n  y_subset = overlap_y_df.iloc[idx]\n  params_ls = []\n  \n  for j in tickers:\n  \n    params = min_rss_optim(x_subset.values, y_subset.loc[:, j].values)\n    params_ls.append(params)\n  \n  result_ls.append(np.mean(params_ls, axis = 0))\n\n\nposition_df = pd.DataFrame(result_ls, index = overlap_df.index[(width - 1):],\n                           columns = factors)\nposition_df.tail()\n\n\n\n\n\n\n\n\nSP500\nSOFR\n\n\nDATE\n\n\n\n\n\n\n2024-02-06\n0.648071\n0.351929\n\n\n2024-02-07\n0.650647\n0.349353\n\n\n2024-02-08\n0.653656\n0.346344\n\n\n2024-02-09\n0.659915\n0.340085\n\n\n2024-02-12\n0.671116\n0.328884"
  },
  {
    "objectID": "posts/risk-r/index.html#partial-least-squares",
    "href": "posts/risk-r/index.html#partial-least-squares",
    "title": "Risk",
    "section": "Partial least squares",
    "text": "Partial least squares"
  },
  {
    "objectID": "posts/blpapi/index.html",
    "href": "posts/blpapi/index.html",
    "title": "Securities",
    "section": "",
    "text": "# !pip install --index-url=https://blpapi.bloomberg.com/repository/releases/python/simple/ blpapi\nimport blpapi\nimport pandas as pd\n\n\nhttps://www.bloomberg.com/professional/support/api-library/\nPage 80: https://bloomberg.github.io/blpapi-docs/\n\n\nData point\n\ndef bdp(securities, fields, host = \"localhost\", port = 8194):\n  \n    # Initialize Bloomberg API session\n    options = blpapi.SessionOptions()\n    options.setServerHost(host)\n    options.setServerPort(port)\n    session = blpapi.Session(options)\n    \n    session.start()\n    session.openService(\"//blp/refdata\")\n    \n    service = session.getService(\"//blp/refdata\")\n    request = service.createRequest(\"ReferenceDataRequest\")\n\n    # Add securities and fields to the request\n    for security in securities:\n        request.append(\"securities\", security)\n    \n    for field in fields:\n        request.append(\"fields\", field)\n\n    # Send request\n    session.sendRequest(request)\n    \n    data_dict = dict.fromkeys([\"security\"] + fields)\n\n    while True:\n      \n        event = session.nextEvent()\n\n        if event.eventType() in [blpapi.Event.RESPONSE, blpapi.Event.PARTIAL_RESPONSE]:\n          \n            for msg in event:\n              \n                data = msg.getElement(\"securityData\")\n                data_dict[\"security\"] = [x.getElementAsString(\"security\") for x in data.values()]\n                data_ls = [x.getElement(\"fieldData\") for x in data.values()]\n\n                for field in fields:\n                    try:\n                        data_dict[field] = [x.getElement(field).getValue() for x in data_ls]\n                    except:\n                        data_dict[field] = [None] * len(data_ls)  # Handle missing values\n\n        if event.eventType() == blpapi.Event.RESPONSE:\n            break\n\n    return pd.DataFrame(data_dict)\n\n\nsecurities = [\"IBM US Equity\", \"GOOG US Equity\", \"MSFT US Equity\", \"BA US Equity\"]\nfields = [\"MARKET_SECTOR_DES\", \"GICS_SECTOR_NAME\", \"ID_CUSIP\", \"PX_LAST\"]\n\n\nbdp_df = bdp(securities, fields)\nprint(bdp_df)\n\n\n\nData point\n\ndef bdp(securities, fields, host = \"localhost\", port = 8194):\n  \n    # Start Bloomberg session\n    options = blpapi.SessionOptions()\n    options.setServerHost(host)\n    options.setServerPort(port)\n    session = blpapi.Session(options)\n    \n    session.start()\n    session.openService(\"//blp/refdata\")\n    \n    service = session.getService(\"//blp/refdata\")\n    request = service.createRequest(\"ReferenceDataRequest\")\n\n    # Add securities and fields to the request\n    for security in securities:\n        request.append(\"securities\", security)\n    for field in fields:\n        request.append(\"fields\", field)\n\n    # Send request\n    session.sendRequest(request)\n\n    # Initialize data storage\n    data_dict = {\"security\": []}\n    for field in fields:\n        data_dict[field] = []\n\n    while True:\n      \n        event = session.nextEvent()\n        \n        if event.eventType() in {blpapi.Event.RESPONSE, blpapi.Event.PARTIAL_RESPONSE}:\n          \n            for msg in event:\n              \n                for security_data in msg.getElement(\"securityData\").values():\n                  \n                    sec_name = security_data.getElementAsString(\"security\")\n                    data_dict[\"security\"].append(sec_name)\n\n                    field_data = security_data.getElement(\"fieldData\")\n                    \n                    for field in fields:\n                        try:\n                            data_dict[field].append(field_data.getElement(field).getValue())\n                        except:\n                            data_dict[field].append(None)  # Handle missing data\n\n        if event.eventType() == blpapi.Event.RESPONSE:\n            break\n\n    # Stop session\n    session.stop()\n\n    return pd.DataFrame(data_dict)\n\n\nbdp_df = bdp(securities, fields)\nprint(bdp_df)\n\n\n\nData history\n\ndef bdh(securities, fields, start_date, end_date, host = \"localhost\", port = 8194):\n  \n    # Initialize Bloomberg session\n    options = blpapi.SessionOptions()\n    options.setServerHost(host)\n    options.setServerPort(port)\n    session = blpapi.Session(options)\n\n    session.start()\n    session.openService(\"//blp/refdata\")\n\n    service = session.getService(\"//blp/refdata\")\n    request = service.createRequest(\"HistoricalDataRequest\")\n\n    # Add securities and fields to the request\n    for security in securities:\n        request.append(\"securities\", security)\n    for field in fields:\n        request.append(\"fields\", field)\n\n    # Set request parameters\n    request.set(\"startDate\", start_date)\n    request.set(\"endDate\", end_date)\n\n    # Send request\n    session.sendRequest(request)\n\n    # Initialize data storage\n    data_dict = dict.fromkeys(securities, dict.fromkeys([\"date\"] + fields))\n\n    while True:\n      \n        event = session.nextEvent()\n        \n        if event.eventType() in {blpapi.Event.RESPONSE, blpapi.Event.PARTIAL_RESPONSE}:\n          \n            for msg in event:\n              \n                data = msg.getElement(\"securityData\")\n                security = security_data.getElementAsString(\"security\").getValue()\n                security_dict = {\"date\": [x.getElementAsDatetime(\"date\") for x in data.getElement(\"fieldData\")]}\n\n                for field in fields:\n                    try:\n                        security_dict[field] = [x.getElement(field).getValue() for x in data.getElement(\"fieldData\")]\n                    except:\n                        passs\n\n        if event.eventType() == blpapi.Event.RESPONSE:\n            break\n    \n    result = {key: pd.DataFrame(value).set_index(\"date\") for key, value in data_dict.items()}\n    \n    return result\n\n\nfields = [\"PX_LAST\", \"PX_BID\", \"PX_ASK\"]\nstart_date = \"20231201\"\nend_date = \"20231205\"\n\n\nbdh_df = bdh(securities, fields, start_date, end_date)\nprint(bdh_df[0])\n\n\n\nData history\n\ndef bdh(securities, fields, start_date, end_date, host = \"localhost\", port = 8194):\n  \n    # Initialize Bloomberg session\n    options = blpapi.SessionOptions()\n    options.setServerHost(host)\n    options.setServerPort(port)\n    session = blpapi.Session(options)\n\n    session.start()\n    session.openService(\"//blp/refdata\")\n\n    service = session.getService(\"//blp/refdata\")\n    request = service.createRequest(\"HistoricalDataRequest\")\n\n    # Add securities and fields to the request\n    for security in securities:\n        request.append(\"securities\", security)\n    for field in fields:\n        request.append(\"fields\", field)\n\n    # Set request parameters\n    request.set(\"startDate\", start_date)\n    request.set(\"endDate\", end_date)\n\n    # Send request\n    session.sendRequest(request)\n\n    # Initialize data storage\n    data_dict = {security: {\"date\": [], **{field: [] for field in fields}} for security in securities}\n\n    while True:\n      \n        event = session.nextEvent()\n        \n        if event.eventType() in {blpapi.Event.RESPONSE, blpapi.Event.PARTIAL_RESPONSE}:\n          \n            for msg in event:\n              \n                data = msg.getElement(\"securityData\")\n                \n                for security_data in data.values():\n                  \n                    security = security_data.getElementAsString(\"security\")\n                    field_data = security_data.getElement(\"fieldData\")\n\n                    data_dict[security][\"date\"] = [x.getElementAsDatetime(\"date\") for x in field_data.values()]\n\n                    for field in fields:\n                        try:\n                            data_dict[security][field] = [x.getElement(field).getValue() for x in field_data.values()]\n                        except:\n                            data_dict[security][field] = [None] * len(data_dict[security][\"date\"])  # Handle missing values\n\n        if event.eventType() == blpapi.Event.RESPONSE:\n            break\n\n    session.stop()  # Properly close the Bloomberg session\n\n    # Convert results to Pandas DataFrame\n    return {sec: pd.DataFrame(data).set_index(\"date\") for sec, data in data_dict.items()}\n\n\nbdh_df = bdh(securities, fields, start_date, end_date)\nprint(bdh_df[0])"
  },
  {
    "objectID": "posts/cloud/index.html",
    "href": "posts/cloud/index.html",
    "title": "Cloud",
    "section": "",
    "text": "EC2 &gt; Network & Security &gt; Key Pairs &gt; Create Key Pair &gt; *.pem\nPuTTYgen &gt; Load an existing private key file &gt; Save private key &gt; *.ppk\nHost Name: ec2-user\\@ec2-1-23-456-789.us-east-2.compute.amazonaws.com\nConnection &gt; SSH &gt; Auth & WinSCP &gt; Auth &gt; Private key file for authentication\nsudo yum update\n\n\n\nInstall Amazon Linux 2\nwget https://repo.anaconda.com/archive/Anaconda3-2023.03-Linux-x86_64.sh\n\nbash Anaconda3-2023.03-Linux-x86_64.sh\nThen close and re-open current shell\n\nhttps://pypi.org/project/ipython/#history\nhttps://stackoverflow.com/a/76521018\nhttps://stackoverflow.com/a/77117477\n\npip install 'ipython==7.34.0' --force-reinstall\nipython\nfrom IPython.lib import passwd\npasswd(\"12345\")\n# sha1:asdfasdf:asfasdfasdf\nexit()\njupyter notebook --generate-config \nvi ~/.jupyter/jupyter_notebook_config.py\n\n# [i]\nc = get_config()  # Get the config object.\n# c.NotebookApp.certfile = u'/home/ubuntu/ssl/cert.pem' # path to the certificate we generated\n# c.NotebookApp.keyfile = u'/home/ubuntu/ssl/cert.key' # path to the certificate key we generated\nc.NotebookApp.ip = '0.0.0.0' # serve notebooks locally\nc.NotebookApp.open_browser = False # do not open a browser window by default when using notebooks.\nc.NotebookApp.password = 'sha1:asdfasdf:asfasdfasdf'\n# [ESC][:wq][ENTER]\n# nohup jupyter notebook\nnohup jupyter lab\nThen close and re-open current shell\njupyter nbconvert --to python &lt;notebook&gt;.ipynb\nsudo service crond start\ncrontab -e\n# [i]\n# https://crontab.guru/\n# * * * * * /home/ec2-user/anaconda3/bin/python /home/ec2-user/&lt;notebook&gt;.py\n# [ESC][:wq][ENTER]\n\nhttp://1.23.456.789:8888\n\n\n\n\n\nhttps://aws.amazon.com/blogs/big-data/running-r-on-aws/\n\nInstall R\nsudo su\n# yum install -y R # Amazon Linux\n# amazon-linux-extras list\nsudo amazon-linux-extras install R3.4 # Amazon Linux 2\nInstall RStudio Server\n\nhttps://posit.co/download/rstudio-server/\n\nwget https://download2.rstudio.org/server/centos6/x86_64/rstudio-server-rhel-1.2.5019-x86_64.rpm\nyum install rstudio-server-rhel-1.2.5019-x86_64.rpm\nrm rstudio-server-rhel-1.2.5019-x86_64.rpm\nInstall Shiny Server\n\nhttps://posit.co/download/shiny-server/\n\nR -e \"install.packages('shiny', repos = 'http://cran.rstudio.com/')\"\nwget https://download3.rstudio.org/centos6.3/x86_64/shiny-server-1.5.12.933-x86_64.rpm\nyum install --nogpgcheck shiny-server-1.5.12.933-x86_64.rpm\nrm shiny-server-1.5.12.933-x86_64.rpm\nAdd user(s)\nuseradd -m jjf234\nsudo passwd jjf234 # prompt to enter password\nNeed to create /ShinyApps under /ec2-user and then subfolders with code, e.g. /home/ec2-user/ShinyApps/hello\n# sudo /opt/shiny-server/bin/deploy-example user-dirs\n# mkdir ~/ShinyApps\n# sudo cp -R /opt/shiny-server/samples/sample-apps/hello ~/ShinyApps\nInstall devtools\nsudo yum install libcurl-devel\nsudo yum install openssl-devel\nsudo yum install libxml2-devel\nR -e \"install.packages('devtools', repos = 'http://cran.rstudio.com/')\"\nInstall RcppParallel\nR -e \"install.packages('RcppParallel', repos = 'http://cran.rstudio.com/')\"\nInstall RcppArmadillo\nR -e \"install.packages('RcppArmadillo', repos = 'http://cran.rstudio.com/')\"\nInstall other packages\nR -e \"install.packages(c('data.table', 'xts', 'testthat', 'microbenchmark'),\nrepos = 'http://cran.rstudio.com/')\"\nInstall roll\nR -e \"devtools::install_github('jjf234/roll')\"\n\nhttp://1.23.456.789:8787/\nhttp://1.23.456.789:3838/ec2-user/hello/"
  },
  {
    "objectID": "posts/cloud/index.html#secure-shell",
    "href": "posts/cloud/index.html#secure-shell",
    "title": "Cloud",
    "section": "",
    "text": "EC2 &gt; Network & Security &gt; Key Pairs &gt; Create Key Pair &gt; *.pem\nPuTTYgen &gt; Load an existing private key file &gt; Save private key &gt; *.ppk\nHost Name: ec2-user\\@ec2-1-23-456-789.us-east-2.compute.amazonaws.com\nConnection &gt; SSH &gt; Auth & WinSCP &gt; Auth &gt; Private key file for authentication\nsudo yum update"
  },
  {
    "objectID": "posts/cloud/index.html#jupyter-server",
    "href": "posts/cloud/index.html#jupyter-server",
    "title": "Cloud",
    "section": "",
    "text": "Install Amazon Linux 2\nwget https://repo.anaconda.com/archive/Anaconda3-2023.03-Linux-x86_64.sh\n\nbash Anaconda3-2023.03-Linux-x86_64.sh\nThen close and re-open current shell\n\nhttps://pypi.org/project/ipython/#history\nhttps://stackoverflow.com/a/76521018\nhttps://stackoverflow.com/a/77117477\n\npip install 'ipython==7.34.0' --force-reinstall\nipython\nfrom IPython.lib import passwd\npasswd(\"12345\")\n# sha1:asdfasdf:asfasdfasdf\nexit()\njupyter notebook --generate-config \nvi ~/.jupyter/jupyter_notebook_config.py\n\n# [i]\nc = get_config()  # Get the config object.\n# c.NotebookApp.certfile = u'/home/ubuntu/ssl/cert.pem' # path to the certificate we generated\n# c.NotebookApp.keyfile = u'/home/ubuntu/ssl/cert.key' # path to the certificate key we generated\nc.NotebookApp.ip = '0.0.0.0' # serve notebooks locally\nc.NotebookApp.open_browser = False # do not open a browser window by default when using notebooks.\nc.NotebookApp.password = 'sha1:asdfasdf:asfasdfasdf'\n# [ESC][:wq][ENTER]\n# nohup jupyter notebook\nnohup jupyter lab\nThen close and re-open current shell\njupyter nbconvert --to python &lt;notebook&gt;.ipynb\nsudo service crond start\ncrontab -e\n# [i]\n# https://crontab.guru/\n# * * * * * /home/ec2-user/anaconda3/bin/python /home/ec2-user/&lt;notebook&gt;.py\n# [ESC][:wq][ENTER]\n\nhttp://1.23.456.789:8888"
  },
  {
    "objectID": "posts/cloud/index.html#rstudio-server",
    "href": "posts/cloud/index.html#rstudio-server",
    "title": "Cloud",
    "section": "",
    "text": "https://aws.amazon.com/blogs/big-data/running-r-on-aws/\n\nInstall R\nsudo su\n# yum install -y R # Amazon Linux\n# amazon-linux-extras list\nsudo amazon-linux-extras install R3.4 # Amazon Linux 2\nInstall RStudio Server\n\nhttps://posit.co/download/rstudio-server/\n\nwget https://download2.rstudio.org/server/centos6/x86_64/rstudio-server-rhel-1.2.5019-x86_64.rpm\nyum install rstudio-server-rhel-1.2.5019-x86_64.rpm\nrm rstudio-server-rhel-1.2.5019-x86_64.rpm\nInstall Shiny Server\n\nhttps://posit.co/download/shiny-server/\n\nR -e \"install.packages('shiny', repos = 'http://cran.rstudio.com/')\"\nwget https://download3.rstudio.org/centos6.3/x86_64/shiny-server-1.5.12.933-x86_64.rpm\nyum install --nogpgcheck shiny-server-1.5.12.933-x86_64.rpm\nrm shiny-server-1.5.12.933-x86_64.rpm\nAdd user(s)\nuseradd -m jjf234\nsudo passwd jjf234 # prompt to enter password\nNeed to create /ShinyApps under /ec2-user and then subfolders with code, e.g. /home/ec2-user/ShinyApps/hello\n# sudo /opt/shiny-server/bin/deploy-example user-dirs\n# mkdir ~/ShinyApps\n# sudo cp -R /opt/shiny-server/samples/sample-apps/hello ~/ShinyApps\nInstall devtools\nsudo yum install libcurl-devel\nsudo yum install openssl-devel\nsudo yum install libxml2-devel\nR -e \"install.packages('devtools', repos = 'http://cran.rstudio.com/')\"\nInstall RcppParallel\nR -e \"install.packages('RcppParallel', repos = 'http://cran.rstudio.com/')\"\nInstall RcppArmadillo\nR -e \"install.packages('RcppArmadillo', repos = 'http://cran.rstudio.com/')\"\nInstall other packages\nR -e \"install.packages(c('data.table', 'xts', 'testthat', 'microbenchmark'),\nrepos = 'http://cran.rstudio.com/')\"\nInstall roll\nR -e \"devtools::install_github('jjf234/roll')\"\n\nhttp://1.23.456.789:8787/\nhttp://1.23.456.789:3838/ec2-user/hello/"
  },
  {
    "objectID": "posts/dev-r/index.html",
    "href": "posts/dev-r/index.html",
    "title": "Software",
    "section": "",
    "text": ".libPaths()\n# &gt; C:\\Users\\username\\AppData\\Local\n# &gt; C:\\Program Files\\R\\R-1.2.3\\library\n\nSet environment variable: PATH=c:\\rtools&lt;123&gt;\\usr\\bin"
  },
  {
    "objectID": "posts/dev-r/index.html#quarto",
    "href": "posts/dev-r/index.html#quarto",
    "title": "Software",
    "section": "Quarto",
    "text": "Quarto\npip install pyyaml jupyter"
  },
  {
    "objectID": "posts/dev-r/index.html#issues",
    "href": "posts/dev-r/index.html#issues",
    "title": "Software",
    "section": "Issues",
    "text": "Issues\n\nhttps://github.com/osqp/osqp/issues/385\n\nUse python 3.10 for qdldl\n\nhttps://github.com/pydata/pandas-datareader/issues/965\n\npip install git+https://github.com/pydata/pandas-datareader.git"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-mean",
    "href": "posts/optim-r/index.html#maximize-mean",
    "title": "Optimization",
    "section": "Maximize mean",
    "text": "Maximize mean\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmax_mean_optim &lt;- function(mu, sigma, target) {\n  \n  params &lt;- CVXR::Variable(length(mu))\n  \n  obj &lt;- CVXR::Maximize(t(params) %*% mu)\n  \n  cons &lt;- list(sum(params) == 1, params &gt;= 0,\n               CVXR::quad_form(params, sigma) &lt;= target ^ 2)\n  \n  prob &lt;- CVXR::Problem(obj, cons)\n  \n  result &lt;- CVXR::solve(prob)$getValue(params)\n  \n  return(result)\n\n}\n\n\ntarget &lt;- 0.06\n\n\nparams1 &lt;- t(max_mean_optim(mu, sigma, target))\nparams1\n\n          [,1]      [,2]      [,3]         [,4]\n[1,] 0.3464269 0.1094585 0.5441146 3.010859e-08\n\n\n\nparams1 %*% mu\n\n           [,1]\n[1,] 0.03655833\n\n\n\nsqrt(params1 %*% sigma %*% t(params1))\n\n     [,1]\n[1,] 0.06\n\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\") # roll (&gt;= 1.1.7)\n# library(rolloptim)\n# \n# mu &lt;- roll_mean(returns_x_xts, 5)\n# sigma &lt;- roll_cov(returns_x_xts, width = 5)\n# \n# xx &lt;- roll_crossprod(returns_x_xts, returns_x_xts, 5)\n# xy &lt;- roll_crossprod(returns_x_xts, returns_y_xts, 5)\n# \n# roll_max_mean(mu)"
  },
  {
    "objectID": "posts/alpha-r/index.html",
    "href": "posts/alpha-r/index.html",
    "title": "Alpha Distribution Analysis",
    "section": "",
    "text": "Question\nCould an optimally combined portfolio of hedge funds have won Warren Buffet’s 10-year bet against the S&P 500?\n\nhttps://longbets.org/362/\n\n\nload(\"returns.rda\")\n\n\n\nOptimization\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\")\n\n\n# library(CVXR)\n\n\n# min_rss_optim &lt;- function(x, y) {\n# \n#   n_rows &lt;- nrow(x)\n#   x &lt;- as.matrix(x)\n#   y &lt;- as.numeric(y)\n#   params &lt;- Variable(ncol(x))\n#   \n#   obj &lt;- Minimize(sum_squares(y - x %*% params))\n#   cons &lt;- list(sum(params) == 1, params &gt;= 0, params &lt;= 1)\n#   prob &lt;- Problem(obj, cons)\n#   \n#   result &lt;- solve(prob)$getValue(params)\n#   \n#   return(result)\n# \n# }\n\n\n\nBootstrapping\n\n# library(NNS)\n\n\nboot &lt;- function(n, p = 1) {\n\n  idx &lt;- sample(1:n, 1)\n  idx_ls &lt;- list(idx)\n  \n  for (i in 2:n) {\n    if (runif(1) &lt; 1 / p) {\n      idx &lt;- sample(1:n, 1)\n    } else {\n\n      idx &lt;- idx + 1\n\n      if (idx &gt; n) {\n        idx &lt;- 1\n      }\n\n    }\n    \n    idx_ls &lt;- append(idx_ls, list(idx))\n\n  }\n\n  result &lt;- do.call(rbind, idx_ls)\n  \n  return(result)\n\n}\n\n\nset.seed(5640)\nn_rows &lt;- nrow(returns)\nn_cols &lt;- ncol(returns)\nn_boot &lt;- 1000\noptim_weights_ls &lt;- list()\noptim_alpha_ls &lt;- list()\noptim_te_ls &lt;- list()\n# returns_boot &lt;- array(NA, dim = c(n_rows, n_cols, n_boot)) # time, asset, and bootstrap\n\n\n# for (j in 1:n_cols) {\n# \n#   col_boot &lt;- NNS.meboot(as.vector(returns[ , j]), reps = n_boot,\n#                          rho = 1, type = \"pearson\")[\"replicates\", ]$replicates\n# \n#   returns_boot[ , j, ] &lt;- col_boot\n# \n# }\n\n\n# colnames(returns_boot) &lt;- colnames(returns)\n\n\nfor (z in 1:n_boot) {\n  \n  # idx &lt;- sample(1:n_rows, n_rows, replace = TRUE)\n  idx &lt;- boot(n_rows, p = 2)\n  # boot_sample &lt;- returns_boot[ , , z]\n  \n  x_boot &lt;- returns[idx, colnames(returns) != \"S&P Index Fund\"]\n  y_boot &lt;- returns[idx, \"S&P Index Fund\"]\n  # x_boot &lt;- boot_sample[ , colnames(boot_sample) != \"S&P Index Fund\"]\n  # y_boot &lt;- boot_sample[ , \"S&P Index Fund\"]\n  \n  # optim_weights &lt;- min_rss_optim(x_boot, y_boot)\n  \n  xx &lt;- roll::roll_crossprod(x_boot, x_boot, width = nrow(x_boot))\n  xy &lt;- roll::roll_crossprod(x_boot, y_boot, width = nrow(x_boot))\n  \n  optim_weights &lt;- rolloptim::roll_min_rss(xx, xy, total = 1, lower = 0, upper = 1)[nrow(x_boot), ]\n  \n  optim_returns &lt;- x_boot %*% optim_weights\n  optim_port &lt;- prod(1 + optim_returns) ^ (1 / n_rows) - 1\n  optim_bench &lt;- prod(1 + y_boot) ^ (1 / n_rows) - 1\n  optim_alpha &lt;- optim_port - optim_bench\n  optim_te &lt;- sd(optim_returns - y_boot)\n  \n  optim_weights_ls &lt;- append(optim_weights_ls, list(t(optim_weights)))\n  optim_alpha_ls &lt;- append(optim_alpha_ls, list(optim_alpha))\n  optim_te_ls &lt;- append(optim_te_ls, list(optim_te))\n\n}\n\n\noptim_weights &lt;- do.call(rbind, optim_weights_ls)\noptim_alpha &lt;- do.call(rbind, optim_alpha_ls)\noptim_te &lt;- do.call(rbind, optim_te_ls)\n\n\n\nAnswer\nAnalysis shows that 3% of the optimized portfolios produced alpha values greater than zero, which means that 97% of optimized combinations did not outperform the S&P 500."
  },
  {
    "objectID": "posts/crowds-r/index.html",
    "href": "posts/crowds-r/index.html",
    "title": "Crowds",
    "section": "",
    "text": "# factors_r &lt;- c(\"SP500\") # \"SP500\" does not contain dividends\n# factors_d &lt;- c(\"DTB3\")\n\n\nParse web\n\nfilters &lt;- list(\"eq\", list(\"categoryname\", \"Tactical Allocation\"))\nquery &lt;- yfscreen::create_query(filters)\npayload &lt;- yfscreen::create_payload(\"mutualfund\", query, 250)\ndata &lt;- yfscreen::get_data(payload)\n\n\nsorted_df &lt;- data[order(\n  -data[[\"netAssets.raw\"]],\n  data[[\"netExpenseRatio.raw\"]],\n  data[[\"firstTradeDateMilliseconds\"]],\n  data[[\"longName\"]],\n  data[[\"symbol\"]]\n), ]\ntickers &lt;- sorted_df[!duplicated(sorted_df[[\"netAssets.raw\"]]), \"symbol\"]\n\n\nallocations &lt;- c(\"IVV\", \"IDEV\", \"IUSB\", \"IEMG\", \"IJH\", \"IAGG\", \"IJR\")\ntickers &lt;- c(tickers, allocations)\n\n\n\nOptimization\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\")\n\n\n# library(CVXR)\n\n\n# min_rss_optim &lt;- function(x, y) {\n# \n#   params &lt;- Variable(ncol(x))\n# \n#   obj &lt;- Minimize(sum_squares(y - x %*% params))\n# \n#   cons &lt;- list(sum(params) == 1, params &gt;= 0, params &lt;= 1)\n# \n#   prob &lt;- Problem(obj, cons)\n# \n#   result &lt;- solve(prob)$getValue(params)\n# \n#   return(result)\n# \n# }\n\n\nperformance_xts &lt;- roll::roll_prod(1 + returns_xts, width, min_obs = 1) - 1\n\n\nn_rows &lt;- nrow(overlap_xts)\nresult_ls &lt;- list()\nindex_ls &lt;- list()\n\n# for (i in width:n_rows) {\nfor (i in n_rows) {\n    \n  idx &lt;- max(i - width + 1, 1):i\n  x_subset &lt;- zoo::coredata(overlap_x_xts[idx, ])\n  y_subset &lt;- zoo::coredata(overlap_y_xts[idx, ])\n  params_ls &lt;- list()\n  tickers_ls &lt;- list()\n  performance_ls &lt;- list()\n  \n  for (j in tickers[!tickers %in% allocations]) {\n    \n    idx &lt;- complete.cases(x_subset, y_subset[ , j])\n    x_complete &lt;- x_subset[idx, , drop = FALSE]\n    y_complete &lt;- y_subset[idx, j]\n    \n    if ((nrow(x_complete) &gt; 0) && (length(y_complete) &gt; 0)) {\n      \n      xx &lt;- roll::roll_crossprod(x_complete, x_complete, width = nrow(x_complete))\n      xy &lt;- roll::roll_crossprod(x_complete, y_complete, width = nrow(x_complete))\n      \n      # params &lt;- t(min_rss_optim(x_complete, y_complete))\n      params &lt;- rolloptim::roll_min_rss(xx, xy, total = 1, lower = 0, upper = 1)[nrow(x_complete), ]\n      params_ls &lt;- append(params_ls, list(params))\n      \n      tickers_ls &lt;- append(tickers_ls, list(j))\n      \n      performance_ls &lt;- append(performance_ls, list(performance_xts[i, j]))\n        \n    }\n      \n  }\n  \n  if (length(params_ls) &gt; 0) {\n      \n    result &lt;- do.call(rbind, params_ls)\n    rownames(result) &lt;- unlist(tickers_ls)\n    \n    result &lt;- cbind(result, performance = unlist(performance_ls))\n    \n    result_ls &lt;- append(result_ls, list(result))\n    index_ls &lt;- append(index_ls, list(zoo::index(overlap_xts)[i]))\n      \n  }\n  \n}\n\n\n# save(result_ls, file = \"result_ls.rda\")\n# save(index_ls, file = \"index_ls.rda\")\n\n\n\nPerformance\n\n# load(\"result_ls.rda\")\n# load(\"index_ls.rda\")\n\n\nquantile_cut &lt;- function(x) {\n\n  result &lt;- cut(\n    -x,\n    breaks = quantile(-x, probs = c(0, 0.25, 0.5, 0.75, 1),\n                      na.rm = TRUE),\n    labels = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"),\n    include.lowest = TRUE\n  )\n\n  return(result)\n\n}\n\n\nn_rows &lt;- length(result_ls)\nscore_ls &lt;- list()\n\nfor (i in 1:n_rows) {\n  \n  score_df &lt;- data.frame(result_ls[[i]])\n  colnames(score_df) &lt;- c(allocations, \"performance\")\n  \n  score_df[[\"date\"]] &lt;- index_ls[[i]]\n  score_df[[\"quantile\"]] &lt;- quantile_cut(score_df[[\"performance\"]])\n  \n  score_form &lt;- as.formula(paste0(\"cbind(\", paste(c(allocations, \"performance\"), collapse = \",\"), \") ~ date + quantile\"))\n  overall_form &lt;- as.formula(paste0(\"cbind(\", paste(c(allocations, \"performance\"), collapse = \",\"), \") ~ date\"))\n  \n  score &lt;- aggregate(score_form, data = score_df, FUN = function(x) mean(x, na.rm = TRUE))\n  overall &lt;- aggregate(overall_form, data = score_df, FUN = function(x) mean(x, na.rm = TRUE))\n  overall[[\"quantile\"]] &lt;- \"Overall\"\n  overall &lt;- overall[ , c(\"date\", \"quantile\", allocations, \"performance\")]\n  \n  score &lt;- rbind(score, overall)\n  \n  score_ls &lt;- append(score_ls, list(score))\n  \n}\n\n\nscore_df &lt;- do.call(rbind, score_ls)\nprint(score_df)\n\n        date quantile       IVV       IDEV       IUSB       IEMG         IJH\n1 2025-11-21       Q1 0.3440322 0.18802729 0.15740512 0.16990174 0.005644199\n2 2025-11-21       Q2 0.3140775 0.16199268 0.18826551 0.10380047 0.005734540\n3 2025-11-21       Q3 0.3876704 0.08915638 0.04651193 0.05760997 0.033547032\n4 2025-11-21       Q4 0.2353967 0.14270583 0.09664845 0.01899065 0.055491094\n5 2025-11-21  Overall 0.3205873 0.14599594 0.12264229 0.08859208 0.024863969\n       IAGG        IJR  performance\n1 0.1184955 0.01649395  0.046051238\n2 0.1788969 0.04723236  0.025532830\n3 0.3535519 0.03195237  0.014213997\n4 0.3311749 0.11959231 -0.006730357\n5 0.2439615 0.05335696  0.020091425\n\n\n\n# save(score_df, file = \"score_df.rda\") # see test-crowds-r\n# score_xts &lt;- xts(score_df[score_df[[\"quantile\"]] == \"Q1\", \"weight\"],\n#                  score_df[score_df[[\"quantile\"]] == \"Q1\", \"date\"])\n# plot(score_xts)"
  },
  {
    "objectID": "posts/crowds-py/index.html",
    "href": "posts/crowds-py/index.html",
    "title": "Crowds",
    "section": "",
    "text": "factors_r = [\"SP500\"] # \"SP500\" does not contain dividends\nfactors_d = [\"DTB3\"]\n\n\nParse web\n\nimport yfscreen as yfs\n\n\nfilters = [\"eq\", [\"categoryname\", \"Tactical Allocation\"]]\nquery = yfs.create_query(filters)\npayload = yfs.create_payload(\"mutualfund\", query, 250)\ndata = yfs.get_data(payload)\n\n\nsorted_df = data.sort_values(\n  by = [\n    \"netAssets.raw\",\n    \"netExpenseRatio.raw\",\n    \"firstTradeDateMilliseconds\",\n    \"longName\",\n    \"symbol\"\n  ],\n  ascending = [False, True, True, True, True],\n  kind = \"stable\"\n)\ntickers = sorted_df.loc[~sorted_df[\"netAssets.raw\"].duplicated(), \"symbol\"].tolist()\n\n\n# allocations = [\"AOK\", \"AOM\", \"AOR\", \"AOA\"]\n# tickers = tickers + allocations\n\n\n\nOptimization\n\nimport json\nimport cvxpy as cp\n\n\ndef min_rss_optim(x, y):\n    \n  w = cp.Variable(x.shape[1])\n    \n  objective = cp.Minimize(cp.sum_squares(y - x @ w))\n    \n  constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= 1]\n    \n  problem = cp.Problem(objective, constraints)\n  problem.solve()\n    \n  return w.value\n\n\ndef pnl(x):\n  return np.nanprod(1 + x) - 1\n\n\nperformance_df = returns_df.rolling(width, min_periods = 1).apply(pnl, raw = False)\n\n\nn_rows = overlap_df.shape[0]\nresult_ls = []\nindex_ls = []\n\n# for i in range(width - 1, n_rows):\nfor i in range(n_rows - 1, n_rows):\n  \n  idx = range(max(i - width + 1, 0), i + 1)\n  x_subset = overlap_x_df.iloc[idx]\n  y_subset = overlap_y_df.iloc[idx]\n  params_ls = []\n  tickers_ls = []\n  performance_ls = []\n  \n  # for j in [ticker for ticker in tickers if ticker not in allocations]:\n  for j in tickers:\n    \n    idx = ~x_subset.isna().any(axis = 1) & ~y_subset[j].isna()\n    x_complete = x_subset.loc[idx]\n    y_complete = y_subset.loc[idx, j]\n    \n    if (x_complete.shape[0] &gt; 0) and (y_complete.size &gt; 0):\n        \n      params = min_rss_optim(x_complete.values, y_complete.values)\n      params_ls.append(params)\n      \n      tickers_ls.append(j)\n      \n      performance_ls.append(performance_df[j].iloc[i])\n\n  if params_ls:\n    \n    result = pd.DataFrame(params_ls, index = tickers_ls)\n    result[\"performance\"] = performance_ls\n    \n    result_ls.append(result)\n    index_ls.append(overlap_x_df.index[i])\n\n\n# json.dump([x.to_dict() for x in result_ls], open(\"result_ls.json\", \"w\"))\n# json.dump([x.isoformat() for x in index_ls], open(\"index_ls.json\", \"w\"))\n\n\n\nPerformance\n\n# result_ls = [pd.DataFrame(x) for x in json.load(open(\"result_ls.json\", \"r\"))]\n# index_ls = [pd.Timestamp(x) for x in json.load(open(\"index_ls.json\", \"r\"))]\n\n\ndef quantile_cut(x):\n  \n  result = pd.qcut(\n    -x,\n    q = [0, 0.25, 0.5, 0.75, 1],\n    labels = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n  )\n  \n  return result\n\n\nn_rows = len(result_ls)\nscore_ls = []\n\nfor i in range(n_rows):\n  \n  score_df = pd.DataFrame(result_ls[i])\n  score_df.columns = factors + [\"performance\"]\n  \n  score_df[\"date\"] = index_ls[i]\n  score_df[\"quantile\"] = quantile_cut(score_df[\"performance\"])\n  \n  score = score_df.groupby([\"date\", \"quantile\"], observed = True).agg(\n    weight = (factors[0], \"mean\"),\n    performance = (\"performance\", \"mean\")\n  ).reset_index()\n  \n  overall = pd.DataFrame({\n    \"date\": [index_ls[i]],\n    \"quantile\": [\"Overall\"],\n    \"weight\": [score_df[factors[0]].mean()],\n    \"performance\": [score_df[\"performance\"].mean()]\n  })\n  \n  score = pd.concat([score, overall], ignore_index = True)\n  \n  score_ls.append(score)\n\n\nscore_df = pd.concat(score_ls, ignore_index = True)\nprint(score_df)\n\n        date quantile    weight  performance\n0 2025-11-21       Q1  0.760244     0.046051\n1 2025-11-21       Q2  0.649859     0.025533\n2 2025-11-21       Q3  0.602750     0.014214\n3 2025-11-21       Q4  0.544587    -0.006730\n4 2025-11-21  Overall  0.638979     0.020091\n\n\n\n# score_df.to_json(\"score_df.json\", date_format = \"iso\")"
  }
]