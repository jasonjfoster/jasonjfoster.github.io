[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jjf234.github.io",
    "section": "",
    "text": "Securities\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCloud\n\n\n\ncomputing\n\n\npython\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware\n\n\n\ndevelopment\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nalgorithms\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nalgorithms\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 7, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Software development",
    "section": "",
    "text": "System setup\n\nhttps://r-pkgs.org/setup.html\n\n# environment variable: PATH=c:\\rtools40\\usr\\bin\n\n\nDevelopment workflow\n\nhttps://r-pkgs.org/workflows101.html\n\n\nRcppArmadillo::RcppArmadillo.package.skeleton(name = \"anRpackage\",\n                                              path = \".\",\n                                              example_code = FALSE)\n\n\n\nObject documentation\n\nhttps://r-pkgs.org/man.html\n\n\nroxygen2::roxygenize()\n\n\n\nCompiled code\n\nhttps://r-pkgs.org/src.html\n\n\nRcpp::compileAttributes(verbose = TRUE)\ntools::package_native_routine_registration_skeleton(\".\", character_only = FALSE)\n\n\n\nGit and GitHub\n\nhttps://r-pkgs.org/git.html\n\nTell Git your name and email address:\ngit config --global user.name \"YOUR FULL NAME\"\ngit config --global user.email \"YOUR EMAIL ADDRESS\"\nIf needed, generate an SSH key: RStudio &gt; Tools &gt; Git/SVN &gt; Create RSA Key…\nThen give GitHub your SSH public key: GitHub &gt; SSH and GPG Keys &gt; New SSH key\ngit remote add origin git@github.com:hadley/r-pkgs.git\ngit push -u origin master\nAlso, if needed, change the URL of remote ‘origin’:\ngit remote set-url origin git@github.com:hadley/r-foo.git\n\n\nAutomated checking\n\nhttps://r-pkgs.org/r-cmd-check.html\n\n\nusethis::use_github_action(\"check-standard\")\nusethis::use_github_action(\"test-coverage\")"
  },
  {
    "objectID": "posts/dev/dev.html",
    "href": "posts/dev/dev.html",
    "title": "Software development",
    "section": "",
    "text": "System setup\n\nhttps://r-pkgs.org/setup.html\n\n# environment variable: PATH=c:\\rtools40\\usr\\bin\n\n\nDevelopment workflow\n\nhttps://r-pkgs.org/workflows101.html\n\n\nRcppArmadillo::RcppArmadillo.package.skeleton(name = \"anRpackage\",\n                                              path = \".\",\n                                              example_code = FALSE)\n\n\n\nObject documentation\n\nhttps://r-pkgs.org/man.html\n\n\nroxygen2::roxygenize()\n\n\n\nCompiled code\n\nhttps://r-pkgs.org/src.html\n\n\nRcpp::compileAttributes(verbose = TRUE)\ntools::package_native_routine_registration_skeleton(\".\", character_only = FALSE)\n\n\n\nGit and GitHub\n\nhttps://r-pkgs.org/git.html\n\nTell Git your name and email address:\ngit config --global user.name \"YOUR FULL NAME\"\ngit config --global user.email \"YOUR EMAIL ADDRESS\"\nIf needed, generate an SSH key: RStudio &gt; Tools &gt; Git/SVN &gt; Create RSA Key…\nThen give GitHub your SSH public key: GitHub &gt; SSH and GPG Keys &gt; New SSH key\ngit remote add origin git@github.com:hadley/r-pkgs.git\ngit push -u origin master\nAlso, if needed, change the URL of remote ‘origin’:\ngit remote set-url origin git@github.com:hadley/r-foo.git\n\n\nAutomated checking\n\nhttps://r-pkgs.org/r-cmd-check.html\n\n\nusethis::use_github_action(\"check-standard\")\nusethis::use_github_action(\"test-coverage\")"
  },
  {
    "objectID": "posts/cloud/index.html",
    "href": "posts/cloud/index.html",
    "title": "Cloud",
    "section": "",
    "text": "EC2 &gt; Network & Security &gt; Key Pairs &gt; Create Key Pair &gt; *.pem\nPuTTYgen &gt; Load an existing private key file &gt; Save private key &gt; *.ppk\nHost Name: ec2-user@ec2-1-23-456-789.us-east-2.compute.amazonaws.com\nConnection &gt; SSH &gt; Auth & WinSCP &gt; Auth &gt; Private key file for authentication\nsudo yum update\n\n\n\nAmazon Linux 2\nwget https://repo.anaconda.com/archive/Anaconda3-2023.03-Linux-x86_64.sh\n\nbash Anaconda3-2023.03-Linux-x86_64.sh\nThen close and re-open current shell\n\nhttps://pypi.org/project/ipython/#history\nhttps://stackoverflow.com/a/76521018\nhttps://stackoverflow.com/a/77117477\n\npip install 'ipython==7.34.0' --force-reinstall\nipython\nfrom IPython.lib import passwd\npasswd(\"12345\")\n# sha1:asdfasdf:asfasdfasdf\nexit()\njupyter notebook --generate-config \nvi ~/.jupyter/jupyter_notebook_config.py\n\n# [i]\nc = get_config()  # Get the config object.\n# c.NotebookApp.certfile = u'/home/ubuntu/ssl/cert.pem' # path to the certificate we generated\n# c.NotebookApp.keyfile = u'/home/ubuntu/ssl/cert.key' # path to the certificate key we generated\nc.NotebookApp.ip = '0.0.0.0' # serve notebooks locally\nc.NotebookApp.open_browser = False # do not open a browser window by default when using notebooks.\nc.NotebookApp.password = 'sha1:asdfasdf:asfasdfasdf'\n# [ESC][:wq][ENTER]\n# nohup jupyter notebook\nnohup jupyter lab\nThen close and re-open current shell\njupyter nbconvert --to python &lt;notebook&gt;.ipynb\nsudo service crond start\ncrontab -e\n# [i]\n# https://crontab.guru/\n# * * * * * /home/ec2-user/anaconda3/bin/python /home/ec2-user/&lt;notebook&gt;.py\n# [ESC][:wq][ENTER]\n\nhttp://1.23.456.789:8888\n\n\n\n\n\nhttps://aws.amazon.com/blogs/big-data/running-r-on-aws/\n\nInstall R\nsudo su\n# yum install -y R # Amazon Linux\n# amazon-linux-extras list\nsudo amazon-linux-extras install R3.4 # Amazon Linux 2\nInstall RStudio Server\n\nhttps://posit.co/download/rstudio-server/\n\nwget https://download2.rstudio.org/server/centos6/x86_64/rstudio-server-rhel-1.2.5019-x86_64.rpm\nyum install rstudio-server-rhel-1.2.5019-x86_64.rpm\nrm rstudio-server-rhel-1.2.5019-x86_64.rpm\nInstall Shiny Server\n\nhttps://posit.co/download/shiny-server/\n\nR -e \"install.packages('shiny', repos = 'http://cran.rstudio.com/')\"\nwget https://download3.rstudio.org/centos6.3/x86_64/shiny-server-1.5.12.933-x86_64.rpm\nyum install --nogpgcheck shiny-server-1.5.12.933-x86_64.rpm\nrm shiny-server-1.5.12.933-x86_64.rpm\nAdd user(s)\nuseradd -m jjf234\nsudo passwd jjf234 # prompt to enter password\nNeed to create ‘/ShinyApps’ under ‘/ec2-user’ and then subfolders with code, e.g. /home/ec2-user/ShinyApps/hello\n# sudo /opt/shiny-server/bin/deploy-example user-dirs\n# mkdir ~/ShinyApps\n# sudo cp -R /opt/shiny-server/samples/sample-apps/hello ~/ShinyApps\nInstall devtools\nsudo yum install libcurl-devel\nsudo yum install openssl-devel\nsudo yum install libxml2-devel\nR -e \"install.packages('devtools', repos = 'http://cran.rstudio.com/')\"\nInstall RcppParallel\nR -e \"install.packages('RcppParallel', repos = 'http://cran.rstudio.com/')\"\nInstall RcppArmadillo\nR -e \"install.packages('RcppArmadillo', repos = 'http://cran.rstudio.com/')\"\nInstall other packages\nR -e \"install.packages(c('data.table', 'xts', 'testthat', 'microbenchmark'),\nrepos = 'http://cran.rstudio.com/')\"\nInstall roll\nR -e \"devtools::install_github('jjf234/roll')\"\n\nhttp://1.23.456.789:8787/\nhttp://1.23.456.789:3838/ec2-user/hello/"
  },
  {
    "objectID": "posts/cloud/index.html#secure-shell",
    "href": "posts/cloud/index.html#secure-shell",
    "title": "Cloud",
    "section": "",
    "text": "EC2 &gt; Network & Security &gt; Key Pairs &gt; Create Key Pair &gt; *.pem\nPuTTYgen &gt; Load an existing private key file &gt; Save private key &gt; *.ppk\nHost Name: ec2-user@ec2-1-23-456-789.us-east-2.compute.amazonaws.com\nConnection &gt; SSH &gt; Auth & WinSCP &gt; Auth &gt; Private key file for authentication\nsudo yum update"
  },
  {
    "objectID": "posts/cloud/index.html#jupyter-server",
    "href": "posts/cloud/index.html#jupyter-server",
    "title": "Cloud",
    "section": "",
    "text": "Amazon Linux 2\nwget https://repo.anaconda.com/archive/Anaconda3-2023.03-Linux-x86_64.sh\n\nbash Anaconda3-2023.03-Linux-x86_64.sh\nThen close and re-open current shell\n\nhttps://pypi.org/project/ipython/#history\nhttps://stackoverflow.com/a/76521018\nhttps://stackoverflow.com/a/77117477\n\npip install 'ipython==7.34.0' --force-reinstall\nipython\nfrom IPython.lib import passwd\npasswd(\"12345\")\n# sha1:asdfasdf:asfasdfasdf\nexit()\njupyter notebook --generate-config \nvi ~/.jupyter/jupyter_notebook_config.py\n\n# [i]\nc = get_config()  # Get the config object.\n# c.NotebookApp.certfile = u'/home/ubuntu/ssl/cert.pem' # path to the certificate we generated\n# c.NotebookApp.keyfile = u'/home/ubuntu/ssl/cert.key' # path to the certificate key we generated\nc.NotebookApp.ip = '0.0.0.0' # serve notebooks locally\nc.NotebookApp.open_browser = False # do not open a browser window by default when using notebooks.\nc.NotebookApp.password = 'sha1:asdfasdf:asfasdfasdf'\n# [ESC][:wq][ENTER]\n# nohup jupyter notebook\nnohup jupyter lab\nThen close and re-open current shell\njupyter nbconvert --to python &lt;notebook&gt;.ipynb\nsudo service crond start\ncrontab -e\n# [i]\n# https://crontab.guru/\n# * * * * * /home/ec2-user/anaconda3/bin/python /home/ec2-user/&lt;notebook&gt;.py\n# [ESC][:wq][ENTER]\n\nhttp://1.23.456.789:8888"
  },
  {
    "objectID": "posts/cloud/index.html#rstudio-server",
    "href": "posts/cloud/index.html#rstudio-server",
    "title": "Cloud",
    "section": "",
    "text": "https://aws.amazon.com/blogs/big-data/running-r-on-aws/\n\nInstall R\nsudo su\n# yum install -y R # Amazon Linux\n# amazon-linux-extras list\nsudo amazon-linux-extras install R3.4 # Amazon Linux 2\nInstall RStudio Server\n\nhttps://posit.co/download/rstudio-server/\n\nwget https://download2.rstudio.org/server/centos6/x86_64/rstudio-server-rhel-1.2.5019-x86_64.rpm\nyum install rstudio-server-rhel-1.2.5019-x86_64.rpm\nrm rstudio-server-rhel-1.2.5019-x86_64.rpm\nInstall Shiny Server\n\nhttps://posit.co/download/shiny-server/\n\nR -e \"install.packages('shiny', repos = 'http://cran.rstudio.com/')\"\nwget https://download3.rstudio.org/centos6.3/x86_64/shiny-server-1.5.12.933-x86_64.rpm\nyum install --nogpgcheck shiny-server-1.5.12.933-x86_64.rpm\nrm shiny-server-1.5.12.933-x86_64.rpm\nAdd user(s)\nuseradd -m jjf234\nsudo passwd jjf234 # prompt to enter password\nNeed to create ‘/ShinyApps’ under ‘/ec2-user’ and then subfolders with code, e.g. /home/ec2-user/ShinyApps/hello\n# sudo /opt/shiny-server/bin/deploy-example user-dirs\n# mkdir ~/ShinyApps\n# sudo cp -R /opt/shiny-server/samples/sample-apps/hello ~/ShinyApps\nInstall devtools\nsudo yum install libcurl-devel\nsudo yum install openssl-devel\nsudo yum install libxml2-devel\nR -e \"install.packages('devtools', repos = 'http://cran.rstudio.com/')\"\nInstall RcppParallel\nR -e \"install.packages('RcppParallel', repos = 'http://cran.rstudio.com/')\"\nInstall RcppArmadillo\nR -e \"install.packages('RcppArmadillo', repos = 'http://cran.rstudio.com/')\"\nInstall other packages\nR -e \"install.packages(c('data.table', 'xts', 'testthat', 'microbenchmark'),\nrepos = 'http://cran.rstudio.com/')\"\nInstall roll\nR -e \"devtools::install_github('jjf234/roll')\"\n\nhttp://1.23.456.789:8787/\nhttp://1.23.456.789:3838/ec2-user/hello/"
  },
  {
    "objectID": "posts/dev/index.html",
    "href": "posts/dev/index.html",
    "title": "Software",
    "section": "",
    "text": "System setup\n# environment variable: PATH=c:\\rtools&lt;123&gt;\\usr\\bin\n\n\nGit and GitHub\ngit config --global user.name \"&lt;NAME&gt;\"\ngit config --global user.email \"&lt;EMAIL&gt;\"\nIf needed, generate an SSH key: RStudio &gt; Tools &gt; Git/SVN &gt; Create RSA Key…\nThen give GitHub your SSH public key: GitHub &gt; SSH and GPG Keys &gt; New SSH key\nCreate a new repository on the command line\necho \"# &lt;REPO&gt;\" &gt;&gt; README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin git@github.com:&lt;OWNER&gt;/&lt;REPO&gt;.git\ngit push -u origin main\n…or push an existing repository from the command line\ngit remote add origin git@github.com:&lt;OWNER&gt;/&lt;REPO&gt;.git\ngit branch -M main\ngit push -u origin main\n# git remote set-url origin git@github.com:&lt;OWNER&gt;/&lt;REPO&gt;.git\n\n\nDevelopment workflow\n\nRcppArmadillo::RcppArmadillo.package.skeleton(name = \"anRpackage\",\n                                              path = \".\",\n                                              example_code = FALSE)\n\n\n\nCompiled code\n\nRcpp::compileAttributes(verbose = TRUE)\ntools::package_native_routine_registration_skeleton(\".\", character_only = FALSE)\n\n\n\nObject documentation\n\nroxygen2::roxygenize()\n\n\n\nAutomated checking\n\nusethis::use_github_action(\"check-standard\")\nusethis::use_github_action(\"test-coverage\")"
  },
  {
    "objectID": "posts/roll/index.html",
    "href": "posts/roll/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Usage\n\nlibrary(roll)\nlibrary(microbenchmark)\noptions(microbenchmark.unit = \"us\")\n\n\nn_vars &lt;- 10\nn_obs &lt;- 1000\nweights &lt;- 0.9 ^ (n_obs:1)\n\nx &lt;- matrix(rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)\ny &lt;- matrix(rnorm(n_obs), nrow = n_obs, ncol = 1)\nx_lgl &lt;- x &lt; 0\n\n\n\nRolling any\n\nresult &lt;- microbenchmark(\"125\" = roll_any(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_any(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_any(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_any(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 100.9 131.50 147.534 141.50 155.35  282.1   100\n  250 116.9 132.75 156.166 141.95 154.95 1119.2   100\n  500  93.2 127.35 143.660 138.70 152.95  217.2   100\n 1000 102.6 126.10 139.524 135.45 147.30  227.3   100\n\n\n\n\nRolling all\n\nresult &lt;- microbenchmark(\"125\" = roll_all(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_all(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_all(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_all(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 102.6 129.15 144.509 139.25 149.20 374.0   100\n  250  98.3 130.90 146.548 139.65 150.65 282.4   100\n  500  97.1 128.70 142.922 137.25 149.90 257.9   100\n 1000 108.4 129.65 142.897 136.30 152.70 236.6   100\n\n\n\n\nRolling sums\n\\[\n\\begin{aligned}\n&\\text{Expanding window} \\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_sum(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sum(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sum(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sum(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125  97.3 116.45 131.576 127.45 140.80 200.8   100\n  250 100.1 115.95 129.606 123.90 132.90 382.3   100\n  500  97.3 114.85 126.365 121.65 131.85 241.8   100\n 1000  93.4 114.50 123.485 120.95 129.85 177.9   100\n\n\n\n\nRolling products\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{prod}_{w}\\leftarrow\\text{prod}_{w}\\times\\text{w}_{new}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}/\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_prod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_prod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_prod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_prod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 220.9 236.35 271.061  257.5 284.10 526.5   100\n  250 222.6 245.10 278.764  259.4 281.00 620.1   100\n  500 151.8 177.50 201.508  187.4 213.60 545.5   100\n 1000 147.3 172.00 197.124  184.3 206.15 575.2   100\n\n\n\n\nRolling means\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times \\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_mean(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_mean(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_mean(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_mean(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125  96.2 118.45 130.005 126.20 136.00 194.1   100\n  250  95.5 117.95 132.266 127.50 140.05 187.7   100\n  500 100.8 116.95 125.618 121.80 130.45 177.5   100\n 1000  98.2 113.55 126.878 118.35 130.75 408.7   100\n\n\n\n\nRolling minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_min(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_min(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_min(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_min(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 107.9 131.30 156.383 143.05 171.30 315.1   100\n  250 114.1 133.45 148.953 142.00 155.15 289.9   100\n  500 113.0 132.90 150.993 141.95 160.75 362.4   100\n 1000 119.7 137.85 154.477 147.20 155.30 454.0   100\n\n\n\n\nRolling maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_max(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_max(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_max(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_max(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 113.7 131.25 146.899 139.05 153.05 401.7   100\n  250 109.4 134.65 147.073 142.25 152.25 229.7   100\n  500 108.1 136.30 149.703 143.50 156.55 239.5   100\n 1000 110.0 133.55 145.957 146.05 156.30 207.1   100\n\n\n\n\nRolling index of minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmin(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmin(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmin(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmin(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 104.1 115.25 130.789 121.25 130.50 225.4   100\n  250 103.6 115.60 133.341 120.65 136.75 343.1   100\n  500 103.1 117.25 132.180 123.10 136.20 302.0   100\n 1000 100.9 120.10 137.035 126.15 144.05 261.2   100\n\n\n\n\nRolling index of maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmax(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmax(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmax(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmax(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 100.3 115.45 140.336 121.70 141.55 373.8   100\n  250 103.4 114.45 130.265 119.05 132.30 348.6   100\n  500 103.5 114.10 131.351 119.55 130.65 256.3   100\n 1000 105.4 118.30 134.958 124.25 135.80 263.4   100\n\n\n\n\nRolling medians\n\nresult &lt;- microbenchmark(\"125\" = roll_median(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_median(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_median(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_median(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr     min       lq      mean   median       uq     max neval\n  125  4599.7  5482.75  7698.712  6229.65  8027.80 18081.3   100\n  250  8730.7 10744.00 14895.029 13272.45 18087.30 27164.4   100\n  500 16728.6 20769.00 27494.023 26526.60 34311.20 40460.8   100\n 1000 22308.3 35583.90 40242.298 41314.70 46052.25 50372.8   100\n\n\n\n\nRolling quantiles\n\nresult &lt;- microbenchmark(\"125\" = roll_quantile(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_quantile(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_quantile(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_quantile(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr     min       lq     mean  median      uq     max neval\n  125  4729.8  5240.35  7676.22  6085.0  8855.2 17907.3   100\n  250  8941.7 11134.45 15170.24 13553.9 17722.8 27089.2   100\n  500 16328.3 20147.60 27557.76 26549.0 34689.3 40910.2   100\n 1000 23694.9 30086.55 37129.21 38408.9 43262.2 48889.0   100\n\n\n\n\nRolling variances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{x}_{old}-\\text{mean}_{prev_x})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_var(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_var(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_var(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_var(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 124.9 152.65 168.985 162.35 177.45 238.6   100\n  250 141.4 152.65 167.993 162.25 175.35 306.0   100\n  500 113.1 144.90 163.236 156.50 168.50 402.3   100\n 1000 105.6 145.35 162.694 155.35 171.95 350.3   100\n\n\n\n\nRolling standard deviations\n\nresult &lt;- microbenchmark(\"125\" = roll_sd(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sd(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sd(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sd(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 143.7 152.55 165.386 162.45 171.80 272.8   100\n  250 138.9 148.75 164.558 159.15 169.25 264.9   100\n  500 140.5 146.40 165.517 157.70 167.50 481.8   100\n 1000 131.7 140.45 154.087 150.05 158.35 239.6   100\n\n\n\n\nRolling scaling and centering\n\nresult &lt;- microbenchmark(\"125\" = roll_scale(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_scale(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_scale(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_scale(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 152.4 173.05 222.879 184.65 207.40 994.0   100\n  250 156.0 172.00 205.115 180.35 203.75 880.2   100\n  500 156.1 170.85 211.344 179.05 199.05 881.0   100\n 1000 147.0 164.35 208.481 176.40 206.55 848.1   100\n\n\n\n\nRolling covariances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{y}_{old}-\\text{mean}_{prev_y})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_cov(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cov(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cov(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cov(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq    max neval\n  125 1032.5 1154.95 1274.447 1211.05 1290.40 6003.6   100\n  250 1024.0 1122.05 1257.095 1198.65 1279.90 5818.0   100\n  500  876.5 1084.45 1152.625 1132.30 1232.20 1641.3   100\n 1000  888.0  997.80 1211.518 1056.75 1113.05 6448.1   100\n\n\n\n\nRolling correlations\n\nresult &lt;- microbenchmark(\"125\" = roll_cor(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cor(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cor(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cor(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq     max neval\n  125 1156.7 1348.55 1873.940 1477.85 1611.25 13501.6   100\n  250 1136.6 1332.00 2051.621 1449.40 1735.15  6172.6   100\n  500 1005.2 1270.15 1681.855 1351.00 1477.85  6514.3   100\n 1000 1009.5 1121.15 1466.576 1183.50 1275.70  5742.3   100\n\n\n\n\nRolling crossproducts\n\nresult &lt;- microbenchmark(\"125\" = roll_crossprod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_crossprod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_crossprod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_crossprod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq     mean  median      uq    max neval\n  125 760.7 959.80 1208.778 1017.75 1073.40 5887.9   100\n  250 770.0 943.25 1010.253 1004.75 1061.05 1491.5   100\n  500 828.2 912.65  972.969  971.45 1015.10 1506.5   100\n 1000 614.1 827.05  927.984  868.80  925.90 5604.2   100\n\n\n\n\nRolling linear models\n\\[\n\\begin{aligned}\n&\\text{coef}=\\text{cov}_{xx}^{-1}\\times\\text{cov}_{xy}\\\\\n&\\text{intercept}=\\text{mean}_{y}-\\text{coef}\\times\\text{mean}_{x}\\\\\n&\\text{rsq}=\\frac{\\text{coef}^{T}\\times\\text{cov}_{xx}\\times\\text{coef}}{\\text{var}_{y}}\\\\\n&\\text{var}_{resid}=\\frac{(1-\\text{rsq})(\\text{var}_{y})(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})}{\\text{n}_{rows}-\\text{n}_{cols}}\\\\\n&\\text{xx}=\\text{cov}_{xx}\\times(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})\\\\\n&\\text{se}_{coef}=\\sqrt{\\text{var}_{resid}\\times\\text{diag}(\\text{xx}^{-1})}\\\\\n&\\text{se}_{intercept}=\\sqrt{\\text{var}_{resid}\\left(1/\\text{sum}_{w}+\\text{mean}_{x}^{T}\\text{xx}^{-1}\\text{mean}_{x}\\right)}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_lm(x, y, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_lm(x, y, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_lm(x, y, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_lm(x, y, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median     uq     max neval\n  125 2260.5 7207.10 7421.314 7551.45 7930.3 10909.0   100\n  250 2295.2 7098.45 7722.951 7454.40 7888.1 23681.8   100\n  500 2251.6 6967.50 7331.274 7233.75 7778.8 10878.0   100\n 1000 2098.4 6372.60 6629.760 6664.15 7062.0 23237.5   100\n\n\n\n\nReferences\n\nhttps://stackoverflow.com/a/9933794\nhttps://stackoverflow.com/a/11316626\nhttps://stackoverflow.com/a/34363187\nhttps://stackoverflow.com/a/243342\nhttps://stats.stackexchange.com/a/64217\nhttps://stackoverflow.com/a/51992954\nhttps://stackoverflow.com/a/25921772\nhttps://stackoverflow.com/a/40416506\nhttps://stackoverflow.com/a/5970314\nhttps://gist.github.com/ashelly/5665911\nhttps://stackoverflow.com/a/51992954"
  },
  {
    "objectID": "posts/optim/index.html",
    "href": "posts/optim/index.html",
    "title": "Optimization",
    "section": "",
    "text": "Usage\n\nlibrary(rolloptim)\n\n\nn_vars &lt;- 3\nn_obs &lt;- 15\nx &lt;- matrix(rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)\n\nmu &lt;- roll::roll_mean(x, 5)\nsigma &lt;- roll::roll_cov(x, width = 5)\n\n\n\nMinimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nroll_min_var(sigma)\n\n\n\nMaximize mean\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nroll_max_mean(mu)\n\n\n\nMaximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nroll_max_utility(mu, sigma)\n\n\n\nMinimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nroll_min_rss(xx, xy)\n\n\n\nReferences\n\nhttps://www.adrian.idv.hk/2021-06-22-kkt/\nhttps://or.stackexchange.com/a/3738\nhttps://bookdown.org/compfinezbook/introFinRbook/Portfolio-Theory-with-Matrix-Algebra.html#algorithm-for-computing-efficient-frontier"
  },
  {
    "objectID": "posts/finance/index.html#expected-value",
    "href": "posts/finance/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/finance/index.html#variance",
    "href": "posts/finance/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\n# http://scipp.ucsc.edu/~haber/ph116C/iid.pdf\ndef risk(x):\n    \n    n_rows = sum(~np.isnan(x))\n        \n    if n_rows &gt; 1:\n        result = np.sqrt(np.nansum(x ** 2) / (n_rows - 1))\n    else:\n        result = np.nan\n        \n    return result\n\n\n# volatility scale only\nscore_df = (momentum_df / momentum_df.rolling(width, min_periods = 1).apply(risk, raw = False)).dropna()\n\n\n# overall_df = score_df.mean(axis = 1)\n# overall_df = overall_df / overall_df.rolling(width, min_periods = 1).apply(risk, raw = False)\n\n\n# score_df.insert(loc = 0, column = \"Overall\", value = overall_df)\n# score_df = score_df.dropna()\n\n\nscore_mlt &lt;- melt(as.data.table(py$score_df, keep.rownames = \"index\"), id.vars = \"index\")\nscore_mlt[ , index := as.Date(index)]\nscore_plt &lt;- plot_ts_decomp(score_mlt, decomp = \"Overall\", title = \"Score 1Y\", multiple = 1) +\n  facet_wrap(~ variable)\nprint(score_plt)"
  },
  {
    "objectID": "posts/finance/index.html",
    "href": "posts/finance/index.html",
    "title": "Markets",
    "section": "",
    "text": "library(reticulate)\nlibrary(data.table)\nsource(\"../plot/theme_jjf.R\")\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\n# https://pandas-datareader.readthedocs.io/en/latest/remote_data.html\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)"
  },
  {
    "objectID": "posts/markets/index.html",
    "href": "posts/markets/index.html",
    "title": "Markets",
    "section": "",
    "text": "library(reticulate)\nlibrary(data.table)\nsource(\"../plot/theme_jjf.R\")\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\n# https://pandas-datareader.readthedocs.io/en/latest/remote_data.html\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)"
  },
  {
    "objectID": "posts/markets/index.html#expected-value",
    "href": "posts/markets/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets/index.html#variance",
    "href": "posts/markets/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\n# http://scipp.ucsc.edu/~haber/ph116C/iid.pdf\ndef risk(x):\n    \n    n_rows = sum(~np.isnan(x))\n        \n    if n_rows &gt; 1:\n        result = np.sqrt(np.nansum(x ** 2) / (n_rows - 1))\n    else:\n        result = np.nan\n        \n    return result\n\n\n# volatility scale only\nscore_df = (momentum_df / momentum_df.rolling(width, min_periods = 1).apply(risk, raw = False)).dropna()\n\n\n# overall_df = score_df.mean(axis = 1)\n# overall_df = overall_df / overall_df.rolling(width, min_periods = 1).apply(risk, raw = False)\n\n\n# score_df.insert(loc = 0, column = \"Overall\", value = overall_df)\n# score_df = score_df.dropna()\n\n\nscore_mlt &lt;- melt(as.data.table(py$score_df, keep.rownames = \"index\"), id.vars = \"index\")\nscore_mlt[ , index := as.Date(index)]\nscore_plt &lt;- plot_ts_decomp(score_mlt, decomp = \"Overall\", title = \"Score 1Y\", multiple = 1) +\n  facet_wrap(~ variable)\nprint(score_plt)"
  },
  {
    "objectID": "posts/statistics/index.html",
    "href": "posts/statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Usage\n\nlibrary(roll)\nlibrary(microbenchmark)\noptions(microbenchmark.unit = \"us\")\n\n\nn_vars &lt;- 10\nn_obs &lt;- 1000\nweights &lt;- 0.9 ^ (n_obs:1)\n\nx &lt;- matrix(rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)\ny &lt;- matrix(rnorm(n_obs), nrow = n_obs, ncol = 1)\nx_lgl &lt;- x &lt; 0\n\n\n\nRolling any\n\nresult &lt;- microbenchmark(\"125\" = roll_any(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_any(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_any(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_any(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 103.9 128.85 144.181  142.6 157.80  224.4   100\n  250 111.3 131.90 231.834  149.0 160.95 8548.0   100\n  500 113.1 130.95 146.818  143.2 160.60  194.3   100\n 1000 109.0 124.20 143.241  140.4 160.65  210.7   100\n\n\n\n\nRolling all\n\nresult &lt;- microbenchmark(\"125\" = roll_all(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_all(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_all(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_all(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 120.3 141.55 161.455 152.40 172.35 526.1   100\n  250 118.9 141.45 159.021 160.45 173.30 228.0   100\n  500 114.4 139.55 158.135 155.55 173.90 219.8   100\n 1000 112.0 137.05 155.731 152.95 166.15 245.6   100\n\n\n\n\nRolling sums\n\\[\n\\begin{aligned}\n&\\text{Expanding window} \\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_sum(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sum(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sum(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sum(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr  min     lq    mean median     uq   max neval\n  125 93.7 111.40 124.095 119.95 134.65 190.5   100\n  250 94.7 110.85 124.654 117.95 132.60 192.4   100\n  500 95.4 109.50 122.803 116.70 131.10 308.1   100\n 1000 95.5 109.70 124.298 115.40 134.35 206.4   100\n\n\n\n\nRolling products\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{prod}_{w}\\leftarrow\\text{prod}_{w}\\times\\text{w}_{new}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}/\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_prod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_prod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_prod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_prod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 222.6 234.30 256.250 244.00 269.90 378.4   100\n  250 217.8 234.70 249.864 243.00 254.05 352.8   100\n  500 147.8 168.70 184.928 178.75 198.30 253.0   100\n 1000 151.4 165.95 184.265 176.10 193.15 407.8   100\n\n\n\n\nRolling means\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times \\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_mean(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_mean(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_mean(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_mean(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125  99.5 119.75 133.644 126.55 145.70 193.1   100\n  250 102.6 117.80 131.421 125.65 145.45 171.6   100\n  500 100.6 114.35 133.149 128.75 144.20 386.2   100\n 1000 104.8 116.10 130.722 123.85 143.15 210.4   100\n\n\n\n\nRolling minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_min(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_min(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_min(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_min(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 114.8 139.35 151.308 148.50 160.85  226.8   100\n  250 110.4 136.55 151.550 146.65 164.35  316.8   100\n  500 109.3 139.80 157.480 148.70 170.05  389.8   100\n 1000 113.9 143.30 187.273 152.55 171.15 3147.1   100\n\n\n\n\nRolling maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_max(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_max(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_max(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_max(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 110.7 128.35 138.595 135.35 144.50 283.0   100\n  250 108.8 127.50 139.425 136.75 146.85 205.3   100\n  500 105.8 132.10 143.038 141.55 153.90 187.4   100\n 1000 108.5 133.95 146.798 142.05 156.25 208.9   100\n\n\n\n\nRolling index of minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmin(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmin(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmin(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmin(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125  96.9 116.15 127.061 123.15 131.20 193.1   100\n  250 108.0 117.90 132.211 123.30 131.55 246.2   100\n  500  99.8 117.25 133.133 122.35 137.60 305.4   100\n 1000 102.4 120.50 137.555 127.80 140.80 228.8   100\n\n\n\n\nRolling index of maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmax(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmax(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmax(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmax(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 105.0 119.40 135.222 126.00 143.00 367.1   100\n  250 102.8 117.00 134.750 122.95 139.35 257.8   100\n  500 100.6 119.05 136.248 125.85 142.40 276.9   100\n 1000 105.8 123.30 145.538 133.95 158.85 236.9   100\n\n\n\n\nRolling medians\n\nresult &lt;- microbenchmark(\"125\" = roll_median(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_median(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_median(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_median(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr     min       lq      mean   median       uq     max neval\n  125  5049.7  5677.45  8701.194  6796.90 11291.55 18998.6   100\n  250  9872.1 11961.60 16440.857 14402.90 20366.75 28773.2   100\n  500 18660.2 24411.85 29721.478 29960.75 35295.55 43186.0   100\n 1000 26709.3 36005.95 41646.918 43280.45 47841.55 53199.4   100\n\n\n\n\nRolling quantiles\n\nresult &lt;- microbenchmark(\"125\" = roll_quantile(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_quantile(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_quantile(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_quantile(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr     min       lq      mean   median       uq     max neval\n  125  4450.6  5542.55  8483.886  7392.35  9786.55 19952.5   100\n  250  9229.2 11823.40 16449.410 14480.40 21311.75 29143.9   100\n  500 15916.3 20071.30 27060.605 27390.45 34057.05 40634.8   100\n 1000 22438.4 30954.70 38321.424 39968.25 44804.15 56898.6   100\n\n\n\n\nRolling variances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{x}_{old}-\\text{mean}_{prev_x})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_var(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_var(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_var(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_var(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 139.5 149.10 166.382 163.00 172.35 258.7   100\n  250 130.2 148.45 164.559 160.35 169.75 284.4   100\n  500 131.7 149.25 167.055 160.45 171.45 480.3   100\n 1000 115.0 139.10 154.313 151.25 160.30 252.4   100\n\n\n\n\nRolling standard deviations\n\nresult &lt;- microbenchmark(\"125\" = roll_sd(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sd(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sd(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sd(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 134.2 151.10 166.464 160.45 174.10 382.0   100\n  250 123.7 150.70 169.012 159.80 174.95 496.7   100\n  500 136.1 147.20 161.212 154.10 168.30 274.7   100\n 1000 128.9 140.45 150.573 146.50 156.75 214.4   100\n\n\n\n\nRolling scaling and centering\n\nresult &lt;- microbenchmark(\"125\" = roll_scale(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_scale(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_scale(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_scale(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 147.8 157.65 171.824 166.35 175.75 265.9   100\n  250 144.6 157.20 168.209 164.10 174.25 249.9   100\n  500 143.9 154.10 165.708 162.85 172.15 268.1   100\n 1000 137.1 147.20 165.066 156.60 166.70 412.8   100\n\n\n\n\nRolling covariances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{y}_{old}-\\text{mean}_{prev_y})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_cov(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cov(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cov(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cov(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min      lq     mean  median      uq    max neval\n  125 992.4 1087.60 1181.662 1147.05 1208.40 4539.5   100\n  250 966.9 1071.60 1228.098 1131.85 1185.25 6065.2   100\n  500 848.9 1020.65 1190.394 1082.70 1137.20 5936.6   100\n 1000 777.3  946.65 1052.764  971.25 1016.15 5841.1   100\n\n\n\n\nRolling correlations\n\nresult &lt;- microbenchmark(\"125\" = roll_cor(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cor(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cor(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cor(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq     max neval\n  125 1167.7 1377.75 2143.177 1496.45 2452.30  8919.2   100\n  250 1114.8 1345.70 1971.568 1437.15 1865.40  6661.5   100\n  500 1130.4 1294.90 2055.076 1381.85 2598.75 15026.4   100\n 1000 1011.6 1130.25 1685.279 1208.20 1377.55  4670.2   100\n\n\n\n\nRolling crossproducts\n\nresult &lt;- microbenchmark(\"125\" = roll_crossprod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_crossprod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_crossprod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_crossprod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq     mean  median      uq    max neval\n  125 679.2 976.65 1112.208 1036.75 1078.10 8325.8   100\n  250 830.0 932.45 1047.441  996.00 1051.70 5885.7   100\n  500 793.8 913.05 1075.653  966.60 1018.30 6909.6   100\n 1000 646.4 813.25  918.169  845.00  937.75 5655.9   100\n\n\n\n\nRolling linear models\n\\[\n\\begin{aligned}\n&\\text{coef}=\\text{cov}_{xx}^{-1}\\times\\text{cov}_{xy}\\\\\n&\\text{intercept}=\\text{mean}_{y}-\\text{coef}\\times\\text{mean}_{x}\\\\\n&\\text{rsq}=\\frac{\\text{coef}^{T}\\times\\text{cov}_{xx}\\times\\text{coef}}{\\text{var}_{y}}\\\\\n&\\text{var}_{resid}=\\frac{(1-\\text{rsq})(\\text{var}_{y})(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})}{\\text{n}_{rows}-\\text{n}_{cols}}\\\\\n&\\text{xx}=\\text{cov}_{xx}\\times(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})\\\\\n&\\text{se}_{coef}=\\sqrt{\\text{var}_{resid}\\times\\text{diag}(\\text{xx}^{-1})}\\\\\n&\\text{se}_{intercept}=\\sqrt{\\text{var}_{resid}\\left(1/\\text{sum}_{w}+\\text{mean}_{x}^{T}\\text{xx}^{-1}\\text{mean}_{x}\\right)}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_lm(x, y, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_lm(x, y, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_lm(x, y, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_lm(x, y, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq     max neval\n  125 2250.6 3185.80 5073.430 3999.65 5857.75 23729.6   100\n  250 2386.5 2884.85 4569.928 3939.60 5859.95 11480.4   100\n  500 2161.5 3150.70 4549.807 3914.45 5589.60 10888.2   100\n 1000 2181.1 2772.20 4277.718 3660.90 5287.90 15622.6   100\n\n\n\n\nReferences\n\nhttps://stackoverflow.com/a/9933794\nhttps://stackoverflow.com/a/11316626\nhttps://stackoverflow.com/a/34363187\nhttps://stackoverflow.com/a/243342\nhttps://stats.stackexchange.com/a/64217\nhttps://stackoverflow.com/a/51992954\nhttps://stackoverflow.com/a/25921772\nhttps://stackoverflow.com/a/40416506\nhttps://stackoverflow.com/a/5970314\nhttps://gist.github.com/ashelly/5665911\nhttps://stackoverflow.com/a/51992954"
  },
  {
    "objectID": "posts/markets-py/index.html",
    "href": "posts/markets-py/index.html",
    "title": "Markets",
    "section": "",
    "text": "library(reticulate)\nlibrary(data.table)\nsource(\"../plot/theme_jjf.R\")\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)"
  },
  {
    "objectID": "posts/markets-py/index.html#expected-value",
    "href": "posts/markets-py/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-py/index.html#variance",
    "href": "posts/markets-py/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\ndef risk(x):\n    \n    n_rows = sum(~np.isnan(x))\n        \n    if n_rows &gt; 1:\n        result = np.sqrt(np.nansum(x ** 2) / (n_rows - 1))\n    else:\n        result = np.nan\n        \n    return result\n\n\n# volatility scale only\nscore_df = (momentum_df / momentum_df.rolling(width, min_periods = 1).apply(risk, raw = False)).dropna()\n\n\n# overall_df = score_df.mean(axis = 1)\n# overall_df = overall_df / overall_df.rolling(width, min_periods = 1).apply(risk, raw = False)\n\n\n# score_df.insert(loc = 0, column = \"Overall\", value = overall_df)\n# score_df = score_df.dropna()\n\n\nscore_mlt &lt;- melt(as.data.table(py$score_df, keep.rownames = \"index\"), id.vars = \"index\")\nscore_mlt[ , index := as.Date(index)]\nscore_plt &lt;- plot_ts_decomp(score_mlt, decomp = \"Overall\", title = \"Score 1Y\", multiple = 1) +\n  facet_wrap(~ variable)\nprint(score_plt)"
  },
  {
    "objectID": "posts/markets-r/index.html",
    "href": "posts/markets-r/index.html",
    "title": "Markets",
    "section": "",
    "text": "library(quantmod)\nlibrary(roll)\nlibrary(data.table)\nsource(\"../plot/theme_jjf.R\")\nfactors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\nfactors &lt;- c(factors_r, factors_d)\nwidth &lt;- 252\nscale &lt;- list(\"periods\" = 252, \"overlap\" = 5)\ngetSymbols(factors, src = \"FRED\")\nlevels_xts &lt;- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))\nreturns_xts &lt;- do.call(merge, lapply(factors, function(i) {\n    if (i %in% factors_r) {\n        diff(log((levels_xts[ , i])))\n    } else if (i %in% factors_d) {\n        -diff(levels_xts[ , i]) / 100\n    }    \n}))\noverlap_xts &lt;- roll_mean(returns_xts, scale[[\"overlap\"]], min_obs = 1, na_restore = TRUE)"
  },
  {
    "objectID": "posts/markets-r/index.html#expected-value",
    "href": "posts/markets-r/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-r/index.html#variance",
    "href": "posts/markets-r/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\n# volatility scale only\nscore_xts &lt;- na.omit(momentum_xts / roll_sd(momentum_xts, width, center = FALSE, min_obs = 1))\n\n\n# overall_xts &lt;- xts(rowMeans(score_xts), index(score_xts))\n# overall_xts &lt;- overall_xts / roll_sd(overall_xts, width, center = FALSE, min_obs = 1)\n# colnames(overall_xts) &lt;- \"Overall\"\n\n\n# score_xts &lt;- na.omit(merge(overall_xts, score_xts))\n\n\nscore_mlt &lt;- melt(as.data.table(score_xts), id.vars = \"index\")\nscore_plt &lt;- plot_ts_decomp(score_mlt, decomp = \"Overall\", title = \"Score 1Y\", multiple = 1) +\n  facet_wrap(~ variable)\nprint(score_plt)"
  },
  {
    "objectID": "posts/securities-py/index.html",
    "href": "posts/securities-py/index.html",
    "title": "Securities",
    "section": "",
    "text": "library(reticulate)\nlibrary(data.table)\nsource(\"../plot/theme_jjf.R\")\nimport pandas as pd\nimport numpy as np\n# import statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm # , chi2\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\nimport datetime\nfrom scipy.optimize import minimize\nsd_df = returns_df[\"overlap\"].rolling(width, min_periods = 1).std() * \\\n    np.sqrt(scale[\"periods\"]) * np.sqrt(scale[\"overlap\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#value",
    "href": "posts/securities-py/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    if (type == \"call\"):\n        result =  S * np.exp(-q * tau) * Phi(d1) - np.exp(-r * tau) * K * Phi(d2)\n    elif (type == \"put\"):\n        result = np.exp(-r * tau) * K * Phi(-d2) - S * np.exp(-q * tau) * Phi(-d1)\n        \n    return result\n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\ndef bs_d1(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q + sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n\ndef bs_d2(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q - sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n    \ndef phi(x):\n    \n    result = norm.pdf(x)\n    \n    return result\n\ndef Phi(x):\n    \n    result = norm.cdf(x)\n    \n    return result\n\n\ngreeks_df[\"d1\"] = greeks_df.apply(lambda x: bs_d1(x[\"spot\"], K, r, q, tau, sigma), axis = 1)\ngreeks_df[\"d2\"] = greeks_df.apply(lambda x: bs_d2(x[\"spot\"], K, r, q, tau, sigma), axis = 1)\ngreeks_df[\"value\"] = greeks_df.apply(lambda x: bs_value(x[\"type\"], x[\"spot\"], K, r, q, tau,\n                                                        sigma, x[\"d1\"], x[\"d2\"]), axis = 1)"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order",
    "href": "posts/securities-py/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_delta(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    if (type == \"call\"):\n        result = np.exp(-q * tau) * Phi(d1)\n    elif (type == \"put\"):\n        result = -np.exp(-q * tau) * Phi(-d1)\n    \n    return result\n\n\ngreeks_df[\"delta\"] = greeks_df.apply(lambda x: bs_delta(x[\"type\"], x[\"spot\"], K, r, q, tau,\n                                                        sigma, x[\"d1\"], x[\"d2\"]), axis = 1)\n\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\ndef bs_vega(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    result = S * np.exp(-q * tau) * phi(d1) * np.sqrt(tau)\n    \n    return result\n\n\ngreeks_df[\"vega\"] = greeks_df.apply(lambda x: bs_vega(x[\"type\"], x[\"spot\"], K, r, q, tau,\n                                                      sigma, x[\"d1\"], x[\"d2\"]), axis = 1)\n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_theta(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    if (type == \"call\"):\n        \n        result = -np.exp(-q * tau) * S * phi(d1) * sigma / (2 * np.sqrt(tau)) - \\\n            r * K * np.exp(-r * tau) * Phi(d2) + q * S * np.exp(-q * tau) * Phi(d1)\n        \n    elif (type == \"put\"):\n        \n        result = -np.exp(-q * tau) * S * phi(d1) * sigma / (2 * np.sqrt(tau)) + \\\n            r * K * np.exp(-r * tau) * Phi(-d2) - q * S * np.exp(-q * tau) * Phi(-d1)\n    \n    return result\n\n\ngreeks_df[\"theta\"] = greeks_df.apply(lambda x: bs_theta(x[\"type\"], x[\"spot\"], K, r, q, tau,\n                                                        sigma, x[\"d1\"], x[\"d2\"]), axis = 1)\n\n\n\nNonlinear beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\n# https://en.wikipedia.org/wiki/Notional_amount\ndef beta_option(type, S, K, r, q, tau, sigma, sec):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    notional_mv = sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * delta\n    notional_mv0 = sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * sec[\"delta\"]\n    \n    if (type == \"call\"):\n        result = sec[\"beta\"] * (notional_mv - notional_mv0)\n    elif (type == \"put\"):\n        result = sec[\"beta\"] * (notional_mv0 - notional_mv)\n        \n    return result\n\n\nbeta = 0.35\ntype = \"call\"\nn = 200\nmultiple = 100\nnav = 1000000000\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\ndelta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\nsec = {\n    \"n\": n,\n    \"multiple\": multiple,\n    \"S\": S,\n    \"delta\": delta,\n    \"beta\": 1\n}\n\n\nbeta_df = pd.DataFrame([(x, y) for x in [type] for y in shocks]) \\\n    .rename(columns = {0: \"type\", 1: \"shock\"})\nbeta_df[\"spot\"] = beta_df[\"shock\"].apply(level_shock, args = (S, tau, sigma))\nbeta_df[\"static\"] = beta\nbeta_df[\"dynamic\"] = beta_df \\\n    .apply(lambda x:  beta + \n           beta_option(type, x[\"spot\"], K, r, q, tau, sigma, sec) / nav, axis = 1)\n\n\nbeta_mlt &lt;- as.data.table(py$beta_df)[ , c(\"type\", \"spot\") := NULL]\nbeta_mlt &lt;- melt(beta_mlt, id.vars = \"shock\")\nbeta_plt &lt;- plot_scen(beta_mlt, title = \"Beta\", xlab = \"Shock\")\nprint(beta_plt)\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order",
    "href": "posts/securities-py/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\ndef bs_gamma(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    result = np.exp(-q * tau) * phi(d1) / (S * sigma * np.sqrt(tau))\n    \n    return result\n\n\ngreeks_df[\"gamma\"] = greeks_df.apply(lambda x: bs_gamma(x[\"type\"], x[\"spot\"], K, r, q, tau,\n                                                        sigma, x[\"d1\"], x[\"d2\"]), axis = 1)\n\n\ngreeks_mlt &lt;- as.data.table(py$greeks_df)[ , c(\"spot\", \"d1\", \"d2\") := NULL]\ngreeks_mlt &lt;- melt(greeks_mlt, id.vars = c(\"type\", \"shock\"))\ngreeks_plt &lt;- plot_scen(greeks_mlt, z = \"type\", title = \"Greeks\", xlab = \"Shock\") +\n  facet_wrap(~ variable, nrow = 1, scales = \"free\", labeller = labeller(variable = capitalize))\nprint(greeks_plt)"
  },
  {
    "objectID": "posts/securities-py/index.html#price-yield-formula",
    "href": "posts/securities-py/index.html#price-yield-formula",
    "title": "Securities",
    "section": "Price-yield formula",
    "text": "Price-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\n\ndef pnl_bond(duration, convexity, dy):\n    \n    duration_pnl = -duration * dy\n    convexity_pnl = (convexity / 2) * dy ** 2\n    \n    result = {\"duration_pnl\": duration_pnl,\n              \"convexity_pnl\": convexity_pnl}\n    \n    return result\n\n\n# https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nduration = 6.5 * (2 / 3)\nconvexity = 0.65 * (2 / 3)\ndy = 0.0025 # 25 bps\n\n\npnl_bond(duration, convexity, dy)\n\n{'duration_pnl': -0.010833333333333332, 'convexity_pnl': 1.3541666666666667e-06}\n\n\n\nDuration drift\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=-\\frac{\\partial D}{\\partial y}\\\\\n&=\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}-\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=C-D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(C\\times 100-D^{2}\\right)=C-\\frac{D^{2}}{100}\\)\n\ndef yield_shock(shock, tau, sigma):\n    \n    result = shock * sigma * np.sqrt(tau)\n    \n    return result\n\n\ndef duration_drift(duration, convexity, dy):\n    \n    drift = convexity - duration ** 2 / 100\n    change = -drift * dy * 100\n    \n    result = {\"drift\": drift,\n              \"change\": change}\n    \n    return result\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor = \"DGS10\"\nsigma = sd_df[factor][-1]\n\n\nduration_df = pd.DataFrame(shocks).rename(columns = {0: \"shock\"})\nduration_df[\"spot\"] = duration_df[\"shock\"].apply(yield_shock, args = (tau, sigma))\nduration_df[\"static\"] = duration\nduration_df[\"dynamic\"] = duration_df \\\n    .apply(lambda x: duration +\n           duration_drift(duration, convexity, x[\"spot\"])[\"change\"], axis = 1)\n\n\nduration_mlt &lt;- as.data.table(py$duration_df)[ , \"spot\" := NULL]\nduration_mlt &lt;- melt(duration_mlt, id.vars = \"shock\")\nduration_plt &lt;- plot_scen(duration_mlt, title = \"Duration\", xlab = \"Shock\")\nprint(duration_plt)"
  },
  {
    "objectID": "posts/securities-py/index.html#blacks-formula",
    "href": "posts/securities-py/index.html#blacks-formula",
    "title": "Securities",
    "section": "Black’s formula",
    "text": "Black’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\n# unable to verify the result\ndef pnl_option(type, S, K, r, q, tau, sigma, dS, dt, dsigma):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    value = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega = bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta = bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma = bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl = delta * dS / value\n    gamma_pnl = gamma * dS ** 2 / value\n    vega_pnl = vega * dsigma / value\n    theta_pnl = theta * dt / value\n    \n    result = {\"total\": delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n              \"delta\": delta_pnl,\n              \"gamma\": gamma_pnl,\n              \"vega\": vega_pnl,\n              \"theta\": theta_pnl}\n    \n    return result\n\n\nfactor = \"SP500\"\ntype = \"call\"\nS = levels_df.fillna(method = \"ffill\")[factor][-width]\nK = S * (1 + 0.05)\ntau = 1 # = 252 / 252\nsigma = sd_df[factor][-width]\n\n\noptions_df = pd.concat(dict(spot = levels_df.fillna(method = \"ffill\")[factor][-width:],\n                            sigma = sd_df[factor][-width:]), axis = 1)\noptions_df[\"dS\"] = options_df[\"spot\"] - S\noptions_df[\"dt\"] = (options_df.index - options_df.index[0]).days / 360\noptions_df[\"dsigma\"] = options_df[\"sigma\"] - sigma\n\n\nattrib_cols = [\"total\", \"delta\", \"gamma\", \"vega\", \"theta\"]\nattrib_df = options_df.apply(lambda x: pnl_option(type, S, K, r, q, tau, sigma,\n                                                  x[\"dS\"], x[\"dt\"], x[\"dsigma\"]), axis = 1)\nattrib_df = pd.DataFrame.from_records(attrib_df, index = attrib_df.index)\nattrib_df = attrib_df[attrib_cols]\n\n\nattrib_mlt &lt;- melt(as.data.table(py$attrib_df, keep.rownames = \"index\"), id.vars = \"index\")\nattrib_mlt[ , index := as.Date(index)]\nattrib_plt &lt;- plot_ts_decomp(attrib_mlt, decomp = \"Total\", title = \"Attribution 1Y (%)\")\nprint(attrib_plt)"
  },
  {
    "objectID": "posts/securities-py/index.html#itos-lemma",
    "href": "posts/securities-py/index.html#itos-lemma",
    "title": "Securities",
    "section": "Ito’s lemma",
    "text": "Ito’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\ndef sim_gbm(n_sim, S, mu, sigma, dt):\n    \n    result = S * np.exp(np.cumsum(sigma * np.sqrt(dt) * np.random.normal(size = n_sim)) + \\\n                        (mu - 0.5 * sigma ** 2) * dt)\n    \n    return result\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\ndef sim_multi_gbm(n_sim, S, mu, sigma, dt):\n    \n    n_cols = sigma.shape[1]\n    \n    Z = np.random.normal(size = n_sim * n_cols).reshape((n_sim, n_cols))\n    X = np.add(np.sqrt(dt) * np.matmul(np.matrix(Z), np.linalg.cholesky(sigma).T),\n               ((mu - 0.5 * np.diag(sigma)) * dt))\n    \n    result = np.multiply(S, np.exp(X.cumsum(axis = 0)))\n    \n    return np.asmatrix(result)\n\n\n# https://arxiv.org/pdf/0812.4210.pdf\n# https://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\n# https://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\n# https://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\n# https://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\nS = [1] * len(factors)\nsigma = np.cov(returns_df[\"returns\"].dropna().T, ddof = 1) * scale[\"periods\"]\nmu = returns_df[\"returns\"].dropna().mean().tolist()\nmu = [x * scale[\"periods\"] for x in mu]\nmu = mu + np.diag(sigma) / 2 # drift\ndt = 1 / scale[\"periods\"]\n\n\nmu_df = pd.DataFrame()\nsigma_df = pd.DataFrame()\n\n\nfor i in range(1000):\n    \n    # assumes stock prices\n    levels_sim = pd.DataFrame(sim_multi_gbm(width + 1, S, mu, sigma, dt))\n    returns_sim = np.log(levels_sim).diff().dropna()\n    \n    mu_sim = returns_sim.mean() * scale[\"periods\"]\n    sigma_sim = returns_sim.std() * np.sqrt(scale[\"periods\"])\n    \n    mu_df = mu_df.append(pd.DataFrame(mu_sim).T)\n    sigma_df = sigma_df.append(pd.DataFrame(sigma_sim).T)\n\n\npd.DataFrame.from_dict({\"empirical\": np.array(returns_df[\"returns\"].dropna().mean()) * scale[\"periods\"],\n                        \"theoretical\": np.array(mu_df.mean())})\n\n   empirical  theoretical\n0   0.086493     0.086453\n1   0.018691     0.017543\n2  -0.001022    -0.000660\n3   0.000805     0.000840\n\n\n\npd.DataFrame.from_dict({\"empirical\": np.sqrt(np.diag(sigma)),\n                        \"theoretical\": np.array(sigma_df.mean())})\n\n   empirical  theoretical\n0   0.179687     0.179698\n1   0.062054     0.061841\n2   0.008137     0.008130\n3   0.016926     0.016922"
  },
  {
    "objectID": "posts/securities-py/index.html#newtons-method",
    "href": "posts/securities-py/index.html#newtons-method",
    "title": "Securities",
    "section": "Newton’s method",
    "text": "Newton’s method\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\ndef implied_vol_newton(params, type, S, K, r, q, tau):\n    \n    target0 = 0\n    sigma0 = params[\"sigma\"]\n    \n    while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n        \n        d1 = bs_d1(S, K, r, q, tau, sigma0)\n        d2 = bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 = bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        vega0 = bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma = sigma0 - (target0 - params[\"target\"]) / vega0\n        sigma0 = sigma\n        \n    return sigma\n\n\n# http://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\n# https://books.google.com/books?id=VLi61POD61IC&pg=PA104\nS = levels_df.fillna(method = \"ffill\")[factor][-1]\nK = S * (1 + 0.05)\nsigma = sd_df[factor][-1] # overrides matrix\nstart = 0.2\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\ntarget = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams = {\n    \"target\": target,\n    \"sigma\": start,\n    \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nimplied_vol_newton(params, type, S, K, r, q, tau) \n\n0.14224011090559865"
  },
  {
    "objectID": "posts/securities-py/index.html#optimization",
    "href": "posts/securities-py/index.html#optimization",
    "title": "Securities",
    "section": "Optimization",
    "text": "Optimization\nIf the derivative is unknown, try optimization:\n\ndef implied_vol_obj(param, type, S, K, r, q, tau, target):\n    \n    d1 = bs_d1(S, K, r, q, tau, param)\n    d2 = bs_d2(S, K, r, q, tau, param)\n    target0 = bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result = abs(target0 - target)\n    \n    return result\n\ndef implied_vol_optim(param, type, S, K, r, q, tau, target):\n    \n    result = minimize(implied_vol_obj, param, args = (type, S, K, r, q, tau, target))\n    \n    return result.x.item()\n\n\nimplied_vol_optim(start, type, S, K, r, q, tau, target)\n\n0.14224010626277114"
  },
  {
    "objectID": "posts/securities-py/index.html#variance-swaps",
    "href": "posts/securities-py/index.html#variance-swaps",
    "title": "Securities",
    "section": "Variance swaps",
    "text": "Variance swaps\nA variance swap can be written as:\n\\[\n\\begin{aligned}\nK_{var}={\\frac{2e^{rT}}{T}}\\left(\\int\\limits_{0}^{F_{0}}{\\frac{1}{K^{2}}}P(K)dK+\\int\\limits_{F_{0}}^{\\infty}{\\frac{1}{K^{2}}} C(K)dK\\right)\n\\end{aligned}\n\\]\nThe CBOE Volatility Index (VIX) is calculated as a variance swap on the 30-day variance of the S&P 500 with an adjustment term:\n\\[\n\\begin{aligned}\n\\sigma^{2}={\\frac{2e^{rT}}{T}}\\left(\\sum_{i=0}^{K_{0}}\\frac{\\Delta K_{i}}{K_{i}^{2}}P(K_{i})+\\sum_{i=K_{0}}^{\\infty}\\frac{\\Delta K_{i}}{K_{i}^{2}}C(K_{i})\\right)-\\frac{1}{T}\\left(\\frac{F}{K_{0}}-1\\right)^{2}\n\\end{aligned}\n\\]\n\ndef rle(x):\n    \n    n = len(x)\n    y = x.iloc[1:].reset_index(drop = True) != x.iloc[:-1].reset_index(drop = True)\n    i = y[y].index.tolist() + [n - 1]\n    \n    result = {\"lengths\": np.diff([0] + i),\n              \"values\": x.reset_index(drop = True)[i]}\n    \n    return result\n\n\n# https://cdn.cboe.com/resources/vix/vixwhite.pdf\n# https://en.wikipedia.org/wiki/VIX\n# https://en.wikipedia.org/wiki/Variance_swap\n# https://www.ivolatility.com/doc/VarianceSwaps.pdf\ndef implied_vol_vix(calls_df, puts_df, r, tau):\n    \n    # time to expiration \n    t = ((tau[\"exp_time\"] - tau[\"sys_time\"]).total_seconds() / 60) / (365 * 24 * 60)\n    \n    # midpoint of bid and ask\n    calls_df[\"Mid\"] = calls_df[[\"Bid\", \"Ask\"]].mean(axis = 1)\n    puts_df[\"Mid\"] = puts_df[[\"Bid\", \"Ask\"]].mean(axis = 1)\n    \n    options_df = calls_df[[\"Strike\", \"Mid\"]] \\\n        .merge(puts_df[[\"Strike\", \"Mid\"]], on = \"Strike\") \\\n        .rename(columns = {\"Mid_x\": \"Call\", \"Mid_y\": \"Put\"})\n    \n    options_df[\"Diff\"] = abs(options_df[\"Call\"] - options_df[\"Put\"])\n    \n    # minimum absolute difference is forward index level\n    forward_df = options_df.loc[options_df[\"Diff\"] == min(options_df[\"Diff\"])]\n    k = forward_df[\"Strike\"]\n    c = forward_df[\"Call\"]\n    p = forward_df[\"Put\"]\n    f = k + np.exp(r * t) * (c - p)\n    \n    # strike price equal or below forward index level\n    k0 = options_df.loc[options_df[\"Strike\"] &lt;= int(f), \"Strike\"].iloc[-1]\n    \n    # out-of-the-money options\n    puts_otm_df = puts_df.loc[puts_df[\"Strike\"] &lt; k0]\n    calls_otm_df = calls_df.loc[calls_df[\"Strike\"] &gt; k0]\n\n    # stop after two consecutive strike prices with zero bid prices\n    # https://stackoverflow.com/a/50311890\n    puts_otm_rle = rle(puts_otm_df[\"Bid\"])\n    idx = puts_otm_rle[\"lengths\"].cumsum() # end\n    try:\n        idx = idx[(puts_otm_rle[\"lengths\"] &gt; 1) & (puts_otm_rle[\"values\"] == 0)].max()\n        puts_otm_df = puts_otm_df.iloc[(idx + 1):]\n    except:\n        pass\n    \n    calls_otm_rle = rle(calls_otm_df[\"Bid\"])\n    idx = calls_otm_rle[\"lengths\"].cumsum() # end\n    idx = idx - calls_otm_rle[\"lengths\"] + 1 # start\n    try:\n        idx = idx[(calls_otm_rle[\"lengths\"] &gt; 1) & (calls_otm_rle[\"values\"] == 0)].max()\n        calls_otm_df = calls_otm_df.iloc[:idx]\n    except:\n        pass\n\n    # average put and call prices for k0\n    # note: exclude options with zero bid price\n    result_df = puts_otm_df.loc[puts_otm_df[\"Bid\"] != 0, [\"Strike\", \"Mid\"]] \\\n        .append(pd.DataFrame(puts_df.loc[puts_df[\"Strike\"] == k0, [\"Strike\", \"Mid\"]] \\\n                             .append(calls_df.loc[calls_df[\"Strike\"] == k0,\n                                                  [\"Strike\", \"Mid\"]]).mean(axis = 0)).transpose() \\\n                .append(calls_otm_df.loc[calls_otm_df[\"Bid\"] != 0,\n                                         [\"Strike\", \"Mid\"]])).reset_index(drop = True)\n    \n    # differences between strike prices\n    # note: create new column\n    result_df.loc[0, \"Diff\"] = result_df.loc[1, \"Strike\"] - result_df.loc[0, \"Strike\"]\n    result_df[\"Diff\"].iloc[-1] = result_df[\"Strike\"].iloc[-1] - result_df[\"Strike\"].iloc[-2]\n    result_df[\"Diff\"].iloc[1:-1] = (result_df[\"Strike\"].iloc[2:].values -\n                                    result_df[\"Strike\"].iloc[:-2].values) / 2\n\n    # variance\n    v = sum(result_df[\"Diff\"] / result_df[\"Strike\"] ** 2 * np.exp(r * t) *\n            result_df[\"Mid\"]) * (2 / t) - (1 / t) * (f / k0 - 1) ** 2\n        \n    result = {\"t\": t,\n              \"v\": v}\n            \n    return result\n\n\nsys_time = datetime.datetime.now().replace(hour = 9, minute = 46, second = 0)\n\n\nv1 = implied_vol_vix(pd.read_csv(\"../securities/near_calls.csv\"), pd.read_csv(\"../securities/near_puts.csv\"), 0.000305,\n                     {\"sys_time\": sys_time,\n                      \"exp_time\": sys_time.replace(hour = 8, minute = 30, second = 0) +\n                                  datetime.timedelta(days = 25)})\nv2 = implied_vol_vix(pd.read_csv(\"../securities/next_calls.csv\"), pd.read_csv(\"../securities/next_puts.csv\"), 0.000286,\n                     {\"sys_time\": sys_time,\n                      \"exp_time\": sys_time.replace(hour = 15, minute = 0, second = 0) +\n                                  datetime.timedelta(days = 32)})\n\n\nnt1 = v1[\"t\"] * (365 * 24 * 60)\nnt2 = v2[\"t\"] * (365 * 24 * 60)\nn30 = 30 * (24 * 60)\nn365 = 365 * (24 * 60)\n\n\nvix = np.sqrt(((v1[\"t\"] * v1[\"v\"] * ((nt2 - n30) / (nt2 - nt1))).values +\n               (v2[\"t\"] * v2[\"v\"] * ((n30 - nt1) / (nt2 - nt1))).values) * (n365 / n30))\nvix\n\narray([0.13685821])"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-yield",
    "href": "posts/securities-py/index.html#implied-yield",
    "title": "Securities",
    "section": "Implied yield",
    "text": "Implied yield\n\ndef yield_option(type, S, K, r, q, tau, sigma):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    value = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    if (type == \"call\"):\n        result = (value / S) / tau\n    elif (type == \"put\"):\n        result = (value / K) / tau\n        \n    return result\n\n\nsigmas = [x / 100 for x in range(10, 31, 4)]\ntaus = [x / 252 for x in range(20, 127, 20)]\n\n\nyield_df = pd.DataFrame([(x, y) for x in sigmas for y in taus]) \\\n    .rename(columns = {0: \"sigma\", 1: \"tau\"})\nyield_df[\"yield\"] = yield_df.apply(lambda x: yield_option(type, S, K, r, q, x[\"tau\"], x[\"sigma\"]),\n                                   axis = 1)\nyield_df[\"sigma\"] = yield_df[\"sigma\"] * 100\nyield_df[\"tau\"] = yield_df[\"tau\"] * scale[\"periods\"]\n\n\nyield_plt &lt;- plot_heatmap(py$yield_df, x = \"tau\", y = \"sigma\", z = \"yield\",\n                          title = \"Yield (%)\", xlab = \"Tau\", ylab = \"Sigma\")\nprint(yield_plt)"
  },
  {
    "objectID": "posts/securities-r/index.html",
    "href": "posts/securities-r/index.html",
    "title": "Securities",
    "section": "",
    "text": "library(quantmod)\nlibrary(roll)\nlibrary(data.table)\nsource(\"../plot/theme_jjf.R\")\nfactors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\nfactors &lt;- c(factors_r, factors_d)\nwidth &lt;- 252\nscale &lt;- list(\"periods\" = 252, \"overlap\" = 5)\ngetSymbols(factors, src = \"FRED\")\nlevels_xts &lt;- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))\nreturns_xts &lt;- do.call(merge, lapply(factors, function(i) {\n    if (i %in% factors_r) {\n        diff(log((levels_xts[ , i])))\n    } else if (i %in% factors_d) {\n        -diff(levels_xts[ , i]) / 100\n    }    \n}))\noverlap_xts &lt;- roll_mean(returns_xts, scale[[\"overlap\"]], min_obs = 1, na_restore = TRUE)\nsd_xts &lt;- roll_sd(overlap_xts, width, min_obs = 1) * sqrt(scale[[\"periods\"]]) * sqrt(scale[[\"overlap\"]])"
  },
  {
    "objectID": "posts/securities-r/index.html#value",
    "href": "posts/securities-r/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_value &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    if (type == \"call\") {\n        result &lt;- S * exp(-q * tau) * Phi(d1) - exp(-r * tau) * K * Phi(d2)\n    } else if (type == \"put\") {\n        result &lt;- exp(-r * tau) * K * Phi(-d2) - S * exp(-q * tau) * Phi(-d1)\n    }\n    \n    return(result)\n    \n} \n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\nbs_d1 &lt;- function(S, K, r, q, tau, sigma) {\n    \n    result &lt;- (log(S / K) + (r - q + sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\nbs_d2 &lt;- function(S, K, r, q, tau, sigma) {\n    \n    result &lt;- (log(S / K) + (r - q - sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\nphi &lt;- function(x) {\n    \n    result &lt;- dnorm(x)\n    \n    return(result)\n    \n}\n\nPhi &lt;- function(x) {\n    \n    result &lt;- pnorm(x)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , d1 := bs_d1(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , d2 := bs_d2(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , value := bs_value(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order",
    "href": "posts/securities-r/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_delta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    if (type == \"call\") {\n        result &lt;- exp(-q * tau) * Phi(d1)\n    } else if (type == \"put\") {\n        result &lt;- -exp(-q * tau) * Phi(-d1)\n    }\n    \n    return(result)\n    \n} \n\n\ngreeks_dt[ , delta := bs_delta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\nbs_vega &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    result &lt;- S * exp(-q * tau) * phi(d1) * sqrt(tau)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , vega := bs_vega(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_theta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    if (type == \"call\") {\n        \n        result &lt;- -exp(-q * tau) * S * phi(d1) * sigma / (2 * sqrt(tau)) -\n             r * K * exp(-r * tau) * Phi(d2) + q * S * exp(-q * tau) * Phi(d1)\n    \n    } else if (type == \"put\") {\n        \n        result &lt;- -exp(-q * tau) * S * phi(d1) * sigma / (2 * sqrt(tau)) +\n             r * K * exp(-r * tau) * Phi(-d2) - q * S * exp(-q * tau) * Phi(-d1)\n        \n    }\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , theta := bs_theta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nNonlinear beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\n# https://en.wikipedia.org/wiki/Notional_amount\nbeta_option &lt;- function(type, S, K, r, q, tau, sigma, sec) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    notional_mv &lt;- sec[[\"n\"]] * sec[[\"multiple\"]] * sec[[\"S\"]] * delta\n    notional_mv0 &lt;- sec[[\"n\"]] * sec[[\"multiple\"]] * sec[[\"S\"]] * sec[[\"delta\"]]\n    \n    if (type == \"call\") {\n        result &lt;- sec[[\"beta\"]] * (notional_mv - notional_mv0)\n    } else if (type == \"put\") {\n        result &lt;- sec[[\"beta\"]] * (notional_mv0 - notional_mv)\n    }        \n        \n    return(result)\n    \n}\n\n\nbeta &lt;- 0.35\ntype &lt;- \"call\"\nn &lt;- 200\nmultiple &lt;- 100\nnav &lt;- 1000000000\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\ndelta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\nsec &lt;- list(\n    \"n\" = n,\n    \"multiple\" = multiple,\n    \"S\" = S,\n    \"delta\" = delta,\n    \"beta\" = 1\n)\n\n\nbeta_dt &lt;- CJ(type = type, shock = shocks)\nbeta_dt[ , spot := level_shock(shock, S, tau, sigma), by = c(\"type\", \"shock\")]\nbeta_dt[ , static := beta]\nbeta_dt[ , dynamic := beta + beta_option(type, spot, K, r, q, tau, sigma, sec) / nav, by = c(\"type\", \"shock\")]\n\n\nbeta_mlt &lt;- copy(beta_dt)[ , c(\"type\", \"spot\") := NULL]\nbeta_mlt &lt;- melt(beta_mlt, id.vars = \"shock\")\nbeta_plt &lt;- plot_scen(beta_mlt, title = \"Beta\", xlab = \"Shock\")\nprint(beta_plt)\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order",
    "href": "posts/securities-r/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\nbs_gamma &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    result &lt;- exp(-q * tau) * phi(d1) / (S * sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , gamma := bs_gamma(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\ngreeks_mlt &lt;- copy(greeks_dt)[ , c(\"spot\", \"d1\", \"d2\") := NULL]\ngreeks_mlt &lt;- melt(greeks_mlt, id.vars = c(\"type\", \"shock\"))\ngreeks_plt &lt;- plot_scen(greeks_mlt, z = \"type\", title = \"Greeks\", xlab = \"Shock\") +\n  facet_wrap(~ variable, nrow = 1, scales = \"free\", labeller = labeller(variable = capitalize))\nprint(greeks_plt)"
  },
  {
    "objectID": "posts/securities-r/index.html#price-yield-formula",
    "href": "posts/securities-r/index.html#price-yield-formula",
    "title": "Securities",
    "section": "Price-yield formula",
    "text": "Price-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\n\npnl_bond &lt;- function(duration, convexity, dy) {\n    \n    duration_pnl &lt;- -duration * dy\n    convexity_pnl &lt;- (convexity / 2) * dy ^ 2\n    \n    result &lt;- list(\"duration_pnl\" = duration_pnl,\n                   \"convexity_pnl\" = convexity_pnl)\n    \n    return(result)\n    \n} \n\n\n# https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nduration &lt;- 6.5 * (2 / 3)\nconvexity &lt;- 0.65 * (2 / 3)\ndy &lt;- 0.0025 # 25 bps\n\n\npnl_bond(duration, convexity, dy)\n\n$duration_pnl\n[1] -0.01083333\n\n$convexity_pnl\n[1] 1.354167e-06\n\n\n\nDuration drift\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=-\\frac{\\partial D}{\\partial y}\\\\\n&=\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}-\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=C-D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(C\\times 100-D^{2}\\right)=C-\\frac{D^{2}}{100}\\)\n\nyield_shock &lt;- function(shock, tau, sigma) {\n    \n    result &lt;- shock * sigma * sqrt(tau)\n    \n    return(result)\n    \n}\n\n\nduration_drift &lt;- function(duration, convexity, dy) {\n    \n    drift &lt;- convexity - duration ^ 2 / 100\n    change &lt;- -drift * dy * 100\n    \n    result &lt;- list(\"drift\" = drift,\n                   \"change\" = change)\n    \n    return(result)\n    \n}\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor &lt;- \"DGS10\"\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor])\n\n\nduration_dt &lt;- CJ(shock = shocks)\nduration_dt[ , spot := yield_shock(shock, tau, sigma), by = \"shock\"]\nduration_dt[ , static := duration]\nduration_dt[ , dynamic := duration + duration_drift(duration, convexity, spot)[[\"change\"]], by = \"shock\"]\n\n\nduration_mlt &lt;- copy(duration_dt)[ , \"spot\" := NULL]\nduration_mlt &lt;- melt(duration_mlt, id.vars = \"shock\")\nduration_plt &lt;- plot_scen(duration_mlt, title = \"Duration\", xlab = \"Shock\")\nprint(duration_plt)"
  },
  {
    "objectID": "posts/securities-r/index.html#blacks-formula",
    "href": "posts/securities-r/index.html#blacks-formula",
    "title": "Securities",
    "section": "Black’s formula",
    "text": "Black’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\n# unable to verify the result\npnl_option &lt;- function(type, S, K, r, q, tau, sigma, dS, dt, dsigma) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    value &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega &lt;- bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta &lt;- bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma &lt;- bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl &lt;- delta * dS / value\n    gamma_pnl &lt;- gamma * dS ^ 2 / value\n    vega_pnl &lt;- vega * dsigma / value\n    theta_pnl &lt;- theta * dt / value\n    \n    result &lt;- list(\"total\" = delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n                   \"delta\" = delta_pnl,\n                   \"gamma\" = gamma_pnl,\n                   \"vega\" = vega_pnl,\n                   \"theta\" = theta_pnl)\n    \n    return(result)    \n    \n}\n\n\nfactor &lt;- \"SP500\"\ntype &lt;- \"call\"\nS &lt;- coredata(tail(na.locf(levels_xts[ , factor]), width)[1])\nK &lt;- S * (1 + 0.05)\ntau &lt;- 1 # = 252 / 252\nsigma &lt;- coredata(tail(sd_xts[ , factor], width)[1])\n\n\noptions_dt &lt;- data.table(index = index(tail(levels_xts, width)),\n                         spot = na.locf(tail(levels_xts[ , factor], width)),\n                         sigma = tail(sd_xts[ , factor], width))\nsetnames(options_dt, c(\"index\", \"spot\", \"sigma\"))\noptions_dt[ , dS := spot - S, by = index]\noptions_dt[ , dt := (index - index(tail(levels_xts, width))[1]) / 360, by = index]\noptions_dt[ , dsigma := sigma - ..sigma, by = index]\n\n\nattrib_dt &lt;- options_dt[ , as.list(unlist(pnl_option(type, S, K, r, q, tau, ..sigma,\n                                                     dS, dt, dsigma))), by = index]\n\n\nattrib_mlt &lt;- melt(attrib_dt, id.vars = \"index\")\nattrib_plt &lt;- plot_ts_decomp(attrib_mlt, decomp = \"Total\", title = \"Attribution 1Y (%)\")\nprint(attrib_plt)"
  },
  {
    "objectID": "posts/securities-r/index.html#itos-lemma",
    "href": "posts/securities-r/index.html#itos-lemma",
    "title": "Securities",
    "section": "Ito’s lemma",
    "text": "Ito’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\nsim_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n    \n    result &lt;- S * exp(cumsum(sigma * sqrt(dt) * rnorm(n_sim)) +\n                      (mu - 0.5 * sigma ^ 2) * dt)\n    \n    return(result)\n    \n}\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\nsim_multi_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n    \n    n_cols &lt;- ncol(sigma)\n    \n    Z &lt;- matrix(rnorm(n_sim * n_cols), nrow = n_sim, ncol = n_cols)\n    X &lt;- sweep(sqrt(dt) * (Z %*% chol(sigma)), 2, (mu - 0.5 * diag(sigma)) * dt, \"+\")\n    \n    result &lt;- sweep(apply(X, 2, function(x) exp(cumsum(x))), 2, S, \"*\")\n    \n    return(result)\n    \n}\n\n\n# https://arxiv.org/pdf/0812.4210.pdf\n# https://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\n# https://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\n# https://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\n# https://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\nS &lt;- rep(1, length(factors))\nsigma &lt;- cov(returns_xts, use = \"complete.obs\") * scale[[\"periods\"]]\nmu &lt;- colMeans(na.omit(returns_xts)) * scale[[\"periods\"]]\nmu &lt;- mu + diag(sigma) / 2 # drift\ndt &lt;- 1 / scale[[\"periods\"]]\n\n\nmu_ls &lt;- list()\nsigma_ls &lt;- list()\n\n\nfor (i in 1:1000) {\n    \n    # assumes stock prices\n    levels_sim &lt;- sim_multi_gbm(width + 1, S, mu, sigma, dt)\n    returns_sim &lt;- diff(log(levels_sim))\n\n    mu_sim &lt;- colMeans(returns_sim) * scale[[\"periods\"]]\n    sigma_sim &lt;- apply(returns_sim, 2, sd) * sqrt(scale[[\"periods\"]])\n    \n    mu_ls &lt;- append(mu_ls, list(mu_sim))\n    sigma_ls &lt;- append(sigma_ls, list(sigma_sim))\n    \n}\n\n\ndata.frame(\"empirical\" = colMeans(na.omit(returns_xts)) * scale[[\"periods\"]],\n           \"theoretical\" = colMeans(do.call(\"rbind\", mu_ls)))\n\n                 empirical   theoretical\nSP500         0.0864927465  0.0867239995\nDTWEXAFEGS    0.0186906433  0.0181477915\nDGS10        -0.0010223565 -0.0011270716\nBAMLH0A0HYM2  0.0008048338  0.0006892547\n\n\n\ndata.frame(\"empirical\" = sqrt(diag(sigma)),\n           \"theoretical\" = colMeans(do.call(\"rbind\", sigma_ls)))\n\n               empirical theoretical\nSP500        0.179686984 0.179801695\nDTWEXAFEGS   0.062053750 0.061979796\nDGS10        0.008136688 0.008122869\nBAMLH0A0HYM2 0.016926134 0.016948289"
  },
  {
    "objectID": "posts/securities-r/index.html#newtons-method",
    "href": "posts/securities-r/index.html#newtons-method",
    "title": "Securities",
    "section": "Newton’s method",
    "text": "Newton’s method\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\nimplied_vol_newton &lt;- function(params, type, S, K, r, q, tau) {\n    \n    target0 &lt;- 0\n    sigma0 &lt;- params[[\"sigma\"]]\n    \n    while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n        \n        d1 &lt;- bs_d1(S, K, r, q, tau, sigma0)\n        d2 &lt;- bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 &lt;- bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        vega0 &lt;- bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma &lt;- sigma0 - (target0 - params[[\"target\"]]) / vega0\n        sigma0 &lt;- sigma\n        \n    }\n    \n    return(sigma)\n    \n}\n\n\n# http://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\n# https://books.google.com/books?id=VLi61POD61IC&pg=PA104\nS &lt;- coredata(na.locf(levels_xts)[nrow(levels_xts), factor])\nK &lt;- S * (1 + 0.05)\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor]) # overrides matrix\nstart &lt;- 0.2\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\ntarget &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams &lt;- list(\n    \"target\" = target,\n    \"sigma\" = start,\n    \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nimplied_vol_newton(params, type, S, K, r, q, tau)\n\n         SP500\n[1,] 0.1426412"
  },
  {
    "objectID": "posts/securities-r/index.html#optimization",
    "href": "posts/securities-r/index.html#optimization",
    "title": "Securities",
    "section": "Optimization",
    "text": "Optimization\nIf the derivative is unknown, try optimization:\n\nimplied_vol_obj &lt;- function(param, type, S, K, r, q, tau, target) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, param)\n    d2 &lt;- bs_d2(S, K, r, q, tau, param)\n    target0 &lt;- bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result &lt;- abs(target0 - target)\n    \n    return(result)\n    \n}\n\nimplied_vol_optim &lt;- function(param, type, S, K, r, q, tau, target) {\n    \n    result &lt;- optim(param, implied_vol_obj, type = type, S = S, K = K, r = r, q = q,\n                    tau = tau, target = target, method = \"Brent\", lower = 0, upper = 1)\n    \n    return(result$par)\n    \n}\n\n\nimplied_vol_optim(start, type, S, K, r, q, tau, target)\n\n[1] 0.1426412"
  },
  {
    "objectID": "posts/securities-r/index.html#variance-swaps",
    "href": "posts/securities-r/index.html#variance-swaps",
    "title": "Securities",
    "section": "Variance swaps",
    "text": "Variance swaps\nA variance swap can be written as:\n\\[\n\\begin{aligned}\nK_{var}={\\frac{2e^{rT}}{T}}\\left(\\int\\limits_{0}^{F_{0}}{\\frac{1}{K^{2}}}P(K)dK+\\int\\limits_{F_{0}}^{\\infty}{\\frac{1}{K^{2}}} C(K)dK\\right)\n\\end{aligned}\n\\]\nThe CBOE Volatility Index (VIX) is calculated as a variance swap on the 30-day variance of the S&P 500 with an adjustment term:\n\\[\n\\begin{aligned}\n\\sigma^{2}={\\frac{2e^{rT}}{T}}\\left(\\sum_{i=0}^{K_{0}}\\frac{\\Delta K_{i}}{K_{i}^{2}}P(K_{i})+\\sum_{i=K_{0}}^{\\infty}\\frac{\\Delta K_{i}}{K_{i}^{2}}C(K_{i})\\right)-\\frac{1}{T}\\left(\\frac{F}{K_{0}}-1\\right)^{2}\n\\end{aligned}\n\\]\n\n# https://cdn.cboe.com/resources/vix/vixwhite.pdf\n# https://en.wikipedia.org/wiki/VIX\n# https://en.wikipedia.org/wiki/Variance_swap\n# https://www.ivolatility.com/doc/VarianceSwaps.pdf\nimplied_vol_vix &lt;- function(calls_df, puts_df, r, tau) {\n  \n  # time to expiration  \n  t &lt;- as.numeric(difftime(tau[[\"exp_time\"]], tau[[\"sys_time\"]], units = \"mins\")) / (365 * 24 * 60)\n  \n  # midpoint of bid and ask\n  calls_df[[\"Mid\"]] &lt;- rowMeans(calls_df[ , c(\"Bid\", \"Ask\")])\n  puts_df[[\"Mid\"]] &lt;- rowMeans(puts_df[ , c(\"Bid\", \"Ask\")])\n  \n  options_df &lt;- merge(calls_df[ , c(\"Strike\", \"Mid\")],\n                      puts_df[ , c(\"Strike\", \"Mid\")], by = \"Strike\")\n  colnames(options_df) &lt;- c(\"Strike\", \"Call\", \"Put\")\n  \n  options_df[[\"Diff\"]] &lt;- abs(options_df[[\"Call\"]] - options_df[[\"Put\"]])\n  \n  # minimum absolute difference is forward index level\n  forward_df &lt;- options_df[options_df[[\"Diff\"]] == min(options_df[[\"Diff\"]]), ]\n  k &lt;- forward_df[[\"Strike\"]]\n  c &lt;- forward_df[[\"Call\"]]\n  p &lt;- forward_df[[\"Put\"]]\n  f &lt;- k + exp(r * t) * (c - p)\n  \n  # strike price equal or below forward index level\n  k0 &lt;- tail(options_df[options_df[[\"Strike\"]] &lt;= f, \"Strike\"], 1)\n  \n  # out-of-the-money options\n  puts_otm_df &lt;- puts_df[puts_df[[\"Strike\"]] &lt; k0, ]\n  calls_otm_df &lt;- calls_df[calls_df[[\"Strike\"]] &gt; k0, ]\n  \n  # stop after two consecutive strike prices with zero bid prices\n  # https://stackoverflow.com/a/50311890\n  puts_otm_rle &lt;- rle(puts_otm_df[[\"Bid\"]])\n  idx &lt;- cumsum(puts_otm_rle$lengths) # end\n  idx &lt;- idx[max(which(puts_otm_rle$lengths &gt; 1 & puts_otm_rle$values == 0))]\n  puts_otm_df &lt;- puts_otm_df[(idx + 1):nrow(puts_otm_df), ]\n  \n  calls_otm_rle &lt;- rle(calls_otm_df[[\"Bid\"]])\n  idx &lt;- cumsum(calls_otm_rle$lengths) # end\n  idx &lt;- idx - calls_otm_rle$lengths + 1 # start\n  idx &lt;- idx[max(which(calls_otm_rle$lengths &gt; 1 & calls_otm_rle$values == 0))]\n  calls_otm_df &lt;- calls_otm_df[1:(idx - 1), ]\n  \n  # average put and call prices for k0\n  # note: exclude options with zero bid price\n  result_df &lt;- rbind(puts_otm_df[puts_otm_df[[\"Bid\"]] != 0, c(\"Strike\", \"Mid\")],\n                     colMeans(rbind(puts_df[puts_df[[\"Strike\"]] == k0, c(\"Strike\", \"Mid\")],\n                                    calls_df[calls_df[[\"Strike\"]] == k0, c(\"Strike\", \"Mid\")])),\n                     calls_otm_df[calls_otm_df[[\"Bid\"]] != 0, c(\"Strike\", \"Mid\")])\n  \n  # differences between strike prices\n  n_rows &lt;- nrow(result_df)\n  result_df[1, \"Diff\"] &lt;- result_df[2, \"Strike\"] - result_df[1, \"Strike\"]\n  result_df[n_rows, \"Diff\"] &lt;- result_df[n_rows, \"Strike\"] - result_df[n_rows - 1, \"Strike\"]\n  result_df[2:(n_rows - 1), \"Diff\"] &lt;- (result_df[3:n_rows, \"Strike\"] -\n                                          result_df[1:(n_rows - 2), \"Strike\"]) / 2\n  \n  # variance\n  v &lt;- sum(result_df[ , \"Diff\"] / result_df[ , \"Strike\"] ^ 2 * exp(r * t) *\n             result_df[ , \"Mid\"]) * (2 / t) - (1 / t) * (f / k0 - 1) ^ 2\n  \n  result &lt;- list(\"t\" = t,\n                 \"v\" = v)\n  \n  return(result)\n  \n}\n\n\nsys_time &lt;- format(Sys.Date(), paste0(\"%Y-%m-%d\", \"09:46:00\"))\n\n\nv1 &lt;- implied_vol_vix(read.csv(\"../securities/near_calls.csv\"), read.csv(\"../securities/near_puts.csv\"), 0.000305,\n                      list(\"sys_time\" = sys_time,\n                           \"exp_time\" = format(Sys.Date() + 25, paste0(\"%Y-%m-%d\", \"08:30:00\"))))\n\nv2 &lt;- implied_vol_vix(read.csv(\"../securities/next_calls.csv\"), read.csv(\"../securities/next_puts.csv\"), 0.000286,\n                      list(\"sys_time\" = sys_time,\n                           \"exp_time\" = format(Sys.Date() + 32, paste0(\"%Y-%m-%d\", \"15:00:00\"))))\n\n\nnt1 &lt;- v1$t * (365 * 24 * 60)\nnt2 &lt;- v2$t * (365 * 24 * 60)\nn30 &lt;- 30 * (24 * 60)\nn365 &lt;- 365 * (24 * 60)\n\n\nvix &lt;- sqrt((v1$t * v1$v * ((nt2 - n30) / (nt2 - nt1)) +\n               v2$t * v2$v * ((n30 - nt1) / (nt2 - nt1))) * (n365 / n30))\nprint(vix)\n\n[1] 0.1368582"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-yield",
    "href": "posts/securities-r/index.html#implied-yield",
    "title": "Securities",
    "section": "Implied yield",
    "text": "Implied yield\n\nyield_option &lt;- function(type, S, K, r, q, tau, sigma) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    value &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    if (type == \"call\") {\n        result &lt;- (value / S) / tau\n    } else if (type == \"put\") {\n        result &lt;- (value / K) / tau\n    }\n    \n    return(result)\n    \n}\n\n\nsigmas &lt;- seq(0.1, 0.3, by = 0.04)\ntaus &lt;- seq(20, 126, by = 20) / 252\n\n\nyield_dt &lt;- CJ(sigma = sigmas, tau = taus)\nyield_dt[ , yield := yield_option(type, S, K, r, q, tau, sigma), by = c(\"sigma\", \"tau\")]\nyield_dt[ , sigma := sigma * 100]\nyield_dt[ , tau := tau * scale[[\"periods\"]]]\n\n\nyield_plt &lt;- plot_heatmap(yield_dt, x = \"tau\", y = \"sigma\", z = \"yield\",\n                          title = \"Yield (%)\", xlab = \"Tau\", ylab = \"Sigma\")\nprint(yield_plt)"
  }
]