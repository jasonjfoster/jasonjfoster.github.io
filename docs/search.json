[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jasonjfoster.github.io",
    "section": "",
    "text": "Statistics\n\n\n\nalgorithms\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nApr 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nApr 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nApr 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nApr 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nApr 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nApr 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nApr 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nApr 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nApr 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nApr 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nApr 6, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/optim-r/index.html",
    "href": "posts/optim-r/index.html",
    "title": "Optimization",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/optim-r/index.html#random-turnover",
    "href": "posts/optim-r/index.html#random-turnover",
    "title": "Optimization",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\nrand_turnover1 &lt;- function(n_sim, n_assets, lower, upper, target) {\n    \n    rng &lt;- upper - lower\n    \n    result &lt;- rand_weights2b(n_sim, n_assets) * rng\n    result &lt;- result - rng / n_assets\n    \n    return(result)\n    \n}\n\n\nlower &lt;- -0.05\nupper &lt;- 0.05\ntarget &lt;- 0\n\n\napproach1 &lt;- rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\n\n\n\n\nrand_iterative &lt;- function(n_assets, lower, upper, target) {\n    \n    result &lt;- runif(n_assets - 1, min = lower, max = upper)\n    temp &lt;- target - sum(result)\n    \n    while (!((temp &lt;= upper) && (temp &gt;= lower))) {\n        \n        result &lt;- runif(n_assets - 1, min = lower, max = upper)\n        temp &lt;- target - sum(result)\n        \n    }\n    \n    result &lt;- append(result, temp)\n    \n    return(result)\n    \n}\n\n\nrand_turnover2 &lt;- function(n_sim, n_assets, lower, upper, target) {\n  \n    result_ls &lt;- list()\n    \n    for (i in 1:n_sim) {\n        \n        result_sim &lt;- rand_iterative(n_assets, lower, upper, target)\n        result_ls &lt;- append(result_ls, list(result_sim))\n        \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    \n    return(result)\n    \n}\n\n\napproach2 &lt;- rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-means",
    "href": "posts/optim-r/index.html#maximize-means",
    "title": "Optimization",
    "section": "Maximize means",
    "text": "Maximize means\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmax_mean_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Maximize(t(params) %*% mu)\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0,\n                 quad_form(params, sigma) &lt;= target ^ 2)\n    \n    prob &lt;- Problem(obj, cons)\n    \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\ntarget &lt;- 0.06\n\n\nparams1 &lt;- t(max_mean_optim(mu, sigma, target))\nparams1\n\n          [,1]      [,2]      [,3]         [,4]\n[1,] 0.4818745 0.3813092 0.1368158 4.544784e-07\n\n\n\nparams1 %*% mu\n\n           [,1]\n[1,] 0.04796185\n\n\n\nsqrt(params1 %*% sigma %*% t(params1))\n\n     [,1]\n[1,] 0.06\n\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\") # roll (&gt;= 1.1.7)\n# library(rolloptim)\n# \n# mu &lt;- roll_mean(returns_x_xts, 5)\n# sigma &lt;- roll_cov(returns_x_xts, width = 5)\n# \n# xx &lt;- roll_crossprod(returns_x_xts, returns_x_xts, 5)\n# xy &lt;- roll_crossprod(returns_x_xts, returns_y_xts, 5)\n# \n# roll_max_mean(mu)"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-variance",
    "href": "posts/optim-r/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmin_var_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Minimize(quad_form(params, sigma))\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0,\n                 sum(mu * params) &gt;= target)\n    \n    prob &lt;- Problem(obj, cons)\n    \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\ntarget &lt;- 0.03\n\n\nparams2 &lt;- t(min_var_optim(mu, sigma, target))\nparams2\n\n          [,1]      [,2]      [,3]       [,4]\n[1,] 0.3011723 0.2718717 0.3376254 0.08933059\n\n\n\nparams2 %*% mu\n\n     [,1]\n[1,] 0.03\n\n\n\nsqrt(params2 %*% sigma %*% t(params2))\n\n           [,1]\n[1,] 0.03819576\n\n\n\n# roll_min_var(sigma)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-utility",
    "href": "posts/optim-r/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmax_utility_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Minimize(0.5 * target * quad_form(params, sigma) - t(mu) %*% params)\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0)\n    \n    prob &lt;- Problem(obj, cons)\n        \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\nir &lt;- 0.5\ntarget &lt;- ir / 0.06 # ir / std (see Black-Litterman)\n\n\nparams3 &lt;- t(max_utility_optim(mu, sigma, target))\nparams3\n\n          [,1]      [,2]        [,3]          [,4]\n[1,] 0.7212784 0.2787216 1.89488e-24 -8.545144e-24\n\n\n\nparams3 %*% mu \n\n           [,1]\n[1,] 0.07115885\n\n\n\nsqrt(params3 %*% sigma %*% t(params3))\n\n           [,1]\n[1,] 0.08927915\n\n\n\n# roll_max_utility(mu, sigma)"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nhttps://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html\n\n\nmin_rss_optim1 &lt;- function(mu, sigma) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Minimize(0.5 * quad_form(params, sigma) - t(mu) %*% params)\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0)\n    \n    prob &lt;- Problem(obj, cons)\n        \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams4 &lt;- t(min_rss_optim1(crossprod(overlap_x_xts, overlap_y_xts), crossprod(overlap_x_xts)))\nparams4\n\n          [,1]         [,2]      [,3]        [,4]\n[1,] 0.2747327 4.021173e-23 0.7252673 2.43413e-23\n\n\n\nparams4 %*% mu \n\n          [,1]\n[1,] 0.0256603\n\n\n\nsqrt(params4 %*% sigma %*% t(params4))\n\n           [,1]\n[1,] 0.03609715\n\n\n\n# roll_min_rss(xx, xy)\n\n\nmin_rss_optim2 &lt;- function(x, y) {\n    \n    params &lt;- Variable(ncol(x))\n    \n    obj &lt;- Minimize(sum_squares(y - x %*% params))\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0)\n    \n    prob &lt;- Problem(obj, cons)\n        \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams5 &lt;- t(min_rss_optim2(coredata(overlap_x_xts), coredata(overlap_y_xts)))\nparams5\n\n          [,1]          [,2]      [,3]          [,4]\n[1,] 0.2747327 -2.270178e-20 0.7252673 -1.839983e-20\n\n\n\nparams5 %*% mu \n\n          [,1]\n[1,] 0.0256603\n\n\n\nsqrt(params5 %*% sigma %*% t(params5))\n\n           [,1]\n[1,] 0.03609715\n\n\n\nround(data.frame(\"max_pnl\" = t(params1) * 100,\n                 \"min_risk\" = t(params2) * 100,\n                 \"max_utility\" = t(params3) * 100,\n                 \"min_rss1\" = t(params4) * 100,\n                 \"min_rss2\" = t(params5) * 100), 2)\n\n  max_pnl min_risk max_utility min_rss1 min_rss2\n1   48.19    30.12       72.13    27.47    27.47\n2   38.13    27.19       27.87     0.00     0.00\n3   13.68    33.76        0.00    72.53    72.53\n4    0.00     8.93        0.00     0.00     0.00"
  },
  {
    "objectID": "posts/optim-py/index.html",
    "href": "posts/optim-py/index.html",
    "title": "Optimization",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/optim-py/index.html#random-turnover",
    "href": "posts/optim-py/index.html#random-turnover",
    "title": "Optimization",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights2b(n_sim, n_assets) * rng\n    result = result - rng / n_assets\n    \n    return result\n\n\nlower = -0.05\nupper = 0.05\ntarget = 0\n\n\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\n\n\n\n\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n    temp = target - sum(result)\n    \n    while not ((temp &lt;= upper) and (temp &gt;= lower)):\n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n    result = np.append(result, temp)\n    \n    return result\n\n\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n  \n    result_ls = []\n    \n    for i in range(n_sim):\n      \n      result_sim = rand_iterative(n_assets, lower, upper, target)\n      result_ls.append(result_sim)\n      \n    result = pd.DataFrame(result_ls)\n    \n    return result\n\n\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-mean",
    "href": "posts/optim-py/index.html#maximize-mean",
    "title": "Optimization",
    "section": "Maximize mean",
    "text": "Maximize mean\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ndef max_mean_optim(mu, sigma, target):\n    \n    params = cp.Variable(len(mu))\n    \n    obj = cp.Maximize(params.T @ mu)\n    \n    cons = [cp.sum(params) == 1, params &gt;= 0,\n            cp.quad_form(params, sigma) &lt;= target ** 2]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n\n\ntarget = 0.06\n\n\nparams1 = max_mean_optim(mu, sigma, target)\nparams1\n\narray([4.31211361e-01, 2.11082389e-01, 2.44018500e-06, 3.57703809e-01])\n\n\n\nnp.dot(mu, params1)\n\nnp.float64(0.04387993526045541)\n\n\n\nnp.sqrt(np.dot(params1, np.dot(sigma, params1)))\n\nnp.float64(0.059999998673826355)"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-variance",
    "href": "posts/optim-py/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ndef min_var_optim(mu, sigma, target):\n    \n    params = cp.Variable(len(mu))\n    \n    obj = cp.Minimize(cp.quad_form(params, sigma))\n    \n    cons = [cp.sum(params) == 1, params &gt;= 0,\n            params.T @ mu &gt;= target]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n\n\ntarget = 0.03\n\n\nparams2 = min_var_optim(mu, sigma, target)\nparams2\n\narray([ 2.83586236e-01,  1.48145971e-01, -2.59839572e-20,  5.68267794e-01])\n\n\n\nnp.dot(mu, params2)\n\nnp.float64(0.029999999999999992)\n\n\n\nnp.sqrt(np.dot(params2, np.dot(sigma, params2))) \n\nnp.float64(0.04156129001157303)"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-utility",
    "href": "posts/optim-py/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ndef max_utility_optim(mu, sigma, target):\n    \n    params = cp.Variable(len(mu))\n    \n    obj = cp.Minimize(0.5 * target * cp.quad_form(params, sigma) - params.T @ mu)\n    \n    cons = [cp.sum(params) == 1, params &gt;= 0]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n\n\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\n\n\nparams3 = max_utility_optim(mu, sigma, target)\nparams3\n\narray([6.65831063e-01, 3.11106711e-01, 1.73830812e-23, 2.30622261e-02])\n\n\n\nnp.dot(mu, params3)\n\nnp.float64(0.06593926158420194)\n\n\n\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n\nnp.float64(0.08951308848063624)"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nhttps://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html\n\n\ndef min_rss_optim1(mu, sigma):\n    \n    params = cp.Variable(len(mu))\n    \n    obj = cp.Minimize(0.5 * cp.quad_form(params, sigma) - params.T @ mu)\n    \n    cons = [cp.sum(params) == 1, params &gt;= 0]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n\n\nparams4 = min_rss_optim1(np.dot(overlap_x_df.T.values, overlap_y_df.values),\n                         np.dot(overlap_x_df.T.values, overlap_x_df.values))\nparams4\n\narray([2.79223870e-01, 8.10663395e-24, 7.20776130e-01, 1.15062333e-23])\n\n\n\nnp.dot(mu, params4)\n\nnp.float64(0.026104599832123966)\n\n\n\nnp.sqrt(np.matmul(np.transpose(params4), np.matmul(sigma, params4)))\n\nnp.float64(0.03913103713942823)\n\n\n\ndef min_rss_optim2(x, y):\n    \n    params = cp.Variable(x.shape[1])\n    \n    obj = cp.Minimize(cp.sum_squares(y - x @ params))\n    \n    cons = [cp.sum(params) == 1, params &gt;= 0]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n\n\nparams5 = min_rss_optim2(overlap_x_df.values, overlap_y_df.iloc[:, 0].values)\nparams5\n\narray([ 2.79223870e-01, -1.61222438e-20,  7.20776130e-01, -1.43092675e-20])\n\n\n\nnp.dot(mu, params5)\n\nnp.float64(0.02610459983212409)\n\n\n\nnp.sqrt(np.matmul(np.transpose(params5), np.matmul(sigma, params5)))\n\nnp.float64(0.0391310371394284)\n\n\n\npd.DataFrame({\n  \"max_pnl\": params1 * 100,\n  \"min_risk\": params2 * 100,\n  \"max_utility\": params3 * 100,\n  \"min_rss1\": params4 * 100,\n  \"min_rss2\": params5 * 100\n}).round(2)\n\n   max_pnl  min_risk  max_utility  min_rss1  min_rss2\n0    43.12     28.36        66.58     27.92     27.92\n1    21.11     14.81        31.11      0.00     -0.00\n2     0.00     -0.00         0.00     72.08     72.08\n3    35.77     56.83         2.31      0.00     -0.00"
  },
  {
    "objectID": "posts/eigen-r/index.html",
    "href": "posts/eigen-r/index.html",
    "title": "Eigen",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\n\n\nDecomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\neigen_decomp &lt;- function(x, comps) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV[[\"values\"]][1:comps]\n    V &lt;- LV[[\"vectors\"]][ , 1:comps]\n    \n    result &lt;- V %*% sweep(t(V), 1, L, \"*\")\n    \n    return(result)\n    \n}\n\n\ncomps &lt;- 1\n\n\neigen_decomp(overlap_xts, comps) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  0.0323363613 -3.740203e-03 -1.051094e-04  2.700943e-03\n[2,] -0.0037402030  4.326126e-04  1.215753e-05 -3.124061e-04\n[3,] -0.0001051094  1.215753e-05  3.416581e-07 -8.779418e-06\n[4,]  0.0027009432 -3.124061e-04 -8.779418e-06  2.256003e-04\n\n\n\n# cov(overlap_xts) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\nvariance_explained &lt;- function(x) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV[[\"values\"]]\n    \n    result &lt;- cumsum(L) / sum(L)\n    \n    return(result)\n    \n}\n\n\nvariance_explained(overlap_xts)\n\n[1] 0.8876743 0.9925436 0.9983026 1.0000000\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n\\]\n\nsimilarity &lt;- function(V, V0) {\n    \n    n_cols_v &lt;- ncol(V)\n    n_cols_v0 &lt;- ncol(V0)\n    result &lt;- matrix(0, nrow = n_cols_v, ncol = n_cols_v0)\n    \n    for (i in 1:n_cols_v) {\n      for (j in 1:n_cols_v0) {\n        result[i, j] &lt;- crossprod(V[ , i], V0[ , j]) /\n            sqrt(crossprod(V[ , i]) * crossprod(V0[ , j]))\n      }\n    }\n    \n    return(result)\n    \n}\n\n\nroll_eigen1 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        \n        LV &lt;- eigen(cov(x[idx, ]))\n        V &lt;- LV[[\"vectors\"]]\n        \n        result_ls &lt;- append(result_ls, list(V[ , comp]))\n        \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\ncomp &lt;- 1\n\n\nraw_df &lt;- roll_eigen1(overlap_xts, width, comp)\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolleigen\") # roll (&gt;= 1.1.7)\n# library(rolleigen)\n# raw_df &lt;- roll_eigen(overlap_xts, width, order = TRUE)[[\"vectors\"]][ , comp, ]\n# raw_df &lt;- xts(t(raw_df), index(overlap_xts))\n# colnames(raw_df) &lt;- colnames(overlap_xts)\n\n\n\n\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\nroll_eigen2 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    V_ls &lt;- list()\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        \n        LV &lt;- eigen(cov(x[idx, ]))\n        V &lt;- LV[[\"vectors\"]]\n                \n        if (i &gt; width) {\n          \n            # cosine &lt;- crossprod(V, V_ls[[length(V_ls)]])\n            cosine &lt;- similarity(V, V_ls[[length(V_ls)]])\n            order &lt;- apply(abs(cosine), 1, which.max)\n            V &lt;- t(sign(diag(cosine[ , order])) * t(V[ , order]))\n            \n        }\n        \n        V_ls &lt;- append(V_ls, list(V))\n        result_ls &lt;- append(result_ls, list(V[ , comp]))\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\nclean_df &lt;- roll_eigen2(overlap_xts, width, comp)\n\n\n\n\n\n\n\n\n\n\n\n\nImplied shocks\nProduct of the \\(n\\)th eigenvector and square root of the \\(n\\)th eigenvalue:\n\nroll_shocks &lt;- function(x, width, comp) {\n  \n    n_rows &lt;- nrow(x)\n    V_ls &lt;- list()\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        \n        LV &lt;- eigen(cov(x[idx, ]))\n        L &lt;- LV[[\"values\"]]\n        V &lt;- LV[[\"vectors\"]]\n        \n        if (length(V_ls) &gt; 1) {\n          \n            # cosine &lt;- crossprod(V, V_ls[[length(V_ls)]])\n            cosine &lt;- similarity(V, V_ls[[length(V_ls)]])\n            order &lt;- apply(abs(cosine), 1, which.max)\n            L &lt;- L[order]\n            V &lt;- t(sign(diag(cosine[ , order])) * t(V[ , order]))\n          \n        }\n        \n        shocks &lt;- sqrt(L[comp]) * V[ , comp]\n        V_ls &lt;- append(V_ls, list(V))\n        result_ls &lt;- append(result_ls, list(shocks))\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\nshocks_xts &lt;- roll_shocks(overlap_xts, width, comp) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])"
  },
  {
    "objectID": "posts/eigen-py/index.html",
    "href": "posts/eigen-py/index.html",
    "title": "Eigen",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\n\n\nDecomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\ndef eigen(x):\n  \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    V = V[:, idx]\n    \n    result = {\n        \"values\": L,\n        \"vectors\": V\n    }\n    \n    return result\n\n\ndef eigen_decomp(x, comps):\n    \n    LV = eigen(x)\n    L = LV[\"values\"][:comps]\n    V = LV[\"vectors\"][:, :comps]\n    \n    result = np.dot(V, np.multiply(L, V.T))\n    \n    return result\n\n\ncomps = 1\n\n\neigen_decomp(overlap_df, comps) * scale[\"periods\"] * scale[\"overlap\"]\n\narray([[ 3.29798288e-02, -3.68584726e-03, -1.02264338e-04,\n         2.68616394e-03],\n       [-3.68584726e-03,  4.11932703e-04,  1.14291294e-05,\n        -3.00207441e-04],\n       [-1.02264338e-04,  1.14291294e-05,  3.17102763e-07,\n        -8.32929665e-06],\n       [ 2.68616394e-03, -3.00207441e-04, -8.32929665e-06,\n         2.18784542e-04]])\n\n\n\n# np.cov(overlap_df.T) * scale[\"periods\"] * scale[\"overlap\"]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\ndef variance_explained(x):\n    \n    LV = eigen(x)\n    L = LV[\"values\"]\n    \n    result = L.cumsum() / L.sum()\n    \n    return result\n\n\nvariance_explained(overlap_df)\n\narray([0.88555914, 0.9926834 , 0.99830918, 1.        ])\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n\\]\n\ndef similarity(V, V0):\n  \n    n_cols_v = V.shape[1]\n    n_cols_v0 = V0.shape[1]\n    result = np.zeros((n_cols_v, n_cols_v0))\n    \n    for i in range(n_cols_v):\n        for j in range(n_cols_v0):\n            result[i, j] = np.dot(V[:, i], V0[:, j]) / \\\n                np.sqrt(np.dot(V[:, i], V[:, i]) * np.dot(V0[:, j], V0[:, j]))\n    \n    return result\n\n\ndef roll_eigen1(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        \n        LV = eigen(x.iloc[idx])\n        V = LV[\"vectors\"]\n        \n        result_ls.append(V[:, comp - 1])\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df  \n\n\ncomp = 1\n\n\nraw_df = roll_eigen1(overlap_df, width, comp)\n\n\n\n\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\ndef roll_eigen2(x, width, comp):\n    \n    n_rows = len(x)\n    V_ls = []\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        \n        LV = eigen(x.iloc[idx])\n        V = LV[\"vectors\"]\n        \n        if i &gt; width - 1:\n            \n            # cosine = np.dot(V.T, V_ls[-1])\n            cosine = similarity(V.T, V_ls[-1])\n            order = np.argmax(np.abs(cosine), axis = 1)\n            V = np.sign(np.diag(cosine[:, order])) * V[:, order]\n            \n        V_ls.append(V)\n        result_ls.append(V[:, comp - 1])\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df\n\n\nclean_df = roll_eigen2(overlap_df, width, comp)\n\n\n\n\n\n\n\n\n\n\n\n\nImplied shocks\nProduct of the \\(n\\)th eigenvector and square root of the \\(n\\)th eigenvalue:\n\ndef roll_shocks(x, width, comp):\n    \n    n_rows = len(x)\n    V_ls = []\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        \n        LV = eigen(x.iloc[idx])\n        L = LV[\"values\"]\n        V = LV[\"vectors\"]\n        \n        if len(V_ls) &gt; 1:\n            \n            # cosine = np.dot(V.T, V_ls[-1])\n            cosine = similarity(V.T, V_ls[-1])\n            order = np.argmax(np.abs(cosine), axis = 1)\n            L = L[order]\n            V = np.sign(np.diag(cosine[:, order])) * V[:, order]\n        \n        shocks = np.sqrt(L[comp - 1]) * V[:, comp - 1]\n        V_ls.append(V)\n        result_ls.append(shocks)\n        \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df\n\n\nshocks_df = roll_shocks(overlap_df, width, comp) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])"
  },
  {
    "objectID": "posts/markets-py/index.html",
    "href": "posts/markets-py/index.html",
    "title": "Markets",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/markets-py/index.html#expected-value",
    "href": "posts/markets-py/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-py/index.html#variance",
    "href": "posts/markets-py/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\ndef sd(x):\n    \n    n_rows = sum(~np.isnan(x))\n        \n    if n_rows &gt; 1:\n        result = np.sqrt(np.nansum(x ** 2) / (n_rows - 1))\n    else:\n        result = np.nan\n        \n    return result\n\n\n# volatility scale only\nscore_df = (momentum_df / momentum_df.rolling(width, min_periods = 1).apply(sd, raw = False)).dropna()\n\n\n# overall_df = score_df.mean(axis = 1)\n# overall_df = overall_df / overall_df.rolling(width, min_periods = 1).apply(risk, raw = False)\n\n\n# score_df.insert(loc = 0, column = \"Overall\", value = overall_df)\n# score_df = score_df.dropna()"
  },
  {
    "objectID": "posts/markets-py/index.html#interquartile-range",
    "href": "posts/markets-py/index.html#interquartile-range",
    "title": "Markets",
    "section": "Interquartile range",
    "text": "Interquartile range\nOutliers are defined as the regression residuals that fall below \\(Q_{1}−1.5\\times IQR\\) or above \\(Q_{3}+1.5\\times IQR\\):\n\nhttps://stats.stackexchange.com/a/1153\nhttps://stats.stackexchange.com/a/108951\nhttps://robjhyndman.com/hyndsight/tsoutliers/\n\n\ndef outliers(z):\n  \n    n_cols = z.shape[1]\n    result_ls = []\n\n    for j in range(n_cols):\n      \n        y = z.iloc[:, j]\n\n        if (n_cols == 0):\n            x = sm.add_constant(range(len(y)))\n        else:\n            x = sm.add_constant(z.drop(z.columns[j], axis = 1))\n\n        coef = sm.WLS(y, x).fit().params\n        predict = coef.iloc[0] + np.dot(x.iloc[:, 1:], coef[1:])\n        resid = y - predict\n\n        lower = resid.quantile(0.25)\n        upper = resid.quantile(0.75)\n        iqr = upper - lower\n\n        total = y[(resid &lt; lower - 1.5 * iqr) | (resid &gt; upper + 1.5 * iqr)]\n        \n        total = pd.DataFrame({\"date\": total.index, \"symbol\": total.name, \"values\": total})\n        result_ls.append(total)\n\n    result = pd.concat(result_ls, ignore_index = True)\n    result = result.pivot_table(index = \"date\", columns = \"symbol\", values = \"values\")\n\n    return result\n\n\noutliers_df = outliers(score_df)"
  },
  {
    "objectID": "posts/markets-py/index.html#contour-ellipsoid",
    "href": "posts/markets-py/index.html#contour-ellipsoid",
    "title": "Markets",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid"
  },
  {
    "objectID": "posts/risk-py/index.html",
    "href": "posts/risk-py/index.html",
    "title": "Risk",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\nintercept = True"
  },
  {
    "objectID": "posts/risk-py/index.html#ordinary-least-squares",
    "href": "posts/risk-py/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\ndef lm_coef(x, y, weights, intercept):\n    \n    if (intercept): x = sm.add_constant(x)\n        \n    result = np.dot(np.linalg.inv(np.dot(x.T, np.multiply(weights, x))),\n                    np.dot(x.T, np.multiply(weights, y)))\n    \n    return np.ravel(result)\n\n\nlm_coef(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([ 1.63031573e-04,  2.08407851e-01, -1.19139710e-01,  2.90456280e+00,\n        1.10228919e+00])\n\n\n\nif (intercept): overlap_x_df = sm.add_constant(overlap_x_df)\n    \nfit = sm.WLS(overlap_y_df, overlap_x_df, weights = weights).fit()\n\nif (intercept): overlap_x_df = overlap_x_df.iloc[:, 1:]\n\nnp.array(fit.params)\n\narray([ 1.63031573e-04,  2.08407851e-01, -1.19139710e-01,  2.90456280e+00,\n        1.10228919e+00])\n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\ndef lm_rsq(x, y, weights, intercept):\n            \n    coef = np.matrix(lm_coef(x, y, weights, intercept))\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n    result = np.dot(coef, np.dot(np.dot(x.T, np.multiply(weights, x)), coef.T)) / \\\n        np.dot(y.T, np.multiply(weights, y))\n    \n    return result.item()\n\n\nlm_rsq(overlap_x_df, overlap_y_df, weights, intercept)\n\n0.7173255446186038\n\n\n\nfit.rsquared\n\nnp.float64(0.7173255446186044)\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\ndef lm_se(x, y, weights, intercept):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n        df_resid = n_rows - n_cols - 1 \n        \n    else:\n        df_resid = n_rows - n_cols        \n    \n    var_y = np.dot(y.T, np.multiply(weights, y))\n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    result = np.sqrt(var_resid * np.linalg.inv(np.dot(x.T, np.multiply(weights, x))).diagonal())\n    \n    return np.ravel(result)\n\n\nlm_se(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([5.69616557e-05, 1.96667202e-02, 4.43622346e-02, 2.89175173e-01,\n       2.87849110e-01])\n\n\n\nnp.array(fit.bse)\n\narray([5.69616557e-05, 1.96667202e-02, 4.43622346e-02, 2.89175173e-01,\n       2.87849110e-01])\n\n\n\n\nShapley values\n\\[\nR^{2}_{i}=\\sum_{S\\subseteq N\\setminus\\{i\\}}{\\frac{|S|!\\;(n-|S|-1)!}{n!}}(R^{2}(S\\cup\\{i\\})-R^{2}(S))\n\\]\n\nhttps://real-statistics.com/multiple-regression/shapley-owen-decomposition/\n\n\ndef lm_shap(x, y, weights, intercept):\n  \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    n_combn = 2 ** n_cols\n    n_vec = np.zeros(n_combn)\n    ix_mat = np.zeros((n_cols, n_combn))\n    rsq = np.zeros(n_combn)\n    result = np.zeros(n_cols)\n    \n    # number of binary combinations\n    for k in range(n_combn):\n      \n        n = 0\n        n_size = k\n        \n        # find the binary combination\n        for j in range(n_cols):\n          \n            if n_size % 2 == 0:\n              \n                n += 1\n                \n                ix_mat[j, k] = j + 1\n                \n            n_size //= 2\n        \n        n_vec[k] = n\n        \n        if n &gt; 0:\n          \n            ix_subset = np.where(ix_mat[:, k] != 0)[0]\n            x_subset = x.iloc[:, ix_subset]\n            \n            rsq[k] = lm_rsq(x_subset, y, weights, intercept)\n\n    # calculate the exact Shapley value for r-squared\n    for j in range(n_cols):\n      \n        ix_pos = np.where(ix_mat[j, :] != 0)[0]\n        ix_neg = np.where(ix_mat[j, :] == 0)[0]\n        ix_n = n_vec[ix_neg]\n        rsq_diff = rsq[ix_pos] - rsq[ix_neg]\n\n        for k in range(int(n_combn / 2)):\n          \n            s = int(ix_n[k])\n            weight = math.factorial(s) * math.factorial(n_cols - s - 1) \\\n                / math.factorial(n_cols)\n            result[j] += weight * rsq_diff[k]\n\n    return result\n\n\nlm_shap(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([0.30421791, 0.09151911, 0.23433115, 0.08725737])"
  },
  {
    "objectID": "posts/risk-py/index.html#principal-component-regression",
    "href": "posts/risk-py/index.html#principal-component-regression",
    "title": "Risk",
    "section": "Principal component regression",
    "text": "Principal component regression\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\n\n\ncomps = 1\n\n\nCoefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\ndef pcr_coef(x, y, comps):\n    \n    x = x - np.average(x, axis = 0)\n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    V = V[:, idx]\n    \n    W = np.dot(x, V)\n    gamma = np.dot(np.dot(np.linalg.inv(np.dot(W.T, W)), W.T), y)\n    \n    result = np.dot(V[:, :comps], gamma[:comps])\n    \n    return np.ravel(result)\n\n\nscale_x_df = (overlap_x_df - np.average(overlap_x_df, axis = 0)) \\\n    / np.std(overlap_x_df, axis = 0, ddof = 1)\n\n\npcr_coef(scale_x_df, overlap_y_df, comps)\n\narray([-1.10386321e-05, -3.55058217e-04,  3.90631611e-04, -1.74394824e-04])\n\n\n\npcr_coef(overlap_x_df, overlap_y_df, comps)\n\narray([ 0.29296845, -0.01373458,  0.00347208,  0.0121573 ])\n\n\n\npca = PCA(n_components = len(factors))\npca_x_df = pca.fit_transform(scale_x_df)\n\nfit = LinearRegression(fit_intercept = False).fit(pca_x_df, overlap_y_df)\n\ngamma = fit.coef_\nnp.dot(pca.components_.T[:, :comps], gamma.T[:comps]).ravel()\n\narray([-1.10386321e-05, -3.55058217e-04,  3.90631611e-04, -1.74394824e-04])\n\n\n\n\nR-squared\n\ndef pcr_rsq(x, y, comps):\n    \n    coef = np.matrix(pcr_coef(x, y, comps))\n    \n    x = x - np.average(x, axis = 0)\n    y = y - np.average(y, axis = 0)\n    \n    result = np.dot(np.dot(coef, np.dot(x.T, x)), coef.T) / np.dot(y.T, y)\n    \n    return result.item()\n\n\npcr_rsq(scale_x_df, overlap_y_df, comps)\n\n0.19270846187239188\n\n\n\npcr_rsq(overlap_x_df, overlap_y_df, comps)\n\n0.47041343905230315\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\ndef pcr_se(x, y, comps):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = pcr_rsq(x, y, comps)\n    \n    y = y - np.average(y, axis = 0)\n    \n    df_resid = n_rows - n_cols - 1\n    \n    var_y = np.dot(y.T, y)   \n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    # uses statsmodels for illustrative purposes\n    pca = sm.multivariate.PCA(x, standardize = False, demean = True)\n    L = pca.eigenvals[:comps]\n    V = pca.eigenvecs.iloc[:, :comps]\n    \n    result = np.sqrt(var_resid * np.dot(V, np.dot(np.diag(1 / L), V.T)).diagonal())\n    \n    return np.ravel(result)\n\n\npcr_se(scale_x_df, overlap_y_df, comps)\n\narray([1.43757858e-06, 4.62397949e-05, 5.08725744e-05, 2.27117146e-05])\n\n\n\npcr_se(overlap_x_df, overlap_y_df, comps)\n\narray([0.01977885, 0.00092725, 0.00023441, 0.00082076])"
  },
  {
    "objectID": "posts/risk-py/index.html#partial-least-squares",
    "href": "posts/risk-py/index.html#partial-least-squares",
    "title": "Risk",
    "section": "Partial least squares",
    "text": "Partial least squares"
  },
  {
    "objectID": "posts/risk-py/index.html#standalone-risk",
    "href": "posts/risk-py/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\ndef cov_wt(x, weights, center):\n    \n    sum_w = sum(weights)\n    sumsq_w = sum(np.power(weights, 2))\n    \n    if (center):\n    \n        x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n    \n    result = np.dot(x.T, np.multiply(weights, x)) / (sum_w - sumsq_w / sum_w)\n    \n    return result\n\n\ndef lm_sar(x, y, weights, intercept):\n    \n    coef = lm_coef(x, y, weights, intercept)\n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept): x = sm.add_constant(x)\n    \n    # sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n    #                aweights = weights.reshape(-1))\n    sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n    sar = np.multiply(np.power(coef, 2).T, sigma[:-1, :-1].diagonal())\n    sar_eps = (1 - rsq) * sigma[-1, -1]\n    \n    result = np.sqrt(np.concatenate((np.matrix(sigma[-1, -1]),\n                                     np.matrix(sar),\n                                     np.matrix(sar_eps)), axis = 1))\n    \n    return np.ravel(result)\n\n\nlm_sar(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])\n\narray([0.05754123, 0.        , 0.02796634, 0.00659472, 0.02758507,\n       0.01051104, 0.03059302])"
  },
  {
    "objectID": "posts/risk-py/index.html#risk-contribution",
    "href": "posts/risk-py/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttps://bookdown.org/compfinezbook/introcompfinr/Portfolio-risk-reports.html\n\n\ndef lm_mcr(x, y, weights, intercept):\n    \n    coef = np.matrix(lm_coef(x, y, weights, intercept)).T\n    rsq = lm_rsq(x, y, weights, intercept)\n        \n    if (intercept): x = sm.add_constant(x)\n    \n#     sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n#                    aweights = weights.reshape(-1))\n    sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n    mcr = np.multiply(coef, np.dot(sigma[:-1, :-1], coef)) / np.sqrt(sigma[-1, -1])\n    mcr_eps = np.sqrt(sigma[-1, -1]) - sum(mcr)\n    \n    result = np.concatenate((np.sqrt(np.matrix(sigma[-1, -1])),\n                             np.matrix(mcr).T,\n                             np.matrix(mcr_eps)), axis = 1)\n    \n    return np.ravel(result)\n\n\nlm_mcr(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])\n\narray([0.05754123, 0.        , 0.0189779 , 0.00287695, 0.01584286,\n       0.00357808, 0.01626543])"
  },
  {
    "objectID": "posts/risk-py/index.html#implied-shocks",
    "href": "posts/risk-py/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\ndef implied_shocks(shocks, x, z, weights):\n\n    beta = np.linalg.lstsq(np.multiply(weights, z), np.multiply(weights, x), rcond = None)[0]\n                     \n    result = np.dot(shocks, beta)\n    \n    return np.ravel(result)\n\n\nshocks = np.array([-0.1, 0.1])\noverlap_z_df = overlap_x_df.iloc[:, [0, 1]]\n\n\nimplied_shocks(shocks, overlap_x_df, overlap_z_df, weights)\n\narray([-0.1       ,  0.1       , -0.01046187, -0.00305239])"
  },
  {
    "objectID": "posts/risk-py/index.html#stress-pl",
    "href": "posts/risk-py/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\ndef pnl_stress(shocks, x, y, z, weights, intercept):\n    \n    coef = lm_coef(x, y, weights, intercept)\n    \n    if (intercept): x = sm.add_constant(x)\n    \n    result = np.multiply(coef.T, implied_shocks(shocks, x, z, weights))\n    \n    return np.ravel(result)\n\n\npnl_stress(shocks, overlap_x_df, overlap_y_df, overlap_z_df, weights, intercept)\n\narray([-0.00045368, -0.02084079, -0.01191397, -0.03038717, -0.00336462])"
  },
  {
    "objectID": "posts/markets-r/index.html",
    "href": "posts/markets-r/index.html",
    "title": "Markets",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/markets-r/index.html#expected-value",
    "href": "posts/markets-r/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-r/index.html#variance",
    "href": "posts/markets-r/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\n# volatility scale only\nscore_xts &lt;- na.omit(momentum_xts / roll_sd(momentum_xts, width, center = FALSE, min_obs = 1))\n\n\n# overall_xts &lt;- xts(rowMeans(score_xts), index(score_xts))\n# overall_xts &lt;- overall_xts / roll_sd(overall_xts, width, center = FALSE, min_obs = 1)\n# colnames(overall_xts) &lt;- \"Overall\"\n\n\n# score_xts &lt;- na.omit(merge(overall_xts, score_xts))"
  },
  {
    "objectID": "posts/markets-r/index.html#interquartile-range",
    "href": "posts/markets-r/index.html#interquartile-range",
    "title": "Markets",
    "section": "Interquartile range",
    "text": "Interquartile range\nOutliers are defined as the regression residuals that fall below \\(Q_{1}−1.5\\times IQR\\) or above \\(Q_{3}+1.5\\times IQR\\):\n\nhttps://stats.stackexchange.com/a/1153\nhttps://stats.stackexchange.com/a/108951\nhttps://robjhyndman.com/hyndsight/tsoutliers/\n\n\noutliers &lt;- function(z) {\n  \n  n_cols &lt;- ncol(z)\n  result_ls &lt;- list()\n  \n  for (j in 1:n_cols) {\n    \n    y &lt;- z[ , j]\n    \n    if (n_cols == 1) {\n      x &lt;- 1:length(y)\n    } else {\n      x &lt;- cbind(1:length(y), z[ , -j])\n    }\n    \n    coef &lt;- coef(lm(y ~ x))\n    predict &lt;- coef[1] + x %*% as.matrix(coef[-1])\n    resid &lt;- y - predict\n    \n    lower &lt;- quantile(resid, prob = 0.25)\n    upper &lt;- quantile(resid, prob = 0.75)\n    iqr &lt;- upper - lower\n    \n    total &lt;- y[(resid &lt; lower - 1.5 * iqr) | (resid &gt; upper + 1.5 * iqr)]\n    \n    result_ls &lt;- append(result_ls, list(total))\n    \n  }\n  \n  result &lt;- do.call(merge, result_ls)\n  \n  return(result)\n  \n}\n\n\noutliers_xts &lt;- outliers(score_xts)"
  },
  {
    "objectID": "posts/markets-r/index.html#contour-ellipsoid",
    "href": "posts/markets-r/index.html#contour-ellipsoid",
    "title": "Markets",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid"
  },
  {
    "objectID": "posts/risk-r/index.html",
    "href": "posts/risk-r/index.html",
    "title": "Risk",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\ntickers &lt;- \"BAICX\" # fund inception date is \"2011-11-28\"\nintercept &lt;- TRUE"
  },
  {
    "objectID": "posts/risk-r/index.html#ordinary-least-squares",
    "href": "posts/risk-r/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\nlm_coef &lt;- function(x, y, weights, intercept) {\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    result &lt;- solve(crossprod(x, diag(weights)) %*% x) %*% crossprod(x, diag(weights) %*% y)\n    \n    return(result)\n    \n}\n\n\nt(lm_coef(overlap_x_xts, overlap_y_xts, weights, intercept))\n\n      (Intercept)    xSP500 xDTWEXAFEGS   xDGS10 xBAMLH0A0HYM2\nBAICX 7.30012e-05 0.1913491   -0.145976 3.230495      1.220434\n\n\n\nif (intercept) {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts, weights = weights)\n} else {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts - 1, weights = weights)\n}\n    \ncoef(fit)\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n             0.0000730012              0.1913490751             -0.1459759507 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             3.2304951748              1.2204339943 \n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\nlm_rsq &lt;- function(x, y, weights, intercept) {\n        \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        x &lt;- sweep(x, 2, apply(x, 2, weighted.mean, w = weights), \"-\")\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n    }\n    \n    result &lt;- (t(coef) %*% (crossprod(x, diag(weights)) %*% x) %*% coef) / (crossprod(y, diag(weights)) %*% y)\n    \n    return(result)\n    \n}\n\n\nlm_rsq(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n          BAICX\nBAICX 0.7576384\n\n\n\nsummary(fit)$r.squared\n\n[1] 0.7576384\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\nlm_se &lt;- function(x, y, weights, intercept) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n        df_resid &lt;- n_rows - n_cols - 1\n        \n    } else {\n        df_resid &lt;- n_rows - n_cols\n    }\n    \n    var_y &lt;- crossprod(y, diag(weights)) %*% y\n    var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n    \n    result &lt;- sqrt(var_resid * diag(solve(crossprod(x, diag(weights)) %*% x)))\n    \n    return(result)\n}\n\n\nlm_se(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n  (Intercept)        xSP500   xDTWEXAFEGS        xDGS10 xBAMLH0A0HYM2 \n 5.121335e-05  1.940601e-02  4.495683e-02  2.662767e-01  2.582900e-01 \n\n\n\ncoef(summary(fit))[ , \"Std. Error\"]\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n             5.121335e-05              1.940601e-02              4.495683e-02 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             2.662767e-01              2.582900e-01 \n\n\n\n\nShapley values\n\\[\nR^{2}_{i}=\\sum_{S\\subseteq N\\setminus\\{i\\}}{\\frac{|S|!\\;(n-|S|-1)!}{n!}}(R^{2}(S\\cup\\{i\\})-R^{2}(S))\n\\]\n\nhttps://real-statistics.com/multiple-regression/shapley-owen-decomposition/\n\n\nlm_shap &lt;- function(x, y, weights, intercept) {\n  \n  n_rows &lt;- nrow(x)\n  n_cols &lt;- ncol(x)\n  n_combn &lt;- 2 ^ n_cols\n  n_vec &lt;- array(0, n_combn)\n  ix_mat &lt;- matrix(0, nrow = n_cols, ncol = n_combn)\n  rsq &lt;- array(0, n_combn)\n  result &lt;- array(0, n_cols)\n  \n  # number of binary combinations\n  for (k in 1:n_combn) {\n    \n    n &lt;- 0\n    n_size &lt;- k - 1\n    \n    # find the binary combination\n    for (j in 1:n_cols) {\n      \n      if (n_size %% 2 == 0) {\n        \n        n &lt;- n + 1\n        \n        ix_mat[j, k] = j - 1 + 1\n        \n      }\n      \n      n_size &lt;- n_size %/% 2\n      \n    }\n    \n    n_vec[k] &lt;- n\n    \n    if (n &gt; 0) {\n      \n      ix_subset&lt;- which(ix_mat[ , k] != 0)\n      x_subset &lt;- x[ , ix_subset]\n      \n      rsq[k] &lt;- lm_rsq(x_subset, y, weights, intercept)\n\n    }\n    \n  }\n\n  # calculate the exact Shapley value for r-squared\n  for (j in 1:n_cols) {\n\n    ix_pos &lt;- which(ix_mat[j, ] != 0)\n    ix_neg &lt;- which(ix_mat[j, ] == 0)\n    ix_n &lt;- n_vec[ix_neg]\n    rsq_diff &lt;- rsq[ix_pos] - rsq[ix_neg]\n\n    for (k in 1:(n_combn / 2)) {\n\n      s &lt;- ix_n[k]\n      weight &lt;- factorial(s) * factorial(n_cols - s - 1) / factorial(n_cols)\n      result[j] &lt;- result[j] + weight * rsq_diff[k]\n\n    }\n    \n  }\n\n  return(result)\n  \n}\n\n\nlm_shap(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n[1] 0.26603748 0.13080190 0.26329100 0.09750806"
  },
  {
    "objectID": "posts/risk-r/index.html#principal-component-regression",
    "href": "posts/risk-r/index.html#principal-component-regression",
    "title": "Risk",
    "section": "Principal component regression",
    "text": "Principal component regression\n\nlibrary(pls)\n\n\ncomps &lt;- 1\n\n\nCoefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\npcr_coef &lt;- function(x, y, comps) {\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    LV &lt;- eigen(cov(x))\n    V &lt;- LV[[\"vectors\"]]\n    \n    W &lt;- x %*% V\n    gamma &lt;- solve(crossprod(W)) %*% (crossprod(W, y))\n    \n    result &lt;- V[ , 1:comps] %*% as.matrix(gamma[1:comps])\n    \n    return(result)\n    \n}\n\n\nscale_x_xts &lt;- scale(overlap_x_xts)\n\n\nt(pcr_coef(scale_x_xts, overlap_y_xts, comps))\n\n             [,1]          [,2]         [,3]         [,4]\n[1,] 0.0006077055 -0.0004782704 0.0003097628 0.0004990806\n\n\n\nt(pcr_coef(overlap_x_xts, overlap_y_xts, comps))\n\n          [,1]        [,2]        [,3]       [,4]\n[1,] 0.2976711 -0.02689162 0.003195201 0.01504472\n\n\n\nfit &lt;- pcr(reformulate(termlabels = \".\", response = tickers), \n           data = merge(scale_x_xts, overlap_y_xts), ncomp = comps)\ncoef(fit)[ , , 1]\n\n        SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 0.0006077055 -0.0004782704  0.0003097628  0.0004990806 \n\n\n\n\nR-squared\n\npcr_rsq &lt;- function(x, y, comps) {\n    \n    coef &lt;- pcr_coef(x, y, comps)\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    result &lt;- (t(coef) %*% crossprod(x) %*% coef) / crossprod(y)\n    \n    return(result)\n    \n}\n\n\npcr_rsq(scale_x_xts, overlap_y_xts, comps)\n\n          BAICX\nBAICX 0.6555066\n\n\n\npcr_rsq(overlap_x_xts, overlap_y_xts, comps)\n\n         BAICX\nBAICX 0.456853\n\n\n\nR2(fit)$val[comps + 1]\n\n[1] 0.6555066\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\npcr_se &lt;- function(x, y, comps) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- pcr_rsq(x, y, comps)\n    \n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    df_resid &lt;- n_rows - n_cols - 1\n    \n    var_y &lt;- crossprod(y)\n    var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values[1:comps] * (n_rows - 1)\n    V &lt;- LV$vectors[ , 1:comps]\n    \n    result &lt;- sqrt(var_resid * diag(V %*% sweep(t(V), 1, 1 / L, \"*\")))\n    \n    return(result)\n    \n}\n\n\npcr_se(scale_x_xts, overlap_y_xts, comps)\n\n[1] 2.803152e-05 2.206109e-05 1.428837e-05 2.302100e-05\n\n\n\npcr_se(overlap_x_xts, overlap_y_xts, comps)\n\n[1] 0.0206518316 0.0018656873 0.0002216767 0.0010437727"
  },
  {
    "objectID": "posts/risk-r/index.html#standalone-risk",
    "href": "posts/risk-r/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\nlm_sar &lt;- function(x, y, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n    sar &lt;- coef ^ 2 * diag(sigma[-ncol(sigma), -ncol(sigma)])\n    sar_eps &lt;- (1 - rsq) * sigma[ncol(sigma), ncol(sigma)]\n    \n    result &lt;- sqrt(c(sigma[ncol(sigma), ncol(sigma)],\n                     sar,\n                     sar_eps))\n    \n    return(result)\n    \n}\n\n\nlm_sar(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])\n\n[1] 0.056078921 0.000000000 0.024119624 0.007299644 0.029654129 0.012305724\n[7] 0.027607781"
  },
  {
    "objectID": "posts/risk-r/index.html#risk-contribution",
    "href": "posts/risk-r/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttps://bookdown.org/compfinezbook/introcompfinr/Portfolio-risk-reports.html\n\n\nlm_mcr &lt;- function(x, y, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n    mcr &lt;- coef * sigma[-ncol(sigma), -ncol(sigma)] %*% coef / sqrt(sigma[ncol(sigma), ncol(sigma)])\n    mcr_eps &lt;- sqrt(sigma[ncol(sigma), ncol(sigma)]) - sum(mcr)\n    \n    result &lt;- c(sqrt(sigma[ncol(sigma), ncol(sigma)]),\n                mcr,\n                mcr_eps)\n    \n    return(result)\n    \n}\n\n\nlm_mcr(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])\n\n[1] 0.056078921 0.000000000 0.015956899 0.003914750 0.017939151 0.004676747\n[7] 0.013591374"
  },
  {
    "objectID": "posts/risk-r/index.html#implied-shocks",
    "href": "posts/risk-r/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\nimplied_shocks &lt;- function(shocks, x, z, weights) {\n    \n    beta &lt;- solve(crossprod(z, diag(weights) %*% z)) %*% crossprod(z, diag(weights) %*% x)\n    \n    result &lt;- shocks %*% beta\n    \n    return(result)\n    \n}\n\n\nshocks &lt;- c(-0.1, 0.1)\noverlap_z_xts &lt;- overlap_x_xts[ , 1:2]\n\n\nimplied_shocks(shocks, overlap_x_xts, overlap_z_xts, weights)\n\n     SP500 DTWEXAFEGS      DGS10 BAMLH0A0HYM2\n[1,]  -0.1        0.1 -0.0103585 -0.005015706"
  },
  {
    "objectID": "posts/risk-r/index.html#stress-pl",
    "href": "posts/risk-r/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\npnl_stress &lt;- function(shocks, x, y, z, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    result &lt;- t(coef) * implied_shocks(shocks, x, z, weights)\n    \n    return(result)    \n    \n}\n\n\npnl_stress(shocks, overlap_x_xts, overlap_y_xts, overlap_z_xts, weights, intercept)\n\n        (Intercept)      xSP500 xDTWEXAFEGS      xDGS10 xBAMLH0A0HYM2\nBAICX -0.0001049635 -0.01913491  -0.0145976 -0.03346309  -0.006121339"
  },
  {
    "objectID": "posts/securities-r/index.html",
    "href": "posts/securities-r/index.html",
    "title": "Securities",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/securities-r/index.html#value",
    "href": "posts/securities-r/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_value &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n    r_df &lt;- exp(-r * tau)\n    q_df &lt;- exp(-q * tau)\n    \n    call_value &lt;- S * q_df * Phi(d1) - r_df * K * Phi(d2)\n    put_value &lt;- r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n} \n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\nbs_d1 &lt;- function(S, K, r, q, tau, sigma) {\n    \n    result &lt;- (log(S / K) + (r - q + sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\nbs_d2 &lt;- function(S, K, r, q, tau, sigma) {\n    \n    result &lt;- (log(S / K) + (r - q - sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\nphi &lt;- function(x) {\n    \n    result &lt;- dnorm(x)\n    \n    return(result)\n    \n}\n\nPhi &lt;- function(x) {\n    \n    result &lt;- pnorm(x)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , d1 := bs_d1(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , d2 := bs_d2(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , value := bs_value(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order",
    "href": "posts/securities-r/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_delta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    q_df &lt;- exp(-q * tau)\n  \n    call_value &lt;- q_df * Phi(d1)\n    put_value &lt;- -q_df * Phi(-d1)\n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n} \n\n\ngreeks_dt[ , delta := bs_delta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nDelta-beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\nbs_delta_diff &lt;- function(type, S, K, r, q, tau, sigma, delta0) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    call_value &lt;- delta - delta0\n    put_value &lt;- delta0 - delta\n    \n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n        \n    return(result)\n    \n}\n\n\nbeta &lt;- 0.35\ntype &lt;- \"call\"\nn &lt;- 1\nmultiple &lt;- 100\ntotal &lt;- 1000000\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\nsec &lt;- list(\n    \"n\" = n,\n    \"multiple\" = multiple,\n    \"S\" = S,\n    \"delta\" = bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n    \"beta\" = 1\n)\n\n\nbeta_dt &lt;- CJ(type = type, shock = shocks)\nbeta_dt[ , spot := level_shock(shock, S, tau, sigma), by = c(\"type\", \"shock\")]\nbeta_dt[ , static := beta]\nbeta_dt[ , diff := bs_delta_diff(type, spot, K, r, q, tau, sigma, sec[[\"delta\"]])]\nbeta_dt[ , dynamic := beta + sec[[\"n\"]] * sec[[\"multiple\"]] * sec[[\"S\"]] * sec[[\"beta\"]] * diff / total, by = c(\"type\", \"shock\")]\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\nbs_vega &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    q_df &lt;- exp(-q * tau)\n    \n    result &lt;- S * q_df * phi(d1) * sqrt(tau)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , vega := bs_vega(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_theta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    r_df &lt;- exp(r * tau)\n    q_df &lt;- exp(q * tau)\n  \n    call_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) -\n      r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n    \n    put_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) +\n      r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n        \n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , theta := bs_theta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order",
    "href": "posts/securities-r/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\nbs_gamma &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n    q_df &lt;- exp(-q * tau)\n    \n    result &lt;- q_df * phi(d1) / (S * sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , gamma := bs_gamma(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order-1",
    "href": "posts/securities-r/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\n\npnl_bond &lt;- function(duration, convexity, dy) {\n    \n    duration_pnl &lt;- -duration * dy\n    convexity_pnl &lt;- (convexity / 2) * dy ^ 2\n    income_pnl &lt;- dy\n    \n    result &lt;- list(\n        \"total\" = duration_pnl + convexity_pnl + income_pnl,\n        \"duration\" = duration_pnl,\n        \"convexity\" = convexity_pnl,\n        \"income\" = income_pnl\n    )\n    \n    return(result)\n    \n} \n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor &lt;- \"DGS10\"\nduration &lt;- 6.5\nconvexity &lt;- 0.65\ny &lt;- coredata(tail(na.locf(levels_xts[ , factor]), width)[1])\n\n\nbonds_dt &lt;- data.table(index = index(tail(levels_xts, width)),\n                       duration = duration, convexity = convexity,\n                       dy = na.locf(tail(levels_xts[ , factor], width)))\nsetnames(bonds_dt, c(\"index\", \"duration\", \"convexity\", \"dy\"))\nbonds_dt[ , dy := (dy - y) / 100, by = index]\n\n\nattrib_dt &lt;- bonds_dt[ , as.list(unlist(pnl_bond(duration, convexity, dy))), by = index]\n\n\n\n\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=\\frac{\\partial D}{\\partial y}=\\frac{\\partial}{\\partial y}\\left(-\\frac{1}{P}\\frac{\\partial D}{\\partial y}\\right)\\\\\n&=-\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}+\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=-C+D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(-C\\times 100+D^{2}\\right)=-C+\\frac{D^{2}}{100}\\). For example, if convexity and yield are percent then \\(\\text{Drift}=\\left(-0.65+\\frac{6.5^{2}}{100}\\right)\\partial y\\times100\\) or basis points then \\(\\text{Drift}=\\left(-65+6.5^{2}\\right)\\partial y\\).\n\nyield_shock &lt;- function(shock, tau, sigma) {\n    \n    result &lt;- shock * sigma * sqrt(tau)\n    \n    return(result)\n    \n}\n\n\nduration_drift &lt;- function(duration, convexity, dy) {\n    \n    drift &lt;- -convexity + duration ^ 2 / 100\n    change &lt;- drift * dy * 100\n    \n    result &lt;- list(\n        \"drift\" = drift,\n        \"change\" = change\n    )\n    \n    return(result)\n    \n}\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor &lt;- \"DGS10\"\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor])\n\n\nduration_dt &lt;- CJ(shock = shocks)\nduration_dt[ , spot := yield_shock(shock, tau, sigma), by = \"shock\"]\nduration_dt[ , static := duration]\nduration_dt[ , dynamic := duration + duration_drift(duration, convexity, spot)[[\"change\"]], by = \"shock\"]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order-1",
    "href": "posts/securities-r/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlack’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\npnl_option &lt;- function(type, S, K, r, q, tau, sigma, dS, dt, dsigma) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    value &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega &lt;- bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta &lt;- bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma &lt;- bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl &lt;- delta * dS / value\n    gamma_pnl &lt;- gamma / 2 * dS ^ 2 / value\n    vega_pnl &lt;- vega * dsigma / value\n    theta_pnl &lt;- theta * dt / value\n    \n    result &lt;- list(\n        \"total\" = delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n        \"delta\" = delta_pnl,\n        \"gamma\" = gamma_pnl,\n        \"vega\" = vega_pnl,\n        \"theta\" = theta_pnl\n    )\n    \n    return(result)    \n    \n}\n\n\nfactor &lt;- \"SP500\"\ntype &lt;- \"call\"\nS &lt;- coredata(tail(na.locf(levels_xts[ , factor]), width)[1])\nK &lt;- S # * (1 + 0.05)\ntau &lt;- 1 # = 252 / 252\nsigma &lt;- coredata(tail(sd_xts[ , factor], width)[1])\n\n\noptions_dt &lt;- data.table(index = index(tail(levels_xts, width)),\n                         spot = na.locf(tail(levels_xts[ , factor], width)),\n                         sigma = tail(sd_xts[ , factor], width))\nsetnames(options_dt, c(\"index\", \"spot\", \"sigma\"))\noptions_dt[ , dS := spot - S, by = index]\noptions_dt[ , dt_diff := as.numeric(index - index[1])]\noptions_dt[ , dt := dt_diff / tail(dt_diff, 1)]\noptions_dt[ , dsigma := sigma - ..sigma, by = index]\n\n\nattrib_dt &lt;- options_dt[ , as.list(unlist(pnl_option(type, S, K, r, q, tau, ..sigma,\n                                                     dS, dt, dsigma))), by = index]\n\n\n\n\n\n\n\n\n\n\n\n\nIto’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\nsim_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n    \n    result &lt;- S * exp(cumsum(sigma * sqrt(dt) * rnorm(n_sim)) +\n                      (mu - 0.5 * sigma ^ 2) * dt)\n    \n    return(result)\n    \n}\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\nsim_multi_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n    \n    n_cols &lt;- ncol(sigma)\n    \n    Z &lt;- matrix(rnorm(n_sim * n_cols), nrow = n_sim, ncol = n_cols)\n    X &lt;- sweep(sqrt(dt) * (Z %*% chol(sigma)), 2, (mu - 0.5 * diag(sigma)) * dt, \"+\")\n    \n    result &lt;- sweep(apply(X, 2, function(x) exp(cumsum(x))), 2, S, \"*\")\n    \n    return(result)\n    \n}\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS &lt;- rep(1, length(factors))\nsigma &lt;- cov(returns_xts, use = \"complete.obs\") * scale[[\"periods\"]]\nmu &lt;- colMeans(na.omit(returns_xts)) * scale[[\"periods\"]]\nmu &lt;- mu + diag(sigma) / 2 # drift\ndt &lt;- 1 / scale[[\"periods\"]]\n\n\nmu_ls &lt;- list()\nsigma_ls &lt;- list()\n\n\nfor (i in 1:1e4) {\n    \n    # assumes stock prices\n    levels_sim &lt;- sim_multi_gbm(width + 1, S, mu, sigma, dt)\n    returns_sim &lt;- diff(log(levels_sim))\n\n    mu_sim &lt;- colMeans(returns_sim) * scale[[\"periods\"]]\n    sigma_sim &lt;- apply(returns_sim, 2, sd) * sqrt(scale[[\"periods\"]])\n    \n    mu_ls &lt;- append(mu_ls, list(mu_sim))\n    sigma_ls &lt;- append(sigma_ls, list(sigma_sim))\n    \n}\n\n\ndata.frame(\n    \"empirical\" = colMeans(na.omit(returns_xts)) * scale[[\"periods\"]],\n    \"theoretical\" = colMeans(do.call(rbind, mu_ls)\n))\n\n                empirical  theoretical\nSP500         0.109332156  0.105825189\nDTWEXAFEGS    0.005189749  0.005011516\nDGS10        -0.001483627 -0.001420068\nBAMLH0A0HYM2  0.003476235  0.003352911\n\n\n\ndata.frame(\n    \"empirical\" = sqrt(diag(sigma)),\n    \"theoretical\" = colMeans(do.call(rbind, sigma_ls))\n)\n\n               empirical theoretical\nSP500        0.180249085 0.180038605\nDTWEXAFEGS   0.062748176 0.062712837\nDGS10        0.008492288 0.008478266\nBAMLH0A0HYM2 0.016843432 0.016819178\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility",
    "href": "posts/securities-r/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\nimplied_vol_newton &lt;- function(params, type, S, K, r, q, tau) {\n    \n    target0 &lt;- 0\n    sigma &lt;- params[[\"sigma\"]]\n    sigma0 &lt;- sigma\n    \n    while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n        \n        d1 &lt;- bs_d1(S, K, r, q, tau, sigma0)\n        d2 &lt;- bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 &lt;- bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        d_target0 &lt;- bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma &lt;- sigma0 - (target0 - params[[\"target\"]]) / d_target0\n        sigma0 &lt;- sigma\n        \n    }\n    \n    return(sigma)\n    \n}\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS &lt;- coredata(na.locf(levels_xts)[nrow(levels_xts), factor])\nK &lt;- S # * (1 + 0.05)\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor]) # overrides matrix\nstart1 &lt;- 0.2\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\ntarget1 &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams1 &lt;- list(\n    \"target\" = target1,\n    \"sigma\" = start1,\n    \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nimplied_vol_newton(params1, type, S, K, r, q, tau)\n\n         SP500\n[1,] 0.1314908"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity",
    "href": "posts/securities-r/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\nyld_newton &lt;- function(params, cash_flows) {\n  \n    target0 &lt;- 0\n    yld &lt;- params[[\"cpn\"]]\n    yld0 &lt;- yld\n    \n    while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n      \n      target0 &lt;- 0\n      d_target0 &lt;- 0\n      dd_target0 &lt;- 0\n      \n      for (i in 1:length(cash_flows)) {\n        \n        t &lt;- i\n        \n        # present value of cash flows\n        target0 &lt;- target0 + cash_flows[i] / (1 + yld0) ^ t\n        \n        # first derivative of present value of cash flows\n        d_target0 &lt;- d_target0 - t * cash_flows[i] / (1 + yld0) ^ (t + 1) # use t for Macaulay duration\n        \n        # second derivative of present value of cash flows\n        dd_target0 &lt;- dd_target0 - t * (t + 1) * cash_flows[i] / (1 + yld0) ^ (t + 2)\n        \n      }\n      \n      yld &lt;- yld0 - (target0 - params[[\"target\"]]) / d_target0\n      yld0 &lt;- yld\n      \n    }\n    \n    result &lt;- list(\n        \"price\" = target0,\n        \"yield\" = yld * params[[\"freq\"]],\n        \"duration\" = -d_target0 / params[[\"target\"]] / params[[\"freq\"]],\n        \"convexity\" = -dd_target0 / params[[\"target\"]] / params[[\"freq\"]] ^ 2\n    )\n    \n    return(result)\n  \n}\n\n\nhttps://www.bloomberg.com/markets/rates-bonds/government-bonds/us\nhttps://quant.stackexchange.com/a/61025\nhttps://pages.stern.nyu.edu/~igiddy/spreadsheets/duration-convexity.xls\n\n\ntarget2 &lt;- 0.9928 * 1000 # present value\nstart2 &lt;- 0.0438 # coupon\ncash_flows &lt;- rep(start2 * 1000 / 2, 10 * 2)\ncash_flows[10 * 2] &lt;- cash_flows[10 * 2] + 1000\n\n\nparams2 &lt;- list(\n    \"target\" = target2,\n    \"cpn\" = start2,\n    \"freq\" = 2,\n    \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nt(yld_newton(params2, cash_flows))\n\n     price yield      duration convexity\n[1,] 992.8 0.04470076 8.016596 76.6811"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility-1",
    "href": "posts/securities-r/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\nimplied_vol_obj &lt;- function(param, type, S, K, r, q, tau, target) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, param)\n    d2 &lt;- bs_d2(S, K, r, q, tau, param)\n    target0 &lt;- bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result &lt;- abs(target0 - target)\n    \n    return(result)\n    \n}\n\nimplied_vol_optim &lt;- function(param, type, S, K, r, q, tau, target) {\n    \n    result &lt;- optim(param, implied_vol_obj, type = type, S = S, K = K, r = r, q = q,\n                    tau = tau, target = target, method = \"Brent\", lower = 0, upper = 1)\n    \n    return(result$par)\n    \n}\n\n\nimplied_vol_optim(start1, type, S, K, r, q, tau, target1)\n\n[1] 0.1314908"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity-1",
    "href": "posts/securities-r/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\nyld_obj &lt;- function(param, cash_flows, target) {\n    \n    target0 &lt;- 0\n        \n    for (i in 1:length(cash_flows)) {\n      target0 &lt;- target0 + cash_flows[i] / (1 + param) ^ i\n    }\n  \n    result &lt;- abs(target0 - target)\n    \n    return(result)\n    \n}\n\nyld_optim &lt;- function(params, cash_flows, target) {\n    \n    result &lt;- optim(params[[\"cpn\"]], yld_obj, target = target, cash_flows = cash_flows,\n                    method = \"Brent\", lower = 0, upper = 1)\n    \n    return(result$par * params[[\"freq\"]])\n    \n}\n\n\nyld_optim(params2, cash_flows, target2)\n\n[1] 0.04470077"
  },
  {
    "objectID": "posts/securities-py/index.html",
    "href": "posts/securities-py/index.html",
    "title": "Securities",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/securities-py/index.html#value",
    "href": "posts/securities-py/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    if (type == \"call\"):\n        result =  S * np.exp(-q * tau) * Phi(d1) - np.exp(-r * tau) * K * Phi(d2)\n    elif (type == \"put\"):\n        result = np.exp(-r * tau) * K * Phi(-d2) - S * np.exp(-q * tau) * Phi(-d1)\n        \n    return result\n\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    r_df = np.exp(-r * tau)\n    q_df = np.exp(-q * tau)\n    \n    call_value = S * q_df * Phi(d1) - r_df * K * Phi(d2)\n    put_value = r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\ndef bs_d1(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q + sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n\ndef bs_d2(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q - sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n    \ndef phi(x):\n    \n    result = norm.pdf(x)\n    \n    return result\n\ndef Phi(x):\n    \n    result = norm.cdf(x)\n    \n    return result\n\n\ngreeks_df[\"d1\"] = bs_d1(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"d2\"] = bs_d2(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"value\"] = bs_value(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order",
    "href": "posts/securities-py/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_delta(type, S, K, r, q, tau, sigma, d1, d2):\n  \n    q_df = np.exp(-q * tau)\n    \n    call_value = q_df * Phi(d1)\n    put_value = -q_df * Phi(-d1)\n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\n\ngreeks_df[\"delta\"] = bs_delta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\nDelta-beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\ndef bs_delta_diff(type, S, K, r, q, tau, sigma, delta0):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    call_value = delta - delta0\n    put_value = delta0 - delta\n    \n    result = np.where(type == \"call\", call_value, put_value)\n        \n    return result\n\n\nbeta = 0.35\ntype = \"call\"\nn = 1\nmultiple = 100\ntotal = 1000000\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\nsec = {\n    \"n\": n,\n    \"multiple\": multiple,\n    \"S\": S,\n    \"delta\": bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n    \"beta\": 1\n}\n\n\nbeta_df = pd.DataFrame([(x, y) for x in types for y in shocks], \n    columns = [\"type\", \"shock\"])\nbeta_df[\"spot\"] = level_shock(beta_df[\"shock\"], S, tau, sigma)\nbeta_df[\"static\"] = beta\nbeta_df[\"diff\"] = bs_delta_diff(type, beta_df[\"spot\"], K, r, q, tau, sigma, sec[\"delta\"])\nbeta_df[\"dynamic\"] = beta + sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * sec[\"beta\"] * beta_df[\"diff\"] / total\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\ndef bs_vega(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    q_df = np.exp(-q * tau)\n    result = S * q_df * phi(d1) * np.sqrt(tau)\n    \n    return result\n\n\ngreeks_df[\"vega\"] = bs_vega(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                            greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_theta(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    r_df = np.exp(-r * tau)\n    q_df = np.exp(-q * tau)\n    \n    call_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) - \\\n        r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n        \n    put_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) + \\\n        r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n        \n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\n\ngreeks_df[\"theta\"] = bs_theta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order",
    "href": "posts/securities-py/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\ndef bs_gamma(type, S, K, r, q, tau, sigma, d1, d2):\n  \n    q_df = np.exp(-q * tau)\n    \n    result = q_df * phi(d1) / (S * sigma * np.sqrt(tau))\n    \n    return result\n\n\ngreeks_df[\"gamma\"] = bs_gamma(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order-1",
    "href": "posts/securities-py/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\n\ndef pnl_bond(duration, convexity, dy):\n    \n    duration_pnl = -duration * dy\n    convexity_pnl = (convexity / 2) * dy ** 2\n    income_pnl = dy\n    \n    result = pd.DataFrame({\n        \"total\": duration_pnl + convexity_pnl + income_pnl,\n        \"duration\": duration_pnl,\n        \"convexity\": convexity_pnl,\n        \"income\": income_pnl\n    })\n    \n    return result\n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor = \"DGS10\"\nduration = 6.5\nconvexity = 0.65\ny = levels_df.ffill()[factor].iloc[-width]\n\n\nbond_df = pd.DataFrame({\n    \"duration\": duration,\n    \"convexity\": convexity,\n    \"dy\": (levels_df.ffill()[factor].iloc[-width:] - y) / 100\n})\n\n\nattrib_df = pnl_bond(bond_df[\"duration\"], bond_df[\"convexity\"], bond_df[\"dy\"])\n\n\n\n\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=\\frac{\\partial D}{\\partial y}=\\frac{\\partial}{\\partial y}\\left(-\\frac{1}{P}\\frac{\\partial D}{\\partial y}\\right)\\\\\n&=-\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}+\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=-C+D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(-C\\times 100+D^{2}\\right)=-C+\\frac{D^{2}}{100}\\). For example, if convexity and yield are percent then \\(\\text{Drift}=\\left(-0.65+\\frac{6.5^{2}}{100}\\right)\\partial y\\times100\\) or basis points then \\(\\text{Drift}=\\left(-65+6.5^{2}\\right)\\partial y\\).\n\ndef yield_shock(shock, tau, sigma):\n    \n    result = shock * sigma * np.sqrt(tau)\n    \n    return result\n\n\ndef duration_drift(duration, convexity, dy):\n    \n    drift = -convexity + duration ** 2 / 100\n    change = drift * dy * 100\n    \n    result = {\n        \"drift\": drift,\n        \"change\": change\n    }\n    \n    return result\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor = \"DGS10\"\nsigma = sd_df[factor].iloc[-1]\n\n\nduration_df = pd.DataFrame(shocks).rename(columns = {0: \"shock\"})\nduration_df[\"spot\"] = yield_shock(duration_df[\"shock\"], tau, sigma)\nduration_df[\"static\"] = duration\nduration_df[\"dynamic\"] = duration + \\\n    duration_drift(duration, convexity, duration_df[\"spot\"])[\"change\"]"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order-1",
    "href": "posts/securities-py/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlack’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\ndef pnl_option(type, S, K, r, q, tau, sigma, dS, dt, dsigma):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    value = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega = bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta = bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma = bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl = delta * dS / value\n    gamma_pnl = gamma / 2 * dS ** 2 / value\n    vega_pnl = vega * dsigma / value\n    theta_pnl = theta * dt / value\n    \n    result = pd.DataFrame({\n        \"total\": delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n        \"delta\": delta_pnl,\n        \"gamma\": gamma_pnl,\n        \"vega\": vega_pnl,\n        \"theta\": theta_pnl\n    })\n    \n    return result\n\n\nfactor = \"SP500\"\ntype = \"call\"\nS = levels_df.ffill()[factor].iloc[-width]\nK = S # * (1 + 0.05)\ntau = 1 # = 252 / 252\nsigma = sd_df[factor].iloc[-width]\n\n\noptions_df = pd.DataFrame({\n    \"spot\": levels_df.ffill()[factor].iloc[-width:],\n    \"sigma\": sd_df[factor].iloc[-width:]\n})\noptions_df[\"dS\"] = options_df[\"spot\"] - S\noptions_df[\"dt_diff\"] = (options_df.index - options_df.index[0]).days\noptions_df[\"dt\"] = options_df[\"dt_diff\"] / options_df[\"dt_diff\"].iloc[-1]\noptions_df[\"dsigma\"] = options_df[\"sigma\"] - sigma\n\n\nattrib_df = pnl_option(type, S, K, r, q, tau, sigma,\n                       options_df[\"dS\"], options_df[\"dt\"], options_df[\"dsigma\"])\n\n\n\n\n\n\n\n\n\n\n\n\nIto’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\ndef sim_gbm(n_sim, S, mu, sigma, dt):\n    \n    result = S * np.exp(np.cumsum(sigma * np.sqrt(dt) * np.random.normal(size = n_sim)) + \\\n                        (mu - 0.5 * sigma ** 2) * dt)\n    \n    return result\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\ndef sim_multi_gbm(n_sim, S, mu, sigma, dt):\n    \n    n_cols = sigma.shape[1]\n    \n    Z = np.random.normal(size = n_sim * n_cols).reshape((n_sim, n_cols))\n    X = np.sqrt(dt) * Z @ np.linalg.cholesky(sigma).T + (mu - 0.5 * np.diag(sigma)) * dt\n    \n    result = S * np.exp(X.cumsum(axis = 0))\n    \n    return np.asmatrix(result)\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS = [1] * len(factors)\nsigma = np.cov(returns_df.dropna().T, ddof = 1) * scale[\"periods\"]\nmu = np.array(returns_df.dropna().mean()) * scale[\"periods\"]\nmu = mu + np.diag(sigma) / 2 # drift\ndt = 1 / scale[\"periods\"]\n\n\nmu_ls = []\nsigma_ls = []\n\n\nfor i in range(10000): # \"TypeError: 'float' object cannot be interpreted as an integer\"\n  \n    # assumes underlying stock price follows geometric Brownian motion with constant volatility\n    levels_sim = pd.DataFrame(sim_multi_gbm(width + 1, S, mu, sigma, dt))\n    returns_sim = np.log(levels_sim).diff()\n\n    mu_sim = returns_sim.mean() * scale[\"periods\"]\n    sigma_sim = returns_sim.std() * np.sqrt(scale[\"periods\"])\n\n    mu_ls.append(mu_sim)\n    sigma_ls.append(sigma_sim)\n\n\nmu_df = pd.DataFrame(mu_ls)\nsigma_df = pd.DataFrame(sigma_ls)\n\n\npd.DataFrame({\n    \"empirical\": np.array(returns_df.dropna().mean()) * scale[\"periods\"],\n    \"theoretical\": mu_df.mean()\n})\n\n   empirical  theoretical\n0   0.109332     0.110846\n1   0.005190     0.005041\n2  -0.001484    -0.001525\n3   0.003476     0.003511\n\n\n\npd.DataFrame({\n    \"empirical\": np.sqrt(np.diag(sigma)),\n    \"theoretical\": sigma_df.mean()\n})\n\n   empirical  theoretical\n0   0.180249     0.180082\n1   0.062748     0.062673\n2   0.008492     0.008476\n3   0.016843     0.016831\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility",
    "href": "posts/securities-py/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\ndef implied_vol_newton(params, type, S, K, r, q, tau):\n    \n    target0 = 0\n    sigma = params[\"sigma\"]\n    sigma0 = sigma\n    \n    while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n        \n        d1 = bs_d1(S, K, r, q, tau, sigma0)\n        d2 = bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 = bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        d_target0 = bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma = sigma0 - (target0 - params[\"target\"]) / d_target0\n        sigma0 = sigma\n        \n    return sigma.item()\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS = levels_df.ffill()[factor].iloc[-1]\nK = S # * (1 + 0.05)\nsigma = sd_df[factor].iloc[-1] # overrides matrix\nstart1 = 0.2\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\ntarget1 = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams1 = {\n    \"target\": target1,\n    \"sigma\": start1,\n    \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nimplied_vol_newton(params1, type, S, K, r, q, tau) \n\n0.13423486987753291"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity",
    "href": "posts/securities-py/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\ndef yld_newton(params, cash_flows):\n    \n    target0 = 0\n    yld0 = params[\"cpn\"] / params[\"freq\"]\n    yld = yld0 # assignment to `yield` variable is not possible\n    \n    while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n        \n        target0 = 0\n        d_target0 = 0\n        dd_target0 = 0\n        \n        for i in range(len(cash_flows)):\n          \n            t = i + 1\n          \n            # present value of cash flows\n            target0 += cash_flows[i] / (1 + yld0) ** t\n            \n            # first derivative of present value of cash flows\n            d_target0 -= t * cash_flows[i] / (1 + yld0) ** (t + 1) # use t for Macaulay duration\n            \n            # second derivative of present value of cash flows\n            dd_target0 -= t * (t + 1) * cash_flows[i] / (1 + yld0) ** (t + 2)\n        \n        yld = yld0 - (target0 - params[\"target\"]) / d_target0\n        yld0 = yld\n        \n    result = {\n        \"price\": target0,\n        \"yield\": yld * params[\"freq\"],\n        \"duration\": -d_target0 / params[\"target\"] / params[\"freq\"],\n        \"convexity\": -dd_target0 / params[\"target\"] / params[\"freq\"] ** 2\n    }\n        \n    return result\n\n\nhttps://www.bloomberg.com/markets/rates-bonds/government-bonds/us\nhttps://quant.stackexchange.com/a/61025\nhttps://pages.stern.nyu.edu/~igiddy/spreadsheets/duration-convexity.xls\n\n\ntarget2 = 0.9928 * 1000 # present value\nstart2 = 0.0438 # coupon\ncash_flows = [start2 * 1000 / 2] * 10 * 2\ncash_flows[-1] += 1000\n\n\nparams2 = {\n    \"target\": target2,\n    \"cpn\": start2,\n    \"freq\": 2,\n    \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nyld_newton(params2, cash_flows)\n\n{'price': 992.8000005704454, 'yield': 0.044700757710159994, 'duration': 8.0165959605043, 'convexity': 76.68109754287481}"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility-1",
    "href": "posts/securities-py/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\ndef implied_vol_obj(param, type, S, K, r, q, tau, target):\n    \n    d1 = bs_d1(S, K, r, q, tau, param)\n    d2 = bs_d2(S, K, r, q, tau, param)\n    target0 = bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result = abs(target0 - target)\n    \n    return result\n\ndef implied_vol_optim(param, type, S, K, r, q, tau, target):\n    \n    result = minimize(implied_vol_obj, param, args = (type, S, K, r, q, tau, target))\n    \n    return result.x.item()\n\n\nimplied_vol_optim(start1, type, S, K, r, q, tau, target1)\n\n0.13423486750763614"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity-1",
    "href": "posts/securities-py/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\ndef yld_obj(param, cash_flows, target):\n    \n    target0 = 0\n    \n    for i in range(len(cash_flows)):\n        \n        target0 += cash_flows[i] / (1 + param) ** (i + 1)\n    \n    target0 = abs(target0 - target)\n    \n    return target0\n  \ndef yld_optim(params, cash_flows, target):\n    \n    result = minimize(yld_obj, params[\"cpn\"], args = (cash_flows, target))\n    \n    return result.x.item() * params[\"freq\"]\n\n\nyld_optim(params2, cash_flows, target2)\n\n0.04470075001603727"
  },
  {
    "objectID": "posts/statistics/index.html",
    "href": "posts/statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Usage\n\nlibrary(roll)\nlibrary(microbenchmark)\noptions(microbenchmark.unit = \"us\")\n\n\nn_vars &lt;- 10\nn_obs &lt;- 1000\nweights &lt;- 0.9 ^ (n_obs:1)\n\nx &lt;- matrix(rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)\ny &lt;- matrix(rnorm(n_obs), nrow = n_obs, ncol = 1)\nx_lgl &lt;- x &lt; 0\n\n\n\nRolling any\n\nresult &lt;- microbenchmark(\"125\" = roll_any(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_any(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_any(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_any(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 129.0 181.35 214.252 192.25 218.70 1350.9   100\n  250 167.1 178.00 200.785 192.75 214.60  432.0   100\n  500 118.0 172.15 192.657 182.85 210.50  264.9   100\n 1000  95.5 161.05 178.354 172.15 192.05  298.2   100\n\n\n\n\nRolling all\n\nresult &lt;- microbenchmark(\"125\" = roll_all(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_all(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_all(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_all(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 106.6 129.05 148.540 136.45 154.95 384.5   100\n  250 104.8 127.85 152.501 144.05 170.55 265.4   100\n  500 106.4 125.40 150.169 138.55 164.60 247.0   100\n 1000  97.7 114.90 135.077 129.90 149.85 227.5   100\n\n\n\n\nRolling sums\n\\[\n\\begin{aligned}\n&\\text{Expanding window} \\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_sum(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sum(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sum(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sum(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 134.4 147.35 175.617 163.10 179.40 303.3   100\n  250 135.6 152.10 175.233 161.85 178.20 300.2   100\n  500 131.5 142.75 165.794 158.95 169.50 293.5   100\n 1000 125.4 141.85 166.895 159.00 169.05 388.9   100\n\n\n\n\nRolling products\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{prod}_{w}\\leftarrow\\text{prod}_{w}\\times\\text{w}_{new}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}/\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_prod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_prod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_prod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_prod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 347.9 385.75 423.830 414.20 432.80  753.0   100\n  250 366.1 391.35 455.279 418.65 460.45 1932.3   100\n  500 263.7 291.55 320.111 313.80 332.35  966.8   100\n 1000 258.7 288.55 320.861 308.50 331.10 1087.4   100\n\n\n\n\nRolling means\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times \\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_mean(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_mean(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_mean(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_mean(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min    lq    mean median     uq   max neval\n  125 133.3 149.9 178.344 170.70 181.15 420.6   100\n  250 135.7 145.0 173.550 169.75 180.85 297.4   100\n  500 135.1 147.9 173.448 166.40 186.75 295.0   100\n 1000 132.0 142.4 167.098 162.00 179.30 265.6   100\n\n\n\n\nRolling minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_min(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_min(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_min(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_min(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 150.6 166.85 211.422 190.40 239.55  515.0   100\n  250 149.3 161.70 210.911 186.55 212.35 1266.6   100\n  500 152.5 167.95 204.202 188.15 225.30  454.9   100\n 1000 151.8 168.55 202.519 193.65 220.55  319.5   100\n\n\n\n\nRolling maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_max(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_max(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_max(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_max(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 148.3 163.40 195.064 182.15 205.85 368.6   100\n  250 149.2 159.75 200.023 176.30 220.35 342.3   100\n  500 149.2 160.05 198.094 182.95 206.40 433.2   100\n 1000 147.2 158.60 200.071 182.15 226.85 361.5   100\n\n\n\n\nRolling index of minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmin(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmin(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmin(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmin(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 137.5 162.35 182.197 174.45 190.95 381.2   100\n  250 142.6 165.55 186.158 179.60 200.85 271.9   100\n  500 139.1 161.00 176.800 173.05 182.00 264.7   100\n 1000 141.0 162.50 185.109 178.05 195.95 466.2   100\n\n\n\n\nRolling index of maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmax(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmax(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmax(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmax(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 146.8 164.70 192.655 171.30 190.50 393.2   100\n  250 144.1 158.20 182.576 168.75 184.80 436.7   100\n  500 138.8 160.60 190.420 169.35 192.60 363.7   100\n 1000 141.0 155.85 182.651 167.85 184.75 343.6   100\n\n\n\n\nRolling medians\n\n# \"'online' is only supported for equal 'weights'\"\nresult &lt;- microbenchmark(\"125\" = roll_median(x, width = 125, min_obs = 1),\n                         \"250\" = roll_median(x, width = 250, min_obs = 1),\n                         \"500\" = roll_median(x, width = 500, min_obs = 1),\n                         \"1000\" = roll_median(x, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq    max neval\n  125 1103.0 1246.75 1892.713 2002.90 2035.15 9965.7   100\n  250 1111.1 1644.55 1813.722 1983.55 2002.15 2233.4   100\n  500 1041.6 1810.35 1714.452 1841.60 1860.05 2301.7   100\n 1000  782.2  902.55 1269.107 1410.30 1441.25 1822.8   100\n\n\n\n\nRolling quantiles\n\n# \"'online' is only supported for equal 'weights'\"\nresult &lt;- microbenchmark(\"125\" = roll_quantile(x, width = 125, min_obs = 1),\n                         \"250\" = roll_quantile(x, width = 250, min_obs = 1),\n                         \"500\" = roll_quantile(x, width = 500, min_obs = 1),\n                         \"1000\" = roll_quantile(x, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq    max neval\n  125 1077.1 1124.80 1519.669 1401.20 1996.75 4935.5   100\n  250 1075.3 1117.95 1463.520 1392.15 1972.25 2134.1   100\n  500  991.8 1052.00 1417.220 1305.55 1838.20 1948.4   100\n 1000  769.3  828.65 1138.110 1018.80 1423.75 4404.5   100\n\n\n\n\nRolling variances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{x}_{old}-\\text{mean}_{prev_x})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_var(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_var(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_var(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_var(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 167.0 177.00 203.513 187.40 208.10 438.2   100\n  250 165.1 175.20 195.914 184.45 211.45 299.8   100\n  500 161.7 171.45 199.202 184.15 210.80 341.1   100\n 1000 154.1 162.10 186.859 175.85 200.70 303.5   100\n\n\n\n\nRolling standard deviations\n\nresult &lt;- microbenchmark(\"125\" = roll_sd(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sd(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sd(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sd(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median    uq   max neval\n  125 163.9 179.10 201.304  185.0 202.9 492.1   100\n  250 164.7 176.10 199.809  183.9 205.6 317.4   100\n  500 158.9 170.15 187.077  175.2 189.3 286.3   100\n 1000 150.4 163.85 181.687  169.7 184.7 284.9   100\n\n\n\n\nRolling scaling and centering\n\nresult &lt;- microbenchmark(\"125\" = roll_scale(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_scale(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_scale(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_scale(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 187.1 199.90 226.142 210.20 246.45 331.9   100\n  250 187.8 196.20 225.345 207.55 250.65 456.2   100\n  500 180.3 194.65 216.011 202.90 231.75 292.2   100\n 1000 174.0 182.70 208.997 192.15 215.20 380.3   100\n\n\n\n\nRolling covariances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{y}_{old}-\\text{mean}_{prev_y})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_cov(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cov(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cov(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cov(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq     max neval\n  125 1067.2 1230.20 1642.465 1356.25 1568.05 11902.8   100\n  250  968.0 1178.25 1312.100 1240.25 1330.65  2461.4   100\n  500  943.5 1144.75 1400.392 1187.75 1272.00  7728.5   100\n 1000  796.5 1001.85 1214.537 1062.45 1118.60  7784.9   100\n\n\n\n\nRolling correlations\n\nresult &lt;- microbenchmark(\"125\" = roll_cor(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cor(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cor(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cor(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median     uq     max neval\n  125 1173.4 1455.00 2253.283 1687.75 3020.4 11280.9   100\n  250 1145.7 1351.75 2083.530 1548.25 3048.2  3616.3   100\n  500 1169.3 1349.00 2129.608 1548.80 2874.2 11464.6   100\n 1000  876.1 1174.45 1856.686 1465.45 2447.0  6702.3   100\n\n\n\n\nRolling crossproducts\n\nresult &lt;- microbenchmark(\"125\" = roll_crossprod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_crossprod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_crossprod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_crossprod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 785.3 853.25 918.568 913.75 976.15 1193.7   100\n  250 751.3 827.20 897.928 883.05 961.60 1179.6   100\n  500 699.6 787.45 953.820 847.50 932.75 5066.9   100\n 1000 687.6 733.00 920.345 771.80 859.95 5073.7   100\n\n\n\n\nRolling linear models\n\\[\n\\begin{aligned}\n&\\text{coef}=\\text{cov}_{xx}^{-1}\\times\\text{cov}_{xy}\\\\\n&\\text{intercept}=\\text{mean}_{y}-\\text{coef}\\times\\text{mean}_{x}\\\\\n&\\text{rsq}=\\frac{\\text{coef}^{T}\\times\\text{cov}_{xx}\\times\\text{coef}}{\\text{var}_{y}}\\\\\n&\\text{var}_{resid}=\\frac{(1-\\text{rsq})(\\text{var}_{y})(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})}{\\text{n}_{rows}-\\text{n}_{cols}}\\\\\n&\\text{xx}=\\text{cov}_{xx}\\times(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})\\\\\n&\\text{se}_{coef}=\\sqrt{\\text{var}_{resid}\\times\\text{diag}(\\text{xx}^{-1})}\\\\\n&\\text{se}_{intercept}=\\sqrt{\\text{var}_{resid}\\left(1/\\text{sum}_{w}+\\text{mean}_{x}^{T}\\text{xx}^{-1}\\text{mean}_{x}\\right)}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_lm(x, y, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_lm(x, y, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_lm(x, y, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_lm(x, y, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq     max neval\n  125 3017.9 4526.45 5105.695 4817.10 5478.80 12264.4   100\n  250 3023.7 4459.70 5039.013 4690.75 5408.05 10310.0   100\n  500 3109.0 4416.25 4803.363 4585.90 4900.70  7667.2   100\n 1000 2933.2 4290.65 4725.408 4476.65 4928.90 10803.2   100\n\n\n\n\nReferences\n\nWeights: https://stackoverflow.com/a/9933794\nIndex: https://stackoverflow.com/a/11316626\nIndex: https://stackoverflow.com/a/34363187\nIndex: https://stackoverflow.com/a/243342\nQuantile (comparator): https://stackoverflow.com/a/51992954\nQuantile (comparator): https://stackoverflow.com/a/25921772\nQuantile (comparator): https://stackoverflow.com/a/40416506\nMedian: https://stackoverflow.com/a/5970314\nMedian: https://stackoverflow.com/a/5971248\nMedian: https://gist.github.com/ashelly/5665911\nStandard errors: https://stats.stackexchange.com/a/64217"
  },
  {
    "objectID": "posts/crowds-py/crowds-py.html",
    "href": "posts/crowds-py/crowds-py.html",
    "title": "Crowds",
    "section": "",
    "text": "import requests\nfrom lxml import html\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\n\n\nfactors_r = [\"SP500\"] # \"SP500\" does not contain dividends\nfactors_d = [\"SOFR\"]\nfactors = factors_r + factors_d\nwidth = 20 * 3\nscale = {\"periods\": 252, \"overlap\": 5}\n\n\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\n\n\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\n\n\nimport os\nimport cvxpy as cp\n\n\ndef get_nth(x, n, offset = 0):\n    \n    result = x[offset::n]\n    \n    return result\n\n\ndef get_text(x, n = 0):\n    \n    result_ls = []\n    \n    for i in x:\n      \n        if (len(i) == 0):\n            result_ls.append(i.text_content()) # types\n        else:\n            result_ls.append(i[n].text_content()) # names and tickers\n    \n    return result_ls\n\n\ndef get_mstar():\n    \n    i = 0\n    status = True\n    names_ls = []\n    tickers_ls = []\n    types_ls = []\n\n    while status:\n\n        i += 1\n\n        url = \"https://www.morningstar.com/asset-allocation-funds?page=\" + str(i)\n        response = requests.get(url)\n        tree = html.fromstring(response.content)\n\n        table = tree.xpath(\"//div[@class='topic__table-container']\")\n\n        if (len(table) == 0):\n            status = False\n        else:\n\n            names_tickers = tree.xpath(\"//a[@class='mdc-link mds-link mds-link--data-table mdc-link--no-visited']\")\n            types = tree.xpath(\"//span[@class='mdc-data-point mdc-data-point--string mdc-string']\")\n            \n        names_ls.extend(get_text(get_nth(names_tickers, 2)))\n        tickers_ls.extend(get_text(get_nth(names_tickers, 2, 1)))\n        types_ls.extend(get_text(get_nth(types, 5, 2)))\n\n    result = pd.DataFrame({\n      \"name\": names_ls,\n      \"ticker\": tickers_ls,\n      \"type\": types_ls\n    })\n    \n    return result\n\n\nmstar_df = get_mstar()\n\n\ntickers = mstar_df.loc[mstar_df[\"type\"] == \"Tactical Allocation\", \"ticker\"].tolist()\nprices_df = pdr.get_data_tiingo(tickers, start = \"1900-01-01\", api_key = os.getenv(\"TIINGO_API_KEY\"))\nprices_df = prices_df.pivot_table(index = \"date\", columns = \"symbol\", values = \"adjClose\") \\\n    .tz_localize(None)\nprices_df.sort_index(axis = 0, inplace = True)\ntickers = prices_df.columns\n\n\nreturns_cols = [(\"returns\", i) for i in tickers]\noverlap_cols = [(\"overlap\", i) for i in tickers]\nreturns_df[returns_cols] = np.log(prices_df).diff()\nreturns_df[overlap_cols] = returns_df[returns_cols].rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df.sort_index(axis = 1, inplace = True)\n\n\noverlap_df = returns_df.dropna()[\"overlap\"]\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors]\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers]\n\n\ndef pnl(x):\n    return np.nanprod(1 + x) - 1\n\n\nOptimization\n\ndef min_rss_optim(x, y):\n    \n    w = cp.Variable(x.shape[1])\n    \n    objective = cp.Minimize(cp.sum_squares(y - x @ w))\n    \n    constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= 1]\n    \n    problem = cp.Problem(objective, constraints)\n    problem.solve()\n    \n    return w.value\n\n\nn_rows = overlap_df.shape[0]\nresult_ls = []\n\nfor i in range(width - 1, n_rows):\n  \n  idx = range(max(i - width + 1, 0), i + 1)\n  x_subset = overlap_x_df.iloc[idx]\n  y_subset = overlap_y_df.iloc[idx]\n  params_ls = []\n  \n  for j in tickers:\n  \n    params = min_rss_optim(x_subset.values, y_subset.loc[:, j].values)\n    params_ls.append(params)\n  \n  result_ls.append(np.mean(params_ls, axis = 0))\n\n\nposition_df = pd.DataFrame(result_ls, index = overlap_df.index[(width - 1):],\n                           columns = factors)\nposition_df.tail()\n\n\n\n\n\n\n\n\nSP500\nSOFR\n\n\nDATE\n\n\n\n\n\n\n2024-02-06\n0.648071\n0.351929\n\n\n2024-02-07\n0.650647\n0.349353\n\n\n2024-02-08\n0.653656\n0.346344\n\n\n2024-02-09\n0.659915\n0.340085\n\n\n2024-02-12\n0.671116\n0.328884"
  },
  {
    "objectID": "posts/risk-r/index.html#partial-least-squares",
    "href": "posts/risk-r/index.html#partial-least-squares",
    "title": "Risk",
    "section": "Partial least squares",
    "text": "Partial least squares"
  },
  {
    "objectID": "posts/blpapi/index.html",
    "href": "posts/blpapi/index.html",
    "title": "Securities",
    "section": "",
    "text": "# !pip install --index-url=https://blpapi.bloomberg.com/repository/releases/python/simple/ blpapi\nimport blpapi\nimport pandas as pd\n\n\nhttps://www.bloomberg.com/professional/support/api-library/\nPage 80: https://bloomberg.github.io/blpapi-docs/\n\n\nData point\n\ndef bdp(securities, fields, host = \"localhost\", port = 8194):\n  \n    # Initialize Bloomberg API session\n    options = blpapi.SessionOptions()\n    options.setServerHost(host)\n    options.setServerPort(port)\n    session = blpapi.Session(options)\n    \n    session.start()\n    session.openService(\"//blp/refdata\")\n    \n    service = session.getService(\"//blp/refdata\")\n    request = service.createRequest(\"ReferenceDataRequest\")\n\n    # Add securities and fields to the request\n    for security in securities:\n        request.append(\"securities\", security)\n    \n    for field in fields:\n        request.append(\"fields\", field)\n\n    # Send request\n    session.sendRequest(request)\n    \n    data_dict = dict.fromkeys([\"security\"] + fields)\n\n    while True:\n      \n        event = session.nextEvent()\n\n        if event.eventType() in [blpapi.Event.RESPONSE, blpapi.Event.PARTIAL_RESPONSE]:\n          \n            for msg in event:\n              \n                data = msg.getElement(\"securityData\")\n                data_dict[\"security\"] = [x.getElementAsString(\"security\") for x in data.values()]\n                data_ls = [x.getElement(\"fieldData\") for x in data.values()]\n\n                for field in fields:\n                    try:\n                        data_dict[field] = [x.getElement(field).getValue() for x in data_ls]\n                    except:\n                        data_dict[field] = [None] * len(data_ls)  # Handle missing values\n\n        if event.eventType() == blpapi.Event.RESPONSE:\n            break\n\n    return pd.DataFrame(data_dict)\n\n\nsecurities = [\"IBM US Equity\", \"GOOG US Equity\", \"MSFT US Equity\", \"BA US Equity\"]\nfields = [\"MARKET_SECTOR_DES\", \"GICS_SECTOR_NAME\", \"ID_CUSIP\", \"PX_LAST\"]\n\n\nbdp_df = bdp(securities, fields)\nprint(bdp_df)\n\n\n\nData point\n\ndef bdp(securities, fields, host = \"localhost\", port = 8194):\n  \n    # Start Bloomberg session\n    options = blpapi.SessionOptions()\n    options.setServerHost(host)\n    options.setServerPort(port)\n    session = blpapi.Session(options)\n    \n    session.start()\n    session.openService(\"//blp/refdata\")\n    \n    service = session.getService(\"//blp/refdata\")\n    request = service.createRequest(\"ReferenceDataRequest\")\n\n    # Add securities and fields to the request\n    for security in securities:\n        request.append(\"securities\", security)\n    for field in fields:\n        request.append(\"fields\", field)\n\n    # Send request\n    session.sendRequest(request)\n\n    # Initialize data storage\n    data_dict = {\"security\": []}\n    for field in fields:\n        data_dict[field] = []\n\n    while True:\n      \n        event = session.nextEvent()\n        \n        if event.eventType() in {blpapi.Event.RESPONSE, blpapi.Event.PARTIAL_RESPONSE}:\n          \n            for msg in event:\n              \n                for security_data in msg.getElement(\"securityData\").values():\n                  \n                    sec_name = security_data.getElementAsString(\"security\")\n                    data_dict[\"security\"].append(sec_name)\n\n                    field_data = security_data.getElement(\"fieldData\")\n                    \n                    for field in fields:\n                        try:\n                            data_dict[field].append(field_data.getElement(field).getValue())\n                        except:\n                            data_dict[field].append(None)  # Handle missing data\n\n        if event.eventType() == blpapi.Event.RESPONSE:\n            break\n\n    # Stop session\n    session.stop()\n\n    return pd.DataFrame(data_dict)\n\n\nbdp_df = bdp(securities, fields)\nprint(bdp_df)\n\n\n\nData history\n\ndef bdh(securities, fields, start_date, end_date, host = \"localhost\", port = 8194):\n  \n    # Initialize Bloomberg session\n    options = blpapi.SessionOptions()\n    options.setServerHost(host)\n    options.setServerPort(port)\n    session = blpapi.Session(options)\n\n    session.start()\n    session.openService(\"//blp/refdata\")\n\n    service = session.getService(\"//blp/refdata\")\n    request = service.createRequest(\"HistoricalDataRequest\")\n\n    # Add securities and fields to the request\n    for security in securities:\n        request.append(\"securities\", security)\n    for field in fields:\n        request.append(\"fields\", field)\n\n    # Set request parameters\n    request.set(\"startDate\", start_date)\n    request.set(\"endDate\", end_date)\n\n    # Send request\n    session.sendRequest(request)\n\n    # Initialize data storage\n    data_dict = dict.fromkeys(securities, dict.fromkeys([\"date\"] + fields))\n\n    while True:\n      \n        event = session.nextEvent()\n        \n        if event.eventType() in {blpapi.Event.RESPONSE, blpapi.Event.PARTIAL_RESPONSE}:\n          \n            for msg in event:\n              \n                data = msg.getElement(\"securityData\")\n                security = security_data.getElementAsString(\"security\").getValue()\n                security_dict = {\"date\": [x.getElementAsDatetime(\"date\") for x in data.getElement(\"fieldData\")]}\n\n                for field in fields:\n                    try:\n                        security_dict[field] = [x.getElement(field).getValue() for x in data.getElement(\"fieldData\")]\n                    except:\n                        passs\n\n        if event.eventType() == blpapi.Event.RESPONSE:\n            break\n    \n    result = {key: pd.DataFrame(value).set_index(\"date\") for key, value in data_dict.items()}\n    \n    return result\n\n\nfields = [\"PX_LAST\", \"PX_BID\", \"PX_ASK\"]\nstart_date = \"20231201\"\nend_date = \"20231205\"\n\n\nbdh_df = bdh(securities, fields, start_date, end_date)\nprint(bdh_df[0])\n\n\n\nData history\n\ndef bdh(securities, fields, start_date, end_date, host = \"localhost\", port = 8194):\n  \n    # Initialize Bloomberg session\n    options = blpapi.SessionOptions()\n    options.setServerHost(host)\n    options.setServerPort(port)\n    session = blpapi.Session(options)\n\n    session.start()\n    session.openService(\"//blp/refdata\")\n\n    service = session.getService(\"//blp/refdata\")\n    request = service.createRequest(\"HistoricalDataRequest\")\n\n    # Add securities and fields to the request\n    for security in securities:\n        request.append(\"securities\", security)\n    for field in fields:\n        request.append(\"fields\", field)\n\n    # Set request parameters\n    request.set(\"startDate\", start_date)\n    request.set(\"endDate\", end_date)\n\n    # Send request\n    session.sendRequest(request)\n\n    # Initialize data storage\n    data_dict = {security: {\"date\": [], **{field: [] for field in fields}} for security in securities}\n\n    while True:\n      \n        event = session.nextEvent()\n        \n        if event.eventType() in {blpapi.Event.RESPONSE, blpapi.Event.PARTIAL_RESPONSE}:\n          \n            for msg in event:\n              \n                data = msg.getElement(\"securityData\")\n                \n                for security_data in data.values():\n                  \n                    security = security_data.getElementAsString(\"security\")\n                    field_data = security_data.getElement(\"fieldData\")\n\n                    data_dict[security][\"date\"] = [x.getElementAsDatetime(\"date\") for x in field_data.values()]\n\n                    for field in fields:\n                        try:\n                            data_dict[security][field] = [x.getElement(field).getValue() for x in field_data.values()]\n                        except:\n                            data_dict[security][field] = [None] * len(data_dict[security][\"date\"])  # Handle missing values\n\n        if event.eventType() == blpapi.Event.RESPONSE:\n            break\n\n    session.stop()  # Properly close the Bloomberg session\n\n    # Convert results to Pandas DataFrame\n    return {sec: pd.DataFrame(data).set_index(\"date\") for sec, data in data_dict.items()}\n\n\nbdh_df = bdh(securities, fields, start_date, end_date)\nprint(bdh_df[0])"
  },
  {
    "objectID": "posts/crowds-r/index.html",
    "href": "posts/crowds-r/index.html",
    "title": "Crowds",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\") # \"SP500\" does not contain dividends\nfactors_d &lt;- c(\"DTB3\")\n\n\nParse web\n\nlibrary(yfscreen)\n\n\nfilters &lt;- list(\"eq\", list(\"categoryname\", \"Tactical Allocation\"))\nquery &lt;- create_query(filters)\npayload &lt;- create_payload(\"mutualfund\", query, 250)\ndata &lt;- get_data(payload)\n\n\nsorted_df &lt;- data[order(data[[\"netAssets.raw\"]], data[[\"firstTradeDateMilliseconds\"]]), ]\ntickers &lt;- sorted_df[!duplicated(sorted_df[[\"netAssets.raw\"]]), \"symbol\"]\n\n\n# allocations &lt;- c(\"AOK\", \"AOM\", \"AOR\", \"AOA\")\n# tickers &lt;- c(tickers, allocations)\n\n\n\nOptimization\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\")\n\n\n# library(CVXR)\nlibrary(rolloptim)\n\n\n# min_rss_optim &lt;- function(x, y) {\n#     \n#     params &lt;- Variable(ncol(x))\n#     \n#     obj &lt;- Minimize(sum_squares(y - x %*% params))\n#     \n#     cons &lt;- list(sum(params) == 1, params &gt;= 0, params &lt;= 1)\n#     \n#     prob &lt;- Problem(obj, cons)\n#         \n#     result &lt;- solve(prob)$getValue(params)\n#     \n#     return(result)\n# \n# }\n\n\nperformance_xts &lt;- roll_prod(1 + returns_xts, width, min_obs = 1) - 1\n\n\nn_rows &lt;- nrow(overlap_xts)\nresult_ls &lt;- list()\nindex_ls &lt;- list()\n\n# for (i in width:n_rows) {\nfor (i in n_rows) {\n    \n    idx &lt;- max(i - width + 1, 1):i\n    x_subset &lt;- coredata(overlap_x_xts[idx, ])\n    y_subset &lt;- coredata(overlap_y_xts[idx, ])\n    params_ls &lt;- list()\n    tickers_ls &lt;- list()\n    performance_ls &lt;- list()\n    \n    # for (j in tickers[!tickers %in% allocations]) {\n    for (j in tickers) {\n      \n        idx &lt;- complete.cases(x_subset, y_subset[ , j])\n        x_complete &lt;- x_subset[idx, , drop = FALSE]\n        y_complete &lt;- y_subset[idx, j]\n        \n        if ((nrow(x_complete) &gt; 0) && (length(y_complete) &gt; 0)) {\n            \n            # params &lt;- t(min_rss_optim(x_complete, y_complete))\n            # params_ls &lt;- append(params_ls, list(params))\n          \n            xx &lt;- roll_crossprod(x_complete, x_complete, width = nrow(x_complete))\n            xy &lt;- roll_crossprod(x_complete, y_complete, width = nrow(x_complete))\n            \n            params &lt;- roll_min_rss(xx, xy, total = 1, lower = 0, upper = 1)[nrow(x_complete), ]\n            params_ls &lt;- append(params_ls, list(params))\n            \n            tickers_ls &lt;- append(tickers_ls, list(j))\n            \n            performance_ls &lt;- append(performance_ls, list(performance_xts[i, j]))\n            \n        }\n        \n    }\n    \n    if (length(params_ls) &gt; 0) {\n        \n        result &lt;- do.call(rbind, params_ls)\n        rownames(result) &lt;- unlist(tickers_ls)\n        \n        result &lt;- cbind(result, performance = unlist(performance_ls))\n        \n        result_ls &lt;- append(result_ls, list(result))\n        index_ls &lt;- append(index_ls, list(index(overlap_xts)[i]))\n        \n    }\n    \n}\n\n\n# save(result_ls, file = \"result_ls.rda\")\n# save(index_ls, file = \"index_ls.rda\")\n\n\n\nPerformance\n\n# load(\"result_ls.rda\")\n# load(\"index_ls.rda\")\n\n\nquantile_cut &lt;- function(x) {\n  \n  result &lt;- cut(\n    -x,\n    breaks = quantile(-x, probs = c(0, 0.25, 0.5, 0.75, 1)),\n    labels = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"),\n    include.lowest = TRUE\n  )\n  \n  return(result)\n  \n}\n\n\nn_rows &lt;- length(result_ls)\nscore_ls &lt;- list()\n\nfor (i in 1:n_rows) {\n  \n  score_df &lt;- data.frame(result_ls[[i]])\n  colnames(score_df) &lt;- c(factors, \"performance\")\n  \n  score_df[[\"date\"]] &lt;- index_ls[[i]]\n  score_df[[\"quantile\"]] &lt;- quantile_cut(score_df[[\"performance\"]])\n  \n  score &lt;- aggregate(\n    cbind(weight = get(factors[1]), performance) ~ date + quantile,\n    score_df, mean\n  )\n  \n  overall &lt;- data.frame(\n    date = index_ls[[i]],\n    quantile = \"Overall\",\n    weight = mean(score_df[[factors[1]]]),\n    performance = mean(score_df[[\"performance\"]])\n  )\n  \n  score &lt;- rbind(score, overall)\n  \n  score_ls &lt;- append(score_ls, list(score))\n  \n}\n\n\nscore_df &lt;- do.call(rbind, score_ls)\nprint(score_df)\n\n        date quantile    weight performance\n1 2025-09-19       Q1 0.9616480  0.10579881\n2 2025-09-19       Q2 0.7636671  0.07496626\n3 2025-09-19       Q3 0.5488501  0.05702453\n4 2025-09-19       Q4 0.2887171  0.02883465\n5 2025-09-19  Overall 0.6407206  0.06665606\n\n\n\n# save(score_df, file = \"score_df.rda\")\n# score_xts &lt;- xts(score_df[score_df[[\"quantile\"]] == \"Q1\", \"weight\"],\n#                  score_df[score_df[[\"quantile\"]] == \"Q1\", \"date\"])\n# plot(score_xts)"
  },
  {
    "objectID": "posts/crowds-py/index.html",
    "href": "posts/crowds-py/index.html",
    "title": "Crowds",
    "section": "",
    "text": "factors_r = [\"SP500\"] # \"SP500\" does not contain dividends\nfactors_d = [\"DTB3\"]\n\n\nParse web\n\nimport yfscreen as yfs\n\n\nfilters = [\"eq\", [\"categoryname\", \"Tactical Allocation\"]]\nquery = yfs.create_query(filters)\npayload = yfs.create_payload(\"mutualfund\", query, 250)\ndata = yfs.get_data(payload)\n\n\nsorted_df = data.sort_values(by = [\"netAssets.raw\", \"firstTradeDateMilliseconds\"])\ntickers = sorted_df.loc[~data[\"netAssets.raw\"].duplicated(), \"symbol\"].tolist()\n\n\n# allocations = [\"AOK\", \"AOM\", \"AOR\", \"AOA\"]\n# tickers = tickers + allocations\n\n\n\nOptimization\n\nimport json\nimport cvxpy as cp\n\n\ndef min_rss_optim(x, y):\n    \n  w = cp.Variable(x.shape[1])\n    \n  objective = cp.Minimize(cp.sum_squares(y - x @ w))\n    \n  constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= 1]\n    \n  problem = cp.Problem(objective, constraints)\n  problem.solve()\n    \n  return w.value\n\n\ndef pnl(x):\n  return np.nanprod(1 + x) - 1\n\n\nperformance_df = returns_df.rolling(width, min_periods = 1).apply(pnl, raw = False)\n\n\nn_rows = overlap_df.shape[0]\nresult_ls = []\nindex_ls = []\n\n# for i in range(width - 1, n_rows):\nfor i in range(n_rows - 1, n_rows):\n  \n  idx = range(max(i - width + 1, 0), i + 1)\n  x_subset = overlap_x_df.iloc[idx]\n  y_subset = overlap_y_df.iloc[idx]\n  params_ls = []\n  tickers_ls = []\n  performance_ls = []\n  \n  # for j in [ticker for ticker in tickers if ticker not in allocations]:\n  for j in tickers:\n    \n    idx = ~x_subset.isna().any(axis = 1) & ~y_subset[j].isna()\n    x_complete = x_subset.loc[idx]\n    y_complete = y_subset.loc[idx, j]\n    \n    if (x_complete.shape[0] &gt; 0) and (y_complete.size &gt; 0):\n        \n      params = min_rss_optim(x_complete.values, y_complete.values)\n      params_ls.append(params)\n      \n      tickers_ls.append(j)\n      \n      performance_ls.append(performance_df[j].iloc[i])\n\n  if params_ls:\n    \n    result = pd.DataFrame(params_ls, index = tickers_ls)\n    result[\"performance\"] = performance_ls\n    \n    result_ls.append(result)\n    index_ls.append(overlap_x_df.index[i])\n\n\n# json.dump([x.to_dict() for x in result_ls], open(\"result_ls.json\", \"w\"))\n# json.dump([x.isoformat() for x in index_ls], open(\"index_ls.json\", \"w\"))\n\n\n\nPerformance\n\n# result_ls = [pd.DataFrame(x) for x in json.load(open(\"result_ls.json\", \"r\"))]\n# index_ls = [pd.Timestamp(x) for x in json.load(open(\"index_ls.json\", \"r\"))]\n\n\ndef quantile_cut(x):\n  \n  result = pd.qcut(\n    -x,\n    q = [0, 0.25, 0.5, 0.75, 1],\n    labels = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n  )\n  \n  return result\n\n\nn_rows = len(result_ls)\nscore_ls = []\n\nfor i in range(n_rows):\n  \n  score_df = pd.DataFrame(result_ls[i])\n  score_df.columns = factors + [\"performance\"]\n  \n  score_df[\"date\"] = index_ls[i]\n  score_df[\"quantile\"] = quantile_cut(score_df[\"performance\"])\n  \n  score = score_df.groupby([\"date\", \"quantile\"], observed = True).agg(\n    weight = (factors[0], \"mean\"),\n    performance = (\"performance\", \"mean\")\n  ).reset_index()\n  \n  overall = pd.DataFrame({\n    \"date\": [index_ls[i]],\n    \"quantile\": [\"Overall\"],\n    \"weight\": [score_df[factors[0]].mean()],\n    \"performance\": [score_df[\"performance\"].mean()]\n  })\n  \n  score = pd.concat([score, overall], ignore_index = True)\n  \n  score_ls.append(score)\n\n\nscore_df = pd.concat(score_ls, ignore_index = True)\nprint(score_df)\n\n        date quantile    weight  performance\n0 2025-09-19       Q1  0.961389     0.105395\n1 2025-09-19       Q2  0.762925     0.074934\n2 2025-09-19       Q3  0.549677     0.057135\n3 2025-09-19       Q4  0.288999     0.028890\n4 2025-09-19  Overall  0.640747     0.066589\n\n\n\n# score_df.to_json(\"score_df.json\", date_format = \"iso\")"
  }
]