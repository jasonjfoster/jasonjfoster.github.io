[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jjf234.github.io",
    "section": "",
    "text": "Optimization\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCloud\n\n\n\ncomputing\n\n\npython\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware\n\n\n\ndevelopment\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nalgorithms\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nalgorithms\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 7, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Software development",
    "section": "",
    "text": "System setup\n\nhttps://r-pkgs.org/setup.html\n\n# environment variable: PATH=c:\\rtools40\\usr\\bin\n\n\nDevelopment workflow\n\nhttps://r-pkgs.org/workflows101.html\n\n\nRcppArmadillo::RcppArmadillo.package.skeleton(name = \"anRpackage\",\n                                              path = \".\",\n                                              example_code = FALSE)\n\n\n\nObject documentation\n\nhttps://r-pkgs.org/man.html\n\n\nroxygen2::roxygenize()\n\n\n\nCompiled code\n\nhttps://r-pkgs.org/src.html\n\n\nRcpp::compileAttributes(verbose = TRUE)\ntools::package_native_routine_registration_skeleton(\".\", character_only = FALSE)\n\n\n\nGit and GitHub\n\nhttps://r-pkgs.org/git.html\n\nTell Git your name and email address:\ngit config --global user.name \"YOUR FULL NAME\"\ngit config --global user.email \"YOUR EMAIL ADDRESS\"\nIf needed, generate an SSH key: RStudio &gt; Tools &gt; Git/SVN &gt; Create RSA Key…\nThen give GitHub your SSH public key: GitHub &gt; SSH and GPG Keys &gt; New SSH key\ngit remote add origin git@github.com:hadley/r-pkgs.git\ngit push -u origin master\nAlso, if needed, change the URL of remote ‘origin’:\ngit remote set-url origin git@github.com:hadley/r-foo.git\n\n\nAutomated checking\n\nhttps://r-pkgs.org/r-cmd-check.html\n\n\nusethis::use_github_action(\"check-standard\")\nusethis::use_github_action(\"test-coverage\")"
  },
  {
    "objectID": "posts/dev/dev.html",
    "href": "posts/dev/dev.html",
    "title": "Software development",
    "section": "",
    "text": "System setup\n\nhttps://r-pkgs.org/setup.html\n\n# environment variable: PATH=c:\\rtools40\\usr\\bin\n\n\nDevelopment workflow\n\nhttps://r-pkgs.org/workflows101.html\n\n\nRcppArmadillo::RcppArmadillo.package.skeleton(name = \"anRpackage\",\n                                              path = \".\",\n                                              example_code = FALSE)\n\n\n\nObject documentation\n\nhttps://r-pkgs.org/man.html\n\n\nroxygen2::roxygenize()\n\n\n\nCompiled code\n\nhttps://r-pkgs.org/src.html\n\n\nRcpp::compileAttributes(verbose = TRUE)\ntools::package_native_routine_registration_skeleton(\".\", character_only = FALSE)\n\n\n\nGit and GitHub\n\nhttps://r-pkgs.org/git.html\n\nTell Git your name and email address:\ngit config --global user.name \"YOUR FULL NAME\"\ngit config --global user.email \"YOUR EMAIL ADDRESS\"\nIf needed, generate an SSH key: RStudio &gt; Tools &gt; Git/SVN &gt; Create RSA Key…\nThen give GitHub your SSH public key: GitHub &gt; SSH and GPG Keys &gt; New SSH key\ngit remote add origin git@github.com:hadley/r-pkgs.git\ngit push -u origin master\nAlso, if needed, change the URL of remote ‘origin’:\ngit remote set-url origin git@github.com:hadley/r-foo.git\n\n\nAutomated checking\n\nhttps://r-pkgs.org/r-cmd-check.html\n\n\nusethis::use_github_action(\"check-standard\")\nusethis::use_github_action(\"test-coverage\")"
  },
  {
    "objectID": "posts/cloud/index.html",
    "href": "posts/cloud/index.html",
    "title": "Cloud",
    "section": "",
    "text": "EC2 &gt; Network & Security &gt; Key Pairs &gt; Create Key Pair &gt; *.pem\nPuTTYgen &gt; Load an existing private key file &gt; Save private key &gt; *.ppk\nHost Name: ec2-user@ec2-1-23-456-789.us-east-2.compute.amazonaws.com\nConnection &gt; SSH &gt; Auth & WinSCP &gt; Auth &gt; Private key file for authentication\nsudo yum update\n\n\n\nAmazon Linux 2\nwget https://repo.anaconda.com/archive/Anaconda3-2023.03-Linux-x86_64.sh\n\nbash Anaconda3-2023.03-Linux-x86_64.sh\nThen close and re-open current shell\n\nhttps://pypi.org/project/ipython/#history\nhttps://stackoverflow.com/a/76521018\nhttps://stackoverflow.com/a/77117477\n\npip install 'ipython==7.34.0' --force-reinstall\nipython\nfrom IPython.lib import passwd\npasswd(\"12345\")\n# sha1:asdfasdf:asfasdfasdf\nexit()\njupyter notebook --generate-config \nvi ~/.jupyter/jupyter_notebook_config.py\n\n# [i]\nc = get_config()  # Get the config object.\n# c.NotebookApp.certfile = u'/home/ubuntu/ssl/cert.pem' # path to the certificate we generated\n# c.NotebookApp.keyfile = u'/home/ubuntu/ssl/cert.key' # path to the certificate key we generated\nc.NotebookApp.ip = '0.0.0.0' # serve notebooks locally\nc.NotebookApp.open_browser = False # do not open a browser window by default when using notebooks.\nc.NotebookApp.password = 'sha1:asdfasdf:asfasdfasdf'\n# [ESC][:wq][ENTER]\n# nohup jupyter notebook\nnohup jupyter lab\nThen close and re-open current shell\njupyter nbconvert --to python &lt;notebook&gt;.ipynb\nsudo service crond start\ncrontab -e\n# [i]\n# https://crontab.guru/\n# * * * * * /home/ec2-user/anaconda3/bin/python /home/ec2-user/&lt;notebook&gt;.py\n# [ESC][:wq][ENTER]\n\nhttp://1.23.456.789:8888\n\n\n\n\n\nhttps://aws.amazon.com/blogs/big-data/running-r-on-aws/\n\nInstall R\nsudo su\n# yum install -y R # Amazon Linux\n# amazon-linux-extras list\nsudo amazon-linux-extras install R3.4 # Amazon Linux 2\nInstall RStudio Server\n\nhttps://posit.co/download/rstudio-server/\n\nwget https://download2.rstudio.org/server/centos6/x86_64/rstudio-server-rhel-1.2.5019-x86_64.rpm\nyum install rstudio-server-rhel-1.2.5019-x86_64.rpm\nrm rstudio-server-rhel-1.2.5019-x86_64.rpm\nInstall Shiny Server\n\nhttps://posit.co/download/shiny-server/\n\nR -e \"install.packages('shiny', repos = 'http://cran.rstudio.com/')\"\nwget https://download3.rstudio.org/centos6.3/x86_64/shiny-server-1.5.12.933-x86_64.rpm\nyum install --nogpgcheck shiny-server-1.5.12.933-x86_64.rpm\nrm shiny-server-1.5.12.933-x86_64.rpm\nAdd user(s)\nuseradd -m jjf234\nsudo passwd jjf234 # prompt to enter password\nNeed to create ‘/ShinyApps’ under ‘/ec2-user’ and then subfolders with code, e.g. /home/ec2-user/ShinyApps/hello\n# sudo /opt/shiny-server/bin/deploy-example user-dirs\n# mkdir ~/ShinyApps\n# sudo cp -R /opt/shiny-server/samples/sample-apps/hello ~/ShinyApps\nInstall devtools\nsudo yum install libcurl-devel\nsudo yum install openssl-devel\nsudo yum install libxml2-devel\nR -e \"install.packages('devtools', repos = 'http://cran.rstudio.com/')\"\nInstall RcppParallel\nR -e \"install.packages('RcppParallel', repos = 'http://cran.rstudio.com/')\"\nInstall RcppArmadillo\nR -e \"install.packages('RcppArmadillo', repos = 'http://cran.rstudio.com/')\"\nInstall other packages\nR -e \"install.packages(c('data.table', 'xts', 'testthat', 'microbenchmark'),\nrepos = 'http://cran.rstudio.com/')\"\nInstall roll\nR -e \"devtools::install_github('jjf234/roll')\"\n\nhttp://1.23.456.789:8787/\nhttp://1.23.456.789:3838/ec2-user/hello/"
  },
  {
    "objectID": "posts/cloud/index.html#secure-shell",
    "href": "posts/cloud/index.html#secure-shell",
    "title": "Cloud",
    "section": "",
    "text": "EC2 &gt; Network & Security &gt; Key Pairs &gt; Create Key Pair &gt; *.pem\nPuTTYgen &gt; Load an existing private key file &gt; Save private key &gt; *.ppk\nHost Name: ec2-user@ec2-1-23-456-789.us-east-2.compute.amazonaws.com\nConnection &gt; SSH &gt; Auth & WinSCP &gt; Auth &gt; Private key file for authentication\nsudo yum update"
  },
  {
    "objectID": "posts/cloud/index.html#jupyter-server",
    "href": "posts/cloud/index.html#jupyter-server",
    "title": "Cloud",
    "section": "",
    "text": "Amazon Linux 2\nwget https://repo.anaconda.com/archive/Anaconda3-2023.03-Linux-x86_64.sh\n\nbash Anaconda3-2023.03-Linux-x86_64.sh\nThen close and re-open current shell\n\nhttps://pypi.org/project/ipython/#history\nhttps://stackoverflow.com/a/76521018\nhttps://stackoverflow.com/a/77117477\n\npip install 'ipython==7.34.0' --force-reinstall\nipython\nfrom IPython.lib import passwd\npasswd(\"12345\")\n# sha1:asdfasdf:asfasdfasdf\nexit()\njupyter notebook --generate-config \nvi ~/.jupyter/jupyter_notebook_config.py\n\n# [i]\nc = get_config()  # Get the config object.\n# c.NotebookApp.certfile = u'/home/ubuntu/ssl/cert.pem' # path to the certificate we generated\n# c.NotebookApp.keyfile = u'/home/ubuntu/ssl/cert.key' # path to the certificate key we generated\nc.NotebookApp.ip = '0.0.0.0' # serve notebooks locally\nc.NotebookApp.open_browser = False # do not open a browser window by default when using notebooks.\nc.NotebookApp.password = 'sha1:asdfasdf:asfasdfasdf'\n# [ESC][:wq][ENTER]\n# nohup jupyter notebook\nnohup jupyter lab\nThen close and re-open current shell\njupyter nbconvert --to python &lt;notebook&gt;.ipynb\nsudo service crond start\ncrontab -e\n# [i]\n# https://crontab.guru/\n# * * * * * /home/ec2-user/anaconda3/bin/python /home/ec2-user/&lt;notebook&gt;.py\n# [ESC][:wq][ENTER]\n\nhttp://1.23.456.789:8888"
  },
  {
    "objectID": "posts/cloud/index.html#rstudio-server",
    "href": "posts/cloud/index.html#rstudio-server",
    "title": "Cloud",
    "section": "",
    "text": "https://aws.amazon.com/blogs/big-data/running-r-on-aws/\n\nInstall R\nsudo su\n# yum install -y R # Amazon Linux\n# amazon-linux-extras list\nsudo amazon-linux-extras install R3.4 # Amazon Linux 2\nInstall RStudio Server\n\nhttps://posit.co/download/rstudio-server/\n\nwget https://download2.rstudio.org/server/centos6/x86_64/rstudio-server-rhel-1.2.5019-x86_64.rpm\nyum install rstudio-server-rhel-1.2.5019-x86_64.rpm\nrm rstudio-server-rhel-1.2.5019-x86_64.rpm\nInstall Shiny Server\n\nhttps://posit.co/download/shiny-server/\n\nR -e \"install.packages('shiny', repos = 'http://cran.rstudio.com/')\"\nwget https://download3.rstudio.org/centos6.3/x86_64/shiny-server-1.5.12.933-x86_64.rpm\nyum install --nogpgcheck shiny-server-1.5.12.933-x86_64.rpm\nrm shiny-server-1.5.12.933-x86_64.rpm\nAdd user(s)\nuseradd -m jjf234\nsudo passwd jjf234 # prompt to enter password\nNeed to create ‘/ShinyApps’ under ‘/ec2-user’ and then subfolders with code, e.g. /home/ec2-user/ShinyApps/hello\n# sudo /opt/shiny-server/bin/deploy-example user-dirs\n# mkdir ~/ShinyApps\n# sudo cp -R /opt/shiny-server/samples/sample-apps/hello ~/ShinyApps\nInstall devtools\nsudo yum install libcurl-devel\nsudo yum install openssl-devel\nsudo yum install libxml2-devel\nR -e \"install.packages('devtools', repos = 'http://cran.rstudio.com/')\"\nInstall RcppParallel\nR -e \"install.packages('RcppParallel', repos = 'http://cran.rstudio.com/')\"\nInstall RcppArmadillo\nR -e \"install.packages('RcppArmadillo', repos = 'http://cran.rstudio.com/')\"\nInstall other packages\nR -e \"install.packages(c('data.table', 'xts', 'testthat', 'microbenchmark'),\nrepos = 'http://cran.rstudio.com/')\"\nInstall roll\nR -e \"devtools::install_github('jjf234/roll')\"\n\nhttp://1.23.456.789:8787/\nhttp://1.23.456.789:3838/ec2-user/hello/"
  },
  {
    "objectID": "posts/dev/index.html",
    "href": "posts/dev/index.html",
    "title": "Software",
    "section": "",
    "text": "System setup\n# environment variable: PATH=c:\\rtools&lt;123&gt;\\usr\\bin\n\n\nGit and GitHub\ngit config --global user.name \"&lt;NAME&gt;\"\ngit config --global user.email \"&lt;EMAIL&gt;\"\nIf needed, generate an SSH key: RStudio &gt; Tools &gt; Git/SVN &gt; Create RSA Key…\nThen give GitHub your SSH public key: GitHub &gt; SSH and GPG Keys &gt; New SSH key\nCreate a new repository on the command line\necho \"# &lt;REPO&gt;\" &gt;&gt; README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin git@github.com:&lt;OWNER&gt;/&lt;REPO&gt;.git\ngit push -u origin main\n…or push an existing repository from the command line\ngit remote add origin git@github.com:&lt;OWNER&gt;/&lt;REPO&gt;.git\ngit branch -M main\ngit push -u origin main\n# git remote set-url origin git@github.com:&lt;OWNER&gt;/&lt;REPO&gt;.git\n\n\nDevelopment workflow\n\nRcppArmadillo::RcppArmadillo.package.skeleton(name = \"anRpackage\",\n                                              path = \".\",\n                                              example_code = FALSE)\n\n\n\nCompiled code\n\nRcpp::compileAttributes(verbose = TRUE)\ntools::package_native_routine_registration_skeleton(\".\", character_only = FALSE)\n\n\n\nObject documentation\n\nroxygen2::roxygenize()\n\n\n\nAutomated checking\n\nusethis::use_github_action(\"check-standard\")\nusethis::use_github_action(\"test-coverage\")"
  },
  {
    "objectID": "posts/roll/index.html",
    "href": "posts/roll/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Usage\n\nlibrary(roll)\nlibrary(microbenchmark)\noptions(microbenchmark.unit = \"us\")\n\n\nn_vars &lt;- 10\nn_obs &lt;- 1000\nweights &lt;- 0.9 ^ (n_obs:1)\n\nx &lt;- matrix(rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)\ny &lt;- matrix(rnorm(n_obs), nrow = n_obs, ncol = 1)\nx_lgl &lt;- x &lt; 0\n\n\n\nRolling any\n\nresult &lt;- microbenchmark(\"125\" = roll_any(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_any(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_any(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_any(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 100.9 131.50 147.534 141.50 155.35  282.1   100\n  250 116.9 132.75 156.166 141.95 154.95 1119.2   100\n  500  93.2 127.35 143.660 138.70 152.95  217.2   100\n 1000 102.6 126.10 139.524 135.45 147.30  227.3   100\n\n\n\n\nRolling all\n\nresult &lt;- microbenchmark(\"125\" = roll_all(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_all(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_all(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_all(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 102.6 129.15 144.509 139.25 149.20 374.0   100\n  250  98.3 130.90 146.548 139.65 150.65 282.4   100\n  500  97.1 128.70 142.922 137.25 149.90 257.9   100\n 1000 108.4 129.65 142.897 136.30 152.70 236.6   100\n\n\n\n\nRolling sums\n\\[\n\\begin{aligned}\n&\\text{Expanding window} \\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_sum(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sum(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sum(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sum(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125  97.3 116.45 131.576 127.45 140.80 200.8   100\n  250 100.1 115.95 129.606 123.90 132.90 382.3   100\n  500  97.3 114.85 126.365 121.65 131.85 241.8   100\n 1000  93.4 114.50 123.485 120.95 129.85 177.9   100\n\n\n\n\nRolling products\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{prod}_{w}\\leftarrow\\text{prod}_{w}\\times\\text{w}_{new}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}/\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_prod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_prod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_prod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_prod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 220.9 236.35 271.061  257.5 284.10 526.5   100\n  250 222.6 245.10 278.764  259.4 281.00 620.1   100\n  500 151.8 177.50 201.508  187.4 213.60 545.5   100\n 1000 147.3 172.00 197.124  184.3 206.15 575.2   100\n\n\n\n\nRolling means\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times \\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_mean(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_mean(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_mean(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_mean(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125  96.2 118.45 130.005 126.20 136.00 194.1   100\n  250  95.5 117.95 132.266 127.50 140.05 187.7   100\n  500 100.8 116.95 125.618 121.80 130.45 177.5   100\n 1000  98.2 113.55 126.878 118.35 130.75 408.7   100\n\n\n\n\nRolling minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_min(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_min(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_min(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_min(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 107.9 131.30 156.383 143.05 171.30 315.1   100\n  250 114.1 133.45 148.953 142.00 155.15 289.9   100\n  500 113.0 132.90 150.993 141.95 160.75 362.4   100\n 1000 119.7 137.85 154.477 147.20 155.30 454.0   100\n\n\n\n\nRolling maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_max(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_max(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_max(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_max(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 113.7 131.25 146.899 139.05 153.05 401.7   100\n  250 109.4 134.65 147.073 142.25 152.25 229.7   100\n  500 108.1 136.30 149.703 143.50 156.55 239.5   100\n 1000 110.0 133.55 145.957 146.05 156.30 207.1   100\n\n\n\n\nRolling index of minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmin(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmin(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmin(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmin(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 104.1 115.25 130.789 121.25 130.50 225.4   100\n  250 103.6 115.60 133.341 120.65 136.75 343.1   100\n  500 103.1 117.25 132.180 123.10 136.20 302.0   100\n 1000 100.9 120.10 137.035 126.15 144.05 261.2   100\n\n\n\n\nRolling index of maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmax(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmax(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmax(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmax(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 100.3 115.45 140.336 121.70 141.55 373.8   100\n  250 103.4 114.45 130.265 119.05 132.30 348.6   100\n  500 103.5 114.10 131.351 119.55 130.65 256.3   100\n 1000 105.4 118.30 134.958 124.25 135.80 263.4   100\n\n\n\n\nRolling medians\n\nresult &lt;- microbenchmark(\"125\" = roll_median(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_median(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_median(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_median(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr     min       lq      mean   median       uq     max neval\n  125  4599.7  5482.75  7698.712  6229.65  8027.80 18081.3   100\n  250  8730.7 10744.00 14895.029 13272.45 18087.30 27164.4   100\n  500 16728.6 20769.00 27494.023 26526.60 34311.20 40460.8   100\n 1000 22308.3 35583.90 40242.298 41314.70 46052.25 50372.8   100\n\n\n\n\nRolling quantiles\n\nresult &lt;- microbenchmark(\"125\" = roll_quantile(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_quantile(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_quantile(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_quantile(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr     min       lq     mean  median      uq     max neval\n  125  4729.8  5240.35  7676.22  6085.0  8855.2 17907.3   100\n  250  8941.7 11134.45 15170.24 13553.9 17722.8 27089.2   100\n  500 16328.3 20147.60 27557.76 26549.0 34689.3 40910.2   100\n 1000 23694.9 30086.55 37129.21 38408.9 43262.2 48889.0   100\n\n\n\n\nRolling variances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{x}_{old}-\\text{mean}_{prev_x})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_var(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_var(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_var(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_var(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 124.9 152.65 168.985 162.35 177.45 238.6   100\n  250 141.4 152.65 167.993 162.25 175.35 306.0   100\n  500 113.1 144.90 163.236 156.50 168.50 402.3   100\n 1000 105.6 145.35 162.694 155.35 171.95 350.3   100\n\n\n\n\nRolling standard deviations\n\nresult &lt;- microbenchmark(\"125\" = roll_sd(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sd(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sd(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sd(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 143.7 152.55 165.386 162.45 171.80 272.8   100\n  250 138.9 148.75 164.558 159.15 169.25 264.9   100\n  500 140.5 146.40 165.517 157.70 167.50 481.8   100\n 1000 131.7 140.45 154.087 150.05 158.35 239.6   100\n\n\n\n\nRolling scaling and centering\n\nresult &lt;- microbenchmark(\"125\" = roll_scale(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_scale(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_scale(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_scale(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 152.4 173.05 222.879 184.65 207.40 994.0   100\n  250 156.0 172.00 205.115 180.35 203.75 880.2   100\n  500 156.1 170.85 211.344 179.05 199.05 881.0   100\n 1000 147.0 164.35 208.481 176.40 206.55 848.1   100\n\n\n\n\nRolling covariances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{y}_{old}-\\text{mean}_{prev_y})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_cov(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cov(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cov(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cov(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq    max neval\n  125 1032.5 1154.95 1274.447 1211.05 1290.40 6003.6   100\n  250 1024.0 1122.05 1257.095 1198.65 1279.90 5818.0   100\n  500  876.5 1084.45 1152.625 1132.30 1232.20 1641.3   100\n 1000  888.0  997.80 1211.518 1056.75 1113.05 6448.1   100\n\n\n\n\nRolling correlations\n\nresult &lt;- microbenchmark(\"125\" = roll_cor(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cor(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cor(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cor(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq     max neval\n  125 1156.7 1348.55 1873.940 1477.85 1611.25 13501.6   100\n  250 1136.6 1332.00 2051.621 1449.40 1735.15  6172.6   100\n  500 1005.2 1270.15 1681.855 1351.00 1477.85  6514.3   100\n 1000 1009.5 1121.15 1466.576 1183.50 1275.70  5742.3   100\n\n\n\n\nRolling crossproducts\n\nresult &lt;- microbenchmark(\"125\" = roll_crossprod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_crossprod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_crossprod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_crossprod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq     mean  median      uq    max neval\n  125 760.7 959.80 1208.778 1017.75 1073.40 5887.9   100\n  250 770.0 943.25 1010.253 1004.75 1061.05 1491.5   100\n  500 828.2 912.65  972.969  971.45 1015.10 1506.5   100\n 1000 614.1 827.05  927.984  868.80  925.90 5604.2   100\n\n\n\n\nRolling linear models\n\\[\n\\begin{aligned}\n&\\text{coef}=\\text{cov}_{xx}^{-1}\\times\\text{cov}_{xy}\\\\\n&\\text{intercept}=\\text{mean}_{y}-\\text{coef}\\times\\text{mean}_{x}\\\\\n&\\text{rsq}=\\frac{\\text{coef}^{T}\\times\\text{cov}_{xx}\\times\\text{coef}}{\\text{var}_{y}}\\\\\n&\\text{var}_{resid}=\\frac{(1-\\text{rsq})(\\text{var}_{y})(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})}{\\text{n}_{rows}-\\text{n}_{cols}}\\\\\n&\\text{xx}=\\text{cov}_{xx}\\times(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})\\\\\n&\\text{se}_{coef}=\\sqrt{\\text{var}_{resid}\\times\\text{diag}(\\text{xx}^{-1})}\\\\\n&\\text{se}_{intercept}=\\sqrt{\\text{var}_{resid}\\left(1/\\text{sum}_{w}+\\text{mean}_{x}^{T}\\text{xx}^{-1}\\text{mean}_{x}\\right)}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_lm(x, y, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_lm(x, y, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_lm(x, y, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_lm(x, y, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median     uq     max neval\n  125 2260.5 7207.10 7421.314 7551.45 7930.3 10909.0   100\n  250 2295.2 7098.45 7722.951 7454.40 7888.1 23681.8   100\n  500 2251.6 6967.50 7331.274 7233.75 7778.8 10878.0   100\n 1000 2098.4 6372.60 6629.760 6664.15 7062.0 23237.5   100\n\n\n\n\nReferences\n\nhttps://stackoverflow.com/a/9933794\nhttps://stackoverflow.com/a/11316626\nhttps://stackoverflow.com/a/34363187\nhttps://stackoverflow.com/a/243342\nhttps://stats.stackexchange.com/a/64217\nhttps://stackoverflow.com/a/51992954\nhttps://stackoverflow.com/a/25921772\nhttps://stackoverflow.com/a/40416506\nhttps://stackoverflow.com/a/5970314\nhttps://gist.github.com/ashelly/5665911\nhttps://stackoverflow.com/a/51992954"
  },
  {
    "objectID": "posts/optim/index.html",
    "href": "posts/optim/index.html",
    "title": "Optimization",
    "section": "",
    "text": "Usage\n\nlibrary(rolloptim)\n\n\nn_vars &lt;- 3\nn_obs &lt;- 15\nx &lt;- matrix(rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)\n\nmu &lt;- roll::roll_mean(x, 5)\nsigma &lt;- roll::roll_cov(x, width = 5)\n\n\n\nMinimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nroll_min_var(sigma)\n\n\n\nMaximize mean\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nroll_max_mean(mu)\n\n\n\nMaximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nroll_max_utility(mu, sigma)\n\n\n\nMinimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nroll_min_rss(xx, xy)\n\n\n\nReferences\n\nhttps://www.adrian.idv.hk/2021-06-22-kkt/\nhttps://or.stackexchange.com/a/3738\nhttps://bookdown.org/compfinezbook/introFinRbook/Portfolio-Theory-with-Matrix-Algebra.html#algorithm-for-computing-efficient-frontier"
  },
  {
    "objectID": "posts/finance/index.html#expected-value",
    "href": "posts/finance/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/finance/index.html#variance",
    "href": "posts/finance/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\n# http://scipp.ucsc.edu/~haber/ph116C/iid.pdf\ndef risk(x):\n    \n    n_rows = sum(~np.isnan(x))\n        \n    if n_rows &gt; 1:\n        result = np.sqrt(np.nansum(x ** 2) / (n_rows - 1))\n    else:\n        result = np.nan\n        \n    return result\n\n\n# volatility scale only\nscore_df = (momentum_df / momentum_df.rolling(width, min_periods = 1).apply(risk, raw = False)).dropna()\n\n\n# overall_df = score_df.mean(axis = 1)\n# overall_df = overall_df / overall_df.rolling(width, min_periods = 1).apply(risk, raw = False)\n\n\n# score_df.insert(loc = 0, column = \"Overall\", value = overall_df)\n# score_df = score_df.dropna()\n\n\nscore_mlt &lt;- melt(as.data.table(py$score_df, keep.rownames = \"index\"), id.vars = \"index\")\nscore_mlt[ , index := as.Date(index)]\nscore_plt &lt;- plot_ts_decomp(score_mlt, decomp = \"Overall\", title = \"Score 1Y\", multiple = 1) +\n  facet_wrap(~ variable)\nprint(score_plt)"
  },
  {
    "objectID": "posts/finance/index.html",
    "href": "posts/finance/index.html",
    "title": "Markets",
    "section": "",
    "text": "library(reticulate)\nlibrary(data.table)\nsource(\"../plot/theme_jjf.R\")\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\n# https://pandas-datareader.readthedocs.io/en/latest/remote_data.html\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)"
  },
  {
    "objectID": "posts/markets/index.html",
    "href": "posts/markets/index.html",
    "title": "Markets",
    "section": "",
    "text": "library(reticulate)\nlibrary(data.table)\nsource(\"../plot/theme_jjf.R\")\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\n# https://pandas-datareader.readthedocs.io/en/latest/remote_data.html\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)"
  },
  {
    "objectID": "posts/markets/index.html#expected-value",
    "href": "posts/markets/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets/index.html#variance",
    "href": "posts/markets/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\n# http://scipp.ucsc.edu/~haber/ph116C/iid.pdf\ndef risk(x):\n    \n    n_rows = sum(~np.isnan(x))\n        \n    if n_rows &gt; 1:\n        result = np.sqrt(np.nansum(x ** 2) / (n_rows - 1))\n    else:\n        result = np.nan\n        \n    return result\n\n\n# volatility scale only\nscore_df = (momentum_df / momentum_df.rolling(width, min_periods = 1).apply(risk, raw = False)).dropna()\n\n\n# overall_df = score_df.mean(axis = 1)\n# overall_df = overall_df / overall_df.rolling(width, min_periods = 1).apply(risk, raw = False)\n\n\n# score_df.insert(loc = 0, column = \"Overall\", value = overall_df)\n# score_df = score_df.dropna()\n\n\nscore_mlt &lt;- melt(as.data.table(py$score_df, keep.rownames = \"index\"), id.vars = \"index\")\nscore_mlt[ , index := as.Date(index)]\nscore_plt &lt;- plot_ts_decomp(score_mlt, decomp = \"Overall\", title = \"Score 1Y\", multiple = 1) +\n  facet_wrap(~ variable)\nprint(score_plt)"
  },
  {
    "objectID": "posts/statistics/index.html",
    "href": "posts/statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Usage\n\nlibrary(roll)\nlibrary(microbenchmark)\noptions(microbenchmark.unit = \"us\")\n\n\nn_vars &lt;- 10\nn_obs &lt;- 1000\nweights &lt;- 0.9 ^ (n_obs:1)\n\nx &lt;- matrix(rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)\ny &lt;- matrix(rnorm(n_obs), nrow = n_obs, ncol = 1)\nx_lgl &lt;- x &lt; 0\n\n\n\nRolling any\n\nresult &lt;- microbenchmark(\"125\" = roll_any(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_any(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_any(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_any(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 103.9 128.85 144.181  142.6 157.80  224.4   100\n  250 111.3 131.90 231.834  149.0 160.95 8548.0   100\n  500 113.1 130.95 146.818  143.2 160.60  194.3   100\n 1000 109.0 124.20 143.241  140.4 160.65  210.7   100\n\n\n\n\nRolling all\n\nresult &lt;- microbenchmark(\"125\" = roll_all(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_all(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_all(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_all(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 120.3 141.55 161.455 152.40 172.35 526.1   100\n  250 118.9 141.45 159.021 160.45 173.30 228.0   100\n  500 114.4 139.55 158.135 155.55 173.90 219.8   100\n 1000 112.0 137.05 155.731 152.95 166.15 245.6   100\n\n\n\n\nRolling sums\n\\[\n\\begin{aligned}\n&\\text{Expanding window} \\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_sum(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sum(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sum(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sum(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr  min     lq    mean median     uq   max neval\n  125 93.7 111.40 124.095 119.95 134.65 190.5   100\n  250 94.7 110.85 124.654 117.95 132.60 192.4   100\n  500 95.4 109.50 122.803 116.70 131.10 308.1   100\n 1000 95.5 109.70 124.298 115.40 134.35 206.4   100\n\n\n\n\nRolling products\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{prod}_{w}\\leftarrow\\text{prod}_{w}\\times\\text{w}_{new}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}/\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_prod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_prod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_prod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_prod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 222.6 234.30 256.250 244.00 269.90 378.4   100\n  250 217.8 234.70 249.864 243.00 254.05 352.8   100\n  500 147.8 168.70 184.928 178.75 198.30 253.0   100\n 1000 151.4 165.95 184.265 176.10 193.15 407.8   100\n\n\n\n\nRolling means\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times \\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_mean(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_mean(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_mean(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_mean(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125  99.5 119.75 133.644 126.55 145.70 193.1   100\n  250 102.6 117.80 131.421 125.65 145.45 171.6   100\n  500 100.6 114.35 133.149 128.75 144.20 386.2   100\n 1000 104.8 116.10 130.722 123.85 143.15 210.4   100\n\n\n\n\nRolling minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_min(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_min(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_min(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_min(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 114.8 139.35 151.308 148.50 160.85  226.8   100\n  250 110.4 136.55 151.550 146.65 164.35  316.8   100\n  500 109.3 139.80 157.480 148.70 170.05  389.8   100\n 1000 113.9 143.30 187.273 152.55 171.15 3147.1   100\n\n\n\n\nRolling maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_max(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_max(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_max(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_max(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 110.7 128.35 138.595 135.35 144.50 283.0   100\n  250 108.8 127.50 139.425 136.75 146.85 205.3   100\n  500 105.8 132.10 143.038 141.55 153.90 187.4   100\n 1000 108.5 133.95 146.798 142.05 156.25 208.9   100\n\n\n\n\nRolling index of minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmin(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmin(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmin(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmin(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125  96.9 116.15 127.061 123.15 131.20 193.1   100\n  250 108.0 117.90 132.211 123.30 131.55 246.2   100\n  500  99.8 117.25 133.133 122.35 137.60 305.4   100\n 1000 102.4 120.50 137.555 127.80 140.80 228.8   100\n\n\n\n\nRolling index of maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmax(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmax(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmax(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmax(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 105.0 119.40 135.222 126.00 143.00 367.1   100\n  250 102.8 117.00 134.750 122.95 139.35 257.8   100\n  500 100.6 119.05 136.248 125.85 142.40 276.9   100\n 1000 105.8 123.30 145.538 133.95 158.85 236.9   100\n\n\n\n\nRolling medians\n\nresult &lt;- microbenchmark(\"125\" = roll_median(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_median(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_median(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_median(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr     min       lq      mean   median       uq     max neval\n  125  5049.7  5677.45  8701.194  6796.90 11291.55 18998.6   100\n  250  9872.1 11961.60 16440.857 14402.90 20366.75 28773.2   100\n  500 18660.2 24411.85 29721.478 29960.75 35295.55 43186.0   100\n 1000 26709.3 36005.95 41646.918 43280.45 47841.55 53199.4   100\n\n\n\n\nRolling quantiles\n\nresult &lt;- microbenchmark(\"125\" = roll_quantile(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_quantile(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_quantile(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_quantile(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr     min       lq      mean   median       uq     max neval\n  125  4450.6  5542.55  8483.886  7392.35  9786.55 19952.5   100\n  250  9229.2 11823.40 16449.410 14480.40 21311.75 29143.9   100\n  500 15916.3 20071.30 27060.605 27390.45 34057.05 40634.8   100\n 1000 22438.4 30954.70 38321.424 39968.25 44804.15 56898.6   100\n\n\n\n\nRolling variances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{x}_{old}-\\text{mean}_{prev_x})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_var(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_var(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_var(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_var(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 139.5 149.10 166.382 163.00 172.35 258.7   100\n  250 130.2 148.45 164.559 160.35 169.75 284.4   100\n  500 131.7 149.25 167.055 160.45 171.45 480.3   100\n 1000 115.0 139.10 154.313 151.25 160.30 252.4   100\n\n\n\n\nRolling standard deviations\n\nresult &lt;- microbenchmark(\"125\" = roll_sd(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sd(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sd(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sd(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 134.2 151.10 166.464 160.45 174.10 382.0   100\n  250 123.7 150.70 169.012 159.80 174.95 496.7   100\n  500 136.1 147.20 161.212 154.10 168.30 274.7   100\n 1000 128.9 140.45 150.573 146.50 156.75 214.4   100\n\n\n\n\nRolling scaling and centering\n\nresult &lt;- microbenchmark(\"125\" = roll_scale(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_scale(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_scale(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_scale(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 147.8 157.65 171.824 166.35 175.75 265.9   100\n  250 144.6 157.20 168.209 164.10 174.25 249.9   100\n  500 143.9 154.10 165.708 162.85 172.15 268.1   100\n 1000 137.1 147.20 165.066 156.60 166.70 412.8   100\n\n\n\n\nRolling covariances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{y}_{old}-\\text{mean}_{prev_y})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_cov(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cov(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cov(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cov(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min      lq     mean  median      uq    max neval\n  125 992.4 1087.60 1181.662 1147.05 1208.40 4539.5   100\n  250 966.9 1071.60 1228.098 1131.85 1185.25 6065.2   100\n  500 848.9 1020.65 1190.394 1082.70 1137.20 5936.6   100\n 1000 777.3  946.65 1052.764  971.25 1016.15 5841.1   100\n\n\n\n\nRolling correlations\n\nresult &lt;- microbenchmark(\"125\" = roll_cor(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cor(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cor(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cor(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq     max neval\n  125 1167.7 1377.75 2143.177 1496.45 2452.30  8919.2   100\n  250 1114.8 1345.70 1971.568 1437.15 1865.40  6661.5   100\n  500 1130.4 1294.90 2055.076 1381.85 2598.75 15026.4   100\n 1000 1011.6 1130.25 1685.279 1208.20 1377.55  4670.2   100\n\n\n\n\nRolling crossproducts\n\nresult &lt;- microbenchmark(\"125\" = roll_crossprod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_crossprod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_crossprod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_crossprod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq     mean  median      uq    max neval\n  125 679.2 976.65 1112.208 1036.75 1078.10 8325.8   100\n  250 830.0 932.45 1047.441  996.00 1051.70 5885.7   100\n  500 793.8 913.05 1075.653  966.60 1018.30 6909.6   100\n 1000 646.4 813.25  918.169  845.00  937.75 5655.9   100\n\n\n\n\nRolling linear models\n\\[\n\\begin{aligned}\n&\\text{coef}=\\text{cov}_{xx}^{-1}\\times\\text{cov}_{xy}\\\\\n&\\text{intercept}=\\text{mean}_{y}-\\text{coef}\\times\\text{mean}_{x}\\\\\n&\\text{rsq}=\\frac{\\text{coef}^{T}\\times\\text{cov}_{xx}\\times\\text{coef}}{\\text{var}_{y}}\\\\\n&\\text{var}_{resid}=\\frac{(1-\\text{rsq})(\\text{var}_{y})(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})}{\\text{n}_{rows}-\\text{n}_{cols}}\\\\\n&\\text{xx}=\\text{cov}_{xx}\\times(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})\\\\\n&\\text{se}_{coef}=\\sqrt{\\text{var}_{resid}\\times\\text{diag}(\\text{xx}^{-1})}\\\\\n&\\text{se}_{intercept}=\\sqrt{\\text{var}_{resid}\\left(1/\\text{sum}_{w}+\\text{mean}_{x}^{T}\\text{xx}^{-1}\\text{mean}_{x}\\right)}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_lm(x, y, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_lm(x, y, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_lm(x, y, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_lm(x, y, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq     max neval\n  125 2250.6 3185.80 5073.430 3999.65 5857.75 23729.6   100\n  250 2386.5 2884.85 4569.928 3939.60 5859.95 11480.4   100\n  500 2161.5 3150.70 4549.807 3914.45 5589.60 10888.2   100\n 1000 2181.1 2772.20 4277.718 3660.90 5287.90 15622.6   100\n\n\n\n\nReferences\n\nhttps://stackoverflow.com/a/9933794\nhttps://stackoverflow.com/a/11316626\nhttps://stackoverflow.com/a/34363187\nhttps://stackoverflow.com/a/243342\nhttps://stats.stackexchange.com/a/64217\nhttps://stackoverflow.com/a/51992954\nhttps://stackoverflow.com/a/25921772\nhttps://stackoverflow.com/a/40416506\nhttps://stackoverflow.com/a/5970314\nhttps://gist.github.com/ashelly/5665911\nhttps://stackoverflow.com/a/51992954"
  },
  {
    "objectID": "posts/markets-py/index.html",
    "href": "posts/markets-py/index.html",
    "title": "Markets",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)"
  },
  {
    "objectID": "posts/markets-py/index.html#expected-value",
    "href": "posts/markets-py/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-py/index.html#variance",
    "href": "posts/markets-py/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\ndef sd(x):\n    \n    n_rows = sum(~np.isnan(x))\n        \n    if n_rows &gt; 1:\n        result = np.sqrt(np.nansum(x ** 2) / (n_rows - 1))\n    else:\n        result = np.nan\n        \n    return result\n\n\n# volatility scale only\nscore_df = (momentum_df / momentum_df.rolling(width, min_periods = 1).apply(sd, raw = False)).dropna()\n\n\n# overall_df = score_df.mean(axis = 1)\n# overall_df = overall_df / overall_df.rolling(width, min_periods = 1).apply(risk, raw = False)\n\n\n# score_df.insert(loc = 0, column = \"Overall\", value = overall_df)\n# score_df = score_df.dropna()"
  },
  {
    "objectID": "posts/markets-r/index.html",
    "href": "posts/markets-r/index.html",
    "title": "Markets",
    "section": "",
    "text": "library(quantmod)\nlibrary(roll)\nfactors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\nfactors &lt;- c(factors_r, factors_d)\nwidth &lt;- 252\nscale &lt;- list(\"periods\" = 252, \"overlap\" = 5)\ngetSymbols(factors, src = \"FRED\")\nlevels_xts &lt;- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))\nreturns_xts &lt;- do.call(merge, lapply(factors, function(i) {\n    if (i %in% factors_r) {\n        diff(log((levels_xts[ , i])))\n    } else if (i %in% factors_d) {\n        -diff(levels_xts[ , i]) / 100\n    }    \n}))\noverlap_xts &lt;- roll_mean(returns_xts, scale[[\"overlap\"]], min_obs = 1, na_restore = TRUE)"
  },
  {
    "objectID": "posts/markets-r/index.html#expected-value",
    "href": "posts/markets-r/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-r/index.html#variance",
    "href": "posts/markets-r/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\n# volatility scale only\nscore_xts &lt;- na.omit(momentum_xts / roll_sd(momentum_xts, width, center = FALSE, min_obs = 1))\n\n\n# overall_xts &lt;- xts(rowMeans(score_xts), index(score_xts))\n# overall_xts &lt;- overall_xts / roll_sd(overall_xts, width, center = FALSE, min_obs = 1)\n# colnames(overall_xts) &lt;- \"Overall\"\n\n\n# score_xts &lt;- na.omit(merge(overall_xts, score_xts))"
  },
  {
    "objectID": "posts/securities-py/index.html",
    "href": "posts/securities-py/index.html",
    "title": "Securities",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\nsd_df = returns_df[\"overlap\"].rolling(width, min_periods = 1).std() * \\\n    np.sqrt(scale[\"periods\"]) * np.sqrt(scale[\"overlap\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#value",
    "href": "posts/securities-py/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    if (type == \"call\"):\n        result =  S * np.exp(-q * tau) * Phi(d1) - np.exp(-r * tau) * K * Phi(d2)\n    elif (type == \"put\"):\n        result = np.exp(-r * tau) * K * Phi(-d2) - S * np.exp(-q * tau) * Phi(-d1)\n        \n    return result\n\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    r_df = np.exp(-r * tau)\n    q_df = np.exp(-q * tau)\n    \n    call_value = S * q_df * Phi(d1) - r_df * K * Phi(d2)\n    put_value = r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\ndef bs_d1(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q + sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n\ndef bs_d2(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q - sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n    \ndef phi(x):\n    \n    result = norm.pdf(x)\n    \n    return result\n\ndef Phi(x):\n    \n    result = norm.cdf(x)\n    \n    return result\n\n\ngreeks_df[\"d1\"] = bs_d1(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"d2\"] = bs_d2(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"value\"] = bs_value(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order",
    "href": "posts/securities-py/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_delta(type, S, K, r, q, tau, sigma, d1, d2):\n  \n    q_df = np.exp(-q * tau)\n    \n    call_value = q_df * Phi(d1)\n    put_value = -q_df * Phi(-d1)\n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\n\ngreeks_df[\"delta\"] = bs_delta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\nDelta-beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\ndef bs_delta_diff(type, S, K, r, q, tau, sigma, delta0):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    call_value = delta - delta0\n    put_value = delta0 - delta\n    \n    result = np.where(type == \"call\", call_value, put_value)\n        \n    return result\n\n\nbeta = 0.35\ntype = \"call\"\nn = 1\nmultiple = 100\ntotal = 1000000\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\nsec = {\n    \"n\": n,\n    \"multiple\": multiple,\n    \"S\": S,\n    \"delta\": bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n    \"beta\": 1\n}\n\n\nbeta_df = pd.DataFrame([(x, y) for x in types for y in shocks], \n    columns = [\"type\", \"shock\"])\nbeta_df[\"spot\"] = level_shock(beta_df[\"shock\"], S, tau, sigma)\nbeta_df[\"static\"] = beta\nbeta_df[\"diff\"] = bs_delta_diff(type, beta_df[\"spot\"], K, r, q, tau, sigma, sec[\"delta\"])\nbeta_df[\"dynamic\"] = beta + sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * sec[\"beta\"] * beta_df[\"diff\"] / total\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\ndef bs_vega(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    q_df = np.exp(-q * tau)\n    result = S * q_df * phi(d1) * np.sqrt(tau)\n    \n    return result\n\n\ngreeks_df[\"vega\"] = bs_vega(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                            greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_theta(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    r_df = np.exp(-r * tau)\n    q_df = np.exp(-q * tau)\n    \n    call_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) - \\\n        r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n        \n    put_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) + \\\n        r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n        \n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\n\ngreeks_df[\"theta\"] = bs_theta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order",
    "href": "posts/securities-py/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\ndef bs_gamma(type, S, K, r, q, tau, sigma, d1, d2):\n  \n    q_df = np.exp(-q * tau)\n    \n    result = q_df * phi(d1) / (S * sigma * np.sqrt(tau))\n    \n    return result\n\n\ngreeks_df[\"gamma\"] = bs_gamma(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#price-yield-formula",
    "href": "posts/securities-py/index.html#price-yield-formula",
    "title": "Securities",
    "section": "Price-yield formula",
    "text": "Price-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(P(y+\\Delta y)-P(y)\\approx -D\\Delta y +{\\frac{C\\times 100}{2!}}(\\Delta y)^{2}\\)\n\ndef pnl_bond(duration, convexity, dy):\n    \n    duration_pnl = -duration * dy\n    convexity_pnl = (convexity * 100 / 2) * dy ** 2\n    income_pnl = dy\n    \n    result = pd.DataFrame({\"total\": duration_pnl + convexity_pnl + income_pnl,\n                           \"duration\": duration_pnl,\n                           \"convexity\": convexity_pnl,\n                           \"income\": income_pnl})\n    \n    return result\n\n\n# https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nfactor = \"DGS10\"\nduration = 6.5\nconvexity = 0.65\ny = levels_df.fillna(method = \"ffill\")[factor][-width]\n\n\nbond_df = pd.DataFrame({\"duration\": duration, \"convexity\": convexity,\n    \"dy\": (levels_df.fillna(method = \"ffill\")[factor][-width:] - y) / 100})\n\n\nattrib_df = pnl_bond(bond_df[\"duration\"], bond_df[\"convexity\"], bond_df[\"dy\"])\n\n\n\n\n\n\n\nDuration drift\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=-\\frac{\\partial D}{\\partial y}\\\\\n&=\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}-\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=C-D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(C\\times 100-D^{2}\\right)=C-\\frac{D^{2}}{100}\\)\n\ndef yield_shock(shock, tau, sigma):\n    \n    result = shock * sigma * np.sqrt(tau)\n    \n    return result\n\n\ndef duration_drift(duration, convexity, dy):\n    \n    drift = convexity - duration ** 2 / 100\n    change = -drift * dy * 100\n    \n    result = {\"drift\": drift,\n              \"change\": change}\n    \n    return result\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor = \"DGS10\"\nsigma = sd_df[factor][-1]\n\n\nduration_df = pd.DataFrame(shocks).rename(columns = {0: \"shock\"})\nduration_df[\"spot\"] = duration_df[\"shock\"].apply(yield_shock, args = (tau, sigma))\nduration_df[\"static\"] = duration\nduration_df[\"dynamic\"] = duration_df \\\n    .apply(lambda x: duration +\n           duration_drift(duration, convexity, x[\"spot\"])[\"change\"], axis = 1)\n\n\nduration_mlt &lt;- as.data.table(py$duration_df)[ , \"spot\" := NULL]\nduration_mlt &lt;- melt(duration_mlt, id.vars = \"shock\")\nduration_plt &lt;- plot_scen(duration_mlt, title = \"Duration\", xlab = \"Shock\")\nprint(duration_plt)"
  },
  {
    "objectID": "posts/securities-py/index.html#blacks-formula",
    "href": "posts/securities-py/index.html#blacks-formula",
    "title": "Securities",
    "section": "Black’s formula",
    "text": "Black’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\n# unable to verify the result\ndef pnl_option(type, S, K, r, q, tau, sigma, dS, dt, dsigma):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    value = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega = bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta = bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma = bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl = delta * dS / value\n    gamma_pnl = gamma * dS ** 2 / value\n    vega_pnl = vega * dsigma / value\n    theta_pnl = theta * dt / value\n    \n    result = {\"total\": delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n              \"delta\": delta_pnl,\n              \"gamma\": gamma_pnl,\n              \"vega\": vega_pnl,\n              \"theta\": theta_pnl}\n    \n    return result\n\n\nfactor = \"SP500\"\ntype = \"call\"\nS = levels_df.fillna(method = \"ffill\")[factor][-width]\nK = S * (1 + 0.05)\ntau = 1 # = 252 / 252\nsigma = sd_df[factor][-width]\n\n\noptions_df = pd.concat(dict(spot = levels_df.fillna(method = \"ffill\")[factor][-width:],\n                            sigma = sd_df[factor][-width:]), axis = 1)\noptions_df[\"dS\"] = options_df[\"spot\"] - S\noptions_df[\"dt\"] = (options_df.index - options_df.index[0]).days / 360\noptions_df[\"dsigma\"] = options_df[\"sigma\"] - sigma\n\n\nattrib_cols = [\"total\", \"delta\", \"gamma\", \"vega\", \"theta\"]\nattrib_df = options_df.apply(lambda x: pnl_option(type, S, K, r, q, tau, sigma,\n                                                  x[\"dS\"], x[\"dt\"], x[\"dsigma\"]), axis = 1)\nattrib_df = pd.DataFrame.from_records(attrib_df, index = attrib_df.index)\nattrib_df = attrib_df[attrib_cols]\n\n\nattrib_mlt &lt;- melt(as.data.table(py$attrib_df, keep.rownames = \"index\"), id.vars = \"index\")\nattrib_mlt[ , index := as.Date(index)]\nattrib_plt &lt;- plot_ts_decomp(attrib_mlt, decomp = \"Total\", title = \"Attribution 1Y (%)\")\nprint(attrib_plt)"
  },
  {
    "objectID": "posts/securities-py/index.html#itos-lemma",
    "href": "posts/securities-py/index.html#itos-lemma",
    "title": "Securities",
    "section": "Ito’s lemma",
    "text": "Ito’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\ndef sim_gbm(n_sim, S, mu, sigma, dt):\n    \n    result = S * np.exp(np.cumsum(sigma * np.sqrt(dt) * np.random.normal(size = n_sim)) + \\\n                        (mu - 0.5 * sigma ** 2) * dt)\n    \n    return result\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\ndef sim_multi_gbm(n_sim, S, mu, sigma, dt):\n    \n    n_cols = sigma.shape[1]\n    \n    Z = np.random.normal(size = n_sim * n_cols).reshape((n_sim, n_cols))\n    X = np.add(np.sqrt(dt) * np.matmul(np.matrix(Z), np.linalg.cholesky(sigma).T),\n               ((mu - 0.5 * np.diag(sigma)) * dt))\n    \n    result = np.multiply(S, np.exp(X.cumsum(axis = 0)))\n    \n    return np.asmatrix(result)\n\n\n# https://arxiv.org/pdf/0812.4210.pdf\n# https://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\n# https://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\n# https://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\n# https://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\nS = [1] * len(factors)\nsigma = np.cov(returns_df[\"returns\"].dropna().T, ddof = 1) * scale[\"periods\"]\nmu = returns_df[\"returns\"].dropna().mean().tolist()\nmu = [x * scale[\"periods\"] for x in mu]\nmu = mu + np.diag(sigma) / 2 # drift\ndt = 1 / scale[\"periods\"]\n\n\nmu_df = pd.DataFrame()\nsigma_df = pd.DataFrame()\n\n\nfor i in range(1000):\n    \n    # assumes stock prices\n    levels_sim = pd.DataFrame(sim_multi_gbm(width + 1, S, mu, sigma, dt))\n    returns_sim = np.log(levels_sim).diff().dropna()\n    \n    mu_sim = returns_sim.mean() * scale[\"periods\"]\n    sigma_sim = returns_sim.std() * np.sqrt(scale[\"periods\"])\n    \n    mu_df = mu_df.append(pd.DataFrame(mu_sim).T)\n    sigma_df = sigma_df.append(pd.DataFrame(sigma_sim).T)\n\n\npd.DataFrame.from_dict({\"empirical\": np.array(returns_df[\"returns\"].dropna().mean()) * scale[\"periods\"],\n                        \"theoretical\": np.array(mu_df.mean())})\n\n   empirical  theoretical\n0   0.084761     0.086283\n1   0.019915     0.020340\n2  -0.001218    -0.001130\n3   0.000805     0.000902\n\n\n\npd.DataFrame.from_dict({\"empirical\": np.sqrt(np.diag(sigma)),\n                        \"theoretical\": np.array(sigma_df.mean())})\n\n   empirical  theoretical\n0   0.179678     0.179681\n1   0.062071     0.062109\n2   0.008160     0.008145\n3   0.016927     0.016933"
  },
  {
    "objectID": "posts/securities-py/index.html#newtons-method",
    "href": "posts/securities-py/index.html#newtons-method",
    "title": "Securities",
    "section": "Newton’s method",
    "text": "Newton’s method\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\ndef implied_vol_newton(params, type, S, K, r, q, tau):\n    \n    target0 = 0\n    sigma0 = params[\"sigma\"]\n    \n    while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n        \n        d1 = bs_d1(S, K, r, q, tau, sigma0)\n        d2 = bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 = bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        vega0 = bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma = sigma0 - (target0 - params[\"target\"]) / vega0\n        sigma0 = sigma\n        \n    return sigma\n\n\n# http://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\n# https://books.google.com/books?id=VLi61POD61IC&pg=PA104\nS = levels_df.fillna(method = \"ffill\")[factor][-1]\nK = S * (1 + 0.05)\nsigma = sd_df[factor][-1] # overrides matrix\nstart = 0.2\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\ntarget = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams = {\n    \"target\": target,\n    \"sigma\": start,\n    \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nimplied_vol_newton(params, type, S, K, r, q, tau) \n\n0.14334258697457591"
  },
  {
    "objectID": "posts/securities-py/index.html#optimization",
    "href": "posts/securities-py/index.html#optimization",
    "title": "Securities",
    "section": "Optimization",
    "text": "Optimization\nIf the derivative is unknown, try optimization:\n\ndef implied_vol_obj(param, type, S, K, r, q, tau, target):\n    \n    d1 = bs_d1(S, K, r, q, tau, param)\n    d2 = bs_d2(S, K, r, q, tau, param)\n    target0 = bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result = abs(target0 - target)\n    \n    return result\n\ndef implied_vol_optim(param, type, S, K, r, q, tau, target):\n    \n    result = minimize(implied_vol_obj, param, args = (type, S, K, r, q, tau, target))\n    \n    return result.x.item()\n\n\nimplied_vol_optim(start, type, S, K, r, q, tau, target)\n\n0.1433425812588892"
  },
  {
    "objectID": "posts/securities-py/index.html#variance-swaps",
    "href": "posts/securities-py/index.html#variance-swaps",
    "title": "Securities",
    "section": "Variance swaps",
    "text": "Variance swaps\nA variance swap can be written as:\n\\[\n\\begin{aligned}\nK_{var}={\\frac{2e^{rT}}{T}}\\left(\\int\\limits_{0}^{F_{0}}{\\frac{1}{K^{2}}}P(K)dK+\\int\\limits_{F_{0}}^{\\infty}{\\frac{1}{K^{2}}} C(K)dK\\right)\n\\end{aligned}\n\\]\nThe CBOE Volatility Index (VIX) is calculated as a variance swap on the 30-day variance of the S&P 500 with an adjustment term:\n\\[\n\\begin{aligned}\n\\sigma^{2}={\\frac{2e^{rT}}{T}}\\left(\\sum_{i=0}^{K_{0}}\\frac{\\Delta K_{i}}{K_{i}^{2}}P(K_{i})+\\sum_{i=K_{0}}^{\\infty}\\frac{\\Delta K_{i}}{K_{i}^{2}}C(K_{i})\\right)-\\frac{1}{T}\\left(\\frac{F}{K_{0}}-1\\right)^{2}\n\\end{aligned}\n\\]\n\ndef rle(x):\n    \n    n = len(x)\n    y = x.iloc[1:].reset_index(drop = True) != x.iloc[:-1].reset_index(drop = True)\n    i = y[y].index.tolist() + [n - 1]\n    \n    result = {\"lengths\": np.diff([0] + i),\n              \"values\": x.reset_index(drop = True)[i]}\n    \n    return result\n\n\n# https://cdn.cboe.com/resources/vix/vixwhite.pdf\n# https://en.wikipedia.org/wiki/VIX\n# https://en.wikipedia.org/wiki/Variance_swap\n# https://www.ivolatility.com/doc/VarianceSwaps.pdf\ndef implied_vol_vix(calls_df, puts_df, r, tau):\n    \n    # time to expiration \n    t = ((tau[\"exp_time\"] - tau[\"sys_time\"]).total_seconds() / 60) / (365 * 24 * 60)\n    \n    # midpoint of bid and ask\n    calls_df[\"Mid\"] = calls_df[[\"Bid\", \"Ask\"]].mean(axis = 1)\n    puts_df[\"Mid\"] = puts_df[[\"Bid\", \"Ask\"]].mean(axis = 1)\n    \n    options_df = calls_df[[\"Strike\", \"Mid\"]] \\\n        .merge(puts_df[[\"Strike\", \"Mid\"]], on = \"Strike\") \\\n        .rename(columns = {\"Mid_x\": \"Call\", \"Mid_y\": \"Put\"})\n    \n    options_df[\"Diff\"] = abs(options_df[\"Call\"] - options_df[\"Put\"])\n    \n    # minimum absolute difference is forward index level\n    forward_df = options_df.loc[options_df[\"Diff\"] == min(options_df[\"Diff\"])]\n    k = forward_df[\"Strike\"]\n    c = forward_df[\"Call\"]\n    p = forward_df[\"Put\"]\n    f = k + np.exp(r * t) * (c - p)\n    \n    # strike price equal or below forward index level\n    k0 = options_df.loc[options_df[\"Strike\"] &lt;= int(f), \"Strike\"].iloc[-1]\n    \n    # out-of-the-money options\n    puts_otm_df = puts_df.loc[puts_df[\"Strike\"] &lt; k0]\n    calls_otm_df = calls_df.loc[calls_df[\"Strike\"] &gt; k0]\n\n    # stop after two consecutive strike prices with zero bid prices\n    # https://stackoverflow.com/a/50311890\n    puts_otm_rle = rle(puts_otm_df[\"Bid\"])\n    idx = puts_otm_rle[\"lengths\"].cumsum() # end\n    try:\n        idx = idx[(puts_otm_rle[\"lengths\"] &gt; 1) & (puts_otm_rle[\"values\"] == 0)].max()\n        puts_otm_df = puts_otm_df.iloc[(idx + 1):]\n    except:\n        pass\n    \n    calls_otm_rle = rle(calls_otm_df[\"Bid\"])\n    idx = calls_otm_rle[\"lengths\"].cumsum() # end\n    idx = idx - calls_otm_rle[\"lengths\"] + 1 # start\n    try:\n        idx = idx[(calls_otm_rle[\"lengths\"] &gt; 1) & (calls_otm_rle[\"values\"] == 0)].max()\n        calls_otm_df = calls_otm_df.iloc[:idx]\n    except:\n        pass\n\n    # average put and call prices for k0\n    # note: exclude options with zero bid price\n    result_df = puts_otm_df.loc[puts_otm_df[\"Bid\"] != 0, [\"Strike\", \"Mid\"]] \\\n        .append(pd.DataFrame(puts_df.loc[puts_df[\"Strike\"] == k0, [\"Strike\", \"Mid\"]] \\\n                             .append(calls_df.loc[calls_df[\"Strike\"] == k0,\n                                                  [\"Strike\", \"Mid\"]]).mean(axis = 0)).transpose() \\\n                .append(calls_otm_df.loc[calls_otm_df[\"Bid\"] != 0,\n                                         [\"Strike\", \"Mid\"]])).reset_index(drop = True)\n    \n    # differences between strike prices\n    # note: create new column\n    result_df.loc[0, \"Diff\"] = result_df.loc[1, \"Strike\"] - result_df.loc[0, \"Strike\"]\n    result_df[\"Diff\"].iloc[-1] = result_df[\"Strike\"].iloc[-1] - result_df[\"Strike\"].iloc[-2]\n    result_df[\"Diff\"].iloc[1:-1] = (result_df[\"Strike\"].iloc[2:].values -\n                                    result_df[\"Strike\"].iloc[:-2].values) / 2\n\n    # variance\n    v = sum(result_df[\"Diff\"] / result_df[\"Strike\"] ** 2 * np.exp(r * t) *\n            result_df[\"Mid\"]) * (2 / t) - (1 / t) * (f / k0 - 1) ** 2\n        \n    result = {\"t\": t,\n              \"v\": v}\n            \n    return result\n\n\nsys_time = datetime.datetime.now().replace(hour = 9, minute = 46, second = 0)\n\n\nv1 = implied_vol_vix(pd.read_csv(\"../securities/near_calls.csv\"), pd.read_csv(\"../securities/near_puts.csv\"), 0.000305,\n                     {\"sys_time\": sys_time,\n                      \"exp_time\": sys_time.replace(hour = 8, minute = 30, second = 0) +\n                                  datetime.timedelta(days = 25)})\nv2 = implied_vol_vix(pd.read_csv(\"../securities/next_calls.csv\"), pd.read_csv(\"../securities/next_puts.csv\"), 0.000286,\n                     {\"sys_time\": sys_time,\n                      \"exp_time\": sys_time.replace(hour = 15, minute = 0, second = 0) +\n                                  datetime.timedelta(days = 32)})\n\n\nnt1 = v1[\"t\"] * (365 * 24 * 60)\nnt2 = v2[\"t\"] * (365 * 24 * 60)\nn30 = 30 * (24 * 60)\nn365 = 365 * (24 * 60)\n\n\nvix = np.sqrt(((v1[\"t\"] * v1[\"v\"] * ((nt2 - n30) / (nt2 - nt1))).values +\n               (v2[\"t\"] * v2[\"v\"] * ((n30 - nt1) / (nt2 - nt1))).values) * (n365 / n30))\nvix\n\narray([0.13685821])"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-yield",
    "href": "posts/securities-py/index.html#implied-yield",
    "title": "Securities",
    "section": "Implied yield",
    "text": "Implied yield\n\ndef yield_option(type, S, K, r, q, tau, sigma):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    value = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    if (type == \"call\"):\n        result = (value / S) / tau\n    elif (type == \"put\"):\n        result = (value / K) / tau\n        \n    return result\n\n\nsigmas = [x / 100 for x in range(10, 31, 4)]\ntaus = [x / 252 for x in range(20, 127, 20)]\n\n\nyield_df = pd.DataFrame([(x, y) for x in sigmas for y in taus]) \\\n    .rename(columns = {0: \"sigma\", 1: \"tau\"})\nyield_df[\"yield\"] = yield_df.apply(lambda x: yield_option(type, S, K, r, q, x[\"tau\"], x[\"sigma\"]),\n                                   axis = 1)\nyield_df[\"sigma\"] = yield_df[\"sigma\"] * 100\nyield_df[\"tau\"] = yield_df[\"tau\"] * scale[\"periods\"]\n\n\nyield_plt &lt;- plot_heatmap(py$yield_df, x = \"tau\", y = \"sigma\", z = \"yield\",\n                          title = \"Yield (%)\", xlab = \"Tau\", ylab = \"Sigma\")\nprint(yield_plt)"
  },
  {
    "objectID": "posts/securities-r/index.html",
    "href": "posts/securities-r/index.html",
    "title": "Securities",
    "section": "",
    "text": "library(quantmod)\nlibrary(roll)\nlibrary(data.table)\nfactors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\nfactors &lt;- c(factors_r, factors_d)\nwidth &lt;- 252\nscale &lt;- list(\"periods\" = 252, \"overlap\" = 5)\ngetSymbols(factors, src = \"FRED\")\nlevels_xts &lt;- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))\nreturns_xts &lt;- do.call(merge, lapply(factors, function(i) {\n    if (i %in% factors_r) {\n        diff(log((levels_xts[ , i])))\n    } else if (i %in% factors_d) {\n        -diff(levels_xts[ , i]) / 100\n    }    \n}))\noverlap_xts &lt;- roll_mean(returns_xts, scale[[\"overlap\"]], min_obs = 1, na_restore = TRUE)\nsd_xts &lt;- roll_sd(overlap_xts, width, min_obs = 1) * sqrt(scale[[\"periods\"]]) * sqrt(scale[[\"overlap\"]])"
  },
  {
    "objectID": "posts/securities-r/index.html#value",
    "href": "posts/securities-r/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_value &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n    r_df &lt;- exp(-r * tau)\n    q_df &lt;- exp(-q * tau)\n    \n    call_value &lt;- S * q_df * Phi(d1) - r_df * K * Phi(d2)\n    put_value &lt;- r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n} \n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\nbs_d1 &lt;- function(S, K, r, q, tau, sigma) {\n    \n    result &lt;- (log(S / K) + (r - q + sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\nbs_d2 &lt;- function(S, K, r, q, tau, sigma) {\n    \n    result &lt;- (log(S / K) + (r - q - sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\nphi &lt;- function(x) {\n    \n    result &lt;- dnorm(x)\n    \n    return(result)\n    \n}\n\nPhi &lt;- function(x) {\n    \n    result &lt;- pnorm(x)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , d1 := bs_d1(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , d2 := bs_d2(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , value := bs_value(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order",
    "href": "posts/securities-r/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_delta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    q_df &lt;- exp(-q * tau)\n  \n    call_value &lt;- q_df * Phi(d1)\n    put_value &lt;- -q_df * Phi(-d1)\n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n} \n\n\ngreeks_dt[ , delta := bs_delta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nDelta-beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\nbs_delta_diff &lt;- function(type, S, K, r, q, tau, sigma, delta0) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    call_value &lt;- delta - delta0\n    put_value &lt;- delta0 - delta\n    \n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n        \n    return(result)\n    \n}\n\n\nbeta &lt;- 0.35\ntype &lt;- \"call\"\nn &lt;- 1\nmultiple &lt;- 100\ntotal &lt;- 1000000\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\nsec &lt;- list(\n    \"n\" = n,\n    \"multiple\" = multiple,\n    \"S\" = S,\n    \"delta\" = bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n    \"beta\" = 1\n)\n\n\nbeta_dt &lt;- CJ(type = type, shock = shocks)\nbeta_dt[ , spot := level_shock(shock, S, tau, sigma), by = c(\"type\", \"shock\")]\nbeta_dt[ , static := beta]\nbeta_dt[ , diff := bs_delta_diff(type, spot, K, r, q, tau, sigma, sec[[\"delta\"]])]\nbeta_dt[ , dynamic := beta + sec[[\"n\"]] * sec[[\"multiple\"]] * sec[[\"S\"]] * sec[[\"beta\"]] * diff / total, by = c(\"type\", \"shock\")]\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\nbs_vega &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    q_df &lt;- exp(-q * tau)\n    \n    result &lt;- S * q_df * phi(d1) * sqrt(tau)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , vega := bs_vega(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_theta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    r_df &lt;- exp(r * tau)\n    q_df &lt;- exp(q * tau)\n  \n    call_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) -\n      r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n    \n    put_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) +\n      r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n        \n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , theta := bs_theta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order",
    "href": "posts/securities-r/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\nbs_gamma &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n    q_df &lt;- exp(-q * tau)\n    \n    result &lt;- q_df * phi(d1) / (S * sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , gamma := bs_gamma(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#price-yield-formula",
    "href": "posts/securities-r/index.html#price-yield-formula",
    "title": "Securities",
    "section": "Price-yield formula",
    "text": "Price-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(P(y+\\Delta y)-P(y)\\approx -D\\Delta y +{\\frac{C\\times 100}{2!}}(\\Delta y)^{2}\\)\n\npnl_bond &lt;- function(duration, convexity, dy) {\n    \n    duration_pnl &lt;- -duration * dy\n    convexity_pnl &lt;- (convexity * 100 / 2) * dy ^ 2\n    income_pnl &lt;- dy\n    \n    result &lt;- list(\"total\" = duration_pnl + convexity_pnl + income_pnl,\n                   \"duration\" = duration_pnl,\n                   \"convexity\" = convexity_pnl,\n                   \"income\" = income_pnl)\n    \n    return(result)\n    \n} \n\n\n# https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nfactor &lt;- \"DGS10\"\nduration &lt;- 6.5\nconvexity &lt;- 0.65\ny &lt;- coredata(tail(na.locf(levels_xts[ , factor]), width)[1])\n\n\nbonds_dt &lt;- data.table(index = index(tail(levels_xts, width)),\n                       duration = duration, convexity = convexity,\n                       dy = na.locf(tail(levels_xts[ , factor], width)))\nsetnames(bonds_dt, c(\"index\", \"duration\", \"convexity\", \"dy\"))\nbonds_dt[ , dy := (dy - y) / 100, by = index]\n\n\nattrib_dt &lt;- bonds_dt[ , as.list(unlist(pnl_bond(duration, convexity, dy))), by = index]\n\n\nattrib_mlt &lt;- melt(attrib_dt, id.vars = \"index\")\nattrib_plt &lt;- plot_ts_decomp(attrib_mlt, decomp = \"Total\", title = \"Attribution 1Y (%)\")\nprint(attrib_plt)\n\n\n\n\n\nDuration drift\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=-\\frac{\\partial D}{\\partial y}\\\\\n&=\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}-\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=C-D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(C\\times 100-D^{2}\\right)=C-\\frac{D^{2}}{100}\\)\n\nyield_shock &lt;- function(shock, tau, sigma) {\n    \n    result &lt;- shock * sigma * sqrt(tau)\n    \n    return(result)\n    \n}\n\n\nduration_drift &lt;- function(duration, convexity, dy) {\n    \n    drift &lt;- convexity - duration ^ 2 / 100\n    change &lt;- -drift * dy * 100\n    \n    result &lt;- list(\"drift\" = drift,\n                   \"change\" = change)\n    \n    return(result)\n    \n}\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor &lt;- \"DGS10\"\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor])\n\n\nduration_dt &lt;- CJ(shock = shocks)\nduration_dt[ , spot := yield_shock(shock, tau, sigma), by = \"shock\"]\nduration_dt[ , static := duration]\nduration_dt[ , dynamic := duration + duration_drift(duration, convexity, spot)[[\"change\"]], by = \"shock\"]\n\n\nduration_mlt &lt;- copy(duration_dt)[ , \"spot\" := NULL]\nduration_mlt &lt;- melt(duration_mlt, id.vars = \"shock\")\nduration_plt &lt;- plot_scen(duration_mlt, title = \"Duration\", xlab = \"Shock\")\nprint(duration_plt)"
  },
  {
    "objectID": "posts/securities-r/index.html#blacks-formula",
    "href": "posts/securities-r/index.html#blacks-formula",
    "title": "Securities",
    "section": "Black’s formula",
    "text": "Black’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\n# unable to verify the result\npnl_option &lt;- function(type, S, K, r, q, tau, sigma, dS, dt, dsigma) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    value &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega &lt;- bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta &lt;- bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma &lt;- bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl &lt;- delta * dS / value\n    gamma_pnl &lt;- gamma * dS ^ 2 / value\n    vega_pnl &lt;- vega * dsigma / value\n    theta_pnl &lt;- theta * dt / value\n    \n    result &lt;- list(\"total\" = delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n                   \"delta\" = delta_pnl,\n                   \"gamma\" = gamma_pnl,\n                   \"vega\" = vega_pnl,\n                   \"theta\" = theta_pnl)\n    \n    return(result)    \n    \n}\n\n\nfactor &lt;- \"SP500\"\ntype &lt;- \"call\"\nS &lt;- coredata(tail(na.locf(levels_xts[ , factor]), width)[1])\nK &lt;- S * (1 + 0.05)\ntau &lt;- 1 # = 252 / 252\nsigma &lt;- coredata(tail(sd_xts[ , factor], width)[1])\n\n\noptions_dt &lt;- data.table(index = index(tail(levels_xts, width)),\n                         spot = na.locf(tail(levels_xts[ , factor], width)),\n                         sigma = tail(sd_xts[ , factor], width))\nsetnames(options_dt, c(\"index\", \"spot\", \"sigma\"))\noptions_dt[ , dS := spot - S, by = index]\noptions_dt[ , dt := (index - index(tail(levels_xts, width))[1]) / 360, by = index]\noptions_dt[ , dsigma := sigma - ..sigma, by = index]\n\n\nattrib_dt &lt;- options_dt[ , as.list(unlist(pnl_option(type, S, K, r, q, tau, ..sigma,\n                                                     dS, dt, dsigma))), by = index]\n\n\nattrib_mlt &lt;- melt(attrib_dt, id.vars = \"index\")\nattrib_plt &lt;- plot_ts_decomp(attrib_mlt, decomp = \"Total\", title = \"Attribution 1Y (%)\")\nprint(attrib_plt)"
  },
  {
    "objectID": "posts/securities-r/index.html#itos-lemma",
    "href": "posts/securities-r/index.html#itos-lemma",
    "title": "Securities",
    "section": "Ito’s lemma",
    "text": "Ito’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\nsim_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n    \n    result &lt;- S * exp(cumsum(sigma * sqrt(dt) * rnorm(n_sim)) +\n                      (mu - 0.5 * sigma ^ 2) * dt)\n    \n    return(result)\n    \n}\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\nsim_multi_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n    \n    n_cols &lt;- ncol(sigma)\n    \n    Z &lt;- matrix(rnorm(n_sim * n_cols), nrow = n_sim, ncol = n_cols)\n    X &lt;- sweep(sqrt(dt) * (Z %*% chol(sigma)), 2, (mu - 0.5 * diag(sigma)) * dt, \"+\")\n    \n    result &lt;- sweep(apply(X, 2, function(x) exp(cumsum(x))), 2, S, \"*\")\n    \n    return(result)\n    \n}\n\n\n# https://arxiv.org/pdf/0812.4210.pdf\n# https://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\n# https://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\n# https://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\n# https://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\nS &lt;- rep(1, length(factors))\nsigma &lt;- cov(returns_xts, use = \"complete.obs\") * scale[[\"periods\"]]\nmu &lt;- colMeans(na.omit(returns_xts)) * scale[[\"periods\"]]\nmu &lt;- mu + diag(sigma) / 2 # drift\ndt &lt;- 1 / scale[[\"periods\"]]\n\n\nmu_ls &lt;- list()\nsigma_ls &lt;- list()\n\n\nfor (i in 1:1000) {\n    \n    # assumes stock prices\n    levels_sim &lt;- sim_multi_gbm(width + 1, S, mu, sigma, dt)\n    returns_sim &lt;- diff(log(levels_sim))\n\n    mu_sim &lt;- colMeans(returns_sim) * scale[[\"periods\"]]\n    sigma_sim &lt;- apply(returns_sim, 2, sd) * sqrt(scale[[\"periods\"]])\n    \n    mu_ls &lt;- append(mu_ls, list(mu_sim))\n    sigma_ls &lt;- append(sigma_ls, list(sigma_sim))\n    \n}\n\n\ndata.frame(\"empirical\" = colMeans(na.omit(returns_xts)) * scale[[\"periods\"]],\n           \"theoretical\" = colMeans(do.call(\"rbind\", mu_ls)))\n\n                 empirical  theoretical\nSP500         0.0847611377  0.091184184\nDTWEXAFEGS    0.0199149891  0.018494312\nDGS10        -0.0012181269 -0.001486700\nBAMLH0A0HYM2  0.0008048338  0.001880054\n\n\n\ndata.frame(\"empirical\" = sqrt(diag(sigma)),\n           \"theoretical\" = colMeans(do.call(\"rbind\", sigma_ls)))\n\n              empirical theoretical\nSP500        0.17967761 0.179374852\nDTWEXAFEGS   0.06207109 0.062019472\nDGS10        0.00815995 0.008122258\nBAMLH0A0HYM2 0.01692703 0.016891129"
  },
  {
    "objectID": "posts/securities-r/index.html#newtons-method",
    "href": "posts/securities-r/index.html#newtons-method",
    "title": "Securities",
    "section": "Newton’s method",
    "text": "Newton’s method\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\nimplied_vol_newton &lt;- function(params, type, S, K, r, q, tau) {\n    \n    target0 &lt;- 0\n    sigma0 &lt;- params[[\"sigma\"]]\n    \n    while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n        \n        d1 &lt;- bs_d1(S, K, r, q, tau, sigma0)\n        d2 &lt;- bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 &lt;- bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        vega0 &lt;- bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma &lt;- sigma0 - (target0 - params[[\"target\"]]) / vega0\n        sigma0 &lt;- sigma\n        \n    }\n    \n    return(sigma)\n    \n}\n\n\n# http://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\n# https://books.google.com/books?id=VLi61POD61IC&pg=PA104\nS &lt;- coredata(na.locf(levels_xts)[nrow(levels_xts), factor])\nK &lt;- S * (1 + 0.05)\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor]) # overrides matrix\nstart &lt;- 0.2\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\ntarget &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams &lt;- list(\n    \"target\" = target,\n    \"sigma\" = start,\n    \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nimplied_vol_newton(params, type, S, K, r, q, tau)\n\n         SP500\n[1,] 0.1439628"
  },
  {
    "objectID": "posts/securities-r/index.html#optimization",
    "href": "posts/securities-r/index.html#optimization",
    "title": "Securities",
    "section": "Optimization",
    "text": "Optimization\nIf the derivative is unknown, try optimization:\n\nimplied_vol_obj &lt;- function(param, type, S, K, r, q, tau, target) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, param)\n    d2 &lt;- bs_d2(S, K, r, q, tau, param)\n    target0 &lt;- bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result &lt;- abs(target0 - target)\n    \n    return(result)\n    \n}\n\nimplied_vol_optim &lt;- function(param, type, S, K, r, q, tau, target) {\n    \n    result &lt;- optim(param, implied_vol_obj, type = type, S = S, K = K, r = r, q = q,\n                    tau = tau, target = target, method = \"Brent\", lower = 0, upper = 1)\n    \n    return(result$par)\n    \n}\n\n\nimplied_vol_optim(start, type, S, K, r, q, tau, target)\n\n[1] 0.1439628"
  },
  {
    "objectID": "posts/securities-r/index.html#variance-swaps",
    "href": "posts/securities-r/index.html#variance-swaps",
    "title": "Securities",
    "section": "Variance swaps",
    "text": "Variance swaps\nA variance swap can be written as:\n\\[\n\\begin{aligned}\nK_{var}={\\frac{2e^{rT}}{T}}\\left(\\int\\limits_{0}^{F_{0}}{\\frac{1}{K^{2}}}P(K)dK+\\int\\limits_{F_{0}}^{\\infty}{\\frac{1}{K^{2}}} C(K)dK\\right)\n\\end{aligned}\n\\]\nThe CBOE Volatility Index (VIX) is calculated as a variance swap on the 30-day variance of the S&P 500 with an adjustment term:\n\\[\n\\begin{aligned}\n\\sigma^{2}={\\frac{2e^{rT}}{T}}\\left(\\sum_{i=0}^{K_{0}}\\frac{\\Delta K_{i}}{K_{i}^{2}}P(K_{i})+\\sum_{i=K_{0}}^{\\infty}\\frac{\\Delta K_{i}}{K_{i}^{2}}C(K_{i})\\right)-\\frac{1}{T}\\left(\\frac{F}{K_{0}}-1\\right)^{2}\n\\end{aligned}\n\\]\n\n# https://cdn.cboe.com/resources/vix/vixwhite.pdf\n# https://en.wikipedia.org/wiki/VIX\n# https://en.wikipedia.org/wiki/Variance_swap\n# https://www.ivolatility.com/doc/VarianceSwaps.pdf\nimplied_vol_vix &lt;- function(calls_df, puts_df, r, tau) {\n  \n  # time to expiration  \n  t &lt;- as.numeric(difftime(tau[[\"exp_time\"]], tau[[\"sys_time\"]], units = \"mins\")) / (365 * 24 * 60)\n  \n  # midpoint of bid and ask\n  calls_df[[\"Mid\"]] &lt;- rowMeans(calls_df[ , c(\"Bid\", \"Ask\")])\n  puts_df[[\"Mid\"]] &lt;- rowMeans(puts_df[ , c(\"Bid\", \"Ask\")])\n  \n  options_df &lt;- merge(calls_df[ , c(\"Strike\", \"Mid\")],\n                      puts_df[ , c(\"Strike\", \"Mid\")], by = \"Strike\")\n  colnames(options_df) &lt;- c(\"Strike\", \"Call\", \"Put\")\n  \n  options_df[[\"Diff\"]] &lt;- abs(options_df[[\"Call\"]] - options_df[[\"Put\"]])\n  \n  # minimum absolute difference is forward index level\n  forward_df &lt;- options_df[options_df[[\"Diff\"]] == min(options_df[[\"Diff\"]]), ]\n  k &lt;- forward_df[[\"Strike\"]]\n  c &lt;- forward_df[[\"Call\"]]\n  p &lt;- forward_df[[\"Put\"]]\n  f &lt;- k + exp(r * t) * (c - p)\n  \n  # strike price equal or below forward index level\n  k0 &lt;- tail(options_df[options_df[[\"Strike\"]] &lt;= f, \"Strike\"], 1)\n  \n  # out-of-the-money options\n  puts_otm_df &lt;- puts_df[puts_df[[\"Strike\"]] &lt; k0, ]\n  calls_otm_df &lt;- calls_df[calls_df[[\"Strike\"]] &gt; k0, ]\n  \n  # stop after two consecutive strike prices with zero bid prices\n  # https://stackoverflow.com/a/50311890\n  puts_otm_rle &lt;- rle(puts_otm_df[[\"Bid\"]])\n  idx &lt;- cumsum(puts_otm_rle$lengths) # end\n  idx &lt;- idx[max(which(puts_otm_rle$lengths &gt; 1 & puts_otm_rle$values == 0))]\n  puts_otm_df &lt;- puts_otm_df[(idx + 1):nrow(puts_otm_df), ]\n  \n  calls_otm_rle &lt;- rle(calls_otm_df[[\"Bid\"]])\n  idx &lt;- cumsum(calls_otm_rle$lengths) # end\n  idx &lt;- idx - calls_otm_rle$lengths + 1 # start\n  idx &lt;- idx[max(which(calls_otm_rle$lengths &gt; 1 & calls_otm_rle$values == 0))]\n  calls_otm_df &lt;- calls_otm_df[1:(idx - 1), ]\n  \n  # average put and call prices for k0\n  # note: exclude options with zero bid price\n  result_df &lt;- rbind(puts_otm_df[puts_otm_df[[\"Bid\"]] != 0, c(\"Strike\", \"Mid\")],\n                     colMeans(rbind(puts_df[puts_df[[\"Strike\"]] == k0, c(\"Strike\", \"Mid\")],\n                                    calls_df[calls_df[[\"Strike\"]] == k0, c(\"Strike\", \"Mid\")])),\n                     calls_otm_df[calls_otm_df[[\"Bid\"]] != 0, c(\"Strike\", \"Mid\")])\n  \n  # differences between strike prices\n  n_rows &lt;- nrow(result_df)\n  result_df[1, \"Diff\"] &lt;- result_df[2, \"Strike\"] - result_df[1, \"Strike\"]\n  result_df[n_rows, \"Diff\"] &lt;- result_df[n_rows, \"Strike\"] - result_df[n_rows - 1, \"Strike\"]\n  result_df[2:(n_rows - 1), \"Diff\"] &lt;- (result_df[3:n_rows, \"Strike\"] -\n                                          result_df[1:(n_rows - 2), \"Strike\"]) / 2\n  \n  # variance\n  v &lt;- sum(result_df[ , \"Diff\"] / result_df[ , \"Strike\"] ^ 2 * exp(r * t) *\n             result_df[ , \"Mid\"]) * (2 / t) - (1 / t) * (f / k0 - 1) ^ 2\n  \n  result &lt;- list(\"t\" = t,\n                 \"v\" = v)\n  \n  return(result)\n  \n}\n\n\nsys_time &lt;- format(Sys.Date(), paste0(\"%Y-%m-%d\", \"09:46:00\"))\n\n\nv1 &lt;- implied_vol_vix(read.csv(\"../securities/near_calls.csv\"), read.csv(\"../securities/near_puts.csv\"), 0.000305,\n                      list(\"sys_time\" = sys_time,\n                           \"exp_time\" = format(Sys.Date() + 25, paste0(\"%Y-%m-%d\", \"08:30:00\"))))\n\nv2 &lt;- implied_vol_vix(read.csv(\"../securities/next_calls.csv\"), read.csv(\"../securities/next_puts.csv\"), 0.000286,\n                      list(\"sys_time\" = sys_time,\n                           \"exp_time\" = format(Sys.Date() + 32, paste0(\"%Y-%m-%d\", \"15:00:00\"))))\n\n\nnt1 &lt;- v1$t * (365 * 24 * 60)\nnt2 &lt;- v2$t * (365 * 24 * 60)\nn30 &lt;- 30 * (24 * 60)\nn365 &lt;- 365 * (24 * 60)\n\n\nvix &lt;- sqrt((v1$t * v1$v * ((nt2 - n30) / (nt2 - nt1)) +\n               v2$t * v2$v * ((n30 - nt1) / (nt2 - nt1))) * (n365 / n30))\nprint(vix)\n\n[1] 0.1368582"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-yield",
    "href": "posts/securities-r/index.html#implied-yield",
    "title": "Securities",
    "section": "Implied yield",
    "text": "Implied yield\n\nyield_option &lt;- function(type, S, K, r, q, tau, sigma) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    value &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    if (type == \"call\") {\n        result &lt;- (value / S) / tau\n    } else if (type == \"put\") {\n        result &lt;- (value / K) / tau\n    }\n    \n    return(result)\n    \n}\n\n\nsigmas &lt;- seq(0.1, 0.3, by = 0.04)\ntaus &lt;- seq(20, 126, by = 20) / 252\n\n\nyield_dt &lt;- CJ(sigma = sigmas, tau = taus)\nyield_dt[ , yield := yield_option(type, S, K, r, q, tau, sigma), by = c(\"sigma\", \"tau\")]\nyield_dt[ , sigma := sigma * 100]\nyield_dt[ , tau := tau * scale[[\"periods\"]]]\n\n\nyield_plt &lt;- plot_heatmap(yield_dt, x = \"tau\", y = \"sigma\", z = \"yield\",\n                          title = \"Yield (%)\", xlab = \"Tau\", ylab = \"Sigma\")\nprint(yield_plt)"
  },
  {
    "objectID": "posts/portfolios-py/index.html",
    "href": "posts/portfolios-py/index.html",
    "title": "Portfolios",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\n# import datetime\nfrom scipy.optimize import minimize\nimport os\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\nprices_df = pdr.get_data_tiingo(tickers, start = \"1900-01-01\", api_key = os.getenv(\"TIINGO_API_KEY\"))\n\nC:\\Users\\jason\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas_datareader\\tiingo.py:234: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  return pd.concat(dfs, self._concat_axis)\n\nprices_df = prices_df.pivot_table(index = \"date\", columns = \"symbol\", values = \"adjClose\") \\\n    .tz_localize(None)\nreturns_cols = list(zip([\"returns\"], tickers))\noverlap_cols = list(zip([\"overlap\"], tickers))\nreturns_df[returns_cols] = np.log(prices_df).diff()\nreturns_df[overlap_cols] = returns_df[returns_cols].rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df.sort_index(axis = 1, inplace = True)\n# weights = np.array([0.9 ** i for i in range(width - 1, -1, -1)]).reshape((width, 1))\nweights = np.array([1] * width).reshape((width, 1))\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors]\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers]\noverlap_x_mat = np.matrix(overlap_x_df[-width:])\noverlap_y_mat = np.matrix(overlap_y_df[-width:])"
  },
  {
    "objectID": "posts/portfolios-py/index.html#factor-models",
    "href": "posts/portfolios-py/index.html#factor-models",
    "title": "Portfolios",
    "section": "Factor models",
    "text": "Factor models\n\nOrdinary least squares\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\n# https://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\ndef lm_coef(x, y, weights, intercept):\n    \n    if (intercept): x = sm.add_constant(x)\n        \n    result = np.matmul(np.linalg.inv(np.matmul(x.T, np.multiply(weights, x))),\n                       np.matmul(x.T, np.multiply(weights, y)))\n    \n    return np.ravel(result)\n\n\nintercept = True\n# weights = np.matrix([0.9 ** i for i in range(width - 1, -1, -1)]).reshape((width, 1))\nweights = np.matrix([1] * width).reshape((width, 1))\n\n\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors]\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers]\noverlap_x_mat = np.matrix(overlap_x_df[-width:])\noverlap_y_mat = np.matrix(overlap_y_df[-width:])\n\n\nlm_coef(overlap_x_mat, overlap_y_mat, weights, intercept)\n\narray([-1.11871891e-05,  1.99036614e-01, -1.38082089e-01,  2.57225254e+00,\n        1.69074657e+00])\n\n\n\nif (intercept): overlap_x_mat = sm.add_constant(overlap_x_mat)\n    \nfit = sm.WLS(overlap_y_mat, overlap_x_mat, weights = weights).fit()\n\nif (intercept): overlap_x_mat = overlap_x_mat[:, 1:]\n\nfit.params\n\narray([-1.11871891e-05,  1.99036614e-01, -1.38082089e-01,  2.57225254e+00,\n        1.69074657e+00])\n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\ndef lm_rsq(x, y, weights, intercept):\n            \n    coef = np.matrix(lm_coef(x, y, weights, intercept))\n    \n    if (intercept):\n        \n        weights_ls = np.array(weights).reshape(-1).tolist()\n        \n        x = sm.add_constant(x)\n        x = x - np.average(x, axis = 0, weights = weights_ls)\n        y = y - np.average(y, axis = 0, weights = weights_ls)\n        \n    result = np.matmul(coef, np.matmul(np.matmul(x.T, np.multiply(weights, x)), coef.T)) / \\\n        (np.matmul(y.T, np.multiply(weights, y)))\n    \n    return result.item()\n\n\nlm_rsq(overlap_x_mat, overlap_y_mat, weights, intercept)\n\n0.8497723857546378\n\n\n\nfit.rsquared\n\n0.8497723857546384\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\n# http://people.duke.edu/~rnau/mathreg.htm\ndef lm_se(x, y, weights, intercept):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept):\n        \n        weights_ls = np.array(weights).reshape(-1).tolist()\n        \n        x = sm.add_constant(x)\n        y = y - np.average(y, axis = 0, weights = weights_ls)\n        \n        df_resid = n_rows - n_cols - 1 \n        \n    else:\n        df_resid = n_rows - n_cols        \n    \n    var_y = np.matmul(y.T, np.multiply(weights, y))\n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    result = np.sqrt(var_resid * np.linalg.inv(np.matmul(x.T, np.multiply(weights, x))).diagonal())\n    \n    return np.ravel(result)\n\n\nlm_se(overlap_x_mat, overlap_y_mat, weights, intercept)\n\narray([5.17600076e-05, 1.77837174e-02, 3.46178402e-02, 1.78768121e-01,\n       1.51361715e-01])\n\n\n\nfit.bse\n\narray([5.17600076e-05, 1.77837174e-02, 3.46178402e-02, 1.78768121e-01,\n       1.51361715e-01])\n\n\n\n\n\nStandalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\ndef cov_wt(x, weights, center):\n    \n    sum_w = sum(weights)\n    sumsq_w = sum(np.power(weights, 2))\n    \n    if (center):\n    \n        weights_ls = np.array(weights).reshape(-1).tolist()\n        x = x - np.average(x, axis = 0, weights = weights_ls)\n    \n    result = np.matmul(x.T, np.multiply(weights, x)) / (sum_w - sumsq_w / sum_w)\n    \n    return result\n\n\ndef lm_sar(x, y, weights, intercept):\n    \n    coef = lm_coef(x, y, weights, intercept)\n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept): x = sm.add_constant(x)\n    \n#     weights_ls = np.array(weights).reshape(-1)\n#     sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n#                    aweights = weights_ls)\n    sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n    sar = np.multiply(np.power(coef, 2).T, sigma[:-1, :-1].diagonal())\n    sar_eps = (1 - rsq) * sigma[-1, -1]\n    \n    result = np.sqrt(np.concatenate((np.matrix(sigma[-1, -1]),\n                                     np.matrix(sar),\n                                     np.matrix(sar_eps)), axis = 1))\n    \n    return np.ravel(result)\n\n\nlm_sar(overlap_x_mat, overlap_y_mat, weights, intercept) * np.sqrt(scale[\"periods\"]) * np.sqrt(scale[\"overlap\"])\n\narray([0.07370702, 0.        , 0.02986623, 0.00914208, 0.03164923,\n       0.02754505, 0.02856826])\n\n\n\n\nRisk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\n# http://faculty.washington.edu/ezivot/research/factormodelrisklecture_handout.pdf\ndef lm_mcr(x, y, weights, intercept):\n    \n    coef = np.matrix(lm_coef(x, y, weights, intercept)).T\n    rsq = lm_rsq(x, y, weights, intercept)\n        \n    if (intercept): x = sm.add_constant(x)\n    \n#     weights_ls = np.array(weights).reshape(-1)        \n#     sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n#                    aweights = weights_ls)\n    sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n    mcr = np.multiply(coef, np.matmul(sigma[:-1, :-1], coef)) / np.sqrt(sigma[-1, -1])\n    mcr_eps = np.sqrt(sigma[-1, -1]) - sum(mcr)\n    \n    result = np.concatenate((np.sqrt(np.matrix(sigma[-1, -1])),\n                             np.matrix(mcr).T,\n                             np.matrix(mcr_eps)), axis = 1)\n    \n    return np.ravel(result)\n\n\nlm_mcr(overlap_x_mat, overlap_y_mat, weights, intercept) * np.sqrt(scale[\"periods\"]) * np.sqrt(scale[\"overlap\"])\n\narray([ 0.07370702, -0.        ,  0.02368613,  0.00511232,  0.01746792,\n        0.01636783,  0.01107283])\n\n\n\n\nImplied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\ndef implied_shocks(shocks, x, z, weights):\n    \n    beta = np.matmul(np.linalg.inv(np.matrix(np.matmul(z.T, np.multiply(weights, z)))),\n                     np.matrix(np.matmul(z.T, np.multiply(weights, x))))\n    \n    result = np.matmul(shocks, beta)\n    \n    return np.ravel(result)\n\n\nshocks = np.array([-0.1, 0.1])\noverlap_z_mat = overlap_x_mat[:, [0, 1]]\n\n\nimplied_shocks(shocks, overlap_x_mat, overlap_z_mat, weights)\n\narray([-0.1       ,  0.1       , -0.0094656 , -0.00277652])\n\n\n\n\nStress P&L\n\ndef pnl_stress(shocks, x, y, z, weights, intercept):\n    \n    coef = lm_coef(x, y, weights, intercept)\n    \n    if (intercept): x = sm.add_constant(x)\n    \n    result = np.multiply(coef.T, implied_shocks(shocks, x, z, weights))\n    \n    return np.ravel(result)\n\n\npnl_stress(shocks, overlap_x_mat, overlap_y_mat, overlap_z_mat, weights, intercept)\n\narray([ 5.71524767e-05, -1.99036614e-02, -1.38082089e-02, -2.43479180e-02,\n       -4.69439434e-03])"
  },
  {
    "objectID": "posts/portfolios-py/index.html#principal-component-analysis",
    "href": "posts/portfolios-py/index.html#principal-component-analysis",
    "title": "Portfolios",
    "section": "Principal component analysis",
    "text": "Principal component analysis\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\nEigendecomposition\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\n# https://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\ndef eigen_decomp(x, comps):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    V = V[:, idx]\n        \n    L = L[:comps]\n    V = V[:, :comps]\n    \n    result = np.matmul(V, np.multiply(L, V.T))\n    \n    return result\n\n\ncomps = 1\n\n\neigen_decomp(overlap_x_mat, comps) * scale[\"periods\"] * scale[\"overlap\"]\n\narray([[ 2.10177072e-02, -4.77529237e-03,  5.86663972e-04,\n         1.43982775e-03],\n       [-4.77529237e-03,  1.08496217e-03, -1.33291988e-04,\n        -3.27133612e-04],\n       [ 5.86663972e-04, -1.33291988e-04,  1.63754596e-05,\n         4.01896867e-05],\n       [ 1.43982775e-03, -3.27133612e-04,  4.01896867e-05,\n         9.86360661e-05]])\n\n\n\n# np.cov(overlap_x_mat.T) * scale[\"periods\"] * scale[\"overlap\"]\n\n\n\nVariance explained\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\ndef variance_explained(x):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))   \n    idx = L.argsort()[::-1]\n    L = L[idx]\n    \n    result = L.cumsum() / L.sum()\n    \n    return result\n\n\nvariance_explained(overlap_x_mat)\n\narray([0.85645941, 0.99017229, 0.99659713, 1.        ])\n\n\n\n\nCosine similarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\n\\end{aligned}\n\\]\n\ndef eigen_vals(x):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    \n    return pd.DataFrame(L)\n\ndef eigen_vecs(x):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    V = V[:, idx]\n    \n    return pd.DataFrame(V)\n\n\ndef roll_eigen1(x, width, comp):\n    \n    n_rows = len(x)\n    result = pd.DataFrame()\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evec = eigen_vecs(x.iloc[idx]).iloc[:, comp - 1]\n        result = result.append(evec.transpose())\n    \n    result.index = x.index[(width - 1):]\n    result.columns = x.columns\n    \n    return result  \n\n\ncomp = 1\n\n\nraw_df = roll_eigen1(overlap_x_df, width, comp)\n\n\nraw_mlt &lt;- melt(as.data.table(py$raw_df, keep.rownames = \"index\"), id.vars = \"index\")\nraw_mlt[ , index := as.Date(index)]\nraw_plt &lt;- plot_ts(raw_mlt, title = \"Eigenvector 1Y\")\nprint(raw_plt)\n\n\n\n\n\n# https://quant.stackexchange.com/a/3095\ndef roll_eigen2(x, width, comp):\n    \n    n_rows = len(x)\n    result = pd.DataFrame()\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evec = eigen_vecs(x.iloc[idx]).iloc[:, comp - 1]\n        \n        if i &gt; width - 1:\n            \n            similarity = np.matmul(np.matrix(evec),\n                                   np.matrix(result.iloc[-1, :]).T)\n            evec = pd.DataFrame(np.multiply(np.sign(similarity), np.matrix(evec))) \n            result = result.append(evec)\n            \n        else:\n            result = result.append(evec.transpose())\n    \n    result.index = x.index[(width - 1):]\n    result.columns = x.columns\n    \n    return result  \n\n\nclean_df = roll_eigen2(overlap_x_df, width, comp)\n\n\nclean_mlt &lt;- melt(as.data.table(py$clean_df, keep.rownames = \"index\"), id.vars = \"index\")\nclean_mlt[ , index := as.Date(index)]\nclean_plt &lt;- plot_ts(clean_mlt, title = \"Eigenvector 1Y\")\nprint(clean_plt)"
  },
  {
    "objectID": "posts/portfolios-py/index.html#random-portfolios",
    "href": "posts/portfolios-py/index.html#random-portfolios",
    "title": "Portfolios",
    "section": "Random portfolios",
    "text": "Random portfolios\nNeed to generate uniformly distributed weights \\(\\mathbf{w}=(w_{1},w_{2},\\ldots,w_{N})\\) such that \\(\\sum_{j=1}^{N}w_{i}=1\\) and \\(w_{i}\\geq0\\):\n\nApproach 1: tempting to use \\(w_{i}=\\frac{u_{i}}{\\sum_{j=1}^{N}u_{i}}\\) where \\(u_{i}\\sim U(0,1)\\) but the distribution of \\(\\mathbf{w}\\) is not uniform\nApproach 2: instead, generate \\(\\text{Exp}(1)\\) and then normalize\n\nCan also scale random weights by \\(M\\), e.g. if sum of weights must be 10% then multiply weights by 10%.\n\ndef rand_weights1(n_sim, n_assets, lmbda):  \n    \n    rand_exp = np.matrix(np.random.uniform(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = np.divide(rand_exp, rand_exp_sum)\n    \n    return result\n\n\n# Methodology: uniform sampling from the simplex (http://mathoverflow.net/a/76258)\n# z ~ U(0, 1) then -ln(z) is an exponential(1) distribution\n# This is also known as generating a random vector from the symmetric Dirichlet distribution\ndef rand_weights2(n_sim, n_assets, lmbda):   \n    \n    rand_exp = np.matrix(-np.log(1 - np.random.uniform(size = (n_sim, n_assets))) / lmbda)\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = np.divide(rand_exp, rand_exp_sum)\n    \n    return result\n\n\n# 1. Generate n exponential(1) random variables x_1, x_2, ..., x_n\n# 2. Let y_i = x_i / (sum_{i = 1}^{n} x_i)\ndef rand_weights3(n_sim, n_assets, lmbda):\n    \n    rand_exp = np.matrix(np.random.exponential(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = np.divide(rand_exp, rand_exp_sum)\n    \n    return result\n\n\nlmbda = 1\nn_assets = 3\nn_sim = 10000\n\n\napproach1 = rand_weights1(n_sim, n_assets, lmbda)\napproach2 = rand_weights2(n_sim, n_assets, lmbda)\napproach3 = rand_weights3(n_sim, n_assets, lmbda)\n\n\nplot_pairs(as.data.table(py$approach1), title = \"Weight (%)\")\n\n\n\n\n\nplot_pairs(as.data.table(py$approach2), title = \"Weight (%)\")\n\n\n\n\n\nplot_pairs(as.data.table(py$approach3), title = \"Weight (%)\")\n\n\n\n\n\nRandom turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    plug = False\n    \n    while not plug:\n        \n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n        if ((temp &lt;= upper) and (temp &gt;= lower)):\n            plug = True\n        \n    result = np.append(result, temp)\n    \n    return result\n\n\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights3(n_sim, n_assets, lmbda) * rng\n    result = result - rng / n_assets\n    \n    return result\n\n\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n    \n    result = np.matrix(rand_iterative(n_assets, lower, upper, target))\n    \n    while result.shape[0] &lt; n_sim:\n    \n        temp = np.matrix(rand_iterative(n_assets, lower, upper, target))\n        result = np.concatenate((result, temp), axis = 0)\n    \n    return result\n\n\nlower = -0.05\nupper = 0.05\ntarget = 0\n\n\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)\n\n\nplot_pairs(as.data.table(py$approach1), title = \"Weight (%)\")\n\n\n\n\n\nplot_pairs(as.data.table(py$approach2), title = \"Weight (%)\")"
  },
  {
    "objectID": "posts/portfolios-py/index.html#mean-variance",
    "href": "posts/portfolios-py/index.html#mean-variance",
    "title": "Portfolios",
    "section": "Mean-variance",
    "text": "Mean-variance\n\ndef geometric_mean(x, scale):\n    \n    result = np.prod(1 + x) ** (scale / x.shape[1]) - 1\n    \n    return result\n\n\nreturns_x_df = returns_df.dropna()[\"returns\"][factors] # REMOVE LATER\nreturns_x_mat = np.matrix(returns_x_df) # extended history # REMOVE LATER\nmu = np.apply_along_axis(geometric_mean, 0, returns_x_mat, scale[\"periods\"])\nsigma = np.cov(overlap_x_df.T, ddof = 1) * scale[\"periods\"] * scale[\"overlap\"]\n\n\nMaximum return\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\n\ntarget = 0.06\nstart = np.array([1] * len(factors))\nbnds = [(np.finfo(float).eps, 1) for i in range(len(factors))]\ncons = [{\"type\": \"ineq\", \"fun\": lambda params, sigma, target: max_pnl_cons(params, sigma, target),\n         \"args\": (sigma, target)},\n        {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n\n\ndef max_pnl_cons(params, sigma, target):\n    \n    var = np.matmul(np.transpose(params), np.matmul(sigma, params))\n    \n    result = target ** 2 - var\n    \n    return result\n\ndef max_pnl_obj(params, mu):\n    \n    result = np.matmul(mu, params)\n    \n    return -result\n\ndef max_pnl_optim(params, mu):\n    \n    result = minimize(max_pnl_obj, params, args = (mu), bounds = bnds, constraints = cons)\n    \n    return result.x\n\n\nparams1 = max_pnl_optim(start, mu)\nparams1\n\narray([3.33478228e-01, 5.67596349e-01, 9.89254230e-02, 2.27645187e-16])\n\n\n\nnp.matmul(mu, params1)\n\n0.032906364465747885\n\n\n\nnp.sqrt(np.matmul(np.transpose(params1), np.matmul(sigma, params1)))\n\n0.06000003526699834\n\n\n\n\nMinimum variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min_{x}&\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\n\ntarget = 0.03\nstart = np.array([1] * len(factors))\ncons = [{\"type\": \"ineq\", \"fun\": lambda params, mu, target: min_risk_cons(params, mu, target),\n         \"args\": (mu, target)},\n        {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n\n\ndef min_risk_cons(params, mu, target):\n    \n    result = np.matmul(mu, params) - target\n    \n    return result\n\ndef min_risk_obj(params, sigma):\n    \n    result = np.matmul(np.transpose(params), np.matmul(sigma, params))\n    \n    return result\n\ndef min_risk_optim(params, sigma):\n    \n    result = minimize(min_risk_obj, params, args = (sigma), bounds = bnds, constraints = cons)\n    \n    return result.x\n\n\nparams2 = min_risk_optim(start, sigma)\nparams2\n\narray([2.97430685e-01, 5.52140503e-01, 1.50428812e-01, 2.49800181e-16])\n\n\n\nnp.matmul(mu, params2)\n\n0.02999999999411795\n\n\n\nnp.sqrt(np.matmul(np.transpose(params2), np.matmul(sigma, params2))) \n\n0.054626654580512614\n\n\n\n\nMaximum ratio\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}-\\frac{1}{2}\\delta(\\mathbf{w}^T\\Sigma\\mathbf{w})\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\n\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\nstart = np.array([1] * len(factors))\ncons = [{\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n\n\nprint(start)\n\n[1 1 1 1]\n\nprint(start.shape)\n\n(4,)\n\nprint(mu)\n\n[ 0.0741518   0.01444308 -0.00019696  0.00084524]\n\nprint(mu.shape)\n\n(4,)\n\nprint(sigma)\n\n[[ 3.00635294e-02 -3.11798296e-03 -1.77879306e-04  2.54324229e-03]\n [-3.11798296e-03  4.53805294e-03 -1.23898887e-04 -3.78080435e-04]\n [-1.77879306e-04 -1.23898887e-04  7.18234782e-05 -3.96740334e-05]\n [ 2.54324229e-03 -3.78080435e-04 -3.96740334e-05  4.27709578e-04]]\n\nprint(sigma.shape)\n\n(4, 4)\n\nprint(np.matmul(np.transpose(start), np.matmul(sigma, start)))\n\n0.032512568704157904\n\n\n\ndef max_ratio_obj(params, mu, sigma, target):\n    \n    result = np.matmul(mu, params) - 0.5 * target * (np.matmul(np.transpose(params),\n                                                               np.matmul(sigma, params)))\n#     result = np.matmul(mu, params) / np.sqrt(np.matmul(np.transpose(params), np.matmul(sigma, params)))\n    \n    return -result\n\ndef max_ratio_optim(params, mu, sigma, target):\n    \n    result = minimize(max_ratio_obj, params, args = (mu, sigma, target), bounds = bnds,\n                      constraints = cons) \n    \n    return result.x\n\n\nparams3 = max_ratio_optim(start, mu, sigma, target)\nparams3\n\narray([3.55021061e-01, 6.02524365e-01, 4.24545734e-02, 2.78748379e-16])\n\n\n\nnp.matmul(mu, params3)\n\n0.035019395052128664\n\n\n\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n\n0.06396234177264824"
  },
  {
    "objectID": "posts/portfolios-py/index.html#black-litterman",
    "href": "posts/portfolios-py/index.html#black-litterman",
    "title": "Portfolios",
    "section": "Black-Litterman",
    "text": "Black-Litterman\n\nPrior distribution\n\\[\n\\begin{aligned}\n\\text{Risk aversion: } &\\lambda=\\frac{E(r)-r_{f}}{\\sigma^{2}}=\\frac{IR}{\\sigma}\\\\\n\\text{Implied returns: } &\\Pi=\\lambda\\Sigma w\\\\\n\\text{Distribution: } &N\\sim(\\Pi,\\tau\\Sigma)\n\\end{aligned}\n\\]\n\ndef implied_pnl(params, ir, sigma):\n    \n    lmbda = ir / np.sqrt(np.matmul(np.transpose(params), np.matmul(sigma, params)))\n    \n    result = np.matmul(lmbda * sigma, params)\n    \n    return result\n\n\nimplied_pnl(params3, ir, sigma)\n\narray([ 0.06180012,  0.00526111, -0.00039032,  0.00499045])\n\n\n\n\nConditional distribution\n\\[\n\\begin{aligned}\n\\text{Prior mean variance: } &\\tau\\in(0.01, 0.05)\\approx(0.025)\\\\\n\\text{Asset views: } &\\mathbf{P}={\\begin{bmatrix}\np_{11}&\\cdots&p_{1n}\\\\\n\\vdots&\\ddots&\\vdots\\\\\np_{k1}&\\cdots&p_{kn}\n\\end{bmatrix}}=\n{\\begin{bmatrix}\n0&0&0&0&0&0&1&0\\\\\n-1&1&0&0&0&0&0&0\\\\\n0&0&0.5&-0.5&0.5&-0.5&0&0\n\\end{bmatrix}}\\\\\n\\text{View returns: } &\\mathbf{Q}={\\begin{bmatrix}\nq_{1}\\\\\n\\vdots\\\\\nq_{k}\n\\end{bmatrix}}=\n{\\begin{bmatrix}\n0.0525\\\\\n0.0025\\\\\n0.0200\n\\end{bmatrix}}\\\\\n\\text{View confidence: } &\\mathbf{C}={\\begin{bmatrix}\nc_{1}\\\\\n\\vdots\\\\\nc_{k}\n\\end{bmatrix}}=\n{\\begin{bmatrix}\n0.2500\\\\\n0.5000\\\\\n0.6500\n\\end{bmatrix}}\\\\\n\\text{View covariance: } &\\mathbf{\\Omega}={\\begin{bmatrix}\n\\tau\\left(\\frac{1-c_{1}}{c_{1}}\\right)\\left(p_{1}\\Sigma p_{1}^{T}\\right)&0&0\\\\\n0&\\ddots&0\\\\\n0&0&\\tau\\left(\\frac{1-c_{k}}{c_{k}}\\right)\\left(p_{k}\\Sigma p_{k}^{T}\\right)\n\\end{bmatrix}}\\\\\n\\text{Distribution: } &N\\sim(\\mathbf{Q}, \\mathbf{\\Omega})\n\\end{aligned}\n\\]\n\n\nPosterior distribution\n\\[\n\\begin{aligned}\n\\text{Implied returns: } &\\hat{\\Pi}=\\Pi+\\tau\\Sigma \\mathbf{P}^{T}\\left(\\tau \\mathbf{P}\\Sigma \\mathbf{P}^{T}+\\mathbf{\\Omega}\\right)^{-1}\\left(\\mathbf{Q}-\\mathbf{P}\\Pi^{T}\\right)\\\\\n\\text{Covariance: } &\\hat{\\Sigma}=\\Sigma+\\tau\\left[\\Sigma-\\Sigma\\mathbf{P}^{T}\\left(\\tau\\mathbf{P}\\Sigma\\mathbf{P}^{T}+\\mathbf{\\Omega}\\right)^{-1}\\tau\\mathbf{P}\\Sigma\\right]\\\\\n\\text{Weights: } &\\hat{w}=\\hat{\\Pi}\\left(\\lambda\\Sigma\\right)^{-1}\\\\\n\\text{Distribution: } &N\\sim\\left(\\left[\\left(\\tau\\Sigma\\right)^{-1}+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{P}\\right]^{-1}\\left[\\left(\\tau\\Sigma\\right)^{-1}\\Pi+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{Q}\\right],\\left[\\left(\\tau\\Sigma\\right)^{-1}+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{P}\\right]^{-1}\\right)\n\\end{aligned}\n\\]\n\ndef black_litterman(params, ir, sigma, views):\n    \n    # prior distribution\n    weights_prior = params\n    sigma_prior = sigma\n    lmbda = ir / np.sqrt(np.matmul(np.transpose(weights_prior), np.matmul(sigma_prior, weights_prior)))\n    pi_prior = np.transpose(np.matrix(np.matmul(lmbda * sigma_prior, weights_prior)))\n    \n    # matrix calculations\n    matmul_left = np.multiply(views[\"tau\"], np.matmul(sigma_prior, views[\"P\"].T))\n    matmul_mid = np.multiply(views[\"tau\"], np.matmul(views[\"P\"], np.matmul(sigma_prior, views[\"P\"].T)))\n    matmul_right = views[\"Q\"] - np.matmul(views[\"P\"], pi_prior)\n    \n    # conditional distribution\n    omega = np.diag(np.diag(np.matmul(np.diag([(1 - x) / x for x in views[\"C\"]]), matmul_mid)))\n    \n    # posterior distribution\n    pi_posterior = pi_prior + np.matmul(matmul_left, np.matmul(np.linalg.inv(matmul_mid + omega), matmul_right))\n    \n    sigma_posterior = sigma_prior + np.multiply(views[\"tau\"], sigma_prior) - \\\n        np.matmul(matmul_left, np.matmul(np.linalg.inv(matmul_mid + omega),\n                                         np.multiply(views[\"tau\"], np.matmul(views[\"P\"], sigma_prior))))\n    \n    weights_posterior = np.matmul(pi_posterior.T, np.linalg.inv(lmbda * sigma_prior))\n    \n    # implied confidence\n    pi_posterior_100 = pi_prior + np.matmul(matmul_left, np.matmul(np.linalg.inv(matmul_mid), matmul_right))\n    \n    weights_posterior_100 = np.matmul(pi_posterior_100.T, np.linalg.inv(lmbda * sigma_prior))\n    \n    implied_confidence = (weights_posterior - weights_prior) / (weights_posterior_100 - weights_prior)\n    \n    result = {\"implied_confidence\": implied_confidence,\n              \"weights_prior\": np.matrix(weights_prior),\n              \"weights_posterior\": weights_posterior,\n              \"pi_prior\": np.transpose(pi_prior),\n              \"pi_posterior\": np.transpose(pi_posterior),\n              \"sigma_prior\": sigma_prior,\n              \"sigma_posterior\": sigma_posterior}\n        \n    return result\n\n\ntau = 0.025\nP = np.diag([1] * len(factors))\nQ = np.transpose(np.matrix(implied_shocks([0.1], overlap_x_mat, overlap_x_mat[:, 0], 1)))\nC = [0.95] * len(factors)\nviews = {\"tau\": tau, \"P\": P, \"Q\": Q, \"C\": C}\n\n\nbl = black_litterman(params3, ir, sigma, views)\nbl\n\n{'implied_confidence': matrix([[ 0.97159292,  0.93400713, -0.37067746, -0.40614919]]), 'weights_prior': matrix([[4.34472214e-01, 5.65527786e-01, 2.22044605e-16, 3.04010289e-16]]), 'weights_posterior': matrix([[0.52802075, 0.02381873, 0.03199844, 0.01172265]]), 'pi_prior': matrix([[ 0.06180012,  0.00526111, -0.00039032,  0.00499045]]), 'pi_posterior': matrix([[ 0.09907076, -0.01810773,  0.00190707,  0.00671672]]), 'sigma_prior': array([[ 2.25161872e-02, -4.26101245e-03,  4.41178041e-04,\n         1.51807793e-03],\n       [-4.26101245e-03,  4.38344152e-03, -4.21280590e-04,\n        -1.13499562e-04],\n       [ 4.41178041e-04, -4.21280590e-04,  1.51390635e-04,\n        -2.06179097e-05],\n       [ 1.51807793e-03, -1.13499562e-04, -2.06179097e-05,\n         2.65417825e-04]]), 'sigma_posterior': array([[ 2.25429284e-02, -4.26136589e-03,  4.41212808e-04,\n         1.51825854e-03],\n       [-4.26136589e-03,  4.38877437e-03, -4.21313142e-04,\n        -1.13491831e-04],\n       [ 4.41212808e-04, -4.21313142e-04,  1.51575898e-04,\n        -2.06227402e-05],\n       [ 1.51825854e-03, -1.13491831e-04, -2.06227402e-05,\n         2.65737594e-04]])}\n\n\n\nparams4 = np.array(bl[\"weights_posterior\"])[0]\nparams4 = params4 / sum(params4) # no leverage\nparams4\n\narray([0.88659454, 0.0399938 , 0.05372827, 0.01968339])\n\n\n\nnp.matmul(mu, params4)\n\n0.06512940361138682\n\n\n\nnp.sqrt(np.matmul(np.transpose(params4), np.matmul(sigma, params4)))\n\n0.13227699668169068"
  },
  {
    "objectID": "posts/portfolios-py/index.html#risk-parity",
    "href": "posts/portfolios-py/index.html#risk-parity",
    "title": "Portfolios",
    "section": "Risk parity",
    "text": "Risk parity\nRisk parity is an approach to portfolio management that focuses on allocation of risk rather than allocation of capital. In a risk parity strategy, the asset allocations are leveraged, or deleveraged, to have equal risk contributions. Suppose that \\(\\mathbf{R}\\) is a \\(T \\times N\\) matrix of asset returns where the return of the \\(i^{th}\\) asset is \\(R_{i,t}\\) at time \\(t\\). Define \\(\\Sigma\\) to be the covariance matrix of \\(\\mathbf{R}\\) and let \\(\\mathbf{w}=(w_{1},\\dots,w_{N})\\) be a vector of asset weights. Then the volatility of the return of the strategy is \\(\\sigma_{P}=\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}\\) and, by Euler’s Theorem, satisfies:\n\\[\n\\begin{aligned}\n\\sigma_{P}&=\\sum_{i=1}^{N}w_{i}\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}\\\\\n&=w_{1}\\frac{\\partial\\sigma_{P}}{\\partial w_{1}}+\\dots+w_{N}\\frac{\\partial\\sigma_{P}}{\\partial w_{N}}\n\\end{aligned}\n\\]\nwhere each element is the risk contribution of the \\(i^{th}\\) risky asset. The risk parity objective solves for weights such that each asset contributes equal risk using the following nonlinear constrained optimization problem:\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\displaystyle\\sum_{i=1}^{N}\\log(w_{i})\\\\\n\\textrm{s.t.}&\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}&\\leq&\\sigma\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce a new variable \\(\\lambda\\) that is the Lagrange multiplier and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\sum_{i=1}^{N}\\log(w_{i})-\\lambda(\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}-\\sigma)\n\\end{aligned}\n\\]\nThen set the partial derivatives of \\(\\mathcal{L}\\) equal to zero for each asset \\(i\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w_{i}}&=\\frac{1}{w_{i}}-\\lambda\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}=0\n\\Leftrightarrow\nw_{i}\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}=\\frac{1}{\\lambda}\n\\end{aligned}\n\\]\nNotice that \\(1/\\lambda\\) is the risk contribution of the \\(i^{th}\\) asset. Now use Python to maximize the Lagrangian numerically:\n\n# http://faculty.washington.edu/ezivot/econ424/riskbudgetingslides.pdf\n# https://systematicinvestor.wordpress.com/2011/11/16/black-litterman-model/\n# https://cran.r-project.org/web/packages/BLCOP/vignettes/BLCOP.pdf\n# http://math.stackexchange.com/questions/17776/inverse-of-the-sum-of-matrices\ndef risk_parity_obj(params, sigma, target):\n    \n    risk = np.sqrt(np.matmul(np.transpose(params), np.matmul(sigma, params)))\n    risk_contrib = target / len(params)\n    \n    result = -(sum(np.log(params)) - (1 / risk_contrib) * (risk - target))\n    \n    return result\n\ndef risk_parity_optim(params, sigma, target):\n    \n    result = minimize(risk_parity_obj, params, args = (sigma, target)).x\n    result = result / sum(result) # no leverage\n    \n    return result\n\n\ntarget = 1\nstart = np.array([1] * len(factors))\n\n\nparams5 = risk_parity_optim(start, sigma, target)\nparams5\n\narray([0.02197783, 0.09196207, 0.67896   , 0.2071001 ])\n\n\n\nrisk = np.sqrt(np.matmul(np.transpose(params5), np.matmul(sigma, params5)))\nrisk_contrib = np.multiply(params5, np.matmul(sigma, params5)) / risk\nrisk_contrib\n\narray([0.00207012, 0.00206996, 0.00207006, 0.00207012])\n\n\n\nnp.matmul(mu, params5)\n\n0.002999229811662311\n\n\n\nnp.sqrt(np.matmul(np.transpose(params5), np.matmul(sigma, params5))) \n\n0.008280260152428262\n\n\n\npd.DataFrame.from_dict({\"max_pnl\": params1,\n                        \"min_risk\": params2,\n                        \"max_ratio\": params3,\n                        # \"black_litterman\": params4,\n                        \"risk_parity\": params5})\n\n        max_pnl      min_risk     max_ratio  risk_parity\n0  3.334782e-01  2.974307e-01  3.550211e-01     0.021978\n1  5.675963e-01  5.521405e-01  6.025244e-01     0.091962\n2  9.892542e-02  1.504288e-01  4.245457e-02     0.678960\n3  2.276452e-16  2.498002e-16  2.787484e-16     0.207100"
  },
  {
    "objectID": "posts/portfolios-py/index.html#portfolio-attribution",
    "href": "posts/portfolios-py/index.html#portfolio-attribution",
    "title": "Portfolios",
    "section": "Portfolio attribution",
    "text": "Portfolio attribution\n\nSingle-period\nThe arithmetic active return is commonly decomposed using the Brinson-Fachler method:\n\\[\n\\begin{aligned}\n\\text{Allocation: } &r_{a}=\\sum_{k=1}^{n}(w_{p,k}-w_{b,k})(r_{b,k}-r_{b})\\\\\n\\text{Selection: } &r_{s}=\\sum_{k=1}^{n}w_{p,k}(r_{p,k}-r_{b,k})\\\\\n\\end{aligned}\n\\]\nwhere \\(k=1,\\ldots,n\\) is each sector or factor.\n\n\nMulti-period\nArithmetic attributes add to the active return of a single period; however, they cannot be summed or compounded to explain the active return over multiple periods. To solve this problem, the original arithmetic attribute is multiplied by a single scaling coefficient for that period. After all single-period original attributes have been transformed, the adjusted attributes sum to the active return over the periods.\n\\[\n\\begin{aligned}\n\\text{Carino scaling coefficient: } &c_{t}=\\frac{[\\ln(1+r_{p,t})-\\ln(1+r_{b,t})]/(r_{p,t}-r_{b,t})}{[\\ln(1+r_{p})-\\ln(1+r_{b})]/(r_{p}-r_{b})}\n\\end{aligned}\n\\]\nwhere \\(t=1,\\ldots,n\\) is each period.\n\n# http://www.frongello.com/support/Works/Chap20RiskBook.pdf\n# https://github.com/R-Finance/PortfolioAttribution/blob/master/R/Carino.R\ndef pnl_attrib(params, x):\n    \n    total_i = np.sum(x, axis = 1)\n    total = np.prod(1 + total_i) - 1\n    \n    coef = (np.log(1 + total_i) / total_i) / (np.log(1 + total) / total)\n    \n    result = np.sum(np.multiply(x, coef), axis = 0)\n    \n    return np.ravel(result)\n\n\nattrib_mat = np.multiply(params1, np.matrix(returns_x_df)[-width:])\n\n\npnl_attrib(params1, attrib_mat)\n\narray([ 6.28488339e-02, -3.29098189e-02,  3.97256516e-04,  2.34689866e-18])"
  },
  {
    "objectID": "posts/portfolios-r/index.html",
    "href": "posts/portfolios-r/index.html",
    "title": "Portfolios",
    "section": "",
    "text": "library(quantmod)\nlibrary(roll)\nlibrary(data.table)\nfactors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\nfactors &lt;- c(factors_r, factors_d)\nwidth &lt;- 252\nscale &lt;- list(\"periods\" = 252, \"overlap\" = 5)\ngetSymbols(factors, src = \"FRED\")\nlevels_xts &lt;- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))\nreturns_xts &lt;- do.call(merge, lapply(factors, function(i) {\n    if (i %in% factors_r) {\n        diff(log((levels_xts[ , i])))\n    } else if (i %in% factors_d) {\n        -diff(levels_xts[ , i]) / 100\n    }    \n}))\noverlap_xts &lt;- roll_mean(returns_xts, scale[[\"overlap\"]], min_obs = 1, na_restore = TRUE)\nlibrary(pls)\nlibrary(CVXR)\ntickers &lt;- \"BAICX\" # fund inception date is \"2011-11-28\" \ninvisible(getSymbols(tickers, src = \"tiingo\", api.key = Sys.getenv(\"TIINGO_API_KEY\"), adjust = TRUE))\nprices_xts &lt;- do.call(merge, c(lapply(tickers, function(i) Cl(get(i))), all = TRUE))\ncolnames(prices_xts) &lt;- tickers\nindex(prices_xts) &lt;- as.Date(index(prices_xts))\nreturns_xts &lt;- merge(returns_xts, diff(log(prices_xts)))\noverlap_xts &lt;- merge(overlap_xts, roll_mean(returns_xts[ , tickers], scale[[\"overlap\"]], min_obs = 1))\n# weights &lt;- 0.9 ^ ((width - 1):0)\nweights &lt;- rep(1, width)\n# overlap_df &lt;- na.omit(overlap_xts)\noverlap_x_df &lt;- na.omit(overlap_xts)[ , factors]\noverlap_y_df &lt;- na.omit(overlap_xts)[ , tickers]\noverlap_x_xts &lt;- tail(overlap_x_df, width)\noverlap_y_xts &lt;- tail(overlap_y_df, width)"
  },
  {
    "objectID": "posts/portfolios-r/index.html#ordinary-least-squares",
    "href": "posts/portfolios-r/index.html#ordinary-least-squares",
    "title": "Portfolios",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\n# https://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\nlm_coef &lt;- function(x, y, weights, intercept) {\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    result &lt;- solve(t(x) %*% sweep(x, 1, weights, \"*\")) %*% t(x) %*% sweep(y, 1, weights, \"*\")\n    \n    return(result)\n    \n}\n\n\nintercept &lt;- TRUE\n# weights &lt;- 0.9 ^ ((width - 1):0)\nweights &lt;- rep(1, width)\n\n\noverlap_x_df &lt;- na.omit(overlap_xts)[ , factors]\noverlap_y_df &lt;- na.omit(overlap_xts)[ , tickers]\noverlap_x_xts &lt;- tail(overlap_x_df, width)\noverlap_y_xts &lt;- tail(overlap_y_df, width)\n\n\nlm_coef(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n                      BAICX\n(Intercept)   -1.118719e-05\nxSP500         1.990366e-01\nxDTWEXAFEGS   -1.380821e-01\nxDGS10         2.572253e+00\nxBAMLH0A0HYM2  1.690747e+00\n\n\n\nif (intercept) {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts, weights = weights)\n} else {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts - 1, weights = weights)\n}\n    \ncoef(fit)\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n            -1.118719e-05              1.990366e-01             -1.380821e-01 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             2.572253e+00              1.690747e+00 \n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\nlm_rsq &lt;- function(x, y, weights, intercept) {\n        \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        x &lt;- sweep(x, 2, apply(x, 2, weighted.mean, w = weights), \"-\")\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n    }\n    \n    result &lt;- (t(coef) %*% (t(x) %*% sweep(x, 1, weights, \"*\")) %*% coef) / (t(y) %*% sweep(y, 1, weights, \"*\"))\n    \n    return(result)\n    \n}\n\n\nlm_rsq(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n          BAICX\nBAICX 0.8497724\n\n\n\nsummary(fit)$r.squared\n\n[1] 0.8497724\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\n# http://people.duke.edu/~rnau/mathreg.htm\nlm_se &lt;- function(x, y, weights, intercept) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n        df_resid &lt;- n_rows - n_cols - 1\n        \n    } else {\n        df_resid &lt;- n_rows - n_cols\n    }\n    \n    var_y &lt;- t(y) %*% sweep(y, 1, weights, \"*\")\n    var_resid &lt;- as.vector((1 - rsq) * var_y / df_resid)\n    \n    result &lt;- sqrt(var_resid * diag(solve(t(x) %*% sweep(x, 1, weights, \"*\"))))\n    \n    return(result)\n}\n\n\nlm_se(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n  (Intercept)        xSP500   xDTWEXAFEGS        xDGS10 xBAMLH0A0HYM2 \n 5.176001e-05  1.778372e-02  3.461784e-02  1.787681e-01  1.513617e-01 \n\n\n\ncoef(summary(fit))[ , \"Std. Error\"]\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n             5.176001e-05              1.778372e-02              3.461784e-02 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             1.787681e-01              1.513617e-01"
  },
  {
    "objectID": "posts/portfolios-r/index.html#standalone-risk",
    "href": "posts/portfolios-r/index.html#standalone-risk",
    "title": "Portfolios",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\nlm_sar &lt;- function(x, y, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n    sar &lt;- coef ^ 2 * diag(sigma[-ncol(sigma), -ncol(sigma)])\n    sar_eps &lt;- (1 - rsq) * sigma[ncol(sigma), ncol(sigma)]\n    \n    result &lt;- sqrt(c(sigma[ncol(sigma), ncol(sigma)],\n                     sar,\n                     sar_eps))\n    \n    return(result)\n    \n}\n\n\nlm_sar(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]]) * sqrt(scale[[\"overlap\"]])\n\n[1] 0.073707023 0.000000000 0.029866230 0.009142079 0.031649227 0.027545050\n[7] 0.028568258"
  },
  {
    "objectID": "posts/portfolios-r/index.html#risk-contribution",
    "href": "posts/portfolios-r/index.html#risk-contribution",
    "title": "Portfolios",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\n# http://faculty.washington.edu/ezivot/research/factormodelrisklecture_handout.pdf\nlm_mcr &lt;- function(x, y, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n    mcr &lt;- coef * sigma[-ncol(sigma), -ncol(sigma)] %*% coef / sqrt(sigma[ncol(sigma), ncol(sigma)])\n    mcr_eps &lt;- sqrt(sigma[ncol(sigma), ncol(sigma)]) - sum(mcr)\n    \n    result &lt;- c(sqrt(sigma[ncol(sigma), ncol(sigma)]),\n                mcr,\n                mcr_eps)\n    \n    return(result)\n    \n}\n\n\nlm_mcr(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]]) * sqrt(scale[[\"overlap\"]])\n\n[1] 0.073707023 0.000000000 0.023686129 0.005112318 0.017467915 0.016367830\n[7] 0.011072830"
  },
  {
    "objectID": "posts/portfolios-r/index.html#implied-shocks",
    "href": "posts/portfolios-r/index.html#implied-shocks",
    "title": "Portfolios",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\nimplied_shocks &lt;- function(shocks, x, z, weights) {\n    \n    beta &lt;- solve(t(z) %*% sweep(z, 1, weights, \"*\")) %*% t(z) %*% sweep(x, 1, weights, \"*\")\n    \n    result &lt;- shocks %*% beta\n    \n    return(result)\n    \n}\n\n\nshocks &lt;- c(-0.1, 0.1)\noverlap_z_xts &lt;- overlap_x_xts[ , 1:2]\n\n\nimplied_shocks(shocks, overlap_x_xts, overlap_z_xts, weights)\n\n     SP500 DTWEXAFEGS        DGS10 BAMLH0A0HYM2\n[1,]  -0.1        0.1 -0.009465602 -0.002776522"
  },
  {
    "objectID": "posts/portfolios-r/index.html#stress-pl",
    "href": "posts/portfolios-r/index.html#stress-pl",
    "title": "Portfolios",
    "section": "Stress P&L",
    "text": "Stress P&L\n\npnl_stress &lt;- function(shocks, x, y, z, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    result &lt;- t(coef) * implied_shocks(shocks, x, z, weights)\n    \n    return(result)    \n    \n}\n\n\npnl_stress(shocks, overlap_x_xts, overlap_y_xts, overlap_z_xts, weights, intercept)\n\n       (Intercept)      xSP500 xDTWEXAFEGS      xDGS10 xBAMLH0A0HYM2\nBAICX 5.715248e-05 -0.01990366 -0.01380821 -0.02434792  -0.004694394"
  },
  {
    "objectID": "posts/portfolios-r/index.html#eigendecomposition",
    "href": "posts/portfolios-r/index.html#eigendecomposition",
    "title": "Portfolios",
    "section": "Eigendecomposition",
    "text": "Eigendecomposition\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\n# https://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\neigen_decomp &lt;- function(x, comps) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values[1:comps]\n    V &lt;- LV$vectors[ , 1:comps]\n    \n    result &lt;- V %*% sweep(t(V), 1, L, \"*\")\n    \n    return(result)\n    \n}\n\n\ncomps &lt;- 1\n\n\neigen_decomp(overlap_x_xts, comps) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n             [,1]          [,2]          [,3]          [,4]\n[1,]  0.021017707 -0.0047752924  5.866640e-04  1.439828e-03\n[2,] -0.004775292  0.0010849622 -1.332920e-04 -3.271336e-04\n[3,]  0.000586664 -0.0001332920  1.637546e-05  4.018969e-05\n[4,]  0.001439828 -0.0003271336  4.018969e-05  9.863607e-05\n\n\n\n# cov(overlap_x_xts) * scale[[\"periods\"]] * scale[[\"overlap\"]]"
  },
  {
    "objectID": "posts/portfolios-r/index.html#variance-explained",
    "href": "posts/portfolios-r/index.html#variance-explained",
    "title": "Portfolios",
    "section": "Variance explained",
    "text": "Variance explained\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\nvariance_explained &lt;- function(x) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values\n    \n    result &lt;- cumsum(L) / sum(L)\n    \n    return(result)\n    \n}\n\n\nvariance_explained(overlap_x_xts)\n\n[1] 0.8564594 0.9901723 0.9965971 1.0000000"
  },
  {
    "objectID": "posts/portfolios-r/index.html#cosine-similarity",
    "href": "posts/portfolios-r/index.html#cosine-similarity",
    "title": "Portfolios",
    "section": "Cosine similarity",
    "text": "Cosine similarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\n\\end{aligned}\n\\]\n\neigen_vals &lt;- function(x) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values\n    \n    return(L)    \n}\n\neigen_vecs &lt;- function(x) {\n    \n    LV &lt;- eigen(cov(x))\n    V &lt;- LV$vectors\n    \n    return(V)    \n}\n\n\nroll_eigen1 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        evec &lt;- eigen_vecs(x[idx, ])[ , comp]\n        result_ls &lt;- append(result_ls, list(evec))\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\ncomp &lt;- 1\n\n\nraw_df &lt;- roll_eigen1(overlap_x_df, width, comp)\n\n\nraw_mlt &lt;- melt(as.data.table(raw_df), id.vars = \"index\")\nraw_plt &lt;- plot_ts(raw_mlt, title = \"Eigenvector 1Y\")\nprint(raw_plt)\n\n\n\n\n\n# https://quant.stackexchange.com/a/3095\nroll_eigen2 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        evec &lt;- eigen_vecs(x[idx, ])[ , comp]\n                \n        if (i &gt; width) {\n            \n            similarity &lt;- evec %*% result_ls[[length(result_ls)]]\n            evec &lt;- as.vector(sign(similarity)) * evec\n            result_ls &lt;- append(result_ls, list(evec))\n            \n        } else {\n            result_ls &lt;- append(result_ls, list(evec))\n        }\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\nclean_df &lt;- roll_eigen2(overlap_x_df, width, comp)\n\n\nclean_mlt &lt;- melt(as.data.table(clean_df), id.vars = \"index\")\nclean_plt &lt;- plot_ts(clean_mlt, title = \"Eigenvector 1Y\")\nprint(clean_plt)"
  },
  {
    "objectID": "posts/portfolios-r/index.html#contour-ellipsoid",
    "href": "posts/portfolios-r/index.html#contour-ellipsoid",
    "title": "Portfolios",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid\nThe contours of a multivariate normal (MVN) distribution are ellipsoids centered at the mean. The directions of the axes are given by the eigenvectors of the covariance matrix and squared lengths are given by the eigenvalues:\n\\[\n\\begin{aligned}\n({\\mathbf{x}}-{\\boldsymbol{\\mu}})^{\\mathrm{T}}{\\boldsymbol{\\Sigma}}^{-1}({\\mathbf{x}}-{\\boldsymbol{\\mu}})=c^{2}\n\\end{aligned}\n\\]\nOr, in general parametric form:\n\\[\n\\begin{aligned}\nX(t)&=X_{c}+a\\,\\cos t\\,\\cos \\varphi -b\\,\\sin t\\,\\sin \\varphi\\\\\nY(t)&=Y_{c}+a\\,\\cos t\\,\\sin \\varphi +b\\,\\sin t\\,\\cos \\varphi\n\\end{aligned}\n\\] where \\(t\\) varies from \\(0,\\ldots,2\\pi\\). Here \\((X_{c},Y_{c})\\) is the center of the ellipse and \\(\\varphi\\) is the angle between the x-axis and the major axis of the ellipse.\nSpecifically:\n\\[\n\\begin{aligned}\n&\\text{Center: }\\boldsymbol{\\mu}=(X_{c},Y_{c})\\\\\n&\\text{Radius: }c^{2}= \\chi_{\\alpha}^{2}(df)\\\\\n&\\text{Length: }a=c\\sqrt{\\lambda_{k}}\\\\\n&\\text{Angle of rotation: }\\varphi=\\text{atan2}\\left(\\frac{V_{k}(2)}{V_{k}(1)}\\right)\n\\end{aligned}\n\\]\n\n# https://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/\n# https://maitra.public.iastate.edu/stat501/lectures/MultivariateNormalDistribution-I.pdf\n# https://en.wikipedia.org/wiki/Multivariate_normal_distribution\n# https://en.wikipedia.org/wiki/Ellipse#General_parametric_form\nellipse &lt;- function(n_sim, x, y, sigma) {\n    \n    data &lt;- cbind(x, y)\n    LV &lt;- eigen(cov(data))\n    L &lt;- LV$values\n    V &lt;- LV$vectors\n    \n    c &lt;- sqrt(qchisq(pnorm(sigma), 2))\n    t &lt;- seq(0, 2 * pi, len = n_sim)\n    phi &lt;- atan2(V[2, 1], V[1, 1])\n    a &lt;- c * sqrt(L[1]) * cos(t)\n    b &lt;- c * sqrt(L[2]) * sin(t)\n    R &lt;- matrix(c(cos(phi), -sin(phi), sin(phi), cos(phi)), nrow = 2, ncol = 2)\n    r &lt;- t(rbind(a, b)) %*% R\n    \n    result &lt;- sweep(r, 2, colMeans(data), \"+\") # 2D only\n    \n    return(result)\n    \n}\n\n\nreturns_x_xts &lt;- na.omit(returns_xts)[ , factors] # extended history\nellipse_x_xts &lt;- ellipse(1000, returns_x_xts[ , 1], returns_x_xts[ , 3], 1)\n\n\nellipse_plt &lt;- plot_scatter(data.table(returns_x_xts[ , c(1, 3)]), x = \"SP500\", y = \"DGS10\",\n                            title = \"Return 1D (%)\") +\n  geom_point(data = data.table(ellipse_x_xts), aes(x = V1 * 100, y = V2 * 100))\nprint(ellipse_plt)"
  },
  {
    "objectID": "posts/portfolios-r/index.html#principal-component-regression",
    "href": "posts/portfolios-r/index.html#principal-component-regression",
    "title": "Portfolios",
    "section": "Principal component regression",
    "text": "Principal component regression\n\nCoefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\n# https://en.wikipedia.org/wiki/Principal_component_regression\npcr_coef &lt;- function(x, y, comps) {\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    LV &lt;- eigen(cov(x))\n    V &lt;- LV$vectors\n    \n    W &lt;- x %*% V\n    gamma &lt;- solve(t(W) %*% W) %*% (t(W) %*% y)\n    \n    result &lt;- V[ , 1:comps] %*% as.matrix(gamma[1:comps])\n    \n    return(result)\n    \n}\n\n\nscale_x_xts &lt;- scale(overlap_x_xts)\n\n\npcr_coef(scale_x_xts, overlap_y_xts, comps)\n\n              [,1]\n[1,]  0.0008109848\n[2,] -0.0007132832\n[3,]  0.0005759142\n[4,]  0.0005549691\n\n\n\npcr_coef(overlap_x_xts, overlap_y_xts, comps)\n\n            [,1]\n[1,]  0.40426830\n[2,] -0.09185109\n[3,]  0.01128428\n[4,]  0.02769459\n\n\n\nfit &lt;- pcr(reformulate(termlabels = \".\", response = tickers), \n           data = merge(scale_x_xts, overlap_y_xts), ncomp = comps)\ncoef(fit)\n\n, , 1 comps\n\n                     BAICX\nSP500         0.0008109848\nDTWEXAFEGS   -0.0007132832\nDGS10         0.0005759142\nBAMLH0A0HYM2  0.0005549691\n\n\n\n\nR-squared\n\npcr_rsq &lt;- function(x, y, comps) {\n    \n    coef &lt;- pcr_coef(x, y, comps)\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    result &lt;- (t(coef) %*% (t(x) %*% x) %*% coef) / (t(y) %*% y)\n    \n    return(result)\n    \n}\n\n\npcr_rsq(scale_x_xts, overlap_y_xts, comps)\n\n          BAICX\nBAICX 0.8245769\n\n\n\npcr_rsq(overlap_x_xts, overlap_y_xts, comps)\n\n          BAICX\nBAICX 0.6934975\n\n\n\nR2(fit)$val[comps + 1]\n\n[1] 0.8245769\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\npcr_se &lt;- function(x, y, comps) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- pcr_rsq(x, y, comps)\n    \n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    df_resid &lt;- n_rows - n_cols - 1\n    \n    var_y &lt;- t(y) %*% y\n    var_resid &lt;- as.vector((1 - rsq) * var_y / df_resid)\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values[1:comps] * (n_rows - 1)\n    V &lt;- LV$vectors[ , 1:comps]\n    \n    result &lt;- sqrt(var_resid * diag(V %*% sweep(t(V), 1, 1 / L, \"*\")))\n    \n    return(result)\n    \n}\n\n\npcr_se(scale_x_xts, overlap_y_xts, comps)\n\n[1] 2.380083e-05 2.093347e-05 1.690196e-05 1.628726e-05\n\n\n\npcr_se(overlap_x_xts, overlap_y_xts, comps)\n\n[1] 0.0171007759 0.0038853527 0.0004773313 0.0011714966"
  },
  {
    "objectID": "posts/portfolios-r/index.html#marchenkopastur-distribution",
    "href": "posts/portfolios-r/index.html#marchenkopastur-distribution",
    "title": "Portfolios",
    "section": "Marchenko–Pastur distribution",
    "text": "Marchenko–Pastur distribution\nMarchenko–Pastur distribution is the limiting distribution of eigenvalues of Wishart matrices as the matrix dimension \\(m\\) and degrees of freedom \\(n\\) both tend to infinity with ratio \\(m/n\\,\\to \\,\\lambda\\in(0,+\\infty)\\):\n\\[\n\\begin{aligned}\nd\\nu(x)&={\\frac {1}{2\\pi\\sigma ^{2}}}{\\frac{\\sqrt{(\\lambda_{+}-x)(x-\\lambda_{-})}}{\\lambda x}}\\,\\mathbf{1}_{x\\in[\\lambda_{-},\\lambda _{+}]}\\,dx\n\\end{aligned}\n\\]\nwith\n\\[\n\\begin{aligned}\n\\lambda_{\\pm}&=\\sigma^{2}(1\\pm{\\sqrt{\\lambda }})^{2}\n\\end{aligned}\n\\]\n\n# https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution\n# https://faculty.baruch.cuny.edu/jgatheral/RandomMatrixCovariance2008.pdf\ndmp &lt;- function(x, sigma = 1) {\n  \n  LV &lt;- eigen(cov(x))\n  L &lt;- LV$values\n  \n  lmbda &lt;- ncol(x) / nrow(x)\n  lower &lt;- sigma * (1 - sqrt(lmbda)) ^ 2\n  upper &lt;- sigma * (1 + sqrt(lmbda)) ^ 2\n  \n  d &lt;- ifelse((L &lt;= lower) | (L &gt;= upper), 0,\n              1 / (2 * pi * sigma * lmbda * L) * sqrt((upper - L) * (L - lower)))\n  \n  return(d)\n  \n}\n\n\nn_sim &lt;- 5000\nn_cols &lt;- 1000\n\n\ndata_sim &lt;- matrix(rnorm(n_sim * n_cols), nrow = n_sim, ncol = n_cols)\n\n\ndmp_dt &lt;- data.table(evals = eigen(cov(data_sim))$values,\n                     dmp = dmp(data_sim))\n\n\ndmp_plt &lt;- plot_density(dmp_dt, x = \"evals\", y = \"dmp\",\n                        title = \"Marchenko-Pastur distribution\", xlab = \"Eigenvalues\", ylab = \"Density\")\nprint(dmp_plt)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead."
  },
  {
    "objectID": "posts/portfolios-r/index.html#random-turnover",
    "href": "posts/portfolios-r/index.html#random-turnover",
    "title": "Portfolios",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\nrand_iterative &lt;- function(n_assets, lower, upper, target) {\n    \n    plug &lt;- FALSE\n    \n    while (!plug) {\n        \n        result &lt;- as.matrix(runif(n_assets - 1, min = lower, max = upper))\n        temp &lt;- target - sum(result)\n        \n        if ((temp &lt;= upper) && (temp &gt;= lower)) {\n            plug &lt;- TRUE            \n        }\n        \n    }\n    \n    result &lt;- append(result, temp)\n    \n    return(result)\n    \n}\n\n\nrand_turnover1 &lt;- function(n_sim, n_assets, lower, upper, target) {\n    \n    rng &lt;- upper - lower\n    \n    result &lt;- rand_weights3(n_sim, n_assets, lmbda) * rng\n    result &lt;- result - rng / n_assets\n    \n    return(result)\n    \n}\n\n\nrand_turnover2 &lt;- function(n_sim, n_assets, lower, upper, target) {\n    \n    result &lt;- matrix(rand_iterative(n_assets, lower, upper, target), nrow = 1, ncol = n_assets)\n    \n    while (nrow(result) &lt; n_sim) {\n        \n        temp &lt;- matrix(rand_iterative(n_assets, lower, upper, target), nrow = 1, ncol = n_assets)\n        result &lt;- rbind(result, temp)\n        \n    }\n    \n    return(result)\n    \n}\n\n\nlower &lt;- -0.05\nupper &lt;- 0.05\ntarget &lt;- 0\n\n\napproach1 &lt;- rand_turnover1(n_sim, n_assets, lower, upper, target)\napproach2 &lt;- rand_turnover2(n_sim, n_assets, lower, upper, target)\n\n\nplot_pairs(as.data.table(approach1), title = \"Weight (%)\")\n\n\n\n\n\nplot_pairs(as.data.table(approach2), title = \"Weight (%)\")"
  },
  {
    "objectID": "posts/portfolios-r/index.html#maximum-return",
    "href": "posts/portfolios-r/index.html#maximum-return",
    "title": "Portfolios",
    "section": "Maximum return",
    "text": "Maximum return\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\n\ntarget &lt;- 0.06\n\n\n# https://palomar.home.ece.ust.hk/MAFS6010R_lectures/slides_robust_portfolio.html\nmax_pnl_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    cons &lt;- list(params &gt;= 0, sum(params) == 1,\n                 quad_form(params, sigma) &lt;= target ^ 2)\n    \n    obj &lt;- Maximize(t(params) %*% mu)\n        \n    result &lt;- solve(Problem(obj, cons))$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams1 &lt;- max_pnl_optim(mu, sigma, target)\nparams1\n\n             [,1]\n[1,] 4.570765e-01\n[2,] 5.429235e-01\n[3,] 3.283939e-08\n[4,] 1.879820e-08\n\n\n\nmu %*% params1\n\n           [,1]\n[1,] 0.04173453\n\n\n\nsqrt(t(params1) %*% sigma %*% params1)\n\n     [,1]\n[1,] 0.06"
  },
  {
    "objectID": "posts/portfolios-r/index.html#minimum-variance",
    "href": "posts/portfolios-r/index.html#minimum-variance",
    "title": "Portfolios",
    "section": "Minimum variance",
    "text": "Minimum variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min_{x}&\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\n\ntarget &lt;- 0.03\n\n\nmin_risk_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    cons &lt;- list(params &gt;= 0, sum(params) == 1,\n                 sum(mu * params) &gt;= target)\n    \n    obj &lt;- Minimize(quad_form(params, sigma))\n        \n    result &lt;- solve(Problem(obj, cons))$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams2 &lt;- min_risk_optim(mu, sigma, target)\nparams2\n\n             [,1]\n[1,] 3.068265e-01\n[2,] 5.044244e-01\n[3,] 1.887491e-01\n[4,] 2.130983e-21\n\n\n\nmu %*% params2\n\n     [,1]\n[1,] 0.03\n\n\n\nsqrt(t(params2) %*% sigma %*% params2)\n\n           [,1]\n[1,] 0.04220845"
  },
  {
    "objectID": "posts/portfolios-r/index.html#maximum-ratio",
    "href": "posts/portfolios-r/index.html#maximum-ratio",
    "title": "Portfolios",
    "section": "Maximum ratio",
    "text": "Maximum ratio\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}-\\frac{1}{2}\\delta(\\mathbf{w}^T\\Sigma\\mathbf{w})\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\n\nir &lt;- 0.5\ntarget &lt;- ir / 0.06 # ir / std (see Black-Litterman)\n\n\nmax_ratio_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    cons &lt;- list(params &gt;= 0, sum(params) == 1)\n    \n    obj &lt;- Maximize(t(mu) %*% params - 0.5 * target * quad_form(params, sigma))\n        \n    result &lt;- solve(Problem(obj, cons))$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams3 &lt;- max_ratio_optim(mu, sigma, target)\nparams3\n\n              [,1]\n[1,]  4.747901e-01\n[2,]  5.252099e-01\n[3,] -6.324392e-24\n[4,] -5.022383e-23\n\n\n\nmu %*% params3\n\n           [,1]\n[1,] 0.04279219\n\n\n\nsqrt(t(params3) %*% sigma %*% params3)\n\n           [,1]\n[1,] 0.06200391"
  },
  {
    "objectID": "posts/portfolios-r/index.html#prior-distribution",
    "href": "posts/portfolios-r/index.html#prior-distribution",
    "title": "Portfolios",
    "section": "Prior distribution",
    "text": "Prior distribution\n\\[\n\\begin{aligned}\n\\text{Risk aversion: } &\\lambda=\\frac{E(r)-r_{f}}{\\sigma^{2}}=\\frac{IR}{\\sigma}\\\\\n\\text{Implied returns: } &\\Pi=\\lambda\\Sigma w\\\\\n\\text{Distribution: } &N\\sim(\\Pi,\\tau\\Sigma)\n\\end{aligned}\n\\]\n\nimplied_pnl &lt;- function(params, ir, sigma) {\n    \n    lmbda &lt;- as.numeric(ir / sqrt(t(params) %*% sigma %*% params))\n    \n    result &lt;- lmbda * sigma %*% params\n    \n    return(result)    \n    \n}\n\n\nimplied_pnl(params3, ir, sigma)\n\n                      [,1]\nSP500         0.0616647903\nDTWEXAFEGS    0.0053648142\nDGS10        -0.0004003454\nBAMLH0A0HYM2  0.0049851206"
  },
  {
    "objectID": "posts/portfolios-r/index.html#conditional-distribution",
    "href": "posts/portfolios-r/index.html#conditional-distribution",
    "title": "Portfolios",
    "section": "Conditional distribution",
    "text": "Conditional distribution\n\\[\n\\begin{aligned}\n\\text{Prior mean variance: } &\\tau\\in(0.01, 0.05)\\approx(0.025)\\\\\n\\text{Asset views: } &\\mathbf{P}={\\begin{bmatrix}\np_{11}&\\cdots&p_{1n}\\\\\n\\vdots&\\ddots&\\vdots\\\\\np_{k1}&\\cdots&p_{kn}\n\\end{bmatrix}}=\n{\\begin{bmatrix}\n0&0&0&0&0&0&1&0\\\\\n-1&1&0&0&0&0&0&0\\\\\n0&0&0.5&-0.5&0.5&-0.5&0&0\n\\end{bmatrix}}\\\\\n\\text{View returns: } &\\mathbf{Q}={\\begin{bmatrix}\nq_{1}\\\\\n\\vdots\\\\\nq_{k}\n\\end{bmatrix}}=\n{\\begin{bmatrix}\n0.0525\\\\\n0.0025\\\\\n0.0200\n\\end{bmatrix}}\\\\\n\\text{View confidence: } &\\mathbf{C}={\\begin{bmatrix}\nc_{1}\\\\\n\\vdots\\\\\nc_{k}\n\\end{bmatrix}}=\n{\\begin{bmatrix}\n0.2500\\\\\n0.5000\\\\\n0.6500\n\\end{bmatrix}}\\\\\n\\text{View covariance: } &\\mathbf{\\Omega}={\\begin{bmatrix}\n\\tau\\left(\\frac{1-c_{1}}{c_{1}}\\right)\\left(p_{1}\\Sigma p_{1}^{T}\\right)&0&0\\\\\n0&\\ddots&0\\\\\n0&0&\\tau\\left(\\frac{1-c_{k}}{c_{k}}\\right)\\left(p_{k}\\Sigma p_{k}^{T}\\right)\n\\end{bmatrix}}\\\\\n\\text{Distribution: } &N\\sim(\\mathbf{Q}, \\mathbf{\\Omega})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/portfolios-r/index.html#posterior-distribution",
    "href": "posts/portfolios-r/index.html#posterior-distribution",
    "title": "Portfolios",
    "section": "Posterior distribution",
    "text": "Posterior distribution\n\\[\n\\begin{aligned}\n\\text{Implied returns: } &\\hat{\\Pi}=\\Pi+\\tau\\Sigma \\mathbf{P}^{T}\\left(\\tau \\mathbf{P}\\Sigma \\mathbf{P}^{T}+\\mathbf{\\Omega}\\right)^{-1}\\left(\\mathbf{Q}-\\mathbf{P}\\Pi^{T}\\right)\\\\\n\\text{Covariance: } &\\hat{\\Sigma}=\\Sigma+\\tau\\left[\\Sigma-\\Sigma\\mathbf{P}^{T}\\left(\\tau\\mathbf{P}\\Sigma\\mathbf{P}^{T}+\\mathbf{\\Omega}\\right)^{-1}\\tau\\mathbf{P}\\Sigma\\right]\\\\\n\\text{Weights: } &\\hat{w}=\\hat{\\Pi}\\left(\\lambda\\Sigma\\right)^{-1}\\\\\n\\text{Distribution: } &N\\sim\\left(\\left[\\left(\\tau\\Sigma\\right)^{-1}+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{P}\\right]^{-1}\\left[\\left(\\tau\\Sigma\\right)^{-1}\\Pi+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{Q}\\right],\\left[\\left(\\tau\\Sigma\\right)^{-1}+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{P}\\right]^{-1}\\right)\n\\end{aligned}\n\\]\n\nblack_litterman &lt;- function(params, ir, sigma, views) {\n    \n    # prior distribution\n    weights_prior &lt;- params\n    sigma_prior &lt;- sigma    \n    lmbda &lt;- as.numeric(ir / sqrt(t(weights_prior) %*% sigma %*% weights_prior))\n    pi_prior &lt;- lmbda * sigma_prior %*% weights_prior\n    \n    # matrix calculations\n    matmul_left &lt;- views[[\"tau\"]] * sigma_prior %*% t(views[[\"P\"]])\n    matmul_mid &lt;- views[[\"tau\"]] * views[[\"P\"]] %*% sigma_prior %*% t(views[[\"P\"]])\n    matmul_right &lt;- views[[\"Q\"]] - views[[\"P\"]] %*% pi_prior\n    \n    # conditional distribution\n    omega &lt;- diag(diag(diag((1 - views[[\"C\"]]) / views[[\"C\"]]) %*% matmul_mid))\n        \n    # posterior distribution\n    pi_posterior &lt;- pi_prior + matmul_left %*% solve(matmul_mid + omega) %*% matmul_right\n\n    sigma_posterior &lt;- sigma_prior +  views[[\"tau\"]] * sigma_prior -\n        matmul_left %*% solve(matmul_mid + omega) %*% (tau * views[[\"P\"]] %*% sigma_prior)\n    \n    weights_posterior &lt;- t(pi_posterior) %*% solve(lmbda * sigma_prior)\n    \n    # implied confidence\n    pi_posterior_100 &lt;- pi_prior + matmul_left %*% solve(matmul_mid) %*% matmul_right\n    \n    weights_posterior_100 &lt;- t(pi_posterior_100) %*% solve(lmbda * sigma_prior)\n    \n    implied_confidence &lt;- (weights_posterior - weights_prior) / (weights_posterior_100 - weights_prior)\n    \n    result &lt;- list(\"implied_confidence\" = implied_confidence,\n                   \"weights_prior\" = t(as.matrix(weights_prior)),\n                   \"weights_posterior\" = weights_posterior,\n                   \"pi_prior\" = t(pi_prior),\n                   \"pi_posterior\" = t(pi_posterior),\n                   \"sigma_prior\" = sigma_prior,\n                   \"sigma_posterior\" = sigma_posterior)\n    \n    return(result)   \n    \n}\n\n\ntau &lt;- 0.025\nP &lt;- diag(length(factors))\nQ &lt;- t(implied_shocks(0.1, overlap_x_xts, overlap_x_xts[ , 1], 1))\nC &lt;- rep(0.95, length(factors))\nviews &lt;- list(\"tau\" = tau, \"P\" = P, \"Q\" = Q, \"C\" = C)\n\n\nbl &lt;- black_litterman(as.vector(params3), ir, sigma, views)\nbl\n\n$implied_confidence\n         SP500 DTWEXAFEGS      DGS10 BAMLH0A0HYM2\n[1,] 0.9717547  0.9339887 -0.3760611   -0.4081681\n\n$weights_prior\n          [,1]      [,2]         [,3]         [,4]\n[1,] 0.4332169 0.5667831 5.743076e-23 6.211119e-23\n\n$weights_posterior\n         SP500 DTWEXAFEGS      DGS10 BAMLH0A0HYM2\n[1,] 0.5267761 0.02394436 0.03238609   0.01175295\n\n$pi_prior\n          SP500  DTWEXAFEGS         DGS10 BAMLH0A0HYM2\n[1,] 0.06166479 0.005364814 -0.0004003454  0.004985121\n\n$pi_posterior\n          SP500  DTWEXAFEGS       DGS10 BAMLH0A0HYM2\n[1,] 0.09906844 -0.01810305 0.001907041  0.006716716\n\n$sigma_prior\n                    SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2\nSP500         0.022516187 -0.0042610124  4.411780e-04  1.518078e-03\nDTWEXAFEGS   -0.004261012  0.0043834415 -4.212806e-04 -1.134996e-04\nDGS10         0.000441178 -0.0004212806  1.513906e-04 -2.061791e-05\nBAMLH0A0HYM2  0.001518078 -0.0001134996 -2.061791e-05  2.654178e-04\n\n$sigma_posterior\n                     SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2\nSP500         0.0225429284 -0.0042613659  4.412128e-04  1.518259e-03\nDTWEXAFEGS   -0.0042613659  0.0043887744 -4.213131e-04 -1.134918e-04\nDGS10         0.0004412128 -0.0004213131  1.515759e-04 -2.062274e-05\nBAMLH0A0HYM2  0.0015182585 -0.0001134918 -2.062274e-05  2.657376e-04\n\n\n\nparams4 &lt;- as.vector(bl[[\"weights_posterior\"]])\nparams4 &lt;- params4 / sum(params4) # no leverage\nparams4\n\n[1] 0.88554710 0.04025213 0.05444325 0.01975752\n\n\n\nmu %*% params4\n\n           [,1]\n[1,] 0.06505687\n\n\n\nsqrt(t(params4) %*% sigma %*% params4)\n\n          [,1]\n[1,] 0.1321156"
  },
  {
    "objectID": "posts/portfolios-r/index.html#single-period",
    "href": "posts/portfolios-r/index.html#single-period",
    "title": "Portfolios",
    "section": "Single-period",
    "text": "Single-period\nThe arithmetic active return is commonly decomposed using the Brinson-Fachler method:\n\\[\n\\begin{aligned}\n\\text{Allocation: } &r_{a}=\\sum_{k=1}^{n}(w_{p,k}-w_{b,k})(r_{b,k}-r_{b})\\\\\n\\text{Selection: } &r_{s}=\\sum_{k=1}^{n}w_{p,k}(r_{p,k}-r_{b,k})\\\\\n\\end{aligned}\n\\]\nwhere \\(k=1,\\ldots,n\\) is each sector or factor."
  },
  {
    "objectID": "posts/portfolios-r/index.html#multi-period",
    "href": "posts/portfolios-r/index.html#multi-period",
    "title": "Portfolios",
    "section": "Multi-period",
    "text": "Multi-period\nArithmetic attributes add to the active return of a single period; however, they cannot be summed or compounded to explain the active return over multiple periods. To solve this problem, the original arithmetic attribute is multiplied by a single scaling coefficient for that period. After all single-period original attributes have been transformed, the adjusted attributes sum to the active return over the periods.\n\\[\n\\begin{aligned}\n\\text{Carino scaling coefficient: } &c_{t}=\\frac{[\\ln(1+r_{p,t})-\\ln(1+r_{b,t})]/(r_{p,t}-r_{b,t})}{[\\ln(1+r_{p})-\\ln(1+r_{b})]/(r_{p}-r_{b})}\n\\end{aligned}\n\\]\nwhere \\(t=1,\\ldots,n\\) is each period.\n\n# http://www.frongello.com/support/Works/Chap20RiskBook.pdf\n# https://github.com/R-Finance/PortfolioAttribution/blob/master/R/Carino.R\npnl_attrib &lt;- function(params, x) {\n    \n    total_i &lt;- rowSums(x)\n    total &lt;- prod(1 + total_i) - 1\n    \n    coef &lt;- (log(1 + total_i) / total_i) / (log(1 + total) / total)\n    \n    result &lt;- colSums(x * coef)\n    \n    return(result)\n    \n}\n\n\nattrib_mat &lt;- sweep(tail(na.omit(returns_xts)[ , factors], width), 2, params1, \"*\")\n\n\npnl_attrib(params1, attrib_mat)\n\n        SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 8.657379e-02 -3.172667e-02  1.332956e-10  1.944644e-10"
  },
  {
    "objectID": "posts/securities-py/index.html#nonlinear-beta",
    "href": "posts/securities-py/index.html#nonlinear-beta",
    "title": "Securities",
    "section": "Nonlinear beta",
    "text": "Nonlinear beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\n# https://en.wikipedia.org/wiki/Notional_amount\ndef beta_option(type, S, K, r, q, tau, sigma, sec):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    notional_mv = sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * delta\n    notional_mv0 = sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * sec[\"delta\"]\n    \n    call_value = sec[\"beta\"] * (notional_mv - notional_mv0)\n    put_value = sec[\"beta\"] * (notional_mv0 - notional_mv)\n    \n    result = np.where(type == \"call\", call_value, put_value)\n        \n    return result\n\n\nbeta = 0.35\ntype = \"call\"\nn = 200\nmultiple = 100\nnav = 1000000000\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\ndelta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\nsec = {\n    \"n\": n,\n    \"multiple\": multiple,\n    \"S\": S,\n    \"delta\": delta,\n    \"beta\": 1\n}\n\n\nbeta_df = pd.DataFrame([(x, y) for x in types for y in shocks], \n    columns = [\"type\", \"shock\"])\nbeta_df[\"spot\"] = level_shock(beta_df[\"shock\"], S, tau, sigma)\nbeta_df[\"static\"] = beta\nbeta_df[\"dynamic\"] = beta + beta_option(type, beta_df[\"spot\"], K, r, q, tau, sigma, sec) / nav\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/securities-r/index.html#nonlinear-beta",
    "href": "posts/securities-r/index.html#nonlinear-beta",
    "title": "Securities",
    "section": "Nonlinear beta",
    "text": "Nonlinear beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\n# https://en.wikipedia.org/wiki/Notional_amount\nbeta_option &lt;- function(type, S, K, r, q, tau, sigma, sec) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    notional_mv &lt;- sec[[\"n\"]] * sec[[\"multiple\"]] * sec[[\"S\"]] * delta\n    notional_mv0 &lt;- sec[[\"n\"]] * sec[[\"multiple\"]] * sec[[\"S\"]] * sec[[\"delta\"]]\n    \n    if (type == \"call\") {\n        result &lt;- sec[[\"beta\"]] * (notional_mv - notional_mv0)\n    } else if (type == \"put\") {\n        result &lt;- sec[[\"beta\"]] * (notional_mv0 - notional_mv)\n    }        \n        \n    return(result)\n    \n}\n\n\nbeta &lt;- 0.35\ntype &lt;- \"call\"\nn &lt;- 200\nmultiple &lt;- 100\nnav &lt;- 1000000000\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\ndelta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\nsec &lt;- list(\n    \"n\" = n,\n    \"multiple\" = multiple,\n    \"S\" = S,\n    \"delta\" = delta,\n    \"beta\" = 1\n)\n\n\nbeta_dt &lt;- CJ(type = type, shock = shocks)\nbeta_dt[ , spot := level_shock(shock, S, tau, sigma), by = c(\"type\", \"shock\")]\nbeta_dt[ , static := beta]\nbeta_dt[ , dynamic := beta + beta_option(type, spot, K, r, q, tau, sigma, sec) / nav, by = c(\"type\", \"shock\")]\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order-1",
    "href": "posts/securities-py/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(P(y+\\Delta y)-P(y)\\approx -D\\Delta y +{\\frac{C\\times 100}{2!}}(\\Delta y)^{2}\\)\n\ndef pnl_bond(duration, convexity, dy):\n    \n    duration_pnl = -duration * dy\n    convexity_pnl = (convexity * 100 / 2) * dy ** 2\n    income_pnl = dy\n    \n    result = pd.DataFrame({\n        \"total\": duration_pnl + convexity_pnl + income_pnl,\n        \"duration\": duration_pnl,\n        \"convexity\": convexity_pnl,\n        \"income\": income_pnl\n    })\n    \n    return result\n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor = \"DGS10\"\nduration = 6.5\nconvexity = 0.65\ny = levels_df.fillna(method = \"ffill\")[factor][-width]\n\n\nbond_df = pd.DataFrame({\n    \"duration\": duration,\n    \"convexity\": convexity,\n    \"dy\": (levels_df.fillna(method = \"ffill\")[factor][-width:] - y) / 100\n})\n\n\nattrib_df = pnl_bond(bond_df[\"duration\"], bond_df[\"convexity\"], bond_df[\"dy\"])\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=-\\frac{\\partial D}{\\partial y}\\\\\n&=\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}-\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=C-D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(C\\times 100-D^{2}\\right)=C-\\frac{D^{2}}{100}\\)\n\ndef yield_shock(shock, tau, sigma):\n    \n    result = shock * sigma * np.sqrt(tau)\n    \n    return result\n\n\ndef duration_drift(duration, convexity, dy):\n    \n    drift = convexity - duration ** 2 / 100\n    change = -drift * dy * 100\n    \n    result = {\n        \"drift\": drift,\n        \"change\": change\n    }\n    \n    return result\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor = \"DGS10\"\nsigma = sd_df[factor][-1]\n\n\nduration_df = pd.DataFrame(shocks).rename(columns = {0: \"shock\"})\nduration_df[\"spot\"] = yield_shock(duration_df[\"shock\"], tau, sigma)\nduration_df[\"static\"] = duration\nduration_df[\"dynamic\"] = duration + \\\n    duration_drift(duration, convexity, duration_df[\"spot\"])[\"change\"]"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order-1",
    "href": "posts/securities-py/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlack’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\ndef pnl_option(type, S, K, r, q, tau, sigma, dS, dt, dsigma):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    value = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega = bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta = bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma = bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl = delta * dS / value\n    gamma_pnl = gamma / 2 * dS ** 2 / value\n    vega_pnl = vega * dsigma / value\n    theta_pnl = theta * dt / value\n    \n    result = pd.DataFrame({\n        \"total\": delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n        \"delta\": delta_pnl,\n        \"gamma\": gamma_pnl,\n        \"vega\": vega_pnl,\n        \"theta\": theta_pnl\n    })\n    \n    return result\n\n\nfactor = \"SP500\"\ntype = \"call\"\nS = levels_df.fillna(method = \"ffill\")[factor][-width]\nK = S # * (1 + 0.05)\ntau = 1 # = 252 / 252\nsigma = sd_df[factor][-width]\n\n\noptions_df = pd.DataFrame({\n    \"spot\": levels_df.fillna(method = \"ffill\")[factor][-width:],\n    \"sigma\": sd_df[factor][-width:]\n})\noptions_df[\"dS\"] = options_df[\"spot\"] - S\noptions_df[\"dt_diff\"] = (options_df.index - options_df.index[0]).days\noptions_df[\"dt\"] = options_df[\"dt_diff\"] / options_df[\"dt_diff\"][-1]\noptions_df[\"dsigma\"] = options_df[\"sigma\"] - sigma\n\n\nattrib_df = pnl_option(type, S, K, r, q, tau, sigma,\n                       options_df[\"dS\"], options_df[\"dt\"], options_df[\"dsigma\"])\n\n\n\n\n\n\n\n\nIto’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\ndef sim_gbm(n_sim, S, mu, sigma, dt):\n    \n    result = S * np.exp(np.cumsum(sigma * np.sqrt(dt) * np.random.normal(size = n_sim)) + \\\n                        (mu - 0.5 * sigma ** 2) * dt)\n    \n    return result\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\ndef sim_multi_gbm(n_sim, S, mu, sigma, dt):\n    \n    n_cols = sigma.shape[1]\n    \n    Z = np.random.normal(size = n_sim * n_cols).reshape((n_sim, n_cols))\n    X = np.sqrt(dt) * Z @ np.linalg.cholesky(sigma).T + (mu - 0.5 * np.diag(sigma)) * dt\n    \n    result = S * np.exp(X.cumsum(axis = 0))\n    \n    return np.asmatrix(result)\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS = [1] * len(factors)\nsigma = np.cov(returns_df[\"returns\"].dropna().T, ddof = 1) * scale[\"periods\"]\nmu = np.array(returns_df[\"returns\"].dropna().mean()) * scale[\"periods\"]\nmu = mu + np.diag(sigma) / 2 # drift\ndt = 1 / scale[\"periods\"]\n\n\nmu_ls = []\nsigma_ls = []\n\n\nfor i in range(10000): # \"TypeError: 'float' object cannot be interpreted as an integer\"\n  \n    # assumes underlying stock price follows geometric Brownian motion with constant volatility\n    levels_sim = pd.DataFrame(sim_multi_gbm(width + 1, S, mu, sigma, dt))\n    returns_sim = np.log(levels_sim).diff()\n\n    mu_sim = returns_sim.mean() * scale[\"periods\"]\n    sigma_sim = returns_sim.std() * np.sqrt(scale[\"periods\"])\n\n    mu_ls.append(mu_sim)\n    sigma_ls.append(sigma_sim)\n\n\nmu_df = pd.DataFrame(mu_ls)\nsigma_df = pd.DataFrame(sigma_ls)\n\n\npd.DataFrame({\n    \"empirical\": np.array(returns_df[\"returns\"].dropna().mean()) * scale[\"periods\"],\n    \"theoretical\": mu_df.mean()\n})\n\n   empirical  theoretical\n0   0.088257     0.086361\n1   0.018287     0.018006\n2  -0.000576    -0.000571\n3   0.000858     0.000690\n\n\n\npd.DataFrame({\n    \"empirical\": np.sqrt(np.diag(sigma)),\n    \"theoretical\": sigma_df.mean()\n})\n\n   empirical  theoretical\n0   0.179654     0.179456\n1   0.062199     0.062123\n2   0.008224     0.008215\n3   0.016932     0.016934\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility",
    "href": "posts/securities-py/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\ndef implied_vol_newton(params, type, S, K, r, q, tau):\n    \n    target0 = 0\n    sigma0 = params[\"sigma\"]\n    \n    while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n        \n        d1 = bs_d1(S, K, r, q, tau, sigma0)\n        d2 = bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 = bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        vega0 = bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma = sigma0 - (target0 - params[\"target\"]) / vega0\n        sigma0 = sigma\n        \n    return sigma\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS = levels_df.fillna(method = \"ffill\")[factor][-1]\nK = S # * (1 + 0.05)\nsigma = sd_df[factor][-1] # overrides matrix\nstart = 0.2\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\ntarget = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams = {\n    \"target\": target,\n    \"sigma\": start,\n    \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nimplied_vol_newton(params, type, S, K, r, q, tau) \n\n0.13404081832925302"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility-1",
    "href": "posts/securities-py/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\ndef implied_vol_obj(param, type, S, K, r, q, tau, target):\n    \n    d1 = bs_d1(S, K, r, q, tau, param)\n    d2 = bs_d2(S, K, r, q, tau, param)\n    target0 = bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result = abs(target0 - target)\n    \n    return result\n\ndef implied_vol_optim(param, type, S, K, r, q, tau, target):\n    \n    result = minimize(implied_vol_obj, param, args = (type, S, K, r, q, tau, target))\n    \n    return result.x.item()\n\n\nimplied_vol_optim(start, type, S, K, r, q, tau, target)\n\n0.13404081087867226"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order-1",
    "href": "posts/securities-r/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(P(y+\\Delta y)-P(y)\\approx -D\\Delta y +{\\frac{C\\times 100}{2!}}(\\Delta y)^{2}\\)\n\npnl_bond &lt;- function(duration, convexity, dy) {\n    \n    duration_pnl &lt;- -duration * dy\n    convexity_pnl &lt;- (convexity * 100 / 2) * dy ^ 2\n    income_pnl &lt;- dy\n    \n    result &lt;- list(\"total\" = duration_pnl + convexity_pnl + income_pnl,\n                   \"duration\" = duration_pnl,\n                   \"convexity\" = convexity_pnl,\n                   \"income\" = income_pnl)\n    \n    return(result)\n    \n} \n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor &lt;- \"DGS10\"\nduration &lt;- 6.5\nconvexity &lt;- 0.65\ny &lt;- coredata(tail(na.locf(levels_xts[ , factor]), width)[1])\n\n\nbonds_dt &lt;- data.table(index = index(tail(levels_xts, width)),\n                       duration = duration, convexity = convexity,\n                       dy = na.locf(tail(levels_xts[ , factor], width)))\nsetnames(bonds_dt, c(\"index\", \"duration\", \"convexity\", \"dy\"))\nbonds_dt[ , dy := (dy - y) / 100, by = index]\n\n\nattrib_dt &lt;- bonds_dt[ , as.list(unlist(pnl_bond(duration, convexity, dy))), by = index]\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=-\\frac{\\partial D}{\\partial y}\\\\\n&=\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}-\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=C-D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(C\\times 100-D^{2}\\right)=C-\\frac{D^{2}}{100}\\)\n\nyield_shock &lt;- function(shock, tau, sigma) {\n    \n    result &lt;- shock * sigma * sqrt(tau)\n    \n    return(result)\n    \n}\n\n\nduration_drift &lt;- function(duration, convexity, dy) {\n    \n    drift &lt;- convexity - duration ^ 2 / 100\n    change &lt;- -drift * dy * 100\n    \n    result &lt;- list(\"drift\" = drift,\n                   \"change\" = change)\n    \n    return(result)\n    \n}\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor &lt;- \"DGS10\"\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor])\n\n\nduration_dt &lt;- CJ(shock = shocks)\nduration_dt[ , spot := yield_shock(shock, tau, sigma), by = \"shock\"]\nduration_dt[ , static := duration]\nduration_dt[ , dynamic := duration + duration_drift(duration, convexity, spot)[[\"change\"]], by = \"shock\"]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order-1",
    "href": "posts/securities-r/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlack’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\npnl_option &lt;- function(type, S, K, r, q, tau, sigma, dS, dt, dsigma) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    value &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega &lt;- bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta &lt;- bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma &lt;- bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl &lt;- delta * dS / value\n    gamma_pnl &lt;- gamma / 2 * dS ^ 2 / value\n    vega_pnl &lt;- vega * dsigma / value\n    theta_pnl &lt;- theta * dt / value\n    \n    result &lt;- list(\"total\" = delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n                   \"delta\" = delta_pnl,\n                   \"gamma\" = gamma_pnl,\n                   \"vega\" = vega_pnl,\n                   \"theta\" = theta_pnl)\n    \n    return(result)    \n    \n}\n\n\nfactor &lt;- \"SP500\"\ntype &lt;- \"call\"\nS &lt;- coredata(tail(na.locf(levels_xts[ , factor]), width)[1])\nK &lt;- S # * (1 + 0.05)\ntau &lt;- 1 # = 252 / 252\nsigma &lt;- coredata(tail(sd_xts[ , factor], width)[1])\n\n\noptions_dt &lt;- data.table(index = index(tail(levels_xts, width)),\n                         spot = na.locf(tail(levels_xts[ , factor], width)),\n                         sigma = tail(sd_xts[ , factor], width))\nsetnames(options_dt, c(\"index\", \"spot\", \"sigma\"))\noptions_dt[ , dS := spot - S, by = index]\noptions_dt[ , dt_diff := as.numeric(index - index[1])]\noptions_dt[ , dt := dt_diff / tail(dt_diff, 1)]\noptions_dt[ , dsigma := sigma - ..sigma, by = index]\n\n\nattrib_dt &lt;- options_dt[ , as.list(unlist(pnl_option(type, S, K, r, q, tau, ..sigma,\n                                                     dS, dt, dsigma))), by = index]\n\n\n\n\n\n\n\n\nIto’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\nsim_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n    \n    result &lt;- S * exp(cumsum(sigma * sqrt(dt) * rnorm(n_sim)) +\n                      (mu - 0.5 * sigma ^ 2) * dt)\n    \n    return(result)\n    \n}\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\nsim_multi_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n    \n    n_cols &lt;- ncol(sigma)\n    \n    Z &lt;- matrix(rnorm(n_sim * n_cols), nrow = n_sim, ncol = n_cols)\n    X &lt;- sweep(sqrt(dt) * (Z %*% chol(sigma)), 2, (mu - 0.5 * diag(sigma)) * dt, \"+\")\n    \n    result &lt;- sweep(apply(X, 2, function(x) exp(cumsum(x))), 2, S, \"*\")\n    \n    return(result)\n    \n}\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS &lt;- rep(1, length(factors))\nsigma &lt;- cov(returns_xts, use = \"complete.obs\") * scale[[\"periods\"]]\nmu &lt;- colMeans(na.omit(returns_xts)) * scale[[\"periods\"]]\nmu &lt;- mu + diag(sigma) / 2 # drift\ndt &lt;- 1 / scale[[\"periods\"]]\n\n\nmu_ls &lt;- list()\nsigma_ls &lt;- list()\n\n\nfor (i in 1:1e4) {\n    \n    # assumes stock prices\n    levels_sim &lt;- sim_multi_gbm(width + 1, S, mu, sigma, dt)\n    returns_sim &lt;- diff(log(levels_sim))\n\n    mu_sim &lt;- colMeans(returns_sim) * scale[[\"periods\"]]\n    sigma_sim &lt;- apply(returns_sim, 2, sd) * sqrt(scale[[\"periods\"]])\n    \n    mu_ls &lt;- append(mu_ls, list(mu_sim))\n    sigma_ls &lt;- append(sigma_ls, list(sigma_sim))\n    \n}\n\n\ndata.frame(\"empirical\" = colMeans(na.omit(returns_xts)) * scale[[\"periods\"]],\n           \"theoretical\" = colMeans(do.call(rbind, mu_ls)))\n\n                 empirical   theoretical\nSP500         0.0882574632  0.0909617990\nDTWEXAFEGS    0.0182866707  0.0179406059\nDGS10        -0.0005756897 -0.0006698148\nBAMLH0A0HYM2  0.0008581034  0.0011643750\n\n\n\ndata.frame(\"empirical\" = sqrt(diag(sigma)),\n           \"theoretical\" = colMeans(do.call(rbind, sigma_ls)))\n\n               empirical theoretical\nSP500        0.179653728 0.179471194\nDTWEXAFEGS   0.062198908 0.062108972\nDGS10        0.008224016 0.008218401\nBAMLH0A0HYM2 0.016932091 0.016915715\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility",
    "href": "posts/securities-r/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\nimplied_vol_newton &lt;- function(params, type, S, K, r, q, tau) {\n    \n    target0 &lt;- 0\n    sigma0 &lt;- params[[\"sigma\"]]\n    \n    while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n        \n        d1 &lt;- bs_d1(S, K, r, q, tau, sigma0)\n        d2 &lt;- bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 &lt;- bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        vega0 &lt;- bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma &lt;- sigma0 - (target0 - params[[\"target\"]]) / vega0\n        sigma0 &lt;- sigma\n        \n    }\n    \n    return(sigma)\n    \n}\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS &lt;- coredata(na.locf(levels_xts)[nrow(levels_xts), factor])\nK &lt;- S # * (1 + 0.05)\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor]) # overrides matrix\nstart &lt;- 0.2\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\ntarget &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams &lt;- list(\n    \"target\" = target,\n    \"sigma\" = start,\n    \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nimplied_vol_newton(params, type, S, K, r, q, tau)\n\n         SP500\n[1,] 0.1341163"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility-1",
    "href": "posts/securities-r/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\nimplied_vol_obj &lt;- function(param, type, S, K, r, q, tau, target) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, param)\n    d2 &lt;- bs_d2(S, K, r, q, tau, param)\n    target0 &lt;- bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result &lt;- abs(target0 - target)\n    \n    return(result)\n    \n}\n\nimplied_vol_optim &lt;- function(param, type, S, K, r, q, tau, target) {\n    \n    result &lt;- optim(param, implied_vol_obj, type = type, S = S, K = K, r = r, q = q,\n                    tau = tau, target = target, method = \"Brent\", lower = 0, upper = 1)\n    \n    return(result$par)\n    \n}\n\n\nimplied_vol_optim(start, type, S, K, r, q, tau, target)\n\n[1] 0.1341163"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity",
    "href": "posts/securities-py/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity-1",
    "href": "posts/securities-py/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity",
    "href": "posts/securities-r/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity-1",
    "href": "posts/securities-r/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity"
  },
  {
    "objectID": "posts/risk-r/index.html",
    "href": "posts/risk-r/index.html",
    "title": "Risk",
    "section": "",
    "text": "library(quantmod)\nlibrary(roll)\nlibrary(data.table)\nfactors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\nfactors &lt;- c(factors_r, factors_d)\nwidth &lt;- 252\nscale &lt;- list(\"periods\" = 252, \"overlap\" = 5)\ngetSymbols(factors, src = \"FRED\")\nlevels_xts &lt;- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))\nreturns_xts &lt;- do.call(merge, lapply(factors, function(i) {\n    if (i %in% factors_r) {\n        diff(log((levels_xts[ , i])))\n    } else if (i %in% factors_d) {\n        -diff(levels_xts[ , i]) / 100\n    }    \n}))\noverlap_xts &lt;- roll_mean(returns_xts, scale[[\"overlap\"]], min_obs = 1, na_restore = TRUE)\ntickers &lt;- \"BAICX\" # fund inception date is \"2011-11-28\" \ninvisible(getSymbols(tickers, src = \"tiingo\", api.key = Sys.getenv(\"TIINGO_API_KEY\"), adjust = TRUE))\nprices_xts &lt;- do.call(merge, c(lapply(tickers, function(i) Cl(get(i))), all = TRUE))\ncolnames(prices_xts) &lt;- tickers\nindex(prices_xts) &lt;- as.Date(index(prices_xts))\nreturns_xts &lt;- merge(returns_xts, diff(log(prices_xts)))\noverlap_xts &lt;- merge(overlap_xts, roll_mean(returns_xts[ , tickers], scale[[\"overlap\"]], min_obs = 1))\n# weights &lt;- 0.9 ^ ((width - 1):0)\nweights &lt;- rep(1, width)\noverlap_xts &lt;- na.omit(overlap_xts)\noverlap_x_xts &lt;- tail(overlap_xts[ , factors], width) # same dimension as `weights`\noverlap_y_xts &lt;- tail(overlap_xts[ , tickers], width)"
  },
  {
    "objectID": "posts/risk-r/index.html#coefficients",
    "href": "posts/risk-r/index.html#coefficients",
    "title": "Risk",
    "section": "Coefficients",
    "text": "Coefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\nlm_coef &lt;- function(x, y, weights, intercept) {\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    result &lt;- solve(crossprod(x, diag(weights)) %*% x) %*% crossprod(x, diag(weights) %*% y)\n    \n    return(result)\n    \n}\n\n\nt(lm_coef(overlap_x_xts, overlap_y_xts, weights, intercept))\n\n        (Intercept)    xSP500 xDTWEXAFEGS   xDGS10 xBAMLH0A0HYM2\nBAICX -1.576148e-05 0.2233802  -0.1413442 2.505277      1.575168\n\n\n\nif (intercept) {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts, weights = weights)\n} else {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts - 1, weights = weights)\n}\n    \ncoef(fit)\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n            -1.576148e-05              2.233802e-01             -1.413442e-01 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             2.505277e+00              1.575168e+00"
  },
  {
    "objectID": "posts/risk-r/index.html#r-squared",
    "href": "posts/risk-r/index.html#r-squared",
    "title": "Risk",
    "section": "R-squared",
    "text": "R-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\nlm_rsq &lt;- function(x, y, weights, intercept) {\n        \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        x &lt;- sweep(x, 2, apply(x, 2, weighted.mean, w = weights), \"-\")\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n    }\n    \n    result &lt;- (t(coef) %*% (crossprod(x, diag(weights)) %*% x) %*% coef) / (crossprod(y, diag(weights)) %*% y)\n    \n    return(result)\n    \n}\n\n\nlm_rsq(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n          BAICX\nBAICX 0.8595365\n\n\n\nsummary(fit)$r.squared\n\n[1] 0.8595365"
  },
  {
    "objectID": "posts/risk-r/index.html#standard-errors",
    "href": "posts/risk-r/index.html#standard-errors",
    "title": "Risk",
    "section": "Standard errors",
    "text": "Standard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\nlm_se &lt;- function(x, y, weights, intercept) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n        df_resid &lt;- n_rows - n_cols - 1\n        \n    } else {\n        df_resid &lt;- n_rows - n_cols\n    }\n    \n    var_y &lt;- crossprod(y, diag(weights)) %*% y\n    var_resid &lt;- as.vector((1 - rsq) * var_y / df_resid)\n    \n    result &lt;- sqrt(var_resid * diag(solve(crossprod(x, diag(weights)) %*% x)))\n    \n    return(result)\n}\n\n\nlm_se(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n  (Intercept)        xSP500   xDTWEXAFEGS        xDGS10 xBAMLH0A0HYM2 \n 5.062133e-05  1.827216e-02  3.459779e-02  1.762213e-01  1.521949e-01 \n\n\n\ncoef(summary(fit))[ , \"Std. Error\"]\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n             5.062133e-05              1.827216e-02              3.459779e-02 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             1.762213e-01              1.521949e-01"
  },
  {
    "objectID": "posts/risk-py/index.html",
    "href": "posts/risk-py/index.html",
    "title": "Risk",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\nimport os\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\nprices_df = pdr.get_data_tiingo(tickers, start = \"1900-01-01\", api_key = os.getenv(\"TIINGO_API_KEY\"))\n\nC:\\Users\\jason\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas_datareader\\tiingo.py:234: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  return pd.concat(dfs, self._concat_axis)\n\nprices_df = prices_df.pivot_table(index = \"date\", columns = \"symbol\", values = \"adjClose\") \\\n    .tz_localize(None)\nreturns_cols = list(zip([\"returns\"], tickers))\noverlap_cols = list(zip([\"overlap\"], tickers))\nreturns_df[returns_cols] = np.log(prices_df).diff()\nreturns_df[overlap_cols] = returns_df[returns_cols].rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df.sort_index(axis = 1, inplace = True)\n# weights = np.array([0.9 ** i for i in range(width - 1, -1, -1)]).reshape((width, 1))\nweights = np.array([1] * width).reshape((width, 1))\noverlap_df = returns_df.dropna()[\"overlap\"]\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors][-width:]\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers][-width:]"
  },
  {
    "objectID": "posts/risk-py/index.html#coefficients",
    "href": "posts/risk-py/index.html#coefficients",
    "title": "Risk",
    "section": "Coefficients",
    "text": "Coefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\ndef lm_coef(x, y, weights, intercept):\n    \n    if (intercept): x = sm.add_constant(x)\n        \n    result = np.dot(np.linalg.inv(np.dot(x.T, np.multiply(weights, x))),\n                    np.dot(x.T, np.multiply(weights, y)))\n    \n    return np.ravel(result)\n\n\nlm_coef(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([ 3.20013712e-06,  2.22039573e-01, -1.29651349e-01,  2.58862115e+00,\n        1.59982881e+00])\n\n\n\nif (intercept): overlap_x_df = sm.add_constant(overlap_x_df)\n    \nfit = sm.WLS(overlap_y_df, overlap_x_df, weights = weights).fit()\n\nif (intercept): overlap_x_df = overlap_x_df.iloc[:, 1:]\n\nnp.array(fit.params)\n\narray([ 3.20013712e-06,  2.22039573e-01, -1.29651349e-01,  2.58862115e+00,\n        1.59982881e+00])"
  },
  {
    "objectID": "posts/risk-py/index.html#r-squared",
    "href": "posts/risk-py/index.html#r-squared",
    "title": "Risk",
    "section": "R-squared",
    "text": "R-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\ndef lm_rsq(x, y, weights, intercept):\n            \n    coef = np.matrix(lm_coef(x, y, weights, intercept))\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n    result = np.dot(coef, np.dot(np.dot(x.T, np.multiply(weights, x)), coef.T)) / \\\n        np.dot(y.T, np.multiply(weights, y))\n    \n    return result.item()\n\n\nlm_rsq(overlap_x_df, overlap_y_df, weights, intercept)\n\n0.8510258994398467\n\n\n\nfit.rsquared\n\n0.8510258994398463"
  },
  {
    "objectID": "posts/risk-py/index.html#standard-errors",
    "href": "posts/risk-py/index.html#standard-errors",
    "title": "Risk",
    "section": "Standard errors",
    "text": "Standard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\ndef lm_se(x, y, weights, intercept):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n        df_resid = n_rows - n_cols - 1 \n        \n    else:\n        df_resid = n_rows - n_cols        \n    \n    var_y = np.dot(y.T, np.multiply(weights, y))\n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    result = np.sqrt(var_resid * np.linalg.inv(np.dot(x.T, np.multiply(weights, x))).diagonal())\n    \n    return np.ravel(result)\n\n\nlm_se(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([5.25910257e-05, 1.89831347e-02, 3.59440066e-02, 1.83078139e-01,\n       1.58116820e-01])\n\n\n\nnp.array(fit.bse)\n\narray([5.25910257e-05, 1.89831347e-02, 3.59440066e-02, 1.83078139e-01,\n       1.58116820e-01])"
  },
  {
    "objectID": "posts/risk-py/index.html#coefficients-1",
    "href": "posts/risk-py/index.html#coefficients-1",
    "title": "Risk",
    "section": "Coefficients",
    "text": "Coefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\ndef pcr_coef(x, y, comps):\n    \n    x = x - np.average(x, axis = 0)\n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    V = V[:, idx]\n    \n    W = np.dot(x, V)\n    gamma = np.dot(np.dot(np.linalg.inv(np.dot(W.T, W)), W.T), y)\n    \n    result = np.dot(V[:, :comps], gamma[:comps])\n    \n    return np.ravel(result)\n\n\nscale_x_df = (overlap_x_df - np.average(overlap_x_df, axis = 0)) \\\n    / np.std(overlap_x_df, axis = 0, ddof = 1)\n\n\npcr_coef(scale_x_df, overlap_y_df, comps)\n\narray([ 0.00081497, -0.00072018,  0.00056396,  0.00056363])\n\n\n\npcr_coef(overlap_x_df, overlap_y_df, comps)\n\narray([ 0.40829748, -0.09352349,  0.01108694,  0.0280529 ])\n\n\n\npca = PCA(n_components = len(factors))\npca_x_df = pca.fit_transform(scale_x_df)\n\nfit = LinearRegression(fit_intercept = False).fit(pca_x_df, overlap_y_df)\n\ngamma = fit.coef_\nnp.dot(pca.components_.T[:, :comps], gamma.T[:comps]).ravel()\n\narray([ 0.00081497, -0.00072018,  0.00056396,  0.00056363])"
  },
  {
    "objectID": "posts/risk-py/index.html#r-squared-1",
    "href": "posts/risk-py/index.html#r-squared-1",
    "title": "Risk",
    "section": "R-squared",
    "text": "R-squared\n\ndef pcr_rsq(x, y, comps):\n    \n    coef = np.matrix(pcr_coef(x, y, comps))\n    \n    x = x - np.average(x, axis = 0)\n    y = y - np.average(y, axis = 0)\n    \n    result = np.dot(np.dot(coef, np.dot(x.T, x)), coef.T) / np.dot(y.T, y)\n    \n    return result.item()\n\n\npcr_rsq(scale_x_df, overlap_y_df, comps)\n\n0.8119209856908801\n\n\n\npcr_rsq(overlap_x_df, overlap_y_df, comps)\n\n0.6770664200329685"
  },
  {
    "objectID": "posts/risk-py/index.html#standard-errors-1",
    "href": "posts/risk-py/index.html#standard-errors-1",
    "title": "Risk",
    "section": "Standard errors",
    "text": "Standard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\ndef pcr_se(x, y, comps):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = pcr_rsq(x, y, comps)\n    \n    y = y - np.average(y, axis = 0)\n    \n    df_resid = n_rows - n_cols - 1\n    \n    var_y = np.dot(y.T, y)   \n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    # uses statsmodels for illustrative purposes\n    pca = sm.multivariate.PCA(x, standardize = False, demean = True)\n    L = pca.eigenvals[:comps]\n    V = pca.eigenvecs.iloc[:, :comps]\n    \n    result = np.sqrt(var_resid * np.dot(V, np.dot(np.diag(1 / L), V.T)).diagonal())\n    \n    return np.ravel(result)\n\n\npcr_se(scale_x_df, overlap_y_df, comps)\n\narray([2.49576774e-05, 2.20547907e-05, 1.72709143e-05, 1.72606128e-05])\n\n\n\npcr_se(overlap_x_df, overlap_y_df, comps)\n\narray([0.01794193, 0.00410973, 0.0004872 , 0.00123274])"
  },
  {
    "objectID": "posts/risk-r/index.html#coefficients-1",
    "href": "posts/risk-r/index.html#coefficients-1",
    "title": "Risk",
    "section": "Coefficients",
    "text": "Coefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\npcr_coef &lt;- function(x, y, comps) {\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    LV &lt;- eigen(cov(x))\n    V &lt;- LV$vectors\n    \n    W &lt;- x %*% V\n    gamma &lt;- solve(crossprod(W)) %*% (crossprod(W, y))\n    \n    result &lt;- V[ , 1:comps] %*% as.matrix(gamma[1:comps])\n    \n    return(result)\n    \n}\n\n\nscale_x_xts &lt;- scale(overlap_x_xts)\n\n\nt(pcr_coef(scale_x_xts, overlap_y_xts, comps))\n\n             [,1]          [,2]         [,3]         [,4]\n[1,] 0.0008136817 -0.0007190404 0.0005630743 0.0005627385\n\n\n\nt(pcr_coef(overlap_x_xts, overlap_y_xts, comps))\n\n          [,1]        [,2]       [,3]       [,4]\n[1,] 0.4083556 -0.09353679 0.01108851 0.02805689\n\n\n\nfit &lt;- pcr(reformulate(termlabels = \".\", response = tickers), \n           data = merge(scale_x_xts, overlap_y_xts), ncomp = comps)\ncoef(fit)[ , , 1]\n\n        SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 0.0008136817 -0.0007190404  0.0005630743  0.0005627385"
  },
  {
    "objectID": "posts/risk-r/index.html#r-squared-1",
    "href": "posts/risk-r/index.html#r-squared-1",
    "title": "Risk",
    "section": "R-squared",
    "text": "R-squared\n\npcr_rsq &lt;- function(x, y, comps) {\n    \n    coef &lt;- pcr_coef(x, y, comps)\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    result &lt;- (t(coef) %*% crossprod(x) %*% coef) / crossprod(y)\n    \n    return(result)\n    \n}\n\n\npcr_rsq(scale_x_xts, overlap_y_xts, comps)\n\n          BAICX\nBAICX 0.8236699\n\n\n\npcr_rsq(overlap_x_xts, overlap_y_xts, comps)\n\n          BAICX\nBAICX 0.6892296\n\n\n\nR2(fit)$val[comps + 1]\n\n[1] 0.8236699"
  },
  {
    "objectID": "posts/risk-r/index.html#standard-errors-1",
    "href": "posts/risk-r/index.html#standard-errors-1",
    "title": "Risk",
    "section": "Standard errors",
    "text": "Standard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\npcr_se &lt;- function(x, y, comps) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- pcr_rsq(x, y, comps)\n    \n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    df_resid &lt;- n_rows - n_cols - 1\n    \n    var_y &lt;- crossprod(y)\n    var_resid &lt;- as.vector((1 - rsq) * var_y / df_resid)\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values[1:comps] * (n_rows - 1)\n    V &lt;- LV$vectors[ , 1:comps]\n    \n    result &lt;- sqrt(var_resid * diag(V %*% sweep(t(V), 1, 1 / L, \"*\")))\n    \n    return(result)\n    \n}\n\n\npcr_se(scale_x_xts, overlap_y_xts, comps)\n\n[1] 2.395481e-05 2.116857e-05 1.657692e-05 1.656703e-05\n\n\n\npcr_se(overlap_x_xts, overlap_y_xts, comps)\n\n[1] 0.0174472874 0.0039964272 0.0004737647 0.0011987511"
  },
  {
    "objectID": "posts/risk-py/index.html#standalone-risk",
    "href": "posts/risk-py/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\ndef cov_wt(x, weights, center):\n    \n    sum_w = sum(weights)\n    sumsq_w = sum(np.power(weights, 2))\n    \n    if (center):\n    \n        x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n    \n    result = np.dot(x.T, np.multiply(weights, x)) / (sum_w - sumsq_w / sum_w)\n    \n    return result\n\n\ndef lm_sar(x, y, weights, intercept):\n    \n    coef = lm_coef(x, y, weights, intercept)\n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept): x = sm.add_constant(x)\n    \n    # sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n    #                aweights = weights.reshape(-1))\n    sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n    sar = np.multiply(np.power(coef, 2).T, sigma[:-1, :-1].diagonal())\n    sar_eps = (1 - rsq) * sigma[-1, -1]\n    \n    result = np.sqrt(np.concatenate((np.matrix(sigma[-1, -1]),\n                                     np.matrix(sar),\n                                     np.matrix(sar_eps)), axis = 1))\n    \n    return np.ravel(result)\n\n\nlm_sar(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"]) * np.sqrt(scale[\"overlap\"])\n\narray([0.07418344, 0.        , 0.03224959, 0.00811679, 0.03220052,\n       0.02543059, 0.02802299])"
  },
  {
    "objectID": "posts/risk-py/index.html#risk-contribution",
    "href": "posts/risk-py/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttp://faculty.washington.edu/ezivot/research/factormodelrisklecture_handout.pdf\n\n\ndef lm_mcr(x, y, weights, intercept):\n    \n    coef = np.matrix(lm_coef(x, y, weights, intercept)).T\n    rsq = lm_rsq(x, y, weights, intercept)\n        \n    if (intercept): x = sm.add_constant(x)\n    \n#     sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n#                    aweights = weights.reshape(-1))\n    sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n    mcr = np.multiply(coef, np.dot(sigma[:-1, :-1], coef)) / np.sqrt(sigma[-1, -1])\n    mcr_eps = np.sqrt(sigma[-1, -1]) - sum(mcr)\n    \n    result = np.concatenate((np.sqrt(np.matrix(sigma[-1, -1])),\n                             np.matrix(mcr).T,\n                             np.matrix(mcr_eps)), axis = 1)\n    \n    return np.ravel(result)\n\n\nlm_mcr(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"]) * np.sqrt(scale[\"overlap\"])\n\narray([ 0.07418344, -0.        ,  0.02562571,  0.00448002,  0.01807079,\n        0.01542116,  0.01058576])"
  },
  {
    "objectID": "posts/risk-py/index.html#implied-shocks",
    "href": "posts/risk-py/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\ndef implied_shocks(shocks, x, z, weights):\n\n    beta = np.linalg.lstsq(np.multiply(weights, z), np.multiply(weights, x), rcond = None)[0]\n                     \n    result = np.dot(shocks, beta)\n    \n    return np.ravel(result)\n\n\nshocks = np.array([-0.1, 0.1])\noverlap_z_df = overlap_x_df.iloc[:, [0, 1]]\n\n\nimplied_shocks(shocks, overlap_x_df, overlap_z_df, weights)\n\narray([-0.1       ,  0.1       , -0.00947367, -0.00519414])"
  },
  {
    "objectID": "posts/risk-py/index.html#stress-pl",
    "href": "posts/risk-py/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\ndef pnl_stress(shocks, x, y, z, weights, intercept):\n    \n    coef = lm_coef(x, y, weights, intercept)\n    \n    if (intercept): x = sm.add_constant(x)\n    \n    result = np.multiply(coef.T, implied_shocks(shocks, x, z, weights))\n    \n    return np.ravel(result)\n\n\npnl_stress(shocks, overlap_x_df, overlap_y_df, overlap_z_df, weights, intercept)\n\narray([ 6.84171507e-05, -2.29922279e-02, -1.31485402e-02, -2.63403995e-02,\n       -8.20385289e-03])"
  },
  {
    "objectID": "posts/risk-py/index.html#ordinary-least-squares",
    "href": "posts/risk-py/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\n\nintercept = True\n\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\ndef lm_coef(x, y, weights, intercept):\n    \n    if (intercept): x = sm.add_constant(x)\n        \n    result = np.dot(np.linalg.inv(np.dot(x.T, np.multiply(weights, x))),\n                    np.dot(x.T, np.multiply(weights, y)))\n    \n    return np.ravel(result)\n\n\nlm_coef(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([-8.68210746e-06,  2.29922279e-01, -1.31485402e-01,  2.78037999e+00,\n        1.57944387e+00])\n\n\n\nif (intercept): overlap_x_df = sm.add_constant(overlap_x_df)\n    \nfit = sm.WLS(overlap_y_df, overlap_x_df, weights = weights).fit()\n\nif (intercept): overlap_x_df = overlap_x_df.iloc[:, 1:]\n\nnp.array(fit.params)\n\narray([-8.68210746e-06,  2.29922279e-01, -1.31485402e-01,  2.78037999e+00,\n        1.57944387e+00])\n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\ndef lm_rsq(x, y, weights, intercept):\n            \n    coef = np.matrix(lm_coef(x, y, weights, intercept))\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n    result = np.dot(coef, np.dot(np.dot(x.T, np.multiply(weights, x)), coef.T)) / \\\n        np.dot(y.T, np.multiply(weights, y))\n    \n    return result.item()\n\n\nlm_rsq(overlap_x_df, overlap_y_df, weights, intercept)\n\n0.8573029337838813\n\n\n\nfit.rsquared\n\n0.8573029337838821\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\ndef lm_se(x, y, weights, intercept):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n        df_resid = n_rows - n_cols - 1 \n        \n    else:\n        df_resid = n_rows - n_cols        \n    \n    var_y = np.dot(y.T, np.multiply(weights, y))\n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    result = np.sqrt(var_resid * np.linalg.inv(np.dot(x.T, np.multiply(weights, x))).diagonal())\n    \n    return np.ravel(result)\n\n\nlm_se(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([5.08975859e-05, 1.82318654e-02, 3.53527680e-02, 1.85526937e-01,\n       1.51099561e-01])\n\n\n\nnp.array(fit.bse)\n\narray([5.08975859e-05, 1.82318654e-02, 3.53527680e-02, 1.85526937e-01,\n       1.51099561e-01])"
  },
  {
    "objectID": "posts/risk-py/index.html#principal-component-regression",
    "href": "posts/risk-py/index.html#principal-component-regression",
    "title": "Risk",
    "section": "Principal component regression",
    "text": "Principal component regression\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\n\n\ncomps = 1\n\n\nCoefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\ndef pcr_coef(x, y, comps):\n    \n    x = x - np.average(x, axis = 0)\n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    V = V[:, idx]\n    \n    W = np.dot(x, V)\n    gamma = np.dot(np.dot(np.linalg.inv(np.dot(W.T, W)), W.T), y)\n    \n    result = np.dot(V[:, :comps], gamma[:comps])\n    \n    return np.ravel(result)\n\n\nscale_x_df = (overlap_x_df - np.average(overlap_x_df, axis = 0)) \\\n    / np.std(overlap_x_df, axis = 0, ddof = 1)\n\n\npcr_coef(scale_x_df, overlap_y_df, comps)\n\narray([ 0.00081856, -0.00070273,  0.00051127,  0.0006251 ])\n\n\n\npcr_coef(overlap_x_df, overlap_y_df, comps)\n\narray([ 0.41292351, -0.08322167,  0.00889869,  0.03026532])\n\n\n\npca = PCA(n_components = len(factors))\npca_x_df = pca.fit_transform(scale_x_df)\n\nfit = LinearRegression(fit_intercept = False).fit(pca_x_df, overlap_y_df)\n\ngamma = fit.coef_\nnp.dot(pca.components_.T[:, :comps], gamma.T[:comps]).ravel()\n\narray([ 0.00081856, -0.00070273,  0.00051127,  0.0006251 ])\n\n\n\n\nR-squared\n\ndef pcr_rsq(x, y, comps):\n    \n    coef = np.matrix(pcr_coef(x, y, comps))\n    \n    x = x - np.average(x, axis = 0)\n    y = y - np.average(y, axis = 0)\n    \n    result = np.dot(np.dot(coef, np.dot(x.T, x)), coef.T) / np.dot(y.T, y)\n    \n    return result.item()\n\n\npcr_rsq(scale_x_df, overlap_y_df, comps)\n\n0.8154916720699074\n\n\n\npcr_rsq(overlap_x_df, overlap_y_df, comps)\n\n0.6633662970906853\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\ndef pcr_se(x, y, comps):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = pcr_rsq(x, y, comps)\n    \n    y = y - np.average(y, axis = 0)\n    \n    df_resid = n_rows - n_cols - 1\n    \n    var_y = np.dot(y.T, y)   \n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    # uses statsmodels for illustrative purposes\n    pca = sm.multivariate.PCA(x, standardize = False, demean = True)\n    L = pca.eigenvals[:comps]\n    V = pca.eigenvecs.iloc[:, :comps]\n    \n    result = np.sqrt(var_resid * np.dot(V, np.dot(np.diag(1 / L), V.T)).diagonal())\n    \n    return np.ravel(result)\n\n\npcr_se(scale_x_df, overlap_y_df, comps)\n\narray([2.47742572e-05, 2.12684354e-05, 1.54739644e-05, 1.89188976e-05])\n\n\n\npcr_se(overlap_x_df, overlap_y_df, comps)\n\narray([0.01871644, 0.00377216, 0.00040335, 0.00137183])"
  },
  {
    "objectID": "posts/risk-py/index.html#partial-least-squares",
    "href": "posts/risk-py/index.html#partial-least-squares",
    "title": "Risk",
    "section": "Partial least squares",
    "text": "Partial least squares"
  },
  {
    "objectID": "posts/risk-r/index.html#ordinary-least-squares",
    "href": "posts/risk-r/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\n\nintercept &lt;- TRUE\n\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\nlm_coef &lt;- function(x, y, weights, intercept) {\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    result &lt;- solve(crossprod(x, diag(weights)) %*% x) %*% crossprod(x, diag(weights) %*% y)\n    \n    return(result)\n    \n}\n\n\nt(lm_coef(overlap_x_xts, overlap_y_xts, weights, intercept))\n\n        (Intercept)    xSP500 xDTWEXAFEGS  xDGS10 xBAMLH0A0HYM2\nBAICX -8.682107e-06 0.2299223  -0.1314854 2.78038      1.579444\n\n\n\nif (intercept) {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts, weights = weights)\n} else {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts - 1, weights = weights)\n}\n    \ncoef(fit)\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n            -8.682107e-06              2.299223e-01             -1.314854e-01 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             2.780380e+00              1.579444e+00 \n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\nlm_rsq &lt;- function(x, y, weights, intercept) {\n        \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        x &lt;- sweep(x, 2, apply(x, 2, weighted.mean, w = weights), \"-\")\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n    }\n    \n    result &lt;- (t(coef) %*% (crossprod(x, diag(weights)) %*% x) %*% coef) / (crossprod(y, diag(weights)) %*% y)\n    \n    return(result)\n    \n}\n\n\nlm_rsq(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n          BAICX\nBAICX 0.8573029\n\n\n\nsummary(fit)$r.squared\n\n[1] 0.8573029\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\nlm_se &lt;- function(x, y, weights, intercept) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n        df_resid &lt;- n_rows - n_cols - 1\n        \n    } else {\n        df_resid &lt;- n_rows - n_cols\n    }\n    \n    var_y &lt;- crossprod(y, diag(weights)) %*% y\n    var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n    \n    result &lt;- sqrt(var_resid * diag(solve(crossprod(x, diag(weights)) %*% x)))\n    \n    return(result)\n}\n\n\nlm_se(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n  (Intercept)        xSP500   xDTWEXAFEGS        xDGS10 xBAMLH0A0HYM2 \n 5.089759e-05  1.823187e-02  3.535277e-02  1.855269e-01  1.510996e-01 \n\n\n\ncoef(summary(fit))[ , \"Std. Error\"]\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n             5.089759e-05              1.823187e-02              3.535277e-02 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             1.855269e-01              1.510996e-01"
  },
  {
    "objectID": "posts/risk-r/index.html#principal-component-regression",
    "href": "posts/risk-r/index.html#principal-component-regression",
    "title": "Risk",
    "section": "Principal component regression",
    "text": "Principal component regression\n\nlibrary(pls)\n\n\ncomps &lt;- 1\n\n\nCoefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\npcr_coef &lt;- function(x, y, comps) {\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    LV &lt;- eigen(cov(x))\n    V &lt;- LV$vectors\n    \n    W &lt;- x %*% V\n    gamma &lt;- solve(crossprod(W)) %*% (crossprod(W, y))\n    \n    result &lt;- V[ , 1:comps] %*% as.matrix(gamma[1:comps])\n    \n    return(result)\n    \n}\n\n\nscale_x_xts &lt;- scale(overlap_x_xts)\n\n\nt(pcr_coef(scale_x_xts, overlap_y_xts, comps))\n\n             [,1]          [,2]         [,3]         [,4]\n[1,] 0.0008185607 -0.0007027256 0.0005112718 0.0006250951\n\n\n\nt(pcr_coef(overlap_x_xts, overlap_y_xts, comps))\n\n          [,1]        [,2]        [,3]       [,4]\n[1,] 0.4129235 -0.08322167 0.008898687 0.03026532\n\n\n\nfit &lt;- pcr(reformulate(termlabels = \".\", response = tickers), \n           data = merge(scale_x_xts, overlap_y_xts), ncomp = comps)\ncoef(fit)[ , , 1]\n\n        SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 0.0008185607 -0.0007027256  0.0005112718  0.0006250951 \n\n\n\n\nR-squared\n\npcr_rsq &lt;- function(x, y, comps) {\n    \n    coef &lt;- pcr_coef(x, y, comps)\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    result &lt;- (t(coef) %*% crossprod(x) %*% coef) / crossprod(y)\n    \n    return(result)\n    \n}\n\n\npcr_rsq(scale_x_xts, overlap_y_xts, comps)\n\n          BAICX\nBAICX 0.8154917\n\n\n\npcr_rsq(overlap_x_xts, overlap_y_xts, comps)\n\n          BAICX\nBAICX 0.6633663\n\n\n\nR2(fit)$val[comps + 1]\n\n[1] 0.8154917\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\npcr_se &lt;- function(x, y, comps) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- pcr_rsq(x, y, comps)\n    \n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    df_resid &lt;- n_rows - n_cols - 1\n    \n    var_y &lt;- crossprod(y)\n    var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values[1:comps] * (n_rows - 1)\n    V &lt;- LV$vectors[ , 1:comps]\n    \n    result &lt;- sqrt(var_resid * diag(V %*% sweep(t(V), 1, 1 / L, \"*\")))\n    \n    return(result)\n    \n}\n\n\npcr_se(scale_x_xts, overlap_y_xts, comps)\n\n[1] 2.477426e-05 2.126844e-05 1.547396e-05 1.891890e-05\n\n\n\npcr_se(overlap_x_xts, overlap_y_xts, comps)\n\n[1] 0.0187164392 0.0037721595 0.0004033477 0.0013718253"
  },
  {
    "objectID": "posts/risk-r/index.html#partial-least-squares",
    "href": "posts/risk-r/index.html#partial-least-squares",
    "title": "Risk",
    "section": "Partial least squares",
    "text": "Partial least squares"
  },
  {
    "objectID": "posts/risk-r/index.html#standalone-risk",
    "href": "posts/risk-r/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\nlm_sar &lt;- function(x, y, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n    sar &lt;- coef ^ 2 * diag(sigma[-ncol(sigma), -ncol(sigma)])\n    sar_eps &lt;- (1 - rsq) * sigma[ncol(sigma), ncol(sigma)]\n    \n    result &lt;- sqrt(c(sigma[ncol(sigma), ncol(sigma)],\n                     sar,\n                     sar_eps))\n    \n    return(result)\n    \n}\n\n\nlm_sar(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]]) * sqrt(scale[[\"overlap\"]])\n\n[1] 0.074183436 0.000000000 0.032249592 0.008116787 0.032200524 0.025430586\n[7] 0.028022990"
  },
  {
    "objectID": "posts/risk-r/index.html#risk-contribution",
    "href": "posts/risk-r/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttp://faculty.washington.edu/ezivot/research/factormodelrisklecture_handout.pdf\n\n\nlm_mcr &lt;- function(x, y, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n    mcr &lt;- coef * sigma[-ncol(sigma), -ncol(sigma)] %*% coef / sqrt(sigma[ncol(sigma), ncol(sigma)])\n    mcr_eps &lt;- sqrt(sigma[ncol(sigma), ncol(sigma)]) - sum(mcr)\n    \n    result &lt;- c(sqrt(sigma[ncol(sigma), ncol(sigma)]),\n                mcr,\n                mcr_eps)\n    \n    return(result)\n    \n}\n\n\nlm_mcr(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]]) * sqrt(scale[[\"overlap\"]])\n\n[1] 0.074183436 0.000000000 0.025625713 0.004480016 0.018070788 0.015421160\n[7] 0.010585759"
  },
  {
    "objectID": "posts/risk-r/index.html#implied-shocks",
    "href": "posts/risk-r/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\nimplied_shocks &lt;- function(shocks, x, z, weights) {\n    \n    beta &lt;- solve(crossprod(z, diag(weights) %*% z)) %*% crossprod(z, diag(weights) %*% x)\n    \n    result &lt;- shocks %*% beta\n    \n    return(result)\n    \n}\n\n\nshocks &lt;- c(-0.1, 0.1)\noverlap_z_xts &lt;- overlap_x_xts[ , 1:2]\n\n\nimplied_shocks(shocks, overlap_x_xts, overlap_z_xts, weights)\n\n     SP500 DTWEXAFEGS        DGS10 BAMLH0A0HYM2\n[1,]  -0.1        0.1 -0.009473669  -0.00519414"
  },
  {
    "objectID": "posts/risk-r/index.html#stress-pl",
    "href": "posts/risk-r/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\npnl_stress &lt;- function(shocks, x, y, z, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    result &lt;- t(coef) * implied_shocks(shocks, x, z, weights)\n    \n    return(result)    \n    \n}\n\n\npnl_stress(shocks, overlap_x_xts, overlap_y_xts, overlap_z_xts, weights, intercept)\n\n       (Intercept)      xSP500 xDTWEXAFEGS     xDGS10 xBAMLH0A0HYM2\nBAICX 6.841715e-05 -0.02299223 -0.01314854 -0.0263404  -0.008203853"
  },
  {
    "objectID": "posts/markets-r/index.html#interquartile-range",
    "href": "posts/markets-r/index.html#interquartile-range",
    "title": "Markets",
    "section": "Interquartile range",
    "text": "Interquartile range\nOutliers are defined the regression residuals that fall below \\(Q_{1}−1.5\\times IQR\\) or above \\(Q_{3}+1.5\\times IQR\\):\n\nhttps://stats.stackexchange.com/a/1153\nhttps://stats.stackexchange.com/a/108951\nhttps://robjhyndman.com/hyndsight/tsoutliers/\n\n\noutliers &lt;- function(z) {\n  \n  n_cols &lt;- ncol(z)\n  result_ls &lt;- list()\n  \n  for (j in 1:n_cols) {\n    \n    y &lt;- z[ , j]\n    \n    if (n_cols == 1) {\n      x &lt;- 1:length(y)\n    } else {\n      x &lt;- cbind(1:length(y), z[ , -j])\n    }\n    \n    coef &lt;- coef(lm(y ~ x))\n    predict &lt;- coef[1] + x %*% as.matrix(coef[-1])\n    resid &lt;- y - predict\n    \n    lower &lt;- quantile(resid, prob = 0.25)\n    upper &lt;- quantile(resid, prob = 0.75)\n    iqr &lt;- upper - lower\n    \n    total &lt;- y[(resid &lt; lower - 1.5 * iqr) | (resid &gt; upper + 1.5 * iqr)]\n    \n    result_ls &lt;- append(result_ls, list(total))\n    \n  }\n  \n  result &lt;- do.call(merge, result_ls)\n  \n  return(result)\n  \n}\n\n\noutliers_xts &lt;- outliers(score_xts)"
  },
  {
    "objectID": "posts/markets-r/index.html#contour-ellipsoid",
    "href": "posts/markets-r/index.html#contour-ellipsoid",
    "title": "Markets",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid"
  },
  {
    "objectID": "posts/markets-py/index.html#interquartile-range",
    "href": "posts/markets-py/index.html#interquartile-range",
    "title": "Markets",
    "section": "Interquartile range",
    "text": "Interquartile range\nOutliers are defined the regression residuals that fall below \\(Q_{1}−1.5\\times IQR\\) or above \\(Q_{3}+1.5\\times IQR\\):\n\nhttps://stats.stackexchange.com/a/1153\nhttps://stats.stackexchange.com/a/108951\nhttps://robjhyndman.com/hyndsight/tsoutliers/\n\n\ndef outliers(z):\n  \n    n_cols = z.shape[1]\n    result_ls = []\n\n    for j in range(n_cols):\n      \n        y = z.iloc[:, j]\n\n        if (n_cols == 0):\n            x = sm.add_constant(range(len(y)))\n        else:\n            x = sm.add_constant(z.drop(z.columns[j], axis = 1))\n\n        coef = sm.WLS(y, x).fit().params\n        predict = coef[0] + np.dot(x.iloc[:, 1:], coef[1:])\n        resid = y - predict\n\n        lower = resid.quantile(0.25)\n        upper = resid.quantile(0.75)\n        iqr = upper - lower\n\n        total = y[(resid &lt; lower - 1.5 * iqr) | (resid &gt; upper + 1.5 * iqr)]\n        \n        total = pd.DataFrame({\"date\": total.index, \"symbol\": total.name, \"values\": total})\n        result_ls.append(total)\n\n    result = pd.concat(result_ls, ignore_index = True)\n    result = result.pivot_table(index = \"date\", columns = \"symbol\", values = \"values\")\n\n    return result\n\n\noutliers_df = outliers(score_df)"
  },
  {
    "objectID": "posts/markets-py/index.html#contour-ellipsoid",
    "href": "posts/markets-py/index.html#contour-ellipsoid",
    "title": "Markets",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid"
  },
  {
    "objectID": "posts/eigen-py/index.html",
    "href": "posts/eigen-py/index.html",
    "title": "Eigen",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\n\n\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\n\n\nhttps://pandas-datareader.readthedocs.io/en/latest/remote_data.html\n\n\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\n\n\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\n\n\noverlap_df = returns_df.dropna()[\"overlap\"]\n\n\nDecomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\ndef eigen_decomp(x, comps):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    V = V[:, idx]\n        \n    L = L[:comps]\n    V = V[:, :comps]\n    \n    result = np.dot(V, np.multiply(L, V.T))\n    \n    return result\n\n\ncomps = 1\n\n\neigen_decomp(overlap_df, comps) * scale[\"periods\"] * scale[\"overlap\"]\n\narray([[ 3.00046333e-02, -3.63886238e-03, -1.59380727e-04,\n         2.56951663e-03],\n       [-3.63886238e-03,  4.41309156e-04,  1.93291658e-05,\n        -3.11622452e-04],\n       [-1.59380727e-04,  1.93291658e-05,  8.46609789e-07,\n        -1.36489397e-05],\n       [ 2.56951663e-03, -3.11622452e-04, -1.36489397e-05,\n         2.20046539e-04]])\n\n\n\n# np.cov(overlap_df.T) * scale[\"periods\"] * scale[\"overlap\"]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\ndef variance_explained(x):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))   \n    idx = L.argsort()[::-1]\n    L = L[idx]\n    \n    result = L.cumsum() / L.sum()\n    \n    return result\n\n\nvariance_explained(overlap_df)\n\narray([0.87321651, 0.99219698, 0.99827443, 1.        ])\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\n\\end{aligned}\n\\]\n\ndef eigen(x):\n  \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    V = V[:, idx]\n    \n    result = {\n        \"values\": L,\n        \"vectors\": V\n    }\n    \n    return result\n\n\ndef roll_eigen1(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evec = eigen(x.iloc[idx])[\"vectors\"][:, comp - 1]\n        result_ls.append(evec)\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df  \n\n\ncomp = 1\n\n\nraw_df = roll_eigen1(overlap_df, width, comp)\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\ndef roll_eigen2(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evecs = eigen(x.iloc[idx])[\"vectors\"]\n        \n        if i &gt; width - 1:\n            \n            similarity = np.dot(evecs.T, result_ls[-1])\n            order = np.argmax(np.abs(similarity))\n            evec = np.multiply(np.sign(similarity[order]), evecs[:, order])\n            \n            result_ls.append(evec)\n            \n        else:\n            result_ls.append(evecs[:, comp - 1])\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df\n\n\nclean_df = roll_eigen2(overlap_df, width, comp)"
  },
  {
    "objectID": "posts/eigen-py/index.html#eigendecomposition",
    "href": "posts/eigen-py/index.html#eigendecomposition",
    "title": "Eigen",
    "section": "Eigendecomposition",
    "text": "Eigendecomposition\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\ndef eigen_decomp(x, comps):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    V = V[:, idx]\n        \n    L = L[:comps]\n    V = V[:, :comps]\n    \n    result = np.matmul(V, np.multiply(L, V.T))\n    \n    return result\n\n\ncomps = 1\n\n\neigen_decomp(overlap_x_mat, comps) * scale[\"periods\"] * scale[\"overlap\"]\n\narray([[ 1.94794750e-02, -3.81257865e-03,  3.95746280e-04,\n         1.41506503e-03],\n       [-3.81257865e-03,  7.46208817e-04, -7.74565956e-05,\n        -2.76960581e-04],\n       [ 3.95746280e-04, -7.74565956e-05,  8.04000712e-06,\n         2.87485530e-05],\n       [ 1.41506503e-03, -2.76960581e-04,  2.87485530e-05,\n         1.02795842e-04]])\n\n\n\n# np.cov(overlap_x_mat.T) * scale[\"periods\"] * scale[\"overlap\"]"
  },
  {
    "objectID": "posts/eigen-py/index.html#variance-explained",
    "href": "posts/eigen-py/index.html#variance-explained",
    "title": "Eigen",
    "section": "Variance explained",
    "text": "Variance explained\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\ndef variance_explained(x):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))   \n    idx = L.argsort()[::-1]\n    L = L[idx]\n    \n    result = L.cumsum() / L.sum()\n    \n    return result\n\n\nvariance_explained(overlap_x_mat)\n\narray([0.85663169, 0.98971785, 0.99658121, 1.        ])"
  },
  {
    "objectID": "posts/eigen-py/index.html#cosine-similarity",
    "href": "posts/eigen-py/index.html#cosine-similarity",
    "title": "Eigen",
    "section": "Cosine similarity",
    "text": "Cosine similarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\n\\end{aligned}\n\\]\n\ndef eigen_vals(x):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    \n    return pd.DataFrame(L)\n\ndef eigen_vecs(x):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    V = V[:, idx]\n    \n    return pd.DataFrame(V)\n\n\ndef roll_eigen1(x, width, comp):\n    \n    n_rows = len(x)\n    result = pd.DataFrame()\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evec = eigen_vecs(x.iloc[idx]).iloc[:, comp - 1]\n        result = result.append(evec.transpose())\n    \n    result.index = x.index[(width - 1):]\n    result.columns = x.columns\n    \n    return result  \n\n\ncomp = 1\n\n\nraw_df = roll_eigen1(overlap_x_df, width, comp)\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\n\n\ndef roll_eigen2(x, width, comp):\n    \n    n_rows = len(x)\n    result = pd.DataFrame()\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evec = eigen_vecs(x.iloc[idx]).iloc[:, comp - 1]\n        \n        if i &gt; width - 1:\n            \n            similarity = np.matmul(np.matrix(evec),\n                                   np.matrix(result.iloc[-1, :]).T)\n            evec = pd.DataFrame(np.multiply(np.sign(similarity), np.matrix(evec))) \n            result = result.append(evec)\n            \n        else:\n            result = result.append(evec.transpose())\n    \n    result.index = x.index[(width - 1):]\n    result.columns = x.columns\n    \n    return result  \n\n\nclean_df = roll_eigen2(overlap_x_df, width, comp)"
  },
  {
    "objectID": "posts/eigen-r/index.html",
    "href": "posts/eigen-r/index.html",
    "title": "Eigen",
    "section": "",
    "text": "library(quantmod)\nlibrary(roll)\nlibrary(data.table)\n\n\nfactors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\nfactors &lt;- c(factors_r, factors_d)\nwidth &lt;- 252\nscale &lt;- list(\"periods\" = 252, \"overlap\" = 5)\n\n\ngetSymbols(factors, src = \"FRED\")\nlevels_xts &lt;- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))\n\n\nreturns_xts &lt;- do.call(merge, lapply(factors, function(i) {\n    if (i %in% factors_r) {\n        diff(log((levels_xts[ , i])))\n    } else if (i %in% factors_d) {\n        -diff(levels_xts[ , i]) / 100\n    }    \n}))\noverlap_xts &lt;- roll_mean(returns_xts, scale[[\"overlap\"]], min_obs = 1, na_restore = TRUE)\n\n\noverlap_xts &lt;- na.omit(overlap_xts)\n\n\nDecomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\neigen_decomp &lt;- function(x, comps) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values[1:comps]\n    V &lt;- LV$vectors[ , 1:comps]\n    \n    result &lt;- V %*% sweep(t(V), 1, L, \"*\")\n    \n    return(result)\n    \n}\n\n\ncomps &lt;- 1\n\n\neigen_decomp(overlap_xts, comps) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  0.0300046333 -3.638862e-03 -1.593807e-04  2.569517e-03\n[2,] -0.0036388624  4.413092e-04  1.932917e-05 -3.116225e-04\n[3,] -0.0001593807  1.932917e-05  8.466098e-07 -1.364894e-05\n[4,]  0.0025695166 -3.116225e-04 -1.364894e-05  2.200465e-04\n\n\n\n# cov(overlap_xts) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\nvariance_explained &lt;- function(x) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values\n    \n    result &lt;- cumsum(L) / sum(L)\n    \n    return(result)\n    \n}\n\n\nvariance_explained(overlap_xts)\n\n[1] 0.8732165 0.9921970 0.9982744 1.0000000\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\n\\end{aligned}\n\\]\n\nroll_eigen1 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        evec &lt;- eigen(cov(x[idx, ]))[[\"vectors\"]][ , comp]\n        result_ls &lt;- append(result_ls, list(evec))\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\ncomp &lt;- 1\n\n\nraw_df &lt;- roll_eigen1(overlap_xts, width, comp)\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jjf234/rolleigen\") # roll (&gt;= 1.1.7)\n# library(rolleigen)\n# raw_df &lt;- roll_eigen(overlap_xts, width)[[\"vectors\"]][ , comp, ]\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\nroll_eigen2 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        evecs &lt;- eigen(cov(x[idx, ]))[[\"vectors\"]]\n                \n        if (i &gt; width) {\n          \n            similarity &lt;- crossprod(evecs, result_ls[[length(result_ls)]])\n            order &lt;- which.max(abs(similarity))\n            evec &lt;- sign(similarity)[order] * evecs[ , order]\n            \n            result_ls &lt;- append(result_ls, list(evec))\n            \n        } else {\n            result_ls &lt;- append(result_ls, list(evecs[ , comp]))\n        }\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\nclean_df &lt;- roll_eigen2(overlap_xts, width, comp)"
  },
  {
    "objectID": "posts/eigen-r/index.html#eigendecomposition",
    "href": "posts/eigen-r/index.html#eigendecomposition",
    "title": "Eigen",
    "section": "Eigendecomposition",
    "text": "Eigendecomposition\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\neigen_decomp &lt;- function(x, comps) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values[1:comps]\n    V &lt;- LV$vectors[ , 1:comps]\n    \n    result &lt;- V %*% sweep(t(V), 1, L, \"*\")\n    \n    return(result)\n    \n}\n\n\ncomps &lt;- 1\n\n\neigen_decomp(overlap_x_xts, comps) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  0.0194794750 -0.0038125787  3.957463e-04  1.415065e-03\n[2,] -0.0038125787  0.0007462088 -7.745660e-05 -2.769606e-04\n[3,]  0.0003957463 -0.0000774566  8.040007e-06  2.874855e-05\n[4,]  0.0014150650 -0.0002769606  2.874855e-05  1.027958e-04\n\n\n\n# cov(overlap_x_xts) * scale[[\"periods\"]] * scale[[\"overlap\"]]"
  },
  {
    "objectID": "posts/eigen-r/index.html#variance-explained",
    "href": "posts/eigen-r/index.html#variance-explained",
    "title": "Eigen",
    "section": "Variance explained",
    "text": "Variance explained\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\nvariance_explained &lt;- function(x) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values\n    \n    result &lt;- cumsum(L) / sum(L)\n    \n    return(result)\n    \n}\n\n\nvariance_explained(overlap_x_xts)\n\n[1] 0.8566317 0.9897178 0.9965812 1.0000000"
  },
  {
    "objectID": "posts/eigen-r/index.html#cosine-similarity",
    "href": "posts/eigen-r/index.html#cosine-similarity",
    "title": "Eigen",
    "section": "Cosine similarity",
    "text": "Cosine similarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\n\\end{aligned}\n\\]\n\neigen_vals &lt;- function(x) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values\n    \n    return(L)    \n}\n\neigen_vecs &lt;- function(x) {\n    \n    LV &lt;- eigen(cov(x))\n    V &lt;- LV$vectors\n    \n    return(V)    \n}\n\n\nroll_eigen1 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        evec &lt;- eigen_vecs(x[idx, ])[ , comp]\n        result_ls &lt;- append(result_ls, list(evec))\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\ncomp &lt;- 1\n\n\nraw_df &lt;- roll_eigen1(overlap_x_df, width, comp)\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\n\n\nroll_eigen2 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        evec &lt;- eigen_vecs(x[idx, ])[ , comp]\n                \n        if (i &gt; width) {\n            \n            similarity &lt;- evec %*% result_ls[[length(result_ls)]]\n            evec &lt;- as.vector(sign(similarity)) * evec\n            result_ls &lt;- append(result_ls, list(evec))\n            \n        } else {\n            result_ls &lt;- append(result_ls, list(evec))\n        }\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\nclean_df &lt;- roll_eigen2(overlap_x_df, width, comp)"
  },
  {
    "objectID": "posts/eigen-py/index.html#variance",
    "href": "posts/eigen-py/index.html#variance",
    "title": "Eigen",
    "section": "Variance",
    "text": "Variance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\ndef variance_explained(x):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))   \n    idx = L.argsort()[::-1]\n    L = L[idx]\n    \n    result = L.cumsum() / L.sum()\n    \n    return result\n\n\nvariance_explained(overlap_x_mat)\n\narray([0.8568637 , 0.98960048, 0.99654069, 1.        ])"
  },
  {
    "objectID": "posts/eigen-py/index.html#similarity",
    "href": "posts/eigen-py/index.html#similarity",
    "title": "Eigen",
    "section": "Similarity",
    "text": "Similarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\n\\end{aligned}\n\\]\n\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\ndef eigen(x):\n  \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    V = V[:, idx]\n    \n    result = {\n        \"values\": L,\n        \"vectors\": V\n    }\n    \n    return result\n\n\ndef roll_eigen1(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evec = eigen(x.iloc[idx])[\"vectors\"][:, comp - 1]\n        result_ls.append(evec)\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df  \n\n\ncomp = 1\n\n\nraw_df = roll_eigen1(overlap_x_df, width, comp)\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\n\n\ndef roll_eigen2(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evec = eigen(x.iloc[idx])[\"vectors\"][:, comp - 1]\n        \n        if i &gt; width - 1:\n            \n            similarity = np.dot(evec, result_ls[-1].T)\n            evec = np.multiply(np.sign(similarity), evec)\n            result_ls.append(evec)\n            \n        else:\n            result_ls.append(evec)\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df\n\n\nclean_df = roll_eigen2(overlap_x_df, width, comp)"
  },
  {
    "objectID": "posts/optim-r/index.html",
    "href": "posts/optim-r/index.html",
    "title": "Optimization",
    "section": "",
    "text": "library(quantmod)\nlibrary(roll)\nlibrary(data.table)\nfactors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\nfactors &lt;- c(factors_r, factors_d)\nwidth &lt;- 252\nscale &lt;- list(\"periods\" = 252, \"overlap\" = 5)\ngetSymbols(factors, src = \"FRED\")\nlevels_xts &lt;- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))\nreturns_xts &lt;- do.call(merge, lapply(factors, function(i) {\n    if (i %in% factors_r) {\n        diff(log((levels_xts[ , i])))\n    } else if (i %in% factors_d) {\n        -diff(levels_xts[ , i]) / 100\n    }    \n}))\noverlap_xts &lt;- roll_mean(returns_xts, scale[[\"overlap\"]], min_obs = 1, na_restore = TRUE)\nlibrary(CVXR)\ntickers &lt;- \"BAICX\" # fund inception date is \"2011-11-28\" \ninvisible(getSymbols(tickers, src = \"tiingo\", api.key = Sys.getenv(\"TIINGO_API_KEY\"), adjust = TRUE))\nprices_xts &lt;- do.call(merge, c(lapply(tickers, function(i) Cl(get(i))), all = TRUE))\ncolnames(prices_xts) &lt;- tickers\nindex(prices_xts) &lt;- as.Date(index(prices_xts))\nreturns_xts &lt;- merge(returns_xts, diff(log(prices_xts)))\noverlap_xts &lt;- merge(overlap_xts, roll_mean(returns_xts[ , tickers], scale[[\"overlap\"]], min_obs = 1))\n# weights &lt;- 0.9 ^ ((width - 1):0)\nweights &lt;- rep(1, width)\noverlap_xts &lt;- na.omit(overlap_xts)\noverlap_x_xts &lt;- tail(overlap_xts[ , factors], width) # same dimension as `weights`\noverlap_y_xts &lt;- tail(overlap_xts[ , tickers], width)"
  },
  {
    "objectID": "posts/optim-r/index.html#random-turnover",
    "href": "posts/optim-r/index.html#random-turnover",
    "title": "Optimization",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\nrand_iterative &lt;- function(n_assets, lower, upper, target) {\n    \n    plug &lt;- FALSE\n    \n    while (!plug) {\n        \n        result &lt;- as.matrix(runif(n_assets - 1, min = lower, max = upper))\n        temp &lt;- target - sum(result)\n        \n        if ((temp &lt;= upper) && (temp &gt;= lower)) {\n            plug &lt;- TRUE            \n        }\n        \n    }\n    \n    result &lt;- append(result, temp)\n    \n    return(result)\n    \n}\n\n\nrand_turnover1 &lt;- function(n_sim, n_assets, lower, upper, target) {\n    \n    rng &lt;- upper - lower\n    \n    result &lt;- rand_weights3(n_sim, n_assets) * rng\n    result &lt;- result - rng / n_assets\n    \n    return(result)\n    \n}\n\n\nrand_turnover2 &lt;- function(n_sim, n_assets, lower, upper, target) {\n    \n    result &lt;- matrix(rand_iterative(n_assets, lower, upper, target), nrow = 1, ncol = n_assets)\n    \n    while (nrow(result) &lt; n_sim) {\n        \n        temp &lt;- matrix(rand_iterative(n_assets, lower, upper, target), nrow = 1, ncol = n_assets)\n        result &lt;- rbind(result, temp)\n        \n    }\n    \n    return(result)\n    \n}\n\n\nlower &lt;- -0.05\nupper &lt;- 0.05\ntarget &lt;- 0\n\n\napproach1 &lt;- rand_turnover1(n_sim, n_assets, lower, upper, target)\napproach2 &lt;- rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-mean",
    "href": "posts/optim-r/index.html#maximize-mean",
    "title": "Optimization",
    "section": "Maximize mean",
    "text": "Maximize mean\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\n\ntarget &lt;- 0.06\n\n\nhttps://palomar.home.ece.ust.hk/MAFS6010R_lectures/slides_robust_portfolio.html\n\n\nmax_pnl_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    cons &lt;- list(params &gt;= 0, sum(params) == 1,\n                 quad_form(params, sigma) &lt;= target ^ 2)\n    \n    obj &lt;- Maximize(t(params) %*% mu)\n        \n    result &lt;- solve(Problem(obj, cons))$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams1 &lt;- max_pnl_optim(mu, sigma, target)\nparams1\n\n             [,1]\n[1,] 4.587378e-01\n[2,] 5.412619e-01\n[3,] 1.406586e-07\n[4,] 8.392967e-08\n\n\n\nmu %*% params1\n\n           [,1]\n[1,] 0.04181331\n\n\n\nsqrt(t(params1) %*% sigma %*% params1)\n\n     [,1]\n[1,] 0.06"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-variance",
    "href": "posts/optim-r/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min_{x}&\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\n\ntarget &lt;- 0.03\n\n\nmin_risk_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    cons &lt;- list(params &gt;= 0, sum(params) == 1,\n                 sum(mu * params) &gt;= target)\n    \n    obj &lt;- Minimize(quad_form(params, sigma))\n        \n    result &lt;- solve(Problem(obj, cons))$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams2 &lt;- min_risk_optim(mu, sigma, target)\nparams2\n\n             [,1]\n[1,] 3.084858e-01\n[2,] 5.005733e-01\n[3,] 1.909410e-01\n[4,] 2.115583e-21\n\n\n\nmu %*% params2\n\n     [,1]\n[1,] 0.03\n\n\n\nsqrt(t(params2) %*% sigma %*% params2)\n\n           [,1]\n[1,] 0.04214223"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-utility",
    "href": "posts/optim-r/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}-\\frac{1}{2}\\delta(\\mathbf{w}^T\\Sigma\\mathbf{w})\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\n\nir &lt;- 0.5\ntarget &lt;- ir / 0.06 # ir / std (see Black-Litterman)\n\n\nmax_ratio_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    cons &lt;- list(params &gt;= 0, sum(params) == 1)\n    \n    obj &lt;- Maximize(t(mu) %*% params - 0.5 * target * quad_form(params, sigma))\n        \n    result &lt;- solve(Problem(obj, cons))$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams3 &lt;- max_ratio_optim(mu, sigma, target)\nparams3\n\n             [,1]\n[1,] 4.784496e-01\n[2,] 5.215504e-01\n[3,] 1.816669e-22\n[4,] 1.222268e-22\n\n\n\nmu %*% params3\n\n           [,1]\n[1,] 0.04300942\n\n\n\nsqrt(t(params3) %*% sigma %*% params3)\n\n          [,1]\n[1,] 0.0622523"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\ndata.frame(\"max_pnl\" = params1,\n           \"min_risk\" = params2,\n           \"max_ratio\" = params3)\n\n       max_pnl     min_risk    max_ratio\n1 4.587378e-01 3.084858e-01 4.784496e-01\n2 5.412619e-01 5.005733e-01 5.215504e-01\n3 1.406586e-07 1.909410e-01 1.816669e-22\n4 8.392967e-08 2.115583e-21 1.222268e-22"
  },
  {
    "objectID": "posts/optim-py/index.html",
    "href": "posts/optim-py/index.html",
    "title": "Optimization",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\nimport os\nfrom scipy.optimize import minimize\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\nprices_df = pdr.get_data_tiingo(tickers, start = \"1900-01-01\", api_key = os.getenv(\"TIINGO_API_KEY\"))\n\nC:\\Users\\jason\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas_datareader\\tiingo.py:234: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  return pd.concat(dfs, self._concat_axis)\n\nprices_df = prices_df.pivot_table(index = \"date\", columns = \"symbol\", values = \"adjClose\") \\\n    .tz_localize(None)\nreturns_cols = list(zip([\"returns\"], tickers))\noverlap_cols = list(zip([\"overlap\"], tickers))\nreturns_df[returns_cols] = np.log(prices_df).diff()\nreturns_df[overlap_cols] = returns_df[returns_cols].rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df.sort_index(axis = 1, inplace = True)\n# weights = np.array([0.9 ** i for i in range(width - 1, -1, -1)]).reshape((width, 1))\nweights = np.array([1] * width).reshape((width, 1))\noverlap_df = returns_df.dropna()[\"overlap\"]\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors][-width:] # same dimension as `weights`\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers][-width:]"
  },
  {
    "objectID": "posts/optim-py/index.html#random-turnover",
    "href": "posts/optim-py/index.html#random-turnover",
    "title": "Optimization",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    plug = False\n    \n    while not plug:\n        \n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n        if ((temp &lt;= upper) and (temp &gt;= lower)):\n            plug = True\n        \n    result = np.append(result, temp)\n    \n    return result\n\n\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights3(n_sim, n_assets) * rng\n    result = result - rng / n_assets\n    \n    return result\n\n\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n    \n    result = np.matrix(rand_iterative(n_assets, lower, upper, target))\n    \n    while result.shape[0] &lt; n_sim:\n    \n        temp = np.matrix(rand_iterative(n_assets, lower, upper, target))\n        result = np.concatenate((result, temp), axis = 0)\n    \n    return result\n\n\nlower = -0.05\nupper = 0.05\ntarget = 0\n\n\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-mean",
    "href": "posts/optim-py/index.html#maximize-mean",
    "title": "Optimization",
    "section": "Maximize mean",
    "text": "Maximize mean\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\n\ntarget = 0.06\nstart = np.array([1] * len(factors))\nbnds = [(np.finfo(float).eps, 1) for i in range(len(factors))]\ncons = [{\"type\": \"ineq\", \"fun\": lambda params, sigma, target: max_pnl_cons(params, sigma, target),\n         \"args\": (sigma, target)},\n        {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n\n\ndef max_pnl_cons(params, sigma, target):\n    \n    var = np.matmul(np.transpose(params), np.matmul(sigma, params))\n    \n    result = target ** 2 - var\n    \n    return result\n\ndef max_pnl_obj(params, mu):\n    \n    result = np.matmul(mu, params)\n    \n    return -result\n\ndef max_pnl_optim(params, mu):\n    \n    result = minimize(max_pnl_obj, params, args = (mu), bounds = bnds, constraints = cons)\n    \n    return result.x\n\n\nparams1 = max_pnl_optim(start, mu)\nparams1\n\narray([4.58738017e-01, 5.41261983e-01, 2.22044605e-16, 2.32859522e-16])\n\n\n\nnp.matmul(mu, params1)\n\n0.04181332070485769\n\n\n\nnp.sqrt(np.matmul(np.transpose(params1), np.matmul(sigma, params1)))\n\n0.06000001956736139"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-variance",
    "href": "posts/optim-py/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min_{x}&\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\n\ntarget = 0.03\nstart = np.array([1] * len(factors))\ncons = [{\"type\": \"ineq\", \"fun\": lambda params, mu, target: min_risk_cons(params, mu, target),\n         \"args\": (mu, target)},\n        {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n\n\ndef min_risk_cons(params, mu, target):\n    \n    result = np.matmul(mu, params) - target\n    \n    return result\n\ndef min_risk_obj(params, sigma):\n    \n    result = np.matmul(np.transpose(params), np.matmul(sigma, params))\n    \n    return result\n\ndef min_risk_optim(params, sigma):\n    \n    result = minimize(min_risk_obj, params, args = (sigma), bounds = bnds, constraints = cons)\n    \n    return result.x\n\n\nparams2 = min_risk_optim(start, sigma)\nparams2\n\narray([0.30240864, 0.52942142, 0.12462501, 0.04354492])\n\n\n\nnp.matmul(mu, params2)\n\n0.030000000044080797\n\n\n\nnp.sqrt(np.matmul(np.transpose(params2), np.matmul(sigma, params2))) \n\n0.04257030014439174"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-utility",
    "href": "posts/optim-py/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}-\\frac{1}{2}\\delta(\\mathbf{w}^T\\Sigma\\mathbf{w})\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n\\]\n\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\nstart = np.array([1] * len(factors))\ncons = [{\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n\n\ndef max_ratio_obj(params, mu, sigma, target):\n    \n    result = np.matmul(mu, params) - 0.5 * target * (np.matmul(np.transpose(params),\n                                                               np.matmul(sigma, params)))\n#     result = np.matmul(mu, params) / np.sqrt(np.matmul(np.transpose(params), np.matmul(sigma, params)))\n    \n    return -result\n\ndef max_ratio_optim(params, mu, sigma, target):\n    \n    result = minimize(max_ratio_obj, params, args = (mu, sigma, target), bounds = bnds,\n                      constraints = cons) \n    \n    return result.x\n\n\nparams3 = max_ratio_optim(start, mu, sigma, target)\nparams3\n\narray([4.78449628e-01, 5.21550372e-01, 2.46005473e-16, 2.38090797e-16])\n\n\n\nnp.matmul(mu, params3)\n\n0.0430094200561693\n\n\n\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n\n0.06225230173496182"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\npd.DataFrame.from_dict({\"max_pnl\": params1,\n                        \"min_risk\": params2,\n                        \"max_ratio\": params3})\n\n        max_pnl  min_risk     max_ratio\n0  4.587380e-01  0.302409  4.784496e-01\n1  5.412620e-01  0.529421  5.215504e-01\n2  2.220446e-16  0.124625  2.460055e-16\n3  2.328595e-16  0.043545  2.380908e-16"
  }
]