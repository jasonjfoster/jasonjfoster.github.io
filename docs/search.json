[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jasonjfoster.github.io",
    "section": "",
    "text": "Risk\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 1, 2025\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nDec 1, 2025\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nalgorithms\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\nfinance\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\nfinance\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 23, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/optim-r/index.html",
    "href": "posts/optim-r/index.html",
    "title": "Optimization",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/optim-r/index.html#random-turnover",
    "href": "posts/optim-r/index.html#random-turnover",
    "title": "Optimization",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\nrand_turnover1 &lt;- function(n_sim, n_assets, lower, upper, target) {\n    \n  rng &lt;- upper - lower\n  \n  result &lt;- rand_weights2b(n_sim, n_assets) * rng\n  result &lt;- result - rng / n_assets\n  \n  return(result)\n    \n}\n\n\nlower &lt;- -0.05\nupper &lt;- 0.05\ntarget &lt;- 0\n\n\napproach1 &lt;- rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\n\n\n\n\nrand_iterative &lt;- function(n_assets, lower, upper, target) {\n  \n  result &lt;- runif(n_assets - 1, min = lower, max = upper)\n  temp &lt;- target - sum(result)\n  \n  while (!((temp &lt;= upper) && (temp &gt;= lower))) {\n      \n    result &lt;- runif(n_assets - 1, min = lower, max = upper)\n    temp &lt;- target - sum(result)\n      \n  }\n  \n  result &lt;- append(result, temp)\n  \n  return(result)\n  \n}\n\n\nrand_turnover2 &lt;- function(n_sim, n_assets, lower, upper, target) {\n  \n  result_ls &lt;- list()\n  \n  for (i in 1:n_sim) {\n      \n    result_sim &lt;- rand_iterative(n_assets, lower, upper, target)\n    result_ls &lt;- append(result_ls, list(result_sim))\n      \n  }\n  \n  result &lt;- do.call(rbind, result_ls)\n  \n  return(result)\n  \n}\n\n\napproach2 &lt;- rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-means",
    "href": "posts/optim-r/index.html#maximize-means",
    "title": "Optimization",
    "section": "Maximize means",
    "text": "Maximize means\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmax_mean_optim &lt;- function(mu, sigma, target) {\n  \n  params &lt;- CVXR::Variable(length(mu))\n  \n  obj &lt;- CVXR::Maximize(t(params) %*% mu)\n  \n  cons &lt;- list(sum(params) == 1, params &gt;= 0,\n               CVXR::quad_form(params, sigma) &lt;= target ^ 2)\n  \n  prob &lt;- CVXR::Problem(obj, cons)\n  \n  result &lt;- CVXR::solve(prob)$getValue(params)\n  \n  return(result)\n\n}\n\n\ntarget &lt;- 0.06\n\n\nparams1 &lt;- t(max_mean_optim(mu, sigma, target))\nparams1\n\n          [,1]      [,2]      [,3]         [,4]\n[1,] 0.3473083 0.1198965 0.5327951 3.014309e-08\n\n\n\nparams1 %*% mu\n\n           [,1]\n[1,] 0.03741358\n\n\n\nsqrt(params1 %*% sigma %*% t(params1))\n\n     [,1]\n[1,] 0.06\n\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\") # roll (&gt;= 1.1.7)\n# library(rolloptim)\n# \n# mu &lt;- roll_mean(returns_x_xts, 5)\n# sigma &lt;- roll_cov(returns_x_xts, width = 5)\n# \n# xx &lt;- roll_crossprod(returns_x_xts, returns_x_xts, 5)\n# xy &lt;- roll_crossprod(returns_x_xts, returns_y_xts, 5)\n# \n# roll_max_mean(mu)"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-variance",
    "href": "posts/optim-r/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmin_var_optim &lt;- function(mu, sigma, target) {\n  \n  params &lt;- CVXR::Variable(length(mu))\n  \n  obj &lt;- CVXR::Minimize(CVXR::quad_form(params, sigma))\n  \n  cons &lt;- list(sum(params) == 1, params &gt;= 0,\n               sum(mu * params) &gt;= target)\n  \n  prob &lt;- CVXR::Problem(obj, cons)\n  \n  result &lt;- CVXR::solve(prob)$getValue(params)\n  \n  return(result)\n\n}\n\n\ntarget &lt;- 0.03\n\n\nparams2 &lt;- t(min_var_optim(mu, sigma, target))\nparams2\n\n          [,1]      [,2]     [,3]          [,4]\n[1,] 0.2928362 0.1186198 0.588544 -1.232115e-20\n\n\n\nparams2 %*% mu\n\n     [,1]\n[1,] 0.03\n\n\n\nsqrt(params2 %*% sigma %*% t(params2))\n\n           [,1]\n[1,] 0.05042907\n\n\n\n# roll_min_var(sigma)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-utility",
    "href": "posts/optim-r/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmax_utility_optim &lt;- function(mu, sigma, target) {\n  \n  params &lt;- CVXR::Variable(length(mu))\n  \n  obj &lt;- CVXR::Minimize(0.5 * target * CVXR::quad_form(params, sigma) - t(mu) %*% params)\n  \n  cons &lt;- list(sum(params) == 1, params &gt;= 0)\n  \n  prob &lt;- CVXR::Problem(obj, cons)\n      \n  result &lt;- CVXR::solve(prob)$getValue(params)\n  \n  return(result)\n\n}\n\n\nir &lt;- 0.5\ntarget &lt;- ir / 0.06 # ir / std (see Black-Litterman)\n\n\nparams3 &lt;- t(max_utility_optim(mu, sigma, target))\nparams3\n\n         [,1]      [,2]      [,3]         [,4]\n[1,] 0.417415 0.1451334 0.4374516 4.188069e-24\n\n\n\nparams3 %*% mu \n\n           [,1]\n[1,] 0.04316819\n\n\n\nsqrt(params3 %*% sigma %*% t(params3))\n\n           [,1]\n[1,] 0.07224351\n\n\n\n# roll_max_utility(mu, sigma)"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nhttps://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html\n\n\nmin_rss_optim1 &lt;- function(mu, sigma) {\n    \n  params &lt;- CVXR::Variable(length(mu))\n  \n  obj &lt;- CVXR::Minimize(0.5 * CVXR::quad_form(params, sigma) - t(mu) %*% params)\n  \n  cons &lt;- list(sum(params) == 1, params &gt;= 0)\n  \n  prob &lt;- CVXR::Problem(obj, cons)\n      \n  result &lt;- CVXR::solve(prob)$getValue(params)\n  \n  return(result)\n\n}\n\n\nparams4 &lt;- t(min_rss_optim1(crossprod(overlap_x_xts, overlap_y_xts), crossprod(overlap_x_xts)))\nparams4\n\n          [,1]          [,2]      [,3]      [,4]\n[1,] 0.2777089 -2.105969e-23 0.1087466 0.6135446\n\n\n\nparams4 %*% mu \n\n           [,1]\n[1,] 0.03193048\n\n\n\nsqrt(params4 %*% sigma %*% t(params4))\n\n           [,1]\n[1,] 0.05917959\n\n\n\n# roll_min_rss(xx, xy)\n\n\nmin_rss_optim2 &lt;- function(x, y) {\n  \n  params &lt;- CVXR::Variable(ncol(x))\n  \n  obj &lt;- CVXR::Minimize(CVXR::sum_squares(y - x %*% params))\n  \n  cons &lt;- list(sum(params) == 1, params &gt;= 0)\n  \n  prob &lt;- CVXR::Problem(obj, cons)\n      \n  result &lt;- CVXR::solve(prob)$getValue(params)\n  \n  return(result)\n\n}\n\n\nparams5 &lt;- t(min_rss_optim2(zoo::coredata(overlap_x_xts), zoo::coredata(overlap_y_xts)))\nparams5\n\n          [,1]          [,2]      [,3]      [,4]\n[1,] 0.2777089 -8.978014e-21 0.1087466 0.6135446\n\n\n\nparams5 %*% mu \n\n           [,1]\n[1,] 0.03193048\n\n\n\nsqrt(params5 %*% sigma %*% t(params5))\n\n           [,1]\n[1,] 0.05917959\n\n\n\nround(data.frame(\n  \"max_pnl\" = t(params1) * 100,\n  \"min_risk\" = t(params2) * 100,\n  \"max_utility\" = t(params3) * 100,\n  \"min_rss1\" = t(params4) * 100,\n  \"min_rss2\" = t(params5) * 100),\n2)\n\n  max_pnl min_risk max_utility min_rss1 min_rss2\n1   34.76    29.28       41.74    27.77    27.77\n2   13.03    11.86       14.51     0.00     0.00\n3   52.22    58.85       43.75    10.87    10.87\n4    0.00     0.00        0.00    61.35    61.35"
  },
  {
    "objectID": "posts/optim-py/index.html",
    "href": "posts/optim-py/index.html",
    "title": "Optimization",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/optim-py/index.html#random-turnover",
    "href": "posts/optim-py/index.html#random-turnover",
    "title": "Optimization",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n  \n  rng = upper - lower\n  \n  result = rand_weights2b(n_sim, n_assets) * rng\n  result = result - rng / n_assets\n  \n  return result\n\n\nlower = -0.05\nupper = 0.05\ntarget = 0\n\n\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\n\n\n\n\ndef rand_iterative(n_assets, lower, upper, target):\n  \n  result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n  temp = target - sum(result)\n  \n  while not ((temp &lt;= upper) and (temp &gt;= lower)):\n    \n    result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n    temp = target - sum(result)\n      \n  result = np.append(result, temp)\n  \n  return result\n\n\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n\n  result_ls = []\n  \n  for i in range(n_sim):\n    \n    result_sim = rand_iterative(n_assets, lower, upper, target)\n    result_ls.append(result_sim)\n    \n  result = pd.DataFrame(result_ls)\n  \n  return result\n\n\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-mean",
    "href": "posts/optim-py/index.html#maximize-mean",
    "title": "Optimization",
    "section": "Maximize mean",
    "text": "Maximize mean\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ndef max_mean_optim(mu, sigma, target):\n  \n  params = cp.Variable(len(mu))\n  \n  obj = cp.Maximize(params.T @ mu)\n  \n  cons = [cp.sum(params) == 1, params &gt;= 0,\n          cp.quad_form(params, sigma) &lt;= target ** 2]\n  \n  prob = cp.Problem(obj, cons)\n  \n  prob.solve()\n  \n  return params.value\n\n\ntarget = 0.06\n\n\nparams1 = max_mean_optim(mu, sigma, target)\nparams1\n\narray([3.47591127e-01, 1.30236384e-01, 5.22172424e-01, 6.55336409e-08])\n\n\n\nnp.dot(mu, params1)\n\nnp.float64(0.035787932725994794)\n\n\n\nnp.sqrt(np.dot(params1, np.dot(sigma, params1)))\n\nnp.float64(0.05999999958875815)"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-variance",
    "href": "posts/optim-py/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ndef min_var_optim(mu, sigma, target):\n  \n  params = cp.Variable(len(mu))\n  \n  obj = cp.Minimize(cp.quad_form(params, sigma))\n  \n  cons = [cp.sum(params) == 1, params &gt;= 0,\n          params.T @ mu &gt;= target]\n  \n  prob = cp.Problem(obj, cons)\n  \n  prob.solve()\n  \n  return params.value\n\n\ntarget = 0.03\n\n\nparams2 = min_var_optim(mu, sigma, target)\nparams2\n\narray([ 2.92836214e-01,  1.18619775e-01,  5.88544011e-01, -1.23211284e-20])\n\n\n\nnp.dot(mu, params2)\n\nnp.float64(0.030000000000000002)\n\n\n\nnp.sqrt(np.dot(params2, np.dot(sigma, params2))) \n\nnp.float64(0.0504290712100159)"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-utility",
    "href": "posts/optim-py/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ndef max_utility_optim(mu, sigma, target):\n  \n  params = cp.Variable(len(mu))\n  \n  obj = cp.Minimize(0.5 * target * cp.quad_form(params, sigma) - params.T @ mu)\n  \n  cons = [cp.sum(params) == 1, params &gt;= 0]\n  \n  prob = cp.Problem(obj, cons)\n  \n  prob.solve()\n  \n  return params.value\n\n\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\n\n\nparams3 = max_utility_optim(mu, sigma, target)\nparams3\n\narray([ 4.17414991e-01,  1.45133431e-01,  4.37451578e-01, -1.00166234e-23])\n\n\n\nnp.dot(mu, params3)\n\nnp.float64(0.043168186585278205)\n\n\n\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n\nnp.float64(0.07224350666945618)"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nhttps://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html\n\n\ndef min_rss_optim1(mu, sigma):\n  \n  params = cp.Variable(len(mu))\n  \n  obj = cp.Minimize(0.5 * cp.quad_form(params, sigma) - params.T @ mu)\n  \n  cons = [cp.sum(params) == 1, params &gt;= 0]\n  \n  prob = cp.Problem(obj, cons)\n  \n  prob.solve()\n  \n  return params.value\n\n\nparams4 = min_rss_optim1(np.dot(overlap_x_df.T.values, overlap_y_df.values),\n                         np.dot(overlap_x_df.T.values, overlap_x_df.values))\nparams4\n\narray([ 2.77707819e-01, -3.70972748e-24,  1.08739216e-01,  6.13552965e-01])\n\n\n\nnp.dot(mu, params4)\n\nnp.float64(0.031930409833792986)\n\n\n\nnp.sqrt(np.matmul(np.transpose(params4), np.matmul(sigma, params4)))\n\nnp.float64(0.05917954984478293)\n\n\n\ndef min_rss_optim2(x, y):\n  \n  params = cp.Variable(x.shape[1])\n  \n  obj = cp.Minimize(cp.sum_squares(y - x @ params))\n  \n  cons = [cp.sum(params) == 1, params &gt;= 0]\n  \n  prob = cp.Problem(obj, cons)\n  \n  prob.solve()\n  \n  return params.value\n\n\nparams5 = min_rss_optim2(overlap_x_df.values, overlap_y_df.iloc[:, 0].values)\nparams5\n\narray([ 2.77707819e-01, -8.98647193e-21,  1.08739216e-01,  6.13552965e-01])\n\n\n\nnp.dot(mu, params5)\n\nnp.float64(0.031930409833797406)\n\n\n\nnp.sqrt(np.matmul(np.transpose(params5), np.matmul(sigma, params5)))\n\nnp.float64(0.059179549844780376)\n\n\n\npd.DataFrame({\n  \"max_pnl\": params1 * 100,\n  \"min_risk\": params2 * 100,\n  \"max_utility\": params3 * 100,\n  \"min_rss1\": params4 * 100,\n  \"min_rss2\": params5 * 100\n}).round(2)\n\n   max_pnl  min_risk  max_utility  min_rss1  min_rss2\n0    34.76     29.28        41.74     27.77     27.77\n1    13.02     11.86        14.51     -0.00     -0.00\n2    52.22     58.85        43.75     10.87     10.87\n3     0.00     -0.00        -0.00     61.36     61.36"
  },
  {
    "objectID": "posts/eigen-r/index.html",
    "href": "posts/eigen-r/index.html",
    "title": "Eigen",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\n\n\nDecomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\neigen_decomp &lt;- function(x, comps) {\n  \n  LV &lt;- eigen(cov(x))\n  L &lt;- LV[[\"values\"]][1:comps]\n  V &lt;- LV[[\"vectors\"]][ , 1:comps]\n  \n  result &lt;- V %*% sweep(t(V), 1, L, \"*\")\n  \n  return(result)\n    \n}\n\n\ncomps &lt;- 1\n\n\neigen_decomp(overlap_xts, comps) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  3.112361e-02 -3.893592e-03 -8.078094e-05  2.637884e-03\n[2,] -3.893592e-03  4.870920e-04  1.010577e-05 -3.300017e-04\n[3,] -8.078094e-05  1.010577e-05  2.096660e-07 -6.846596e-06\n[4,]  2.637884e-03 -3.300017e-04 -6.846596e-06  2.235741e-04\n\n\n\n# cov(overlap_xts) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\nvariance_explained &lt;- function(x) {\n    \n  LV &lt;- eigen(cov(x))\n  L &lt;- LV[[\"values\"]]\n  \n  result &lt;- cumsum(L) / sum(L)\n  \n  return(result)\n    \n}\n\n\nvariance_explained(overlap_xts)\n\n[1] 0.8879254 0.9925357 0.9982268 1.0000000\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n\\]\n\nsimilarity &lt;- function(V, V0) {\n    \n  n_cols_v &lt;- ncol(V)\n  n_cols_v0 &lt;- ncol(V0)\n  result &lt;- matrix(0, nrow = n_cols_v, ncol = n_cols_v0)\n  \n  for (i in 1:n_cols_v) {\n    for (j in 1:n_cols_v0) {\n      result[i, j] &lt;- crossprod(V[ , i], V0[ , j]) /\n        sqrt(crossprod(V[ , i]) * crossprod(V0[ , j]))\n    }\n  }\n  \n  return(result)\n    \n}\n\n\nroll_eigen1 &lt;- function(x, width, comp) {\n    \n  n_rows &lt;- nrow(x)\n  result_ls &lt;- list()\n  \n  for (i in width:n_rows) {\n    \n    idx &lt;- max(i - width + 1, 1):i\n    \n    LV &lt;- eigen(cov(x[idx, ]))\n    V &lt;- LV[[\"vectors\"]]\n    \n    result_ls &lt;- append(result_ls, list(V[ , comp]))\n    \n  }\n  \n  result &lt;- do.call(rbind, result_ls)\n  result &lt;- xts::xts(result, zoo::index(x)[width:n_rows])\n  colnames(result) &lt;- colnames(x)\n  \n  return(result)\n    \n}\n\n\ncomp &lt;- 1\n\n\nraw_df &lt;- roll_eigen1(overlap_xts, width, comp)\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolleigen\") # roll (&gt;= 1.1.7)\n# raw_df &lt;- rolleigen::roll_eigen(overlap_xts, width, order = TRUE)[[\"vectors\"]][ , comp, ]\n# raw_df &lt;- xts::xts(t(raw_df), zoo::index(overlap_xts))\n# colnames(raw_df) &lt;- colnames(overlap_xts)\n\n\n\n\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\nroll_eigen2 &lt;- function(x, width, comp) {\n    \n  n_rows &lt;- nrow(x)\n  V_ls &lt;- list()\n  result_ls &lt;- list()\n  \n  for (i in width:n_rows) {\n    \n    idx &lt;- max(i - width + 1, 1):i\n    \n    LV &lt;- eigen(cov(x[idx, ]))\n    V &lt;- LV[[\"vectors\"]]\n    \n    if (i &gt; width) {\n      \n      # cosine &lt;- crossprod(V, V_ls[[length(V_ls)]])\n      cosine &lt;- similarity(V, V_ls[[length(V_ls)]])\n      order &lt;- apply(abs(cosine), 1, which.max)\n      V &lt;- t(sign(diag(cosine[ , order])) * t(V[ , order]))\n      \n    }\n    \n    V_ls &lt;- append(V_ls, list(V))\n    result_ls &lt;- append(result_ls, list(V[ , comp]))\n    \n  }\n  \n  result &lt;- do.call(rbind, result_ls)\n  result &lt;- xts::xts(result, zoo::index(x)[width:n_rows])\n  colnames(result) &lt;- colnames(x)\n  \n  return(result)\n    \n}\n\n\nclean_df &lt;- roll_eigen2(overlap_xts, width, comp)\n\n\n\n\n\n\n\n\n\n\n\n\nImplied shocks\nProduct of the \\(n\\)th eigenvector and square root of the \\(n\\)th eigenvalue:\n\nroll_shocks &lt;- function(x, width, comp) {\n  \n  n_rows &lt;- nrow(x)\n  V_ls &lt;- list()\n  result_ls &lt;- list()\n  \n  for (i in width:n_rows) {\n    \n    idx &lt;- max(i - width + 1, 1):i\n    \n    LV &lt;- eigen(cov(x[idx, ]))\n    L &lt;- LV[[\"values\"]]\n    V &lt;- LV[[\"vectors\"]]\n    \n    if (length(V_ls) &gt; 1) {\n      \n      # cosine &lt;- crossprod(V, V_ls[[length(V_ls)]])\n      cosine &lt;- similarity(V, V_ls[[length(V_ls)]])\n      order &lt;- apply(abs(cosine), 1, which.max)\n      L &lt;- L[order]\n      V &lt;- t(sign(diag(cosine[ , order])) * t(V[ , order]))\n      \n    }\n    \n    shocks &lt;- sqrt(L[comp]) * V[ , comp]\n    V_ls &lt;- append(V_ls, list(V))\n    result_ls &lt;- append(result_ls, list(shocks))\n    \n  }\n  \n  result &lt;- do.call(rbind, result_ls)\n  result &lt;- xts::xts(result, zoo::index(x)[width:n_rows])\n  colnames(result) &lt;- colnames(x)\n  \n  return(result)\n    \n}\n\n\nshocks_xts &lt;- roll_shocks(overlap_xts, width, comp) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])"
  },
  {
    "objectID": "posts/eigen-py/index.html",
    "href": "posts/eigen-py/index.html",
    "title": "Eigen",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\n\n\nDecomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\ndef eigen(x):\n  \n  L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n  idx = L.argsort()[::-1]\n  L = L[idx]\n  V = V[:, idx]\n  \n  result = {\n    \"values\": L,\n    \"vectors\": V\n  }\n    \n  return result\n\n\ndef eigen_decomp(x, comps):\n  \n  LV = eigen(x)\n  L = LV[\"values\"][:comps]\n  V = LV[\"vectors\"][:, :comps]\n  \n  result = np.dot(V, np.multiply(L, V.T))\n  \n  return np.ravel(result)\n\n\ncomps = 1\n\n\neigen_decomp(overlap_df, comps) * scale[\"periods\"] * scale[\"overlap\"]\n\narray([ 3.19294657e-02, -3.79444587e-03, -8.04947508e-05,  2.63187781e-03,\n       -3.79444587e-03,  4.50925786e-04,  9.56586550e-06, -3.12768086e-04,\n       -8.04947508e-05,  9.56586550e-06,  2.02928698e-07, -6.63501077e-06,\n        2.63187781e-03, -3.12768086e-04, -6.63501077e-06,  2.16940079e-04])\n\n\n\n# np.cov(overlap_df.T) * scale[\"periods\"] * scale[\"overlap\"]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\ndef variance_explained(x):\n  \n  LV = eigen(x)\n  L = LV[\"values\"]\n  \n  result = L.cumsum() / L.sum()\n  \n  return result\n\n\nvariance_explained(overlap_df)\n\narray([0.8856599 , 0.99267755, 0.99824402, 1.        ])\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n\\]\n\ndef similarity(V, V0):\n  \n  n_cols_v = V.shape[1]\n  n_cols_v0 = V0.shape[1]\n  result = np.zeros((n_cols_v, n_cols_v0))\n  \n  for i in range(n_cols_v):\n    for j in range(n_cols_v0):\n      result[i, j] = np.dot(V[:, i], V0[:, j]) / \\\n      np.sqrt(np.dot(V[:, i], V[:, i]) * np.dot(V0[:, j], V0[:, j]))\n      \n  return result\n\n\ndef roll_eigen1(x, width, comp):\n    \n  n_rows = len(x)\n  result_ls = []\n  \n  for i in range(width - 1, n_rows):\n      \n    idx = range(max(i - width + 1, 0), i + 1)\n    \n    LV = eigen(x.iloc[idx])\n    V = LV[\"vectors\"]\n    \n    result_ls.append(V[:, comp - 1])\n  \n  result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                           columns = x.columns)\n  \n  return result_df  \n\n\ncomp = 1\n\n\nraw_df = roll_eigen1(overlap_df, width, comp)\n\n\n\n\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\ndef roll_eigen2(x, width, comp):\n    \n  n_rows = len(x)\n  V_ls = []\n  result_ls = []\n  \n  for i in range(width - 1, n_rows):\n      \n    idx = range(max(i - width + 1, 0), i + 1)\n    \n    LV = eigen(x.iloc[idx])\n    V = LV[\"vectors\"]\n    \n    if i &gt; width - 1:\n        \n      # cosine = np.dot(V.T, V_ls[-1])\n      cosine = similarity(V.T, V_ls[-1])\n      order = np.argmax(np.abs(cosine), axis = 1)\n      V = np.sign(np.diag(cosine[:, order])) * V[:, order]\n        \n    V_ls.append(V)\n    result_ls.append(V[:, comp - 1])\n  \n  result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                           columns = x.columns)\n  \n  return result_df\n\n\nclean_df = roll_eigen2(overlap_df, width, comp)\n\n\n\n\n\n\n\n\n\n\n\n\nImplied shocks\nProduct of the \\(n\\)th eigenvector and square root of the \\(n\\)th eigenvalue:\n\ndef roll_shocks(x, width, comp):\n    \n  n_rows = len(x)\n  V_ls = []\n  result_ls = []\n  \n  for i in range(width - 1, n_rows):\n    \n    idx = range(max(i - width + 1, 0), i + 1)\n    \n    LV = eigen(x.iloc[idx])\n    L = LV[\"values\"]\n    V = LV[\"vectors\"]\n    \n    if len(V_ls) &gt; 1:\n        \n      # cosine = np.dot(V.T, V_ls[-1])\n      cosine = similarity(V.T, V_ls[-1])\n      order = np.argmax(np.abs(cosine), axis = 1)\n      L = L[order]\n      V = np.sign(np.diag(cosine[:, order])) * V[:, order]\n    \n    shocks = np.sqrt(L[comp - 1]) * V[:, comp - 1]\n    V_ls.append(V)\n    result_ls.append(shocks)\n    \n  result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                           columns = x.columns)\n  \n  return result_df\n\n\nshocks_df = roll_shocks(overlap_df, width, comp) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])"
  },
  {
    "objectID": "posts/markets-py/index.html",
    "href": "posts/markets-py/index.html",
    "title": "Markets",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/markets-py/index.html#expected-value",
    "href": "posts/markets-py/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-py/index.html#variance",
    "href": "posts/markets-py/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\ndef sd(x):\n    \n  n_rows = sum(~np.isnan(x))\n      \n  if n_rows &gt; 1:\n    result = np.sqrt(np.nansum(x ** 2) / (n_rows - 1))\n  else:\n    result = np.nan\n      \n  return result\n\n\n# volatility scale only\nscore_df = (momentum_df / momentum_df.rolling(width, min_periods = 1).apply(sd, raw = False)).dropna()\n\n\n# overall_df = score_df.mean(axis = 1)\n# overall_df = overall_df / overall_df.rolling(width, min_periods = 1).apply(risk, raw = False)\n\n\n# score_df.insert(loc = 0, column = \"Overall\", value = overall_df)\n# score_df = score_df.dropna()"
  },
  {
    "objectID": "posts/markets-py/index.html#interquartile-range",
    "href": "posts/markets-py/index.html#interquartile-range",
    "title": "Markets",
    "section": "Interquartile range",
    "text": "Interquartile range\nOutliers are defined as the regression residuals that fall below \\(Q_{1}−1.5\\times IQR\\) or above \\(Q_{3}+1.5\\times IQR\\):\n\nhttps://stats.stackexchange.com/a/1153\nhttps://stats.stackexchange.com/a/108951\nhttps://robjhyndman.com/hyndsight/tsoutliers/\n\n\ndef outliers(z):\n  \n  n_cols = z.shape[1]\n  result_ls = []\n\n  for j in range(n_cols):\n    \n    y = z.iloc[:, j]\n\n    if (n_cols == 0):\n      x = sm.add_constant(range(len(y)))\n    else:\n      x = sm.add_constant(z.drop(z.columns[j], axis = 1))\n\n    coef = sm.WLS(y, x).fit().params\n    predict = coef.iloc[0] + np.dot(x.iloc[:, 1:], coef[1:])\n    resid = y - predict\n\n    lower = resid.quantile(0.25)\n    upper = resid.quantile(0.75)\n    iqr = upper - lower\n\n    total = y[(resid &lt; lower - 1.5 * iqr) | (resid &gt; upper + 1.5 * iqr)]\n    \n    total = pd.DataFrame({\"date\": total.index, \"symbol\": total.name, \"values\": total})\n    result_ls.append(total)\n\n  result = pd.concat(result_ls, ignore_index = True)\n  result = result.pivot_table(index = \"date\", columns = \"symbol\", values = \"values\")\n\n  return result\n\n\noutliers_df = outliers(score_df)"
  },
  {
    "objectID": "posts/markets-py/index.html#contour-ellipsoid",
    "href": "posts/markets-py/index.html#contour-ellipsoid",
    "title": "Markets",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid"
  },
  {
    "objectID": "posts/risk-py/index.html",
    "href": "posts/risk-py/index.html",
    "title": "Risk",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\nintercept = True"
  },
  {
    "objectID": "posts/risk-py/index.html#ordinary-least-squares",
    "href": "posts/risk-py/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\ndef lm_coef(x, y, weights, intercept):\n  \n  if (intercept): x = sm.add_constant(x)\n      \n  result = np.dot(np.linalg.inv(np.dot(x.T, np.multiply(weights, x))),\n                  np.dot(x.T, np.multiply(weights, y)))\n  \n  return np.ravel(result)\n\n\nlm_coef(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([ 1.30226275e-04,  1.84424216e-01, -1.00653989e-01,  3.11722330e+00,\n        1.14111163e+00])\n\n\n\nif (intercept): overlap_x_df = sm.add_constant(overlap_x_df)\n    \nfit = sm.WLS(overlap_y_df, overlap_x_df, weights = weights).fit()\n\nif (intercept): overlap_x_df = overlap_x_df.iloc[:, 1:]\n\nnp.array(fit.params)\n\narray([ 1.30226275e-04,  1.84424216e-01, -1.00653989e-01,  3.11722330e+00,\n        1.14111163e+00])\n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\ndef lm_rsq(x, y, weights, intercept):\n          \n  coef = np.matrix(lm_coef(x, y, weights, intercept))\n  \n  if (intercept):\n    \n    x = sm.add_constant(x)\n    x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n    y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n      \n  result = np.dot(coef, np.dot(np.dot(x.T, np.multiply(weights, x)), coef.T)) / \\\n    np.dot(y.T, np.multiply(weights, y))\n  \n  return np.float64(result.item())\n\n\nlm_rsq(overlap_x_df, overlap_y_df, weights, intercept)\n\nnp.float64(0.7392064204954406)\n\n\n\nfit.rsquared\n\nnp.float64(0.7392064204954404)\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\ndef lm_se(x, y, weights, intercept):\n  \n  n_rows = x.shape[0]\n  n_cols = x.shape[1]\n  \n  rsq = lm_rsq(x, y, weights, intercept)\n  \n  if (intercept):\n    \n    x = sm.add_constant(x)\n    y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n    \n    df_resid = n_rows - n_cols - 1 \n    \n  else:\n    df_resid = n_rows - n_cols        \n  \n  var_y = np.dot(y.T, np.multiply(weights, y))\n  var_resid = (1 - rsq) * var_y / df_resid\n  \n  result = np.sqrt(var_resid * np.linalg.inv(np.dot(x.T, np.multiply(weights, x))).diagonal())\n  \n  return np.ravel(result)\n\n\nlm_se(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([6.26338599e-05, 1.92317362e-02, 3.35253642e-02, 2.53120524e-01,\n       2.23294694e-01])\n\n\n\nnp.array(fit.bse)\n\narray([6.26338599e-05, 1.92317362e-02, 3.35253642e-02, 2.53120524e-01,\n       2.23294694e-01])\n\n\n\n\nShapley values\n\\[\nR^{2}_{i}=\\sum_{S\\subseteq N\\setminus\\{i\\}}{\\frac{|S|!\\;(n-|S|-1)!}{n!}}(R^{2}(S\\cup\\{i\\})-R^{2}(S))\n\\]\n\nhttps://real-statistics.com/multiple-regression/shapley-owen-decomposition/\n\n\ndef lm_shap(x, y, weights, intercept):\n\n  n_rows = x.shape[0]\n  n_cols = x.shape[1]\n  n_combn = 2 ** n_cols\n  n_vec = np.zeros(n_combn)\n  ix_mat = np.zeros((n_cols, n_combn))\n  rsq = np.zeros(n_combn)\n  result = np.zeros(n_cols)\n  \n  # number of binary combinations\n  for k in range(n_combn):\n    \n    n = 0\n    n_size = k\n    \n    # find the binary combination\n    for j in range(n_cols):\n      \n      if n_size % 2 == 0:\n        \n        n += 1\n        \n        ix_mat[j, k] = j + 1\n          \n      n_size //= 2\n    \n    n_vec[k] = n\n    \n    if n &gt; 0:\n      \n      ix_subset = np.where(ix_mat[:, k] != 0)[0]\n      x_subset = x.iloc[:, ix_subset]\n      \n      rsq[k] = lm_rsq(x_subset, y, weights, intercept)\n\n  # calculate the exact Shapley value for r-squared\n  for j in range(n_cols):\n    \n    ix_pos = np.where(ix_mat[j, :] != 0)[0]\n    ix_neg = np.where(ix_mat[j, :] == 0)[0]\n    ix_n = n_vec[ix_neg]\n    rsq_diff = rsq[ix_pos] - rsq[ix_neg]\n\n    for k in range(int(n_combn / 2)):\n      \n      s = int(ix_n[k])\n      weight = math.factorial(s) * math.factorial(n_cols - s - 1) \\\n        / math.factorial(n_cols)\n      result[j] += weight * rsq_diff[k]\n\n  return result\n\n\nlm_shap(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([0.3248786 , 0.0157933 , 0.15679217, 0.24174235])"
  },
  {
    "objectID": "posts/risk-py/index.html#principal-component-regression",
    "href": "posts/risk-py/index.html#principal-component-regression",
    "title": "Risk",
    "section": "Principal component regression",
    "text": "Principal component regression\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\n\n\ncomps = 1\n\n\nCoefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\ndef pcr_coef(x, y, comps):\n  \n  x = x - np.average(x, axis = 0)\n  L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n  idx = L.argsort()[::-1]\n  V = V[:, idx]\n  \n  W = np.dot(x, V)\n  gamma = np.dot(np.dot(np.linalg.inv(np.dot(W.T, W)), W.T), y)\n  \n  result = np.dot(V[:, :comps], gamma[:comps])\n  \n  return np.ravel(result)\n\n\nscale_x_df = (overlap_x_df - np.average(overlap_x_df, axis = 0)) \\\n  / np.std(overlap_x_df, axis = 0, ddof = 1)\n\n\npcr_coef(scale_x_df, overlap_y_df, comps)\n\narray([ 6.57502594e-04,  4.91887287e-05, -1.26734774e-04,  6.61077549e-04])\n\n\n\npcr_coef(overlap_x_df, overlap_y_df, comps)\n\narray([ 0.25504615,  0.00339423, -0.00051816,  0.01778693])\n\n\n\npca = PCA(n_components = len(factors))\npca_x_df = pca.fit_transform(scale_x_df)\n\nfit = LinearRegression(fit_intercept = False).fit(pca_x_df, overlap_y_df)\n\ngamma = fit.coef_\nnp.dot(pca.components_.T[:, :comps], gamma.T[:comps]).ravel()\n\narray([ 6.57502594e-04,  4.91887287e-05, -1.26734774e-04,  6.61077549e-04])\n\n\n\n\nR-squared\n\ndef pcr_rsq(x, y, comps):\n  \n  coef = np.matrix(pcr_coef(x, y, comps))\n  \n  x = x - np.average(x, axis = 0)\n  y = y - np.average(y, axis = 0)\n  \n  result = np.dot(np.dot(coef, np.dot(x.T, x)), coef.T) / np.dot(y.T, y)\n  \n  return np.float64(result.item())\n\n\npcr_rsq(scale_x_df, overlap_y_df, comps)\n\nnp.float64(0.46506748684183313)\n\n\n\npcr_rsq(overlap_x_df, overlap_y_df, comps)\n\nnp.float64(0.5365459634225618)\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\ndef pcr_se(x, y, comps):\n  \n  n_rows = x.shape[0]\n  n_cols = x.shape[1]\n  \n  rsq = pcr_rsq(x, y, comps)\n  \n  y = y - np.average(y, axis = 0)\n  \n  df_resid = n_rows - n_cols - 1\n  \n  var_y = np.dot(y.T, y)   \n  var_resid = (1 - rsq) * var_y / df_resid\n  \n  # uses statsmodels for illustrative purposes\n  pca = sm.multivariate.PCA(x, standardize = False, demean = True)\n  L = pca.eigenvals[:comps]\n  V = pca.eigenvecs.iloc[:, :comps]\n  \n  result = np.sqrt(var_resid * np.dot(V, np.dot(np.diag(1 / L), V.T)).diagonal())\n  \n  return np.ravel(result)\n\n\npcr_se(scale_x_df, overlap_y_df, comps)\n\narray([4.48683917e-05, 3.35666987e-06, 8.64846087e-06, 4.51123488e-05])\n\n\n\npcr_se(overlap_x_df, overlap_y_df, comps)\n\narray([1.50823898e-02, 2.00720943e-04, 3.06418456e-05, 1.05184683e-03])"
  },
  {
    "objectID": "posts/risk-py/index.html#partial-least-squares",
    "href": "posts/risk-py/index.html#partial-least-squares",
    "title": "Risk",
    "section": "Partial least squares",
    "text": "Partial least squares"
  },
  {
    "objectID": "posts/risk-py/index.html#standalone-risk",
    "href": "posts/risk-py/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\ndef cov_wt(x, weights, center):\n  \n  sum_w = sum(weights)\n  sumsq_w = sum(np.power(weights, 2))\n  \n  if (center):\n  \n    x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n  \n  result = np.dot(x.T, np.multiply(weights, x)) / (sum_w - sumsq_w / sum_w)\n  \n  return result\n\n\ndef lm_sar(x, y, weights, intercept):\n    \n  coef = lm_coef(x, y, weights, intercept)\n  rsq = lm_rsq(x, y, weights, intercept)\n  \n  if (intercept): x = sm.add_constant(x)\n  \n  # sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n  #                aweights = weights.reshape(-1))\n  sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n  sar = np.multiply(np.power(coef, 2).T, sigma[:-1, :-1].diagonal())\n  sar_eps = (1 - rsq) * sigma[-1, -1]\n  \n  result = np.sqrt(np.concatenate((np.matrix(sigma[-1, -1]),\n                                   np.matrix(sar),\n                                   np.matrix(sar_eps)), axis = 1))\n  \n  return np.ravel(result)\n\n\nlm_sar(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])\n\narray([0.06615774, 0.        , 0.03486621, 0.00654608, 0.0270067 ,\n       0.01865794, 0.0337854 ])"
  },
  {
    "objectID": "posts/risk-py/index.html#risk-contribution",
    "href": "posts/risk-py/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttps://bookdown.org/compfinezbook/introcompfinr/Portfolio-risk-reports.html\n\n\ndef lm_mcr(x, y, weights, intercept):\n    \n  coef = np.matrix(lm_coef(x, y, weights, intercept)).T\n  rsq = lm_rsq(x, y, weights, intercept)\n      \n  if (intercept): x = sm.add_constant(x)\n  \n#     sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n#                    aweights = weights.reshape(-1))\n  sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n  mcr = np.multiply(coef, np.dot(sigma[:-1, :-1], coef)) / np.sqrt(sigma[-1, -1])\n  mcr_eps = np.sqrt(sigma[-1, -1]) - sum(mcr)\n  \n  result = np.concatenate((np.sqrt(np.matrix(sigma[-1, -1])),\n                           np.matrix(mcr).T,\n                           np.matrix(mcr_eps)), axis = 1)\n  \n  return np.ravel(result)\n\n\nlm_mcr(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])\n\narray([0.06615774, 0.        , 0.02555203, 0.00093459, 0.0100273 ,\n       0.0123903 , 0.01725351])"
  },
  {
    "objectID": "posts/risk-py/index.html#implied-shocks",
    "href": "posts/risk-py/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\ndef implied_shocks(shocks, x, z, weights):\n\n  beta = np.linalg.lstsq(np.multiply(weights, z), np.multiply(weights, x), rcond = None)[0]\n                   \n  result = np.dot(shocks, beta)\n  \n  return result\n\n\nshocks = np.array([-0.1, 0.1])\noverlap_z_df = overlap_x_df.iloc[:, [0, 1]]\n\n\nimplied_shocks(shocks, overlap_x_df, overlap_z_df, weights)\n\narray([-0.1       ,  0.1       , -0.00198919, -0.00721241])"
  },
  {
    "objectID": "posts/risk-py/index.html#stress-pl",
    "href": "posts/risk-py/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\ndef pnl_stress(shocks, x, y, z, weights, intercept):\n  \n  coef = lm_coef(x, y, weights, intercept)\n  \n  if (intercept): x = sm.add_constant(x)\n  \n  result = np.multiply(coef.T, implied_shocks(shocks, x, z, weights))\n  \n  return result\n\n\npnl_stress(shocks, overlap_x_df, overlap_y_df, overlap_z_df, weights, intercept)\n\narray([-0.00122695, -0.01844242, -0.0100654 , -0.00620074, -0.00823017])"
  },
  {
    "objectID": "posts/markets-r/index.html",
    "href": "posts/markets-r/index.html",
    "title": "Markets",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/markets-r/index.html#expected-value",
    "href": "posts/markets-r/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-r/index.html#variance",
    "href": "posts/markets-r/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\n# volatility scale only\nscore_xts &lt;- na.omit(momentum_xts / roll::roll_sd(momentum_xts, width, center = FALSE, min_obs = 1))\n\n\n# overall_xts &lt;- xts::xts(rowMeans(score_xts), zoo::index(score_xts))\n# overall_xts &lt;- overall_xts / roll::roll_sd(overall_xts, width, center = FALSE, min_obs = 1)\n# colnames(overall_xts) &lt;- \"Overall\"\n\n\n# score_xts &lt;- na.omit(merge(overall_xts, score_xts))"
  },
  {
    "objectID": "posts/markets-r/index.html#interquartile-range",
    "href": "posts/markets-r/index.html#interquartile-range",
    "title": "Markets",
    "section": "Interquartile range",
    "text": "Interquartile range\nOutliers are defined as the regression residuals that fall below \\(Q_{1}−1.5\\times IQR\\) or above \\(Q_{3}+1.5\\times IQR\\):\n\nhttps://stats.stackexchange.com/a/1153\nhttps://stats.stackexchange.com/a/108951\nhttps://robjhyndman.com/hyndsight/tsoutliers/\n\n\noutliers &lt;- function(z) {\n  \n  n_cols &lt;- ncol(z)\n  result_ls &lt;- list()\n  \n  for (j in 1:n_cols) {\n    \n    y &lt;- z[ , j]\n    \n    if (n_cols == 1) {\n      x &lt;- 1:length(y)\n    } else {\n      x &lt;- cbind(1:length(y), z[ , -j])\n    }\n    \n    coef &lt;- coef(lm(y ~ x))\n    predict &lt;- coef[1] + x %*% as.matrix(coef[-1])\n    resid &lt;- y - predict\n    \n    lower &lt;- quantile(resid, prob = 0.25)\n    upper &lt;- quantile(resid, prob = 0.75)\n    iqr &lt;- upper - lower\n    \n    total &lt;- y[(resid &lt; lower - 1.5 * iqr) | (resid &gt; upper + 1.5 * iqr)]\n    \n    result_ls &lt;- append(result_ls, list(total))\n    \n  }\n  \n  result &lt;- do.call(merge, result_ls)\n  \n  return(result)\n  \n}\n\n\noutliers_xts &lt;- outliers(score_xts)"
  },
  {
    "objectID": "posts/markets-r/index.html#contour-ellipsoid",
    "href": "posts/markets-r/index.html#contour-ellipsoid",
    "title": "Markets",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid"
  },
  {
    "objectID": "posts/risk-r/index.html",
    "href": "posts/risk-r/index.html",
    "title": "Risk",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\ntickers &lt;- \"BAICX\" # fund inception date is \"2011-11-28\"\nintercept &lt;- TRUE"
  },
  {
    "objectID": "posts/risk-r/index.html#ordinary-least-squares",
    "href": "posts/risk-r/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\nlm_coef &lt;- function(x, y, weights, intercept) {\n  \n  # cbind.xts() changes names, e.g., \"(Intercept)\" =&gt; \"X.Intercept.\"\n  if (intercept) x &lt;- cbind(\"(Intercept)\" = 1, as.matrix(x))\n  \n  result &lt;- solve(crossprod(x, diag(weights)) %*% x) %*% crossprod(x, diag(weights) %*% y)\n  \n  return(result)\n  \n}\n\n\nt(lm_coef(overlap_x_xts, overlap_y_xts, weights, intercept))\n\n       (Intercept)     SP500  DTWEXAFEGS    DGS10 BAMLH0A0HYM2\nBAICX 0.0001365122 0.1836839 -0.09888612 3.053248      1.13516\n\n\n\nif (intercept) {\n  form &lt;- reformulate(termlabels = factors, response = tickers)\n} else {\n  form &lt;- reformulate(termlabels = factors, response = tickers, intercept = FALSE)\n}\n\nfit &lt;- lm(form, data = overlap_xts, weights = weights)\n    \ncoef(fit)\n\n  (Intercept)         SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 0.0001365122  0.1836838644 -0.0988861207  3.0532475177  1.1351604745 \n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\nlm_rsq &lt;- function(x, y, weights, intercept) {\n        \n  coef &lt;- lm_coef(x, y, weights, intercept)\n  \n  if (intercept) {\n    \n    x &lt;- model.matrix(~ x)\n    x &lt;- sweep(x, 2, apply(x, 2, weighted.mean, w = weights), \"-\")\n    y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n      \n  }\n  \n  result &lt;- (t(coef) %*% (crossprod(x, diag(weights)) %*% x) %*% coef) / (crossprod(y, diag(weights)) %*% y)\n  colnames(result) &lt;- \"R-squared\"\n  \n  return(result)\n    \n}\n\n\nlm_rsq(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n      R-squared\nBAICX 0.7414254\n\n\n\nsummary(fit)$r.squared\n\n[1] 0.7414254\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\nlm_se &lt;- function(x, y, weights, intercept) {\n    \n  n_rows &lt;- nrow(x)\n  n_cols &lt;- ncol(x)\n  \n  rsq &lt;- lm_rsq(x, y, weights, intercept)\n  \n  if (intercept) {\n    \n    x &lt;- cbind(\"(Intercept)\" = 1, as.matrix(x))\n    y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n    \n    df_resid &lt;- n_rows - n_cols - 1\n      \n  } else {\n    df_resid &lt;- n_rows - n_cols\n  }\n  \n  var_y &lt;- crossprod(y, diag(weights)) %*% y\n  var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n  \n  result &lt;- sqrt(var_resid * diag(solve(crossprod(x, diag(weights)) %*% x)))\n  \n  return(result)\n    \n}\n\n\nlm_se(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n (Intercept)        SP500   DTWEXAFEGS        DGS10 BAMLH0A0HYM2 \n6.299406e-05 1.902549e-02 3.334010e-02 2.569485e-01 2.226090e-01 \n\n\n\ncoef(summary(fit))[ , \"Std. Error\"]\n\n (Intercept)        SP500   DTWEXAFEGS        DGS10 BAMLH0A0HYM2 \n6.299406e-05 1.902549e-02 3.334010e-02 2.569485e-01 2.226090e-01 \n\n\n\n\nShapley values\n\\[\nR^{2}_{i}=\\sum_{S\\subseteq N\\setminus\\{i\\}}{\\frac{|S|!\\;(n-|S|-1)!}{n!}}(R^{2}(S\\cup\\{i\\})-R^{2}(S))\n\\]\n\nhttps://real-statistics.com/multiple-regression/shapley-owen-decomposition/\n\n\nlm_shap &lt;- function(x, y, weights, intercept) {\n  \n  n_rows &lt;- nrow(x)\n  n_cols &lt;- ncol(x)\n  n_combn &lt;- 2 ^ n_cols\n  n_vec &lt;- array(0, n_combn)\n  ix_mat &lt;- matrix(0, nrow = n_cols, ncol = n_combn)\n  rsq &lt;- array(0, n_combn)\n  result &lt;- array(0, n_cols)\n  \n  # number of binary combinations\n  for (k in 1:n_combn) {\n    \n    n &lt;- 0\n    n_size &lt;- k - 1\n    \n    # find the binary combination\n    for (j in 1:n_cols) {\n      \n      if (n_size %% 2 == 0) {\n        \n        n &lt;- n + 1\n        \n        ix_mat[j, k] = j - 1 + 1\n        \n      }\n      \n      n_size &lt;- n_size %/% 2\n      \n    }\n    \n    n_vec[k] &lt;- n\n    \n    if (n &gt; 0) {\n      \n      ix_subset&lt;- which(ix_mat[ , k] != 0)\n      x_subset &lt;- x[ , ix_subset]\n      \n      rsq[k] &lt;- lm_rsq(x_subset, y, weights, intercept)\n\n    }\n    \n  }\n\n  # calculate the exact Shapley value for r-squared\n  for (j in 1:n_cols) {\n\n    ix_pos &lt;- which(ix_mat[j, ] != 0)\n    ix_neg &lt;- which(ix_mat[j, ] == 0)\n    ix_n &lt;- n_vec[ix_neg]\n    rsq_diff &lt;- rsq[ix_pos] - rsq[ix_neg]\n\n    for (k in 1:(n_combn / 2)) {\n\n      s &lt;- ix_n[k]\n      weight &lt;- factorial(s) * factorial(n_cols - s - 1) / factorial(n_cols)\n      result[j] &lt;- result[j] + weight * rsq_diff[k]\n\n    }\n    \n  }\n\n  return(result)\n  \n}\n\n\nlm_shap(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n[1] 0.33195117 0.01740582 0.14550822 0.24656019"
  },
  {
    "objectID": "posts/risk-r/index.html#principal-component-regression",
    "href": "posts/risk-r/index.html#principal-component-regression",
    "title": "Risk",
    "section": "Principal component regression",
    "text": "Principal component regression\n\nlibrary(pls) # \"Error in mvrValstats(object = fit, estimate = 'train'): could not find function 'mvrValstats'\"\n\n\ncomps &lt;- 1\n\n\nCoefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\npcr_coef &lt;- function(x, y, comps) {\n  \n  x &lt;- sweep(x, 2, colMeans(x), \"-\")\n  LV &lt;- eigen(cov(x))\n  V &lt;- LV[[\"vectors\"]]\n  \n  W &lt;- x %*% V\n  gamma &lt;- solve(crossprod(W)) %*% (crossprod(W, y))\n  \n  result &lt;- V[ , 1:comps] %*% as.matrix(gamma[1:comps])\n  \n  return(result)\n  \n}\n\n\nscale_x_xts &lt;- scale(overlap_x_xts)\n\n\nt(pcr_coef(scale_x_xts, overlap_y_xts, comps))\n\n             [,1]         [,2]          [,3]         [,4]\n[1,] 0.0006838018 2.316699e-06 -0.0001152147 0.0006882963\n\n\n\nt(pcr_coef(overlap_x_xts, overlap_y_xts, comps))\n\n          [,1]         [,2]          [,3]      [,4]\n[1,] 0.2562742 0.0001241262 -0.0004206692 0.0178099\n\n\n\nfit &lt;- pls::pcr(reformulate(termlabels = \".\", response = tickers),\n                data = merge(scale_x_xts, overlap_y_xts), ncomp = comps)\ncoef(fit)[ , , 1]\n\n        SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 6.838018e-04  2.316699e-06 -1.152147e-04  6.882963e-04 \n\n\n\n\nR-squared\n\npcr_rsq &lt;- function(x, y, comps) {\n  \n  coef &lt;- pcr_coef(x, y, comps)\n  \n  x &lt;- sweep(x, 2, colMeans(x), \"-\")\n  y &lt;- sweep(y, 2, colMeans(y), \"-\")\n  \n  result &lt;- (t(coef) %*% crossprod(x) %*% coef) / crossprod(y)\n  colnames(result) &lt;- \"R-squared\"\n  \n  return(result)\n  \n}\n\n\npcr_rsq(scale_x_xts, overlap_y_xts, comps)\n\n      R-squared\nBAICX 0.4985953\n\n\n\npcr_rsq(overlap_x_xts, overlap_y_xts, comps)\n\n      R-squared\nBAICX 0.5539987\n\n\n\npls::R2(fit)$val[comps + 1]\n\n[1] 0.4985953\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\npcr_se &lt;- function(x, y, comps) {\n  \n  n_rows &lt;- nrow(x)\n  n_cols &lt;- ncol(x)\n  \n  rsq &lt;- pcr_rsq(x, y, comps)\n  \n  y &lt;- sweep(y, 2, colMeans(y), \"-\")\n  \n  df_resid &lt;- n_rows - n_cols - 1\n  \n  var_y &lt;- crossprod(y)\n  var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n  \n  LV &lt;- eigen(cov(x))\n  L &lt;- LV$values[1:comps] * (n_rows - 1)\n  V &lt;- LV$vectors[ , 1:comps]\n  \n  result &lt;- sqrt(var_resid * diag(V %*% sweep(t(V), 1, 1 / L, \"*\")))\n  \n  return(result)\n  \n}\n\n\npcr_se(scale_x_xts, overlap_y_xts, comps)\n\n[1] 4.363167e-05 1.478228e-07 7.351561e-06 4.391846e-05\n\n\n\npcr_se(overlap_x_xts, overlap_y_xts, comps)\n\n[1] 1.463087e-02 7.086450e-06 2.401629e-05 1.016779e-03"
  },
  {
    "objectID": "posts/risk-r/index.html#standalone-risk",
    "href": "posts/risk-r/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\nlm_sar &lt;- function(x, y, weights, intercept) {\n  \n  coef &lt;- lm_coef(x, y, weights, intercept)\n  rsq &lt;- lm_rsq(x, y, weights, intercept)\n  \n  if (intercept) x &lt;- model.matrix(~ x)\n  \n  sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n  sar &lt;- coef ^ 2 * diag(sigma[-ncol(sigma), -ncol(sigma)])\n  sar_eps &lt;- (1 - rsq) * sigma[ncol(sigma), ncol(sigma)]\n  \n  result &lt;- sqrt(c(sigma[ncol(sigma), ncol(sigma)],\n                   sar,\n                   sar_eps))\n  \n  return(result)\n  \n}\n\n\nlm_sar(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])\n\n[1] 0.066230932 0.000000000 0.035163359 0.006440288 0.025984570 0.018668531\n[7] 0.033678582"
  },
  {
    "objectID": "posts/risk-r/index.html#risk-contribution",
    "href": "posts/risk-r/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttps://bookdown.org/compfinezbook/introcompfinr/Portfolio-risk-reports.html\n\n\nlm_mcr &lt;- function(x, y, weights, intercept) {\n  \n  coef &lt;- lm_coef(x, y, weights, intercept)\n  rsq &lt;- lm_rsq(x, y, weights, intercept)\n  \n  if (intercept) x &lt;- model.matrix(~ x)\n  \n  sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n  mcr &lt;- coef * sigma[-ncol(sigma), -ncol(sigma)] %*% coef / sqrt(sigma[ncol(sigma), ncol(sigma)])\n  mcr_eps &lt;- sqrt(sigma[ncol(sigma), ncol(sigma)]) - sum(mcr)\n  \n  result &lt;- c(sqrt(sigma[ncol(sigma), ncol(sigma)]),\n              mcr,\n              mcr_eps)\n  \n  return(result)\n  \n}\n\n\nlm_mcr(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])\n\n[1] 0.066230932 0.000000000 0.026159320 0.001042492 0.009335710 0.012567773\n[7] 0.017125636"
  },
  {
    "objectID": "posts/risk-r/index.html#implied-shocks",
    "href": "posts/risk-r/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\nimplied_shocks &lt;- function(shocks, x, z, weights) {\n  \n  beta &lt;- solve(crossprod(z, diag(weights) %*% z)) %*% crossprod(z, diag(weights) %*% x)\n  \n  result &lt;- shocks %*% beta\n  \n  return(result)\n  \n}\n\n\nshocks &lt;- c(-0.1, 0.1)\noverlap_z_xts &lt;- overlap_x_xts[ , 1:2]\n\n\nimplied_shocks(shocks, overlap_x_xts, overlap_z_xts, weights)\n\n     SP500 DTWEXAFEGS        DGS10 BAMLH0A0HYM2\n[1,]  -0.1        0.1 -0.002071601 -0.006956332"
  },
  {
    "objectID": "posts/risk-r/index.html#stress-pl",
    "href": "posts/risk-r/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\npnl_stress &lt;- function(shocks, x, y, z, weights, intercept) {\n  \n  coef &lt;- lm_coef(x, y, weights, intercept)\n  \n  if (intercept) x &lt;- model.matrix(~ x)\n  \n  result &lt;- t(coef) * implied_shocks(shocks, x, z, weights)\n  \n  return(result)    \n  \n}\n\n\npnl_stress(shocks, overlap_x_xts, overlap_y_xts, overlap_z_xts, weights, intercept)\n\n       (Intercept)       SP500   DTWEXAFEGS        DGS10 BAMLH0A0HYM2\nBAICX -0.001543899 -0.01836839 -0.009888612 -0.006325111 -0.007896553"
  },
  {
    "objectID": "posts/securities-r/index.html",
    "href": "posts/securities-r/index.html",
    "title": "Securities",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/securities-r/index.html#value",
    "href": "posts/securities-r/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_value &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n\n  r_df &lt;- exp(-r * tau)\n  q_df &lt;- exp(-q * tau)\n  \n  call_value &lt;- S * q_df * Phi(d1) - r_df * K * Phi(d2)\n  put_value &lt;- r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n  result &lt;- ifelse(type == \"call\", call_value, put_value)\n  \n  return(result)\n  \n} \n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\nbs_d1 &lt;- function(S, K, r, q, tau, sigma) {\n  \n  result &lt;- (log(S / K) + (r - q + sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n  \n  return(result)\n  \n}\n\nbs_d2 &lt;- function(S, K, r, q, tau, sigma) {\n  \n  result &lt;- (log(S / K) + (r - q - sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n  \n  return(result)\n  \n}\n\nphi &lt;- function(x) {\n  \n  result &lt;- dnorm(x)\n  \n  return(result)\n  \n}\n\nPhi &lt;- function(x) {\n  \n  result &lt;- pnorm(x)\n  \n  return(result)\n  \n}\n\n\ngreeks_dt[ , d1 := bs_d1(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , d2 := bs_d2(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , value := bs_value(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order",
    "href": "posts/securities-r/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_delta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n  q_df &lt;- exp(-q * tau)\n\n  call_value &lt;- q_df * Phi(d1)\n  put_value &lt;- -q_df * Phi(-d1)\n  result &lt;- ifelse(type == \"call\", call_value, put_value)\n  \n  return(result)\n  \n} \n\n\ngreeks_dt[ , delta := bs_delta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nDelta-beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\nbs_delta_diff &lt;- function(type, S, K, r, q, tau, sigma, delta0) {\n  \n  d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n  d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n  delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n  \n  call_value &lt;- delta - delta0\n  put_value &lt;- delta0 - delta\n  \n  result &lt;- ifelse(type == \"call\", call_value, put_value)\n      \n  return(result)\n  \n}\n\n\nbeta &lt;- 0.35\ntype &lt;- \"call\"\nn &lt;- 1\nmultiple &lt;- 100\ntotal &lt;- 1000000\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\nsec &lt;- list(\n  \"n\" = n,\n  \"multiple\" = multiple,\n  \"S\" = S,\n  \"delta\" = bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n  \"beta\" = 1\n)\n\n\nbeta_dt &lt;- data.table::CJ(type = type, shock = shocks)\nbeta_dt[ , spot := level_shock(shock, S, tau, sigma), by = c(\"type\", \"shock\")]\nbeta_dt[ , static := beta]\nbeta_dt[ , diff := bs_delta_diff(type, spot, K, r, q, tau, sigma, sec[[\"delta\"]])]\nbeta_dt[ , dynamic := beta + sec[[\"n\"]] * sec[[\"multiple\"]] * sec[[\"S\"]] * sec[[\"beta\"]] * diff / total, by = c(\"type\", \"shock\")]\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\nbs_vega &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n  q_df &lt;- exp(-q * tau)\n  \n  result &lt;- S * q_df * phi(d1) * sqrt(tau)\n  \n  return(result)\n  \n}\n\n\ngreeks_dt[ , vega := bs_vega(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_theta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n  r_df &lt;- exp(r * tau)\n  q_df &lt;- exp(q * tau)\n\n  call_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) -\n    r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n  \n  put_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) +\n    r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n      \n  result &lt;- ifelse(type == \"call\", call_value, put_value)\n  \n  return(result)\n  \n}\n\n\ngreeks_dt[ , theta := bs_theta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order",
    "href": "posts/securities-r/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\nbs_gamma &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n\n  q_df &lt;- exp(-q * tau)\n  \n  result &lt;- q_df * phi(d1) / (S * sigma * sqrt(tau))\n  \n  return(result)\n  \n}\n\n\ngreeks_dt[ , gamma := bs_gamma(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order-1",
    "href": "posts/securities-r/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\n\npnl_bond &lt;- function(duration, convexity, dy) {\n  \n  duration_pnl &lt;- -duration * dy\n  convexity_pnl &lt;- (convexity / 2) * dy ^ 2\n  income_pnl &lt;- dy\n  \n  result &lt;- list(\n    \"total\" = duration_pnl + convexity_pnl + income_pnl,\n    \"duration\" = duration_pnl,\n    \"convexity\" = convexity_pnl,\n    \"income\" = income_pnl\n  )\n  \n  return(result)\n  \n} \n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor &lt;- \"DGS10\"\nduration &lt;- 6.5\nconvexity &lt;- 0.65\ny &lt;- zoo::coredata(tail(zoo::na.locf(levels_xts[ , factor]), width)[1])\n\n\nbonds_dt &lt;- data.table::data.table(index = zoo::index(tail(levels_xts, width)),\n                                   duration = duration, convexity = convexity,\n                                   dy = zoo::na.locf(tail(levels_xts[ , factor], width)))\ndata.table::setnames(bonds_dt, c(\"index\", \"duration\", \"convexity\", \"dy\"))\nbonds_dt[ , dy := (dy - y) / 100, by = index]\n\n\nattrib_dt &lt;- bonds_dt[ , as.list(unlist(pnl_bond(duration, convexity, dy))), by = index]\n\n\n\n\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=\\frac{\\partial D}{\\partial y}=\\frac{\\partial}{\\partial y}\\left(-\\frac{1}{P}\\frac{\\partial D}{\\partial y}\\right)\\\\\n&=-\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}+\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=-C+D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(-C\\times 100+D^{2}\\right)=-C+\\frac{D^{2}}{100}\\). For example, if convexity and yield are percent then \\(\\text{Drift}=\\left(-0.65+\\frac{6.5^{2}}{100}\\right)\\partial y\\times100\\) or basis points then \\(\\text{Drift}=\\left(-65+6.5^{2}\\right)\\partial y\\).\n\nyield_shock &lt;- function(shock, tau, sigma) {\n  \n  result &lt;- shock * sigma * sqrt(tau)\n  \n  return(result)\n  \n}\n\n\nduration_drift &lt;- function(duration, convexity, dy) {\n  \n  drift &lt;- -convexity + duration ^ 2 / 100\n  change &lt;- drift * dy * 100\n  \n  result &lt;- list(\n    \"drift\" = drift,\n    \"change\" = change\n  )\n  \n  return(result)\n  \n}\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor &lt;- \"DGS10\"\nsigma &lt;- zoo::coredata(sd_xts[nrow(sd_xts), factor])\n\n\nduration_dt &lt;- data.table::CJ(shock = shocks)\nduration_dt[ , spot := yield_shock(shock, tau, sigma), by = \"shock\"]\nduration_dt[ , static := duration]\nduration_dt[ , dynamic := duration + duration_drift(duration, convexity, spot)[[\"change\"]], by = \"shock\"]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order-1",
    "href": "posts/securities-r/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlack’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\npnl_option &lt;- function(type, S, K, r, q, tau, sigma, dS, dt, dsigma) {\n  \n  d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n  d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n  value &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n  delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n  vega &lt;- bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n  theta &lt;- bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n  gamma &lt;- bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n  \n  delta_pnl &lt;- delta * dS / value\n  gamma_pnl &lt;- gamma / 2 * dS ^ 2 / value\n  vega_pnl &lt;- vega * dsigma / value\n  theta_pnl &lt;- theta * dt / value\n  \n  result &lt;- list(\n    \"total\" = delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n    \"delta\" = delta_pnl,\n    \"gamma\" = gamma_pnl,\n    \"vega\" = vega_pnl,\n    \"theta\" = theta_pnl\n  )\n  \n  return(result)    \n  \n}\n\n\nfactor &lt;- \"SP500\"\ntype &lt;- \"call\"\nS &lt;- zoo::coredata(tail(zoo::na.locf(levels_xts[ , factor]), width)[1])\nK &lt;- S # * (1 + 0.05)\ntau &lt;- 1 # = 252 / 252\nsigma &lt;- zoo::coredata(tail(sd_xts[ , factor], width)[1])\n\n\noptions_dt &lt;- data.table::data.table(index = zoo::index(tail(levels_xts, width)),\n                                     spot = zoo::na.locf(tail(levels_xts[ , factor], width)),\n                                     sigma = tail(sd_xts[ , factor], width))\ndata.table::setnames(options_dt, c(\"index\", \"spot\", \"sigma\"))\noptions_dt[ , dS := spot - S, by = index]\noptions_dt[ , dt_diff := as.numeric(index - index[1])]\noptions_dt[ , dt := dt_diff / tail(dt_diff, 1)]\noptions_dt[ , dsigma := sigma - ..sigma, by = index]\n\n\nattrib_dt &lt;- options_dt[ , as.list(unlist(pnl_option(type, S, K, r, q, tau, ..sigma,\n                                                     dS, dt, dsigma))), by = index]\n\n\n\n\n\n\n\n\n\n\n\n\nIto’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\nsim_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n  \n  result &lt;- S * exp(cumsum(sigma * sqrt(dt) * rnorm(n_sim)) +\n                    (mu - 0.5 * sigma ^ 2) * dt)\n  \n  return(result)\n  \n}\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\nsim_multi_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n  \n  n_cols &lt;- ncol(sigma)\n  \n  Z &lt;- matrix(rnorm(n_sim * n_cols), nrow = n_sim, ncol = n_cols)\n  X &lt;- sweep(sqrt(dt) * (Z %*% chol(sigma)), 2, (mu - 0.5 * diag(sigma)) * dt, \"+\")\n  \n  result &lt;- sweep(apply(X, 2, function(x) exp(cumsum(x))), 2, S, \"*\")\n  \n  return(result)\n  \n}\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS &lt;- rep(1, length(factors))\nsigma &lt;- cov(returns_xts, use = \"complete.obs\") * scale[[\"periods\"]]\nmu &lt;- colMeans(na.omit(returns_xts)) * scale[[\"periods\"]]\nmu &lt;- mu + diag(sigma) / 2 # drift\ndt &lt;- 1 / scale[[\"periods\"]]\n\n\nmu_ls &lt;- list()\nsigma_ls &lt;- list()\n\n\nfor (i in 1:1e4) {\n  \n  # assumes stock prices\n  levels_sim &lt;- sim_multi_gbm(width + 1, S, mu, sigma, dt)\n  returns_sim &lt;- diff(log(levels_sim))\n\n  mu_sim &lt;- colMeans(returns_sim) * scale[[\"periods\"]]\n  sigma_sim &lt;- apply(returns_sim, 2, sd) * sqrt(scale[[\"periods\"]])\n  \n  mu_ls &lt;- append(mu_ls, list(mu_sim))\n  sigma_ls &lt;- append(sigma_ls, list(sigma_sim))\n  \n}\n\n\ndata.frame(\n  \"empirical\" = colMeans(na.omit(returns_xts)) * scale[[\"periods\"]],\n  \"theoretical\" = colMeans(do.call(rbind, mu_ls)\n))\n\n                 empirical   theoretical\nSP500         0.1182866969  0.1192013486\nDTWEXAFEGS   -0.0054335166 -0.0049268087\nDGS10        -0.0005403087 -0.0005663181\nBAMLH0A0HYM2  0.0040523156  0.0039961740\n\n\n\ndata.frame(\n  \"empirical\" = sqrt(diag(sigma)),\n  \"theoretical\" = colMeans(do.call(rbind, sigma_ls))\n)\n\n               empirical theoretical\nSP500        0.183914917 0.183748752\nDTWEXAFEGS   0.061168097 0.061124763\nDGS10        0.008470421 0.008464328\nBAMLH0A0HYM2 0.016957011 0.016941305\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility",
    "href": "posts/securities-r/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\nimplied_vol_newton &lt;- function(params, type, S, K, r, q, tau) {\n  \n  target0 &lt;- 0\n  sigma &lt;- params[[\"sigma\"]]\n  sigma0 &lt;- sigma\n  \n  while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma0)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma0)\n    \n    target0 &lt;- bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n    d_target0 &lt;- bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n    \n    sigma &lt;- sigma0 - (target0 - params[[\"target\"]]) / d_target0\n    sigma0 &lt;- sigma\n    \n  }\n  \n  return(sigma)\n  \n}\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS &lt;- zoo::coredata(zoo::na.locf(levels_xts)[nrow(levels_xts), factor])\nK &lt;- S # * (1 + 0.05)\nsigma &lt;- zoo::coredata(sd_xts[nrow(sd_xts), factor]) # overrides matrix\nstart1 &lt;- 0.2\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\ntarget1 &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams1 &lt;- list(\n  \"target\" = target1,\n  \"sigma\" = start1,\n  \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nimplied_vol_newton(params1, type, S, K, r, q, tau)\n\n         SP500\n[1,] 0.1874257"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity",
    "href": "posts/securities-r/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\nyld_newton &lt;- function(params, cash_flows) {\n  \n  target0 &lt;- 0\n  yld &lt;- params[[\"cpn\"]]\n  yld0 &lt;- yld\n  \n  while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n    \n  target0 &lt;- 0\n  d_target0 &lt;- 0\n  dd_target0 &lt;- 0\n  \n  for (i in 1:length(cash_flows)) {\n    \n    t &lt;- i\n    \n    # present value of cash flows\n    target0 &lt;- target0 + cash_flows[i] / (1 + yld0) ^ t\n    \n    # first derivative of present value of cash flows\n    d_target0 &lt;- d_target0 - t * cash_flows[i] / (1 + yld0) ^ (t + 1) # use t for Macaulay duration\n    \n    # second derivative of present value of cash flows\n    dd_target0 &lt;- dd_target0 - t * (t + 1) * cash_flows[i] / (1 + yld0) ^ (t + 2)\n    \n  }\n  \n  yld &lt;- yld0 - (target0 - params[[\"target\"]]) / d_target0\n  yld0 &lt;- yld\n    \n  }\n  \n  result &lt;- list(\n    \"price\" = target0,\n    \"yield\" = yld * params[[\"freq\"]],\n    \"duration\" = -d_target0 / params[[\"target\"]] / params[[\"freq\"]],\n    \"convexity\" = -dd_target0 / params[[\"target\"]] / params[[\"freq\"]] ^ 2\n  )\n  \n  return(result)\n  \n}\n\n\nhttps://www.bloomberg.com/markets/rates-bonds/government-bonds/us\nhttps://quant.stackexchange.com/a/61025\nhttps://pages.stern.nyu.edu/~igiddy/spreadsheets/duration-convexity.xls\n\n\ntarget2 &lt;- 0.9928 * 1000 # present value\nstart2 &lt;- 0.0438 # coupon\ncash_flows &lt;- rep(start2 * 1000 / 2, 10 * 2)\ncash_flows[10 * 2] &lt;- cash_flows[10 * 2] + 1000\n\n\nparams2 &lt;- list(\n  \"target\" = target2,\n  \"cpn\" = start2,\n  \"freq\" = 2,\n  \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nt(yld_newton(params2, cash_flows))\n\n     price yield      duration convexity\n[1,] 992.8 0.04470076 8.016596 76.6811"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility-1",
    "href": "posts/securities-r/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\nimplied_vol_obj &lt;- function(param, type, S, K, r, q, tau, target) {\n  \n  d1 &lt;- bs_d1(S, K, r, q, tau, param)\n  d2 &lt;- bs_d2(S, K, r, q, tau, param)\n  target0 &lt;- bs_value(type, S, K, r, q, tau, param, d1, d2)\n  \n  result &lt;- abs(target0 - target)\n  \n  return(result)\n    \n}\n\nimplied_vol_optim &lt;- function(param, type, S, K, r, q, tau, target) {\n  \n  result &lt;- optim(param, implied_vol_obj, type = type, S = S, K = K, r = r, q = q,\n                  tau = tau, target = target, method = \"Brent\", lower = 0, upper = 1)\n  \n  return(result$par)\n    \n}\n\n\nimplied_vol_optim(start1, type, S, K, r, q, tau, target1)\n\n[1] 0.1874257"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity-1",
    "href": "posts/securities-r/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\nyld_obj &lt;- function(param, cash_flows, target) {\n  \n  target0 &lt;- 0\n      \n  for (i in 1:length(cash_flows)) {\n    target0 &lt;- target0 + cash_flows[i] / (1 + param) ^ i\n  }\n\n  result &lt;- abs(target0 - target)\n  \n  return(result)\n    \n}\n\nyld_optim &lt;- function(params, cash_flows, target) {\n  \n  result &lt;- optim(params[[\"cpn\"]], yld_obj, target = target, cash_flows = cash_flows,\n                  method = \"Brent\", lower = 0, upper = 1)\n  \n  return(result$par * params[[\"freq\"]])\n    \n}\n\n\nyld_optim(params2, cash_flows, target2)\n\n[1] 0.04470077"
  },
  {
    "objectID": "posts/securities-py/index.html",
    "href": "posts/securities-py/index.html",
    "title": "Securities",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/securities-py/index.html#value",
    "href": "posts/securities-py/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n  \n  if (type == \"call\"):\n    result =  S * np.exp(-q * tau) * Phi(d1) - np.exp(-r * tau) * K * Phi(d2)\n  elif (type == \"put\"):\n    result = np.exp(-r * tau) * K * Phi(-d2) - S * np.exp(-q * tau) * Phi(-d1)\n      \n  return result\n\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n  \n  r_df = np.exp(-r * tau)\n  q_df = np.exp(-q * tau)\n  \n  call_value = S * q_df * Phi(d1) - r_df * K * Phi(d2)\n  put_value = r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n  result = np.where(type == \"call\", call_value, put_value)\n  \n  return result\n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\ndef bs_d1(S, K, r, q, tau, sigma):\n    \n  result = (np.log(S / K) + (r - q + sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n  \n  return result\n\ndef bs_d2(S, K, r, q, tau, sigma):\n  \n  result = (np.log(S / K) + (r - q - sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n  \n  return result\n    \ndef phi(x):\n  \n  result = norm.pdf(x)\n  \n  return result\n\ndef Phi(x):\n  \n  result = norm.cdf(x)\n  \n  return result\n\n\ngreeks_df[\"d1\"] = bs_d1(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"d2\"] = bs_d2(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"value\"] = bs_value(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order",
    "href": "posts/securities-py/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_delta(type, S, K, r, q, tau, sigma, d1, d2):\n\n  q_df = np.exp(-q * tau)\n  \n  call_value = q_df * Phi(d1)\n  put_value = -q_df * Phi(-d1)\n  result = np.where(type == \"call\", call_value, put_value)\n  \n  return result\n\n\ngreeks_df[\"delta\"] = bs_delta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\nDelta-beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\ndef bs_delta_diff(type, S, K, r, q, tau, sigma, delta0):\n  \n  d1 = bs_d1(S, K, r, q, tau, sigma)\n  d2 = bs_d2(S, K, r, q, tau, sigma)\n  delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n  \n  call_value = delta - delta0\n  put_value = delta0 - delta\n  \n  result = np.where(type == \"call\", call_value, put_value)\n      \n  return result\n\n\nbeta = 0.35\ntype = \"call\"\nn = 1\nmultiple = 100\ntotal = 1000000\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\nsec = {\n  \"n\": n,\n  \"multiple\": multiple,\n  \"S\": S,\n  \"delta\": bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n  \"beta\": 1\n}\n\n\nbeta_df = pd.DataFrame([(x, y) for x in types for y in shocks], \n  columns = [\"type\", \"shock\"])\nbeta_df[\"spot\"] = level_shock(beta_df[\"shock\"], S, tau, sigma)\nbeta_df[\"static\"] = beta\nbeta_df[\"diff\"] = bs_delta_diff(type, beta_df[\"spot\"], K, r, q, tau, sigma, sec[\"delta\"])\nbeta_df[\"dynamic\"] = beta + sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * sec[\"beta\"] * beta_df[\"diff\"] / total\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\ndef bs_vega(type, S, K, r, q, tau, sigma, d1, d2):\n  \n  q_df = np.exp(-q * tau)\n  result = S * q_df * phi(d1) * np.sqrt(tau)\n  \n  return result\n\n\ngreeks_df[\"vega\"] = bs_vega(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                            greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_theta(type, S, K, r, q, tau, sigma, d1, d2):\n  \n  r_df = np.exp(-r * tau)\n  q_df = np.exp(-q * tau)\n  \n  call_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) - \\\n    r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n      \n  put_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) + \\\n    r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n      \n  result = np.where(type == \"call\", call_value, put_value)\n  \n  return result\n\n\ngreeks_df[\"theta\"] = bs_theta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order",
    "href": "posts/securities-py/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\ndef bs_gamma(type, S, K, r, q, tau, sigma, d1, d2):\n\n  q_df = np.exp(-q * tau)\n  \n  result = q_df * phi(d1) / (S * sigma * np.sqrt(tau))\n  \n  return result\n\n\ngreeks_df[\"gamma\"] = bs_gamma(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order-1",
    "href": "posts/securities-py/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\n\ndef pnl_bond(duration, convexity, dy):\n  \n  duration_pnl = -duration * dy\n  convexity_pnl = (convexity / 2) * dy ** 2\n  income_pnl = dy\n  \n  result = pd.DataFrame({\n    \"total\": duration_pnl + convexity_pnl + income_pnl,\n    \"duration\": duration_pnl,\n    \"convexity\": convexity_pnl,\n    \"income\": income_pnl\n  })\n  \n  return result\n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor = \"DGS10\"\nduration = 6.5\nconvexity = 0.65\ny = levels_df.ffill()[factor].iloc[-width]\n\n\nbond_df = pd.DataFrame({\n  \"duration\": duration,\n  \"convexity\": convexity,\n  \"dy\": (levels_df.ffill()[factor].iloc[-width:] - y) / 100\n})\n\n\nattrib_df = pnl_bond(bond_df[\"duration\"], bond_df[\"convexity\"], bond_df[\"dy\"])\n\n\n\n\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=\\frac{\\partial D}{\\partial y}=\\frac{\\partial}{\\partial y}\\left(-\\frac{1}{P}\\frac{\\partial D}{\\partial y}\\right)\\\\\n&=-\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}+\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=-C+D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(-C\\times 100+D^{2}\\right)=-C+\\frac{D^{2}}{100}\\). For example, if convexity and yield are percent then \\(\\text{Drift}=\\left(-0.65+\\frac{6.5^{2}}{100}\\right)\\partial y\\times100\\) or basis points then \\(\\text{Drift}=\\left(-65+6.5^{2}\\right)\\partial y\\).\n\ndef yield_shock(shock, tau, sigma):\n  \n  result = shock * sigma * np.sqrt(tau)\n  \n  return result\n\n\ndef duration_drift(duration, convexity, dy):\n  \n  drift = -convexity + duration ** 2 / 100\n  change = drift * dy * 100\n  \n  result = {\n    \"drift\": drift,\n    \"change\": change\n  }\n  \n  return result\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor = \"DGS10\"\nsigma = sd_df[factor].iloc[-1]\n\n\nduration_df = pd.DataFrame(shocks).rename(columns = {0: \"shock\"})\nduration_df[\"spot\"] = yield_shock(duration_df[\"shock\"], tau, sigma)\nduration_df[\"static\"] = duration\nduration_df[\"dynamic\"] = duration + \\\n  duration_drift(duration, convexity, duration_df[\"spot\"])[\"change\"]"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order-1",
    "href": "posts/securities-py/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlack’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\ndef pnl_option(type, S, K, r, q, tau, sigma, dS, dt, dsigma):\n  \n  d1 = bs_d1(S, K, r, q, tau, sigma)\n  d2 = bs_d2(S, K, r, q, tau, sigma)\n  value = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n  delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n  vega = bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n  theta = bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n  gamma = bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n  \n  delta_pnl = delta * dS / value\n  gamma_pnl = gamma / 2 * dS ** 2 / value\n  vega_pnl = vega * dsigma / value\n  theta_pnl = theta * dt / value\n  \n  result = pd.DataFrame({\n    \"total\": delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n    \"delta\": delta_pnl,\n    \"gamma\": gamma_pnl,\n    \"vega\": vega_pnl,\n    \"theta\": theta_pnl\n  })\n  \n  return result\n\n\nfactor = \"SP500\"\ntype = \"call\"\nS = levels_df.ffill()[factor].iloc[-width]\nK = S # * (1 + 0.05)\ntau = 1 # = 252 / 252\nsigma = sd_df[factor].iloc[-width]\n\n\noptions_df = pd.DataFrame({\n  \"spot\": levels_df.ffill()[factor].iloc[-width:],\n  \"sigma\": sd_df[factor].iloc[-width:]\n})\noptions_df[\"dS\"] = options_df[\"spot\"] - S\noptions_df[\"dt_diff\"] = (options_df.index - options_df.index[0]).days\noptions_df[\"dt\"] = options_df[\"dt_diff\"] / options_df[\"dt_diff\"].iloc[-1]\noptions_df[\"dsigma\"] = options_df[\"sigma\"] - sigma\n\n\nattrib_df = pnl_option(type, S, K, r, q, tau, sigma,\n                       options_df[\"dS\"], options_df[\"dt\"], options_df[\"dsigma\"])\n\n\n\n\n\n\n\n\n\n\n\n\nIto’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\ndef sim_gbm(n_sim, S, mu, sigma, dt):\n  \n  result = S * np.exp(np.cumsum(sigma * np.sqrt(dt) * np.random.normal(size = n_sim)) + \\\n                      (mu - 0.5 * sigma ** 2) * dt)\n  \n  return result\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\ndef sim_multi_gbm(n_sim, S, mu, sigma, dt):\n  \n  n_cols = sigma.shape[1]\n  \n  Z = np.random.normal(size = n_sim * n_cols).reshape((n_sim, n_cols))\n  X = np.sqrt(dt) * Z @ np.linalg.cholesky(sigma).T + (mu - 0.5 * np.diag(sigma)) * dt\n  \n  result = S * np.exp(X.cumsum(axis = 0))\n  \n  return result\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS = [1] * len(factors)\nsigma = np.cov(returns_df.dropna().T, ddof = 1) * scale[\"periods\"]\nmu = np.array(returns_df.dropna().mean()) * scale[\"periods\"]\nmu = mu + np.diag(sigma) / 2 # drift\ndt = 1 / scale[\"periods\"]\n\n\nmu_ls = []\nsigma_ls = []\n\n\nfor i in range(10000): # \"TypeError: 'float' object cannot be interpreted as an integer\"\n\n  # assumes underlying stock price follows geometric Brownian motion with constant volatility\n  levels_sim = pd.DataFrame(sim_multi_gbm(width + 1, S, mu, sigma, dt))\n  returns_sim = np.log(levels_sim).diff()\n\n  mu_sim = returns_sim.mean() * scale[\"periods\"]\n  sigma_sim = returns_sim.std() * np.sqrt(scale[\"periods\"])\n\n  mu_ls.append(mu_sim)\n  sigma_ls.append(sigma_sim)\n\n\nmu_df = pd.DataFrame(mu_ls)\nsigma_df = pd.DataFrame(sigma_ls)\n\n\npd.DataFrame({\n  \"empirical\": np.array(returns_df.dropna().mean()) * scale[\"periods\"],\n  \"theoretical\": mu_df.mean()\n})\n\n   empirical  theoretical\n0   0.122062     0.121060\n1  -0.006109    -0.005529\n2  -0.000486    -0.000410\n3   0.004276     0.004100\n\n\n\npd.DataFrame({\n  \"empirical\": np.sqrt(np.diag(sigma)),\n  \"theoretical\": sigma_df.mean()\n})\n\n   empirical  theoretical\n0   0.183927     0.183697\n1   0.061157     0.061101\n2   0.008468     0.008457\n3   0.016954     0.016943\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility",
    "href": "posts/securities-py/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\ndef implied_vol_newton(params, type, S, K, r, q, tau):\n  \n  target0 = 0\n  sigma = params[\"sigma\"]\n  sigma0 = sigma\n  \n  while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n      \n    d1 = bs_d1(S, K, r, q, tau, sigma0)\n    d2 = bs_d2(S, K, r, q, tau, sigma0)\n    \n    target0 = bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n    d_target0 = bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n    \n    sigma = sigma0 - (target0 - params[\"target\"]) / d_target0\n    sigma0 = sigma\n      \n  return sigma\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS = levels_df.ffill()[factor].iloc[-1]\nK = S # * (1 + 0.05)\nsigma = sd_df[factor].iloc[-1] # overrides matrix\nstart1 = 0.2\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\ntarget1 = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams1 = {\n  \"target\": target1,\n  \"sigma\": start1,\n  \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nimplied_vol_newton(params1, type, S, K, r, q, tau) \n\nnp.float64(0.19094186709138591)"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity",
    "href": "posts/securities-py/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\ndef yld_newton(params, cash_flows):\n  \n  target0 = 0\n  yld0 = params[\"cpn\"] / params[\"freq\"]\n  yld = yld0 # assignment to `yield` variable is not possible\n  \n  while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n      \n    target0 = 0\n    d_target0 = 0\n    dd_target0 = 0\n    \n    for i in range(len(cash_flows)):\n      \n      t = i + 1\n    \n      # present value of cash flows\n      target0 += cash_flows[i] / (1 + yld0) ** t\n      \n      # first derivative of present value of cash flows\n      d_target0 -= t * cash_flows[i] / (1 + yld0) ** (t + 1) # use t for Macaulay duration\n      \n      # second derivative of present value of cash flows\n      dd_target0 -= t * (t + 1) * cash_flows[i] / (1 + yld0) ** (t + 2)\n    \n    yld = yld0 - (target0 - params[\"target\"]) / d_target0\n    yld0 = yld\n      \n  result = {\n    \"price\": target0,\n    \"yield\": yld * params[\"freq\"],\n    \"duration\": -d_target0 / params[\"target\"] / params[\"freq\"],\n    \"convexity\": -dd_target0 / params[\"target\"] / params[\"freq\"] ** 2\n  }\n      \n  return result\n\n\nhttps://www.bloomberg.com/markets/rates-bonds/government-bonds/us\nhttps://quant.stackexchange.com/a/61025\nhttps://pages.stern.nyu.edu/~igiddy/spreadsheets/duration-convexity.xls\n\n\ntarget2 = 0.9928 * 1000 # present value\nstart2 = 0.0438 # coupon\ncash_flows = [start2 * 1000 / 2] * 10 * 2\ncash_flows[-1] += 1000\n\n\nparams2 = {\n  \"target\": target2,\n  \"cpn\": start2,\n  \"freq\": 2,\n  \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nyld_newton(params2, cash_flows)\n\n{'price': 992.8000005704454, 'yield': 0.044700757710159994, 'duration': 8.0165959605043, 'convexity': 76.68109754287481}"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility-1",
    "href": "posts/securities-py/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\ndef implied_vol_obj(param, type, S, K, r, q, tau, target):\n  \n  d1 = bs_d1(S, K, r, q, tau, param)\n  d2 = bs_d2(S, K, r, q, tau, param)\n  target0 = bs_value(type, S, K, r, q, tau, param, d1, d2)\n  \n  result = abs(target0 - target)\n  \n  return result\n\ndef implied_vol_optim(param, type, S, K, r, q, tau, target):\n  \n  result = minimize(implied_vol_obj, param, args = (type, S, K, r, q, tau, target))\n  \n  return np.float64(result.x.item())\n\n\nimplied_vol_optim(start1, type, S, K, r, q, tau, target1)\n\nnp.float64(0.1909418609673873)"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity-1",
    "href": "posts/securities-py/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity\n\ndef yld_obj(param, cash_flows, target):\n  \n  target0 = 0\n  \n  for i in range(len(cash_flows)):\n      \n    target0 += cash_flows[i] / (1 + param) ** (i + 1)\n  \n  target0 = abs(target0 - target)\n  \n  return target0\n  \ndef yld_optim(params, cash_flows, target):\n  \n  result = minimize(yld_obj, params[\"cpn\"], args = (cash_flows, target))\n  \n  return np.float64(result.x.item()) * params[\"freq\"]\n\n\nyld_optim(params2, cash_flows, target2)\n\nnp.float64(0.04470075001603727)"
  },
  {
    "objectID": "posts/statistics/index.html",
    "href": "posts/statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Usage\n\noptions(microbenchmark.unit = \"us\")\n\n\nn_vars &lt;- 10\nn_obs &lt;- 1000\nweights &lt;- 0.9 ^ (n_obs:1)\n\nx &lt;- matrix(rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)\ny &lt;- matrix(rnorm(n_obs), nrow = n_obs, ncol = 1)\nx_lgl &lt;- x &lt; 0\n\n\n\nRolling any\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_any(x_lgl, width = 125, min_obs = 1),\n  \"250\" = roll::roll_any(x_lgl, width = 250, min_obs = 1),\n  \"500\" = roll::roll_any(x_lgl, width = 500, min_obs = 1),\n  \"1000\" = roll::roll_any(x_lgl, width = 1000, min_obs = 1)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq     max neval\n  125 122.1 139.45 150.118 143.70 156.30   322.3   100\n  250 131.0 138.35 885.063 148.20 162.15 73523.7   100\n  500 128.6 135.25 149.147 143.65 160.35   191.1   100\n 1000 120.3 127.70 143.784 134.30 156.20   367.0   100\n\n\n\n\nRolling all\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_all(x_lgl, width = 125, min_obs = 1),\n  \"250\" = roll::roll_all(x_lgl, width = 250, min_obs = 1),\n  \"500\" = roll::roll_all(x_lgl, width = 500, min_obs = 1),\n  \"1000\" = roll::roll_all(x_lgl, width = 1000, min_obs = 1)\n)\nprint(result)\n\nUnit: microseconds\n expr  min    lq    mean median     uq   max neval\n  125 82.4 90.15 114.051  94.70 134.00 228.1   100\n  250 80.6 88.95 110.066  94.95 134.35 216.0   100\n  500 75.7 85.60 108.501  89.40 135.15 292.7   100\n 1000 72.8 78.70  94.912  82.30 106.70 181.8   100\n\n\n\n\nRolling sums\n\\[\n\\begin{aligned}\n&\\text{Expanding window} \\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_sum(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_sum(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_sum(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_sum(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr  min     lq    mean median     uq   max neval\n  125 95.5 107.60 127.335 114.25 130.00 278.5   100\n  250 94.2 106.95 130.752 113.75 136.90 328.1   100\n  500 95.3 106.80 132.233 117.80 148.15 296.6   100\n 1000 91.6 101.80 127.463 109.20 138.10 291.1   100\n\n\n\n\nRolling products\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{prod}_{w}\\leftarrow\\text{prod}_{w}\\times\\text{w}_{new}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}/\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_prod(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_prod(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_prod(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_prod(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min    lq    mean median     uq   max neval\n  125 207.2 219.7 236.949 228.60 241.55 374.9   100\n  250 207.8 224.7 240.577 236.35 247.40 350.6   100\n  500 136.6 154.1 175.491 165.55 180.55 388.8   100\n 1000 135.6 155.0 178.438 165.30 176.85 557.2   100\n\n\n\n\nRolling means\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times \\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_mean(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_mean(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_mean(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_mean(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125  99.3 107.25 133.575 113.50 147.65 305.0   100\n  250 101.8 109.90 134.233 115.55 149.90 268.7   100\n  500  99.2 106.25 128.771 112.30 143.80 246.4   100\n 1000  96.5 102.70 129.033 115.45 143.80 217.0   100\n\n\n\n\nRolling minimums\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_min(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_min(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_min(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_min(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min    lq    mean median     uq   max neval\n  125 131.5 147.0 167.555 154.40 180.05 268.4   100\n  250 117.8 145.8 163.208 154.75 167.00 304.5   100\n  500 139.9 149.8 173.730 159.40 184.25 366.6   100\n 1000 134.7 147.1 181.030 162.50 194.20 911.0   100\n\n\n\n\nRolling maximums\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_max(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_max(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_max(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_max(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 102.6 115.70 131.797 123.20 134.20 233.4   100\n  250 101.7 114.05 136.393 122.15 137.10 256.8   100\n  500 105.0 113.95 133.193 121.95 135.75 332.2   100\n 1000 101.4 111.05 130.468 117.40 136.45 261.7   100\n\n\n\n\nRolling index of minimums\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_idxmin(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_idxmin(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_idxmin(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_idxmin(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 101.0 116.00 132.410 130.55 142.85 301.5   100\n  250 106.5 120.05 138.459 132.95 148.00 320.5   100\n  500  98.8 119.50 136.189 133.15 144.40 333.5   100\n 1000  97.2 116.05 131.241 128.35 141.80 194.9   100\n\n\n\n\nRolling index of maximums\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_idxmax(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_idxmax(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_idxmax(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_idxmax(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 103.2 111.45 129.493 118.15 137.95 241.2   100\n  250 100.1 111.30 130.007 116.80 132.60 274.4   100\n  500 103.8 111.30 130.981 117.00 134.80 346.0   100\n 1000  97.0 110.70 133.250 117.05 148.35 274.4   100\n\n\n\n\nRolling medians\n\n# \"'online' is only supported for equal 'weights'\"\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_median(x, width = 125, min_obs = 1),\n  \"250\" = roll::roll_median(x, width = 250, min_obs = 1),\n  \"500\" = roll::roll_median(x, width = 500, min_obs = 1),\n  \"1000\" = roll::roll_median(x, width = 1000, min_obs = 1)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 746.3 807.45 910.488 832.10 866.85 7271.2   100\n  250 740.6 810.15 851.863 834.15 924.65  963.7   100\n  500 679.8 735.50 800.554 761.80 790.10 3382.7   100\n 1000 518.2 566.25 598.112 589.10 610.20 1407.7   100\n\n\n\n\nRolling quantiles\n\n# \"'online' is only supported for equal 'weights'\"\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_quantile(x, width = 125, min_obs = 1),\n  \"250\" = roll::roll_quantile(x, width = 250, min_obs = 1),\n  \"500\" = roll::roll_quantile(x, width = 500, min_obs = 1),\n  \"1000\" = roll::roll_quantile(x, width = 1000, min_obs = 1)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq    max neval\n  125 788.7 820.50 849.449 834.00 845.80 1378.2   100\n  250 779.8 806.10 829.049 821.20 837.90 1052.1   100\n  500 728.5 751.65 774.702 766.60 783.90 1007.6   100\n 1000 552.5 583.80 604.054 598.05 612.75  783.1   100\n\n\n\n\nRolling variances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{x}_{old}-\\text{mean}_{prev_x})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_var(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_var(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_var(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_var(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 118.9 137.50 154.914 147.30 156.65 290.7   100\n  250 117.5 135.75 151.086 148.65 155.30 298.6   100\n  500 120.0 132.40 150.285 143.05 151.80 373.9   100\n 1000 108.1 125.20 138.725 134.70 143.15 275.0   100\n\n\n\n\nRolling standard deviations\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_sd(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_sd(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_sd(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_sd(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 122.1 142.80 161.970 153.45 164.25 326.4   100\n  250 132.7 141.90 160.125 151.60 165.45 296.8   100\n  500 129.9 137.45 152.858 146.75 158.05 313.0   100\n 1000 109.0 132.35 156.075 142.70 159.25 406.1   100\n\n\n\n\nRolling scaling and centering\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_scale(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_scale(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_scale(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_scale(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median    uq   max neval\n  125 145.1 152.90 178.613 164.30 191.1 569.6   100\n  250 145.1 150.90 176.003 159.40 178.7 642.6   100\n  500 141.7 148.50 173.040 162.25 190.2 351.5   100\n 1000 135.0 141.05 161.846 154.95 168.9 280.1   100\n\n\n\n\nRolling covariances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{y}_{old}-\\text{mean}_{prev_y})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_cov(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_cov(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_cov(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_cov(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min     lq     mean median      uq    max neval\n  125 742.8 911.80 1038.197 972.50 1036.05 7535.8   100\n  250 672.6 855.65 1017.200 935.00 1020.50 6087.4   100\n  500 653.2 836.15  956.936 905.35  969.30 6041.5   100\n 1000 564.3 732.95  823.702 762.20  813.55 6017.8   100\n\n\n\n\nRolling correlations\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_cor(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_cor(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_cor(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_cor(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min      lq     mean  median      uq    max neval\n  125 861.5 1029.60 1078.781 1077.90 1126.75 1490.8   100\n  250 774.2  980.30 1126.301 1064.40 1147.30 4535.5   100\n  500 739.7  895.50 1000.387  974.70 1042.45 4175.1   100\n 1000 641.4  772.45  909.451  828.05  865.25 4145.3   100\n\n\n\n\nRolling crossproducts\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_crossprod(x, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_crossprod(x, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_crossprod(x, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_crossprod(x, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr   min    lq    mean median     uq    max neval\n  125 551.4 624.5 678.036 675.05 711.45 1168.1   100\n  250 547.9 605.9 755.236 659.45 717.90 3993.3   100\n  500 531.5 579.8 737.054 635.25 680.05 3927.9   100\n 1000 501.6 537.8 623.640 562.30 618.70 3822.3   100\n\n\n\n\nRolling linear models\n\\[\n\\begin{aligned}\n&\\text{coef}=\\text{cov}_{xx}^{-1}\\times\\text{cov}_{xy}\\\\\n&\\text{intercept}=\\text{mean}_{y}-\\text{coef}\\times\\text{mean}_{x}\\\\\n&\\text{rsq}=\\frac{\\text{coef}^{T}\\times\\text{cov}_{xx}\\times\\text{coef}}{\\text{var}_{y}}\\\\\n&\\text{var}_{resid}=\\frac{(1-\\text{rsq})(\\text{var}_{y})(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})}{\\text{n}_{rows}-\\text{n}_{cols}}\\\\\n&\\text{xx}=\\text{cov}_{xx}\\times(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})\\\\\n&\\text{se}_{coef}=\\sqrt{\\text{var}_{resid}\\times\\text{diag}(\\text{xx}^{-1})}\\\\\n&\\text{se}_{intercept}=\\sqrt{\\text{var}_{resid}\\left(1/\\text{sum}_{w}+\\text{mean}_{x}^{T}\\text{xx}^{-1}\\text{mean}_{x}\\right)}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark::microbenchmark(\n  \"125\" = roll::roll_lm(x, y, width = 125, min_obs = 1, weights = weights),\n  \"250\" = roll::roll_lm(x, y, width = 250, min_obs = 1, weights = weights),\n  \"500\" = roll::roll_lm(x, y, width = 500, min_obs = 1, weights = weights),\n  \"1000\" = roll::roll_lm(x, y, width = 1000, min_obs = 1, weights = weights)\n)\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq    max neval\n  125 2140.9 2342.15 2510.482 2432.95 2549.65 4350.2   100\n  250 2078.6 2328.70 2521.667 2442.40 2631.50 3821.7   100\n  500 2092.6 2276.50 2448.099 2384.40 2523.95 3547.1   100\n 1000 2030.9 2235.15 2496.523 2331.20 2457.30 6381.6   100\n\n\n\n\nReferences\n\nWeights: https://stackoverflow.com/a/9933794\nIndex: https://stackoverflow.com/a/11316626\nIndex: https://stackoverflow.com/a/34363187\nIndex: https://stackoverflow.com/a/243342\nQuantile (comparator): https://stackoverflow.com/a/51992954\nQuantile (comparator): https://stackoverflow.com/a/25921772\nQuantile (comparator): https://stackoverflow.com/a/40416506\nMedian: https://stackoverflow.com/a/5970314\nMedian: https://stackoverflow.com/a/5971248\nMedian: https://gist.github.com/ashelly/5665911\nStandard errors: https://stats.stackexchange.com/a/64217"
  },
  {
    "objectID": "posts/crowds-py/crowds-py.html",
    "href": "posts/crowds-py/crowds-py.html",
    "title": "Crowds",
    "section": "",
    "text": "import requests\nfrom lxml import html\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\n\n\nfactors_r = [\"SP500\"] # \"SP500\" does not contain dividends\nfactors_d = [\"SOFR\"]\nfactors = factors_r + factors_d\nwidth = 20 * 3\nscale = {\"periods\": 252, \"overlap\": 5}\n\n\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\n\n\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\n\n\nimport os\nimport cvxpy as cp\n\n\ndef get_nth(x, n, offset = 0):\n    \n    result = x[offset::n]\n    \n    return result\n\n\ndef get_text(x, n = 0):\n    \n    result_ls = []\n    \n    for i in x:\n      \n        if (len(i) == 0):\n            result_ls.append(i.text_content()) # types\n        else:\n            result_ls.append(i[n].text_content()) # names and tickers\n    \n    return result_ls\n\n\ndef get_mstar():\n    \n    i = 0\n    status = True\n    names_ls = []\n    tickers_ls = []\n    types_ls = []\n\n    while status:\n\n        i += 1\n\n        url = \"https://www.morningstar.com/asset-allocation-funds?page=\" + str(i)\n        response = requests.get(url)\n        tree = html.fromstring(response.content)\n\n        table = tree.xpath(\"//div[@class='topic__table-container']\")\n\n        if (len(table) == 0):\n            status = False\n        else:\n\n            names_tickers = tree.xpath(\"//a[@class='mdc-link mds-link mds-link--data-table mdc-link--no-visited']\")\n            types = tree.xpath(\"//span[@class='mdc-data-point mdc-data-point--string mdc-string']\")\n            \n        names_ls.extend(get_text(get_nth(names_tickers, 2)))\n        tickers_ls.extend(get_text(get_nth(names_tickers, 2, 1)))\n        types_ls.extend(get_text(get_nth(types, 5, 2)))\n\n    result = pd.DataFrame({\n      \"name\": names_ls,\n      \"ticker\": tickers_ls,\n      \"type\": types_ls\n    })\n    \n    return result\n\n\nmstar_df = get_mstar()\n\n\ntickers = mstar_df.loc[mstar_df[\"type\"] == \"Tactical Allocation\", \"ticker\"].tolist()\nprices_df = pdr.get_data_tiingo(tickers, start = \"1900-01-01\", api_key = os.getenv(\"TIINGO_API_KEY\"))\nprices_df = prices_df.pivot_table(index = \"date\", columns = \"symbol\", values = \"adjClose\") \\\n    .tz_localize(None)\nprices_df.sort_index(axis = 0, inplace = True)\ntickers = prices_df.columns\n\n\nreturns_cols = [(\"returns\", i) for i in tickers]\noverlap_cols = [(\"overlap\", i) for i in tickers]\nreturns_df[returns_cols] = np.log(prices_df).diff()\nreturns_df[overlap_cols] = returns_df[returns_cols].rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df.sort_index(axis = 1, inplace = True)\n\n\noverlap_df = returns_df.dropna()[\"overlap\"]\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors]\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers]\n\n\ndef pnl(x):\n    return np.nanprod(1 + x) - 1\n\n\nOptimization\n\ndef min_rss_optim(x, y):\n    \n    w = cp.Variable(x.shape[1])\n    \n    objective = cp.Minimize(cp.sum_squares(y - x @ w))\n    \n    constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= 1]\n    \n    problem = cp.Problem(objective, constraints)\n    problem.solve()\n    \n    return w.value\n\n\nn_rows = overlap_df.shape[0]\nresult_ls = []\n\nfor i in range(width - 1, n_rows):\n  \n  idx = range(max(i - width + 1, 0), i + 1)\n  x_subset = overlap_x_df.iloc[idx]\n  y_subset = overlap_y_df.iloc[idx]\n  params_ls = []\n  \n  for j in tickers:\n  \n    params = min_rss_optim(x_subset.values, y_subset.loc[:, j].values)\n    params_ls.append(params)\n  \n  result_ls.append(np.mean(params_ls, axis = 0))\n\n\nposition_df = pd.DataFrame(result_ls, index = overlap_df.index[(width - 1):],\n                           columns = factors)\nposition_df.tail()\n\n\n\n\n\n\n\n\nSP500\nSOFR\n\n\nDATE\n\n\n\n\n\n\n2024-02-06\n0.648071\n0.351929\n\n\n2024-02-07\n0.650647\n0.349353\n\n\n2024-02-08\n0.653656\n0.346344\n\n\n2024-02-09\n0.659915\n0.340085\n\n\n2024-02-12\n0.671116\n0.328884"
  },
  {
    "objectID": "posts/risk-r/index.html#partial-least-squares",
    "href": "posts/risk-r/index.html#partial-least-squares",
    "title": "Risk",
    "section": "Partial least squares",
    "text": "Partial least squares"
  },
  {
    "objectID": "posts/blpapi/index.html",
    "href": "posts/blpapi/index.html",
    "title": "Securities",
    "section": "",
    "text": "# !pip install --index-url=https://blpapi.bloomberg.com/repository/releases/python/simple/ blpapi\nimport blpapi\nimport pandas as pd\n\n\nhttps://www.bloomberg.com/professional/support/api-library/\nPage 80: https://bloomberg.github.io/blpapi-docs/\n\n\nData point\n\ndef bdp(securities, fields, host = \"localhost\", port = 8194):\n  \n    # Initialize Bloomberg API session\n    options = blpapi.SessionOptions()\n    options.setServerHost(host)\n    options.setServerPort(port)\n    session = blpapi.Session(options)\n    \n    session.start()\n    session.openService(\"//blp/refdata\")\n    \n    service = session.getService(\"//blp/refdata\")\n    request = service.createRequest(\"ReferenceDataRequest\")\n\n    # Add securities and fields to the request\n    for security in securities:\n        request.append(\"securities\", security)\n    \n    for field in fields:\n        request.append(\"fields\", field)\n\n    # Send request\n    session.sendRequest(request)\n    \n    data_dict = dict.fromkeys([\"security\"] + fields)\n\n    while True:\n      \n        event = session.nextEvent()\n\n        if event.eventType() in [blpapi.Event.RESPONSE, blpapi.Event.PARTIAL_RESPONSE]:\n          \n            for msg in event:\n              \n                data = msg.getElement(\"securityData\")\n                data_dict[\"security\"] = [x.getElementAsString(\"security\") for x in data.values()]\n                data_ls = [x.getElement(\"fieldData\") for x in data.values()]\n\n                for field in fields:\n                    try:\n                        data_dict[field] = [x.getElement(field).getValue() for x in data_ls]\n                    except:\n                        data_dict[field] = [None] * len(data_ls)  # Handle missing values\n\n        if event.eventType() == blpapi.Event.RESPONSE:\n            break\n\n    return pd.DataFrame(data_dict)\n\n\nsecurities = [\"IBM US Equity\", \"GOOG US Equity\", \"MSFT US Equity\", \"BA US Equity\"]\nfields = [\"MARKET_SECTOR_DES\", \"GICS_SECTOR_NAME\", \"ID_CUSIP\", \"PX_LAST\"]\n\n\nbdp_df = bdp(securities, fields)\nprint(bdp_df)\n\n\n\nData point\n\ndef bdp(securities, fields, host = \"localhost\", port = 8194):\n  \n    # Start Bloomberg session\n    options = blpapi.SessionOptions()\n    options.setServerHost(host)\n    options.setServerPort(port)\n    session = blpapi.Session(options)\n    \n    session.start()\n    session.openService(\"//blp/refdata\")\n    \n    service = session.getService(\"//blp/refdata\")\n    request = service.createRequest(\"ReferenceDataRequest\")\n\n    # Add securities and fields to the request\n    for security in securities:\n        request.append(\"securities\", security)\n    for field in fields:\n        request.append(\"fields\", field)\n\n    # Send request\n    session.sendRequest(request)\n\n    # Initialize data storage\n    data_dict = {\"security\": []}\n    for field in fields:\n        data_dict[field] = []\n\n    while True:\n      \n        event = session.nextEvent()\n        \n        if event.eventType() in {blpapi.Event.RESPONSE, blpapi.Event.PARTIAL_RESPONSE}:\n          \n            for msg in event:\n              \n                for security_data in msg.getElement(\"securityData\").values():\n                  \n                    sec_name = security_data.getElementAsString(\"security\")\n                    data_dict[\"security\"].append(sec_name)\n\n                    field_data = security_data.getElement(\"fieldData\")\n                    \n                    for field in fields:\n                        try:\n                            data_dict[field].append(field_data.getElement(field).getValue())\n                        except:\n                            data_dict[field].append(None)  # Handle missing data\n\n        if event.eventType() == blpapi.Event.RESPONSE:\n            break\n\n    # Stop session\n    session.stop()\n\n    return pd.DataFrame(data_dict)\n\n\nbdp_df = bdp(securities, fields)\nprint(bdp_df)\n\n\n\nData history\n\ndef bdh(securities, fields, start_date, end_date, host = \"localhost\", port = 8194):\n  \n    # Initialize Bloomberg session\n    options = blpapi.SessionOptions()\n    options.setServerHost(host)\n    options.setServerPort(port)\n    session = blpapi.Session(options)\n\n    session.start()\n    session.openService(\"//blp/refdata\")\n\n    service = session.getService(\"//blp/refdata\")\n    request = service.createRequest(\"HistoricalDataRequest\")\n\n    # Add securities and fields to the request\n    for security in securities:\n        request.append(\"securities\", security)\n    for field in fields:\n        request.append(\"fields\", field)\n\n    # Set request parameters\n    request.set(\"startDate\", start_date)\n    request.set(\"endDate\", end_date)\n\n    # Send request\n    session.sendRequest(request)\n\n    # Initialize data storage\n    data_dict = dict.fromkeys(securities, dict.fromkeys([\"date\"] + fields))\n\n    while True:\n      \n        event = session.nextEvent()\n        \n        if event.eventType() in {blpapi.Event.RESPONSE, blpapi.Event.PARTIAL_RESPONSE}:\n          \n            for msg in event:\n              \n                data = msg.getElement(\"securityData\")\n                security = security_data.getElementAsString(\"security\").getValue()\n                security_dict = {\"date\": [x.getElementAsDatetime(\"date\") for x in data.getElement(\"fieldData\")]}\n\n                for field in fields:\n                    try:\n                        security_dict[field] = [x.getElement(field).getValue() for x in data.getElement(\"fieldData\")]\n                    except:\n                        passs\n\n        if event.eventType() == blpapi.Event.RESPONSE:\n            break\n    \n    result = {key: pd.DataFrame(value).set_index(\"date\") for key, value in data_dict.items()}\n    \n    return result\n\n\nfields = [\"PX_LAST\", \"PX_BID\", \"PX_ASK\"]\nstart_date = \"20231201\"\nend_date = \"20231205\"\n\n\nbdh_df = bdh(securities, fields, start_date, end_date)\nprint(bdh_df[0])\n\n\n\nData history\n\ndef bdh(securities, fields, start_date, end_date, host = \"localhost\", port = 8194):\n  \n    # Initialize Bloomberg session\n    options = blpapi.SessionOptions()\n    options.setServerHost(host)\n    options.setServerPort(port)\n    session = blpapi.Session(options)\n\n    session.start()\n    session.openService(\"//blp/refdata\")\n\n    service = session.getService(\"//blp/refdata\")\n    request = service.createRequest(\"HistoricalDataRequest\")\n\n    # Add securities and fields to the request\n    for security in securities:\n        request.append(\"securities\", security)\n    for field in fields:\n        request.append(\"fields\", field)\n\n    # Set request parameters\n    request.set(\"startDate\", start_date)\n    request.set(\"endDate\", end_date)\n\n    # Send request\n    session.sendRequest(request)\n\n    # Initialize data storage\n    data_dict = {security: {\"date\": [], **{field: [] for field in fields}} for security in securities}\n\n    while True:\n      \n        event = session.nextEvent()\n        \n        if event.eventType() in {blpapi.Event.RESPONSE, blpapi.Event.PARTIAL_RESPONSE}:\n          \n            for msg in event:\n              \n                data = msg.getElement(\"securityData\")\n                \n                for security_data in data.values():\n                  \n                    security = security_data.getElementAsString(\"security\")\n                    field_data = security_data.getElement(\"fieldData\")\n\n                    data_dict[security][\"date\"] = [x.getElementAsDatetime(\"date\") for x in field_data.values()]\n\n                    for field in fields:\n                        try:\n                            data_dict[security][field] = [x.getElement(field).getValue() for x in field_data.values()]\n                        except:\n                            data_dict[security][field] = [None] * len(data_dict[security][\"date\"])  # Handle missing values\n\n        if event.eventType() == blpapi.Event.RESPONSE:\n            break\n\n    session.stop()  # Properly close the Bloomberg session\n\n    # Convert results to Pandas DataFrame\n    return {sec: pd.DataFrame(data).set_index(\"date\") for sec, data in data_dict.items()}\n\n\nbdh_df = bdh(securities, fields, start_date, end_date)\nprint(bdh_df[0])"
  },
  {
    "objectID": "posts/cloud/index.html",
    "href": "posts/cloud/index.html",
    "title": "Cloud",
    "section": "",
    "text": "EC2 &gt; Network & Security &gt; Key Pairs &gt; Create Key Pair &gt; *.pem\nPuTTYgen &gt; Load an existing private key file &gt; Save private key &gt; *.ppk\nHost Name: ec2-user\\@ec2-1-23-456-789.us-east-2.compute.amazonaws.com\nConnection &gt; SSH &gt; Auth & WinSCP &gt; Auth &gt; Private key file for authentication\nsudo yum update\n\n\n\nInstall Amazon Linux 2\nwget https://repo.anaconda.com/archive/Anaconda3-2023.03-Linux-x86_64.sh\n\nbash Anaconda3-2023.03-Linux-x86_64.sh\nThen close and re-open current shell\n\nhttps://pypi.org/project/ipython/#history\nhttps://stackoverflow.com/a/76521018\nhttps://stackoverflow.com/a/77117477\n\npip install 'ipython==7.34.0' --force-reinstall\nipython\nfrom IPython.lib import passwd\npasswd(\"12345\")\n# sha1:asdfasdf:asfasdfasdf\nexit()\njupyter notebook --generate-config \nvi ~/.jupyter/jupyter_notebook_config.py\n\n# [i]\nc = get_config()  # Get the config object.\n# c.NotebookApp.certfile = u'/home/ubuntu/ssl/cert.pem' # path to the certificate we generated\n# c.NotebookApp.keyfile = u'/home/ubuntu/ssl/cert.key' # path to the certificate key we generated\nc.NotebookApp.ip = '0.0.0.0' # serve notebooks locally\nc.NotebookApp.open_browser = False # do not open a browser window by default when using notebooks.\nc.NotebookApp.password = 'sha1:asdfasdf:asfasdfasdf'\n# [ESC][:wq][ENTER]\n# nohup jupyter notebook\nnohup jupyter lab\nThen close and re-open current shell\njupyter nbconvert --to python &lt;notebook&gt;.ipynb\nsudo service crond start\ncrontab -e\n# [i]\n# https://crontab.guru/\n# * * * * * /home/ec2-user/anaconda3/bin/python /home/ec2-user/&lt;notebook&gt;.py\n# [ESC][:wq][ENTER]\n\nhttp://1.23.456.789:8888\n\n\n\n\n\nhttps://aws.amazon.com/blogs/big-data/running-r-on-aws/\n\nInstall R\nsudo su\n# yum install -y R # Amazon Linux\n# amazon-linux-extras list\nsudo amazon-linux-extras install R3.4 # Amazon Linux 2\nInstall RStudio Server\n\nhttps://posit.co/download/rstudio-server/\n\nwget https://download2.rstudio.org/server/centos6/x86_64/rstudio-server-rhel-1.2.5019-x86_64.rpm\nyum install rstudio-server-rhel-1.2.5019-x86_64.rpm\nrm rstudio-server-rhel-1.2.5019-x86_64.rpm\nInstall Shiny Server\n\nhttps://posit.co/download/shiny-server/\n\nR -e \"install.packages('shiny', repos = 'http://cran.rstudio.com/')\"\nwget https://download3.rstudio.org/centos6.3/x86_64/shiny-server-1.5.12.933-x86_64.rpm\nyum install --nogpgcheck shiny-server-1.5.12.933-x86_64.rpm\nrm shiny-server-1.5.12.933-x86_64.rpm\nAdd user(s)\nuseradd -m jjf234\nsudo passwd jjf234 # prompt to enter password\nNeed to create /ShinyApps under /ec2-user and then subfolders with code, e.g. /home/ec2-user/ShinyApps/hello\n# sudo /opt/shiny-server/bin/deploy-example user-dirs\n# mkdir ~/ShinyApps\n# sudo cp -R /opt/shiny-server/samples/sample-apps/hello ~/ShinyApps\nInstall devtools\nsudo yum install libcurl-devel\nsudo yum install openssl-devel\nsudo yum install libxml2-devel\nR -e \"install.packages('devtools', repos = 'http://cran.rstudio.com/')\"\nInstall RcppParallel\nR -e \"install.packages('RcppParallel', repos = 'http://cran.rstudio.com/')\"\nInstall RcppArmadillo\nR -e \"install.packages('RcppArmadillo', repos = 'http://cran.rstudio.com/')\"\nInstall other packages\nR -e \"install.packages(c('data.table', 'xts', 'testthat', 'microbenchmark'),\nrepos = 'http://cran.rstudio.com/')\"\nInstall roll\nR -e \"devtools::install_github('jjf234/roll')\"\n\nhttp://1.23.456.789:8787/\nhttp://1.23.456.789:3838/ec2-user/hello/"
  },
  {
    "objectID": "posts/cloud/index.html#secure-shell",
    "href": "posts/cloud/index.html#secure-shell",
    "title": "Cloud",
    "section": "",
    "text": "EC2 &gt; Network & Security &gt; Key Pairs &gt; Create Key Pair &gt; *.pem\nPuTTYgen &gt; Load an existing private key file &gt; Save private key &gt; *.ppk\nHost Name: ec2-user\\@ec2-1-23-456-789.us-east-2.compute.amazonaws.com\nConnection &gt; SSH &gt; Auth & WinSCP &gt; Auth &gt; Private key file for authentication\nsudo yum update"
  },
  {
    "objectID": "posts/cloud/index.html#jupyter-server",
    "href": "posts/cloud/index.html#jupyter-server",
    "title": "Cloud",
    "section": "",
    "text": "Install Amazon Linux 2\nwget https://repo.anaconda.com/archive/Anaconda3-2023.03-Linux-x86_64.sh\n\nbash Anaconda3-2023.03-Linux-x86_64.sh\nThen close and re-open current shell\n\nhttps://pypi.org/project/ipython/#history\nhttps://stackoverflow.com/a/76521018\nhttps://stackoverflow.com/a/77117477\n\npip install 'ipython==7.34.0' --force-reinstall\nipython\nfrom IPython.lib import passwd\npasswd(\"12345\")\n# sha1:asdfasdf:asfasdfasdf\nexit()\njupyter notebook --generate-config \nvi ~/.jupyter/jupyter_notebook_config.py\n\n# [i]\nc = get_config()  # Get the config object.\n# c.NotebookApp.certfile = u'/home/ubuntu/ssl/cert.pem' # path to the certificate we generated\n# c.NotebookApp.keyfile = u'/home/ubuntu/ssl/cert.key' # path to the certificate key we generated\nc.NotebookApp.ip = '0.0.0.0' # serve notebooks locally\nc.NotebookApp.open_browser = False # do not open a browser window by default when using notebooks.\nc.NotebookApp.password = 'sha1:asdfasdf:asfasdfasdf'\n# [ESC][:wq][ENTER]\n# nohup jupyter notebook\nnohup jupyter lab\nThen close and re-open current shell\njupyter nbconvert --to python &lt;notebook&gt;.ipynb\nsudo service crond start\ncrontab -e\n# [i]\n# https://crontab.guru/\n# * * * * * /home/ec2-user/anaconda3/bin/python /home/ec2-user/&lt;notebook&gt;.py\n# [ESC][:wq][ENTER]\n\nhttp://1.23.456.789:8888"
  },
  {
    "objectID": "posts/cloud/index.html#rstudio-server",
    "href": "posts/cloud/index.html#rstudio-server",
    "title": "Cloud",
    "section": "",
    "text": "https://aws.amazon.com/blogs/big-data/running-r-on-aws/\n\nInstall R\nsudo su\n# yum install -y R # Amazon Linux\n# amazon-linux-extras list\nsudo amazon-linux-extras install R3.4 # Amazon Linux 2\nInstall RStudio Server\n\nhttps://posit.co/download/rstudio-server/\n\nwget https://download2.rstudio.org/server/centos6/x86_64/rstudio-server-rhel-1.2.5019-x86_64.rpm\nyum install rstudio-server-rhel-1.2.5019-x86_64.rpm\nrm rstudio-server-rhel-1.2.5019-x86_64.rpm\nInstall Shiny Server\n\nhttps://posit.co/download/shiny-server/\n\nR -e \"install.packages('shiny', repos = 'http://cran.rstudio.com/')\"\nwget https://download3.rstudio.org/centos6.3/x86_64/shiny-server-1.5.12.933-x86_64.rpm\nyum install --nogpgcheck shiny-server-1.5.12.933-x86_64.rpm\nrm shiny-server-1.5.12.933-x86_64.rpm\nAdd user(s)\nuseradd -m jjf234\nsudo passwd jjf234 # prompt to enter password\nNeed to create /ShinyApps under /ec2-user and then subfolders with code, e.g. /home/ec2-user/ShinyApps/hello\n# sudo /opt/shiny-server/bin/deploy-example user-dirs\n# mkdir ~/ShinyApps\n# sudo cp -R /opt/shiny-server/samples/sample-apps/hello ~/ShinyApps\nInstall devtools\nsudo yum install libcurl-devel\nsudo yum install openssl-devel\nsudo yum install libxml2-devel\nR -e \"install.packages('devtools', repos = 'http://cran.rstudio.com/')\"\nInstall RcppParallel\nR -e \"install.packages('RcppParallel', repos = 'http://cran.rstudio.com/')\"\nInstall RcppArmadillo\nR -e \"install.packages('RcppArmadillo', repos = 'http://cran.rstudio.com/')\"\nInstall other packages\nR -e \"install.packages(c('data.table', 'xts', 'testthat', 'microbenchmark'),\nrepos = 'http://cran.rstudio.com/')\"\nInstall roll\nR -e \"devtools::install_github('jjf234/roll')\"\n\nhttp://1.23.456.789:8787/\nhttp://1.23.456.789:3838/ec2-user/hello/"
  },
  {
    "objectID": "posts/dev-r/index.html",
    "href": "posts/dev-r/index.html",
    "title": "Software",
    "section": "",
    "text": ".libPaths()\n# &gt; C:\\Users\\username\\AppData\\Local\n# &gt; C:\\Program Files\\R\\R-1.2.3\\library\n\nSet environment variable: PATH=c:\\rtools&lt;123&gt;\\usr\\bin"
  },
  {
    "objectID": "posts/dev-r/index.html#quarto",
    "href": "posts/dev-r/index.html#quarto",
    "title": "Software",
    "section": "Quarto",
    "text": "Quarto\npip install pyyaml jupyter"
  },
  {
    "objectID": "posts/dev-r/index.html#issues",
    "href": "posts/dev-r/index.html#issues",
    "title": "Software",
    "section": "Issues",
    "text": "Issues\n\nhttps://github.com/osqp/osqp/issues/385\n\nUse python 3.10 for qdldl\n\nhttps://github.com/pydata/pandas-datareader/issues/965\n\npip install git+https://github.com/pydata/pandas-datareader.git"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-mean",
    "href": "posts/optim-r/index.html#maximize-mean",
    "title": "Optimization",
    "section": "Maximize mean",
    "text": "Maximize mean\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmax_mean_optim &lt;- function(mu, sigma, target) {\n  \n  params &lt;- CVXR::Variable(length(mu))\n  \n  obj &lt;- CVXR::Maximize(t(params) %*% mu)\n  \n  cons &lt;- list(sum(params) == 1, params &gt;= 0,\n               CVXR::quad_form(params, sigma) &lt;= target ^ 2)\n  \n  prob &lt;- CVXR::Problem(obj, cons)\n  \n  result &lt;- CVXR::solve(prob)$getValue(params)\n  \n  return(result)\n\n}\n\n\ntarget &lt;- 0.06\n\n\nparams1 &lt;- t(max_mean_optim(mu, sigma, target))\nparams1\n\n          [,1]      [,2]      [,3]         [,4]\n[1,] 0.3475922 0.1302538 0.5221538 1.870625e-07\n\n\n\nparams1 %*% mu\n\n           [,1]\n[1,] 0.03578793\n\n\n\nsqrt(params1 %*% sigma %*% t(params1))\n\n     [,1]\n[1,] 0.06\n\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\") # roll (&gt;= 1.1.7)\n# library(rolloptim)\n# \n# mu &lt;- roll_mean(returns_x_xts, 5)\n# sigma &lt;- roll_cov(returns_x_xts, width = 5)\n# \n# xx &lt;- roll_crossprod(returns_x_xts, returns_x_xts, 5)\n# xy &lt;- roll_crossprod(returns_x_xts, returns_y_xts, 5)\n# \n# roll_max_mean(mu)"
  },
  {
    "objectID": "posts/alpha-r/index.html",
    "href": "posts/alpha-r/index.html",
    "title": "Alpha Distribution Analysis",
    "section": "",
    "text": "Question\nCould an optimally combined portfolio of hedge funds have won Warren Buffet’s 10-year bet against the S&P 500?\n\nhttps://longbets.org/362/\n\n\nload(\"returns.rda\")\n\n\n\nOptimization\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\")\n\n\n# library(CVXR)\n\n\n# min_rss_optim &lt;- function(x, y) {\n# \n#   n_rows &lt;- nrow(x)\n#   x &lt;- as.matrix(x)\n#   y &lt;- as.numeric(y)\n#   params &lt;- Variable(ncol(x))\n#   \n#   obj &lt;- Minimize(sum_squares(y - x %*% params))\n#   cons &lt;- list(sum(params) == 1, params &gt;= 0, params &lt;= 1)\n#   prob &lt;- Problem(obj, cons)\n#   \n#   result &lt;- solve(prob)$getValue(params)\n#   \n#   return(result)\n# \n# }\n\n\n\nBootstrapping\n\n# library(NNS)\n\n\nboot &lt;- function(n, p = 1) {\n\n  idx &lt;- sample(1:n, 1)\n  idx_ls &lt;- list(idx)\n  \n  for (i in 2:n) {\n    if (runif(1) &lt; 1 / p) {\n      idx &lt;- sample(1:n, 1)\n    } else {\n\n      idx &lt;- idx + 1\n\n      if (idx &gt; n) {\n        idx &lt;- 1\n      }\n\n    }\n    \n    idx_ls &lt;- append(idx_ls, list(idx))\n\n  }\n\n  result &lt;- do.call(rbind, idx_ls)\n  \n  return(result)\n\n}\n\n\nset.seed(5640)\nn_rows &lt;- nrow(returns)\nn_cols &lt;- ncol(returns)\nn_boot &lt;- 1000\noptim_weights_ls &lt;- list()\noptim_alpha_ls &lt;- list()\noptim_te_ls &lt;- list()\n# returns_boot &lt;- array(NA, dim = c(n_rows, n_cols, n_boot)) # time, asset, and bootstrap\n\n\n# for (j in 1:n_cols) {\n# \n#   col_boot &lt;- NNS.meboot(as.vector(returns[ , j]), reps = n_boot,\n#                          rho = 1, type = \"pearson\")[\"replicates\", ]$replicates\n# \n#   returns_boot[ , j, ] &lt;- col_boot\n# \n# }\n\n\n# colnames(returns_boot) &lt;- colnames(returns)\n\n\nfor (z in 1:n_boot) {\n  \n  # idx &lt;- sample(1:n_rows, n_rows, replace = TRUE)\n  idx &lt;- boot(n_rows, p = 2)\n  # boot_sample &lt;- returns_boot[ , , z]\n  \n  x_boot &lt;- returns[idx, colnames(returns) != \"S&P Index Fund\"]\n  y_boot &lt;- returns[idx, \"S&P Index Fund\"]\n  # x_boot &lt;- boot_sample[ , colnames(boot_sample) != \"S&P Index Fund\"]\n  # y_boot &lt;- boot_sample[ , \"S&P Index Fund\"]\n  \n  # optim_weights &lt;- min_rss_optim(x_boot, y_boot)\n  \n  xx &lt;- roll::roll_crossprod(x_boot, x_boot, width = nrow(x_boot))\n  xy &lt;- roll::roll_crossprod(x_boot, y_boot, width = nrow(x_boot))\n  \n  optim_weights &lt;- rolloptim::roll_min_rss(xx, xy, total = 1, lower = 0, upper = 1)[nrow(x_boot), ]\n  \n  optim_returns &lt;- x_boot %*% optim_weights\n  optim_port &lt;- prod(1 + optim_returns) ^ (1 / n_rows) - 1\n  optim_bench &lt;- prod(1 + y_boot) ^ (1 / n_rows) - 1\n  optim_alpha &lt;- optim_port - optim_bench\n  optim_te &lt;- sd(optim_returns - y_boot)\n  \n  optim_weights_ls &lt;- append(optim_weights_ls, list(t(optim_weights)))\n  optim_alpha_ls &lt;- append(optim_alpha_ls, list(optim_alpha))\n  optim_te_ls &lt;- append(optim_te_ls, list(optim_te))\n\n}\n\n\noptim_weights &lt;- do.call(rbind, optim_weights_ls)\noptim_alpha &lt;- do.call(rbind, optim_alpha_ls)\noptim_te &lt;- do.call(rbind, optim_te_ls)\n\n\n\nAnswer\nAnalysis shows that 3% of the optimized portfolios produced alpha values greater than zero, which means that 97% of optimized combinations did not outperform the S&P 500."
  },
  {
    "objectID": "posts/crowds-r/index.html",
    "href": "posts/crowds-r/index.html",
    "title": "Crowds",
    "section": "",
    "text": "# factors_r &lt;- c(\"SP500\") # \"SP500\" does not contain dividends\n# factors_d &lt;- c(\"DTB3\")\n\n\nParse web\n\nfilters &lt;- list(\"eq\", list(\"categoryname\", \"Tactical Allocation\"))\nquery &lt;- yfscreen::create_query(filters)\npayload &lt;- yfscreen::create_payload(\"mutualfund\", query, 250)\ndata &lt;- yfscreen::get_data(payload)\n\n\nsorted_df &lt;- data[order(\n  -data[[\"netAssets.raw\"]],\n  data[[\"netExpenseRatio.raw\"]],\n  data[[\"firstTradeDateMilliseconds\"]],\n  data[[\"longName\"]],\n  data[[\"symbol\"]]\n), ]\ntickers &lt;- sorted_df[!duplicated(sorted_df[[\"netAssets.raw\"]]), \"symbol\"]\n\n\nallocations &lt;- c(\"IVV\", \"IDEV\", \"IUSB\", \"IEMG\", \"IJH\", \"IAGG\", \"IJR\")\ntickers &lt;- c(tickers, allocations)\n\n\n\nOptimization\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\")\n\n\n# library(CVXR)\n\n\n# min_rss_optim &lt;- function(x, y) {\n# \n#   params &lt;- Variable(ncol(x))\n# \n#   obj &lt;- Minimize(sum_squares(y - x %*% params))\n# \n#   cons &lt;- list(sum(params) == 1, params &gt;= 0, params &lt;= 1)\n# \n#   prob &lt;- Problem(obj, cons)\n# \n#   result &lt;- solve(prob)$getValue(params)\n# \n#   return(result)\n# \n# }\n\n\nperformance_xts &lt;- roll::roll_prod(1 + returns_xts, width, min_obs = 1) - 1\n\n\nn_rows &lt;- nrow(overlap_xts)\nresult_ls &lt;- list()\nindex_ls &lt;- list()\n\n# for (i in width:n_rows) {\nfor (i in n_rows) {\n    \n  idx &lt;- max(i - width + 1, 1):i\n  x_subset &lt;- zoo::coredata(overlap_x_xts[idx, ])\n  y_subset &lt;- zoo::coredata(overlap_y_xts[idx, ])\n  params_ls &lt;- list()\n  tickers_ls &lt;- list()\n  performance_ls &lt;- list()\n  \n  for (j in tickers[!tickers %in% allocations]) {\n    \n    idx &lt;- complete.cases(x_subset, y_subset[ , j])\n    x_complete &lt;- x_subset[idx, , drop = FALSE]\n    y_complete &lt;- y_subset[idx, j]\n    \n    if ((nrow(x_complete) &gt; 0) && (length(y_complete) &gt; 0)) {\n      \n      xx &lt;- roll::roll_crossprod(x_complete, x_complete, width = nrow(x_complete))\n      xy &lt;- roll::roll_crossprod(x_complete, y_complete, width = nrow(x_complete))\n      \n      # params &lt;- t(min_rss_optim(x_complete, y_complete))\n      params &lt;- rolloptim::roll_min_rss(xx, xy, total = 1, lower = 0, upper = 1)[nrow(x_complete), ]\n      params_ls &lt;- append(params_ls, list(params))\n      \n      tickers_ls &lt;- append(tickers_ls, list(j))\n      \n      performance_ls &lt;- append(performance_ls, list(performance_xts[i, j]))\n        \n    }\n      \n  }\n  \n  if (length(params_ls) &gt; 0) {\n      \n    result &lt;- do.call(rbind, params_ls)\n    rownames(result) &lt;- unlist(tickers_ls)\n    \n    result &lt;- cbind(result, performance = unlist(performance_ls))\n    \n    result_ls &lt;- append(result_ls, list(result))\n    index_ls &lt;- append(index_ls, list(zoo::index(overlap_xts)[i]))\n      \n  }\n  \n}\n\n\n# save(result_ls, file = \"result_ls.rda\")\n# save(index_ls, file = \"index_ls.rda\")\n\n\n\nPerformance\n\n# load(\"result_ls.rda\")\n# load(\"index_ls.rda\")\n\n\nquantile_cut &lt;- function(x) {\n\n  result &lt;- cut(\n    -x,\n    breaks = quantile(-x, probs = c(0, 0.25, 0.5, 0.75, 1),\n                      na.rm = TRUE),\n    labels = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"),\n    include.lowest = TRUE\n  )\n\n  return(result)\n\n}\n\n\nn_rows &lt;- length(result_ls)\nscore_ls &lt;- list()\n\nfor (i in 1:n_rows) {\n  \n  score_df &lt;- data.frame(result_ls[[i]])\n  colnames(score_df) &lt;- c(allocations, \"performance\")\n  \n  score_df[[\"date\"]] &lt;- index_ls[[i]]\n  score_df[[\"quantile\"]] &lt;- quantile_cut(score_df[[\"performance\"]])\n  \n  score_form &lt;- as.formula(paste0(\"cbind(\", paste(c(allocations, \"performance\"), collapse = \",\"), \") ~ date + quantile\"))\n  overall_form &lt;- as.formula(paste0(\"cbind(\", paste(c(allocations, \"performance\"), collapse = \",\"), \") ~ date\"))\n  \n  score &lt;- aggregate(score_form, data = score_df, FUN = function(x) mean(x, na.rm = TRUE))\n  overall &lt;- aggregate(overall_form, data = score_df, FUN = function(x) mean(x, na.rm = TRUE))\n  overall[[\"quantile\"]] &lt;- \"Overall\"\n  overall &lt;- overall[ , c(\"date\", \"quantile\", allocations, \"performance\")]\n  \n  score &lt;- rbind(score, overall)\n  \n  score_ls &lt;- append(score_ls, list(score))\n  \n}\n\n\nscore_df &lt;- do.call(rbind, score_ls)\nprint(score_df)\n\n        date quantile        IVV       IDEV       IUSB       IEMG        IJH\n1 2025-11-28       Q1 0.45772398 0.16798422 0.09136037 0.09455402 0.00839337\n2 2025-11-28       Q2 0.36857392 0.17736427 0.14790194 0.11583411 0.01589267\n3 2025-11-28       Q3 0.37092980 0.09816484 0.18732404 0.05413359 0.02766579\n4 2025-11-28       Q4 0.08743557 0.14143510 0.08072945 0.05507598 0.06206377\n5 2025-11-28  Overall 0.32285172 0.14650559 0.12639107 0.08008035 0.02825562\n        IAGG        IJR performance\n1 0.09638949 0.08359455 0.065910933\n2 0.11954885 0.05488424 0.042256298\n3 0.22266335 0.03911859 0.028265095\n4 0.53538691 0.03787322 0.004268273\n5 0.24168100 0.05423465 0.035554604\n\n\n\n# save(score_df, file = \"score_df.rda\") # see test-crowds-r\n# score_xts &lt;- xts(score_df[score_df[[\"quantile\"]] == \"Q1\", \"weight\"],\n#                  score_df[score_df[[\"quantile\"]] == \"Q1\", \"date\"])\n# plot(score_xts)"
  },
  {
    "objectID": "posts/crowds-py/index.html",
    "href": "posts/crowds-py/index.html",
    "title": "Crowds",
    "section": "",
    "text": "factors_r = [\"SP500\"] # \"SP500\" does not contain dividends\nfactors_d = [\"DTB3\"]\n\n\nParse web\n\nimport yfscreen as yfs\n\n\nfilters = [\"eq\", [\"categoryname\", \"Tactical Allocation\"]]\nquery = yfs.create_query(filters)\npayload = yfs.create_payload(\"mutualfund\", query, 250)\ndata = yfs.get_data(payload)\n\n\nsorted_df = data.sort_values(\n  by = [\n    \"netAssets.raw\",\n    \"netExpenseRatio.raw\",\n    \"firstTradeDateMilliseconds\",\n    \"longName\",\n    \"symbol\"\n  ],\n  ascending = [False, True, True, True, True],\n  kind = \"stable\"\n)\ntickers = sorted_df.loc[~sorted_df[\"netAssets.raw\"].duplicated(), \"symbol\"].tolist()\n\n\n# allocations = [\"AOK\", \"AOM\", \"AOR\", \"AOA\"]\n# tickers = tickers + allocations\n\n\n\nOptimization\n\nimport json\nimport cvxpy as cp\n\n\ndef min_rss_optim(x, y):\n    \n  w = cp.Variable(x.shape[1])\n    \n  objective = cp.Minimize(cp.sum_squares(y - x @ w))\n    \n  constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= 1]\n    \n  problem = cp.Problem(objective, constraints)\n  problem.solve()\n    \n  return w.value\n\n\ndef pnl(x):\n  return np.nanprod(1 + x) - 1\n\n\nperformance_df = returns_df.rolling(width, min_periods = 1).apply(pnl, raw = False)\n\n\nn_rows = overlap_df.shape[0]\nresult_ls = []\nindex_ls = []\n\n# for i in range(width - 1, n_rows):\nfor i in range(n_rows - 1, n_rows):\n  \n  idx = range(max(i - width + 1, 0), i + 1)\n  x_subset = overlap_x_df.iloc[idx]\n  y_subset = overlap_y_df.iloc[idx]\n  params_ls = []\n  tickers_ls = []\n  performance_ls = []\n  \n  # for j in [ticker for ticker in tickers if ticker not in allocations]:\n  for j in tickers:\n    \n    idx = ~x_subset.isna().any(axis = 1) & ~y_subset[j].isna()\n    x_complete = x_subset.loc[idx]\n    y_complete = y_subset.loc[idx, j]\n    \n    if (x_complete.shape[0] &gt; 0) and (y_complete.size &gt; 0):\n        \n      params = min_rss_optim(x_complete.values, y_complete.values)\n      params_ls.append(params)\n      \n      tickers_ls.append(j)\n      \n      performance_ls.append(performance_df[j].iloc[i])\n\n  if params_ls:\n    \n    result = pd.DataFrame(params_ls, index = tickers_ls)\n    result[\"performance\"] = performance_ls\n    \n    result_ls.append(result)\n    index_ls.append(overlap_x_df.index[i])\n\n\n# json.dump([x.to_dict() for x in result_ls], open(\"result_ls.json\", \"w\"))\n# json.dump([x.isoformat() for x in index_ls], open(\"index_ls.json\", \"w\"))\n\n\n\nPerformance\n\n# result_ls = [pd.DataFrame(x) for x in json.load(open(\"result_ls.json\", \"r\"))]\n# index_ls = [pd.Timestamp(x) for x in json.load(open(\"index_ls.json\", \"r\"))]\n\n\ndef quantile_cut(x):\n  \n  result = pd.qcut(\n    -x,\n    q = [0, 0.25, 0.5, 0.75, 1],\n    labels = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n  )\n  \n  return result\n\n\nn_rows = len(result_ls)\nscore_ls = []\n\nfor i in range(n_rows):\n  \n  score_df = pd.DataFrame(result_ls[i])\n  score_df.columns = factors + [\"performance\"]\n  \n  score_df[\"date\"] = index_ls[i]\n  score_df[\"quantile\"] = quantile_cut(score_df[\"performance\"])\n  \n  score = score_df.groupby([\"date\", \"quantile\"], observed = True).agg(\n    weight = (factors[0], \"mean\"),\n    performance = (\"performance\", \"mean\")\n  ).reset_index()\n  \n  overall = pd.DataFrame({\n    \"date\": [index_ls[i]],\n    \"quantile\": [\"Overall\"],\n    \"weight\": [score_df[factors[0]].mean()],\n    \"performance\": [score_df[\"performance\"].mean()]\n  })\n  \n  score = pd.concat([score, overall], ignore_index = True)\n  \n  score_ls.append(score)\n\n\nscore_df = pd.concat(score_ls, ignore_index = True)\nprint(score_df)\n\n        date quantile    weight  performance\n0 2025-11-28       Q1  0.821132     0.065911\n1 2025-11-28       Q2  0.738272     0.042256\n2 2025-11-28       Q3  0.611185     0.028265\n3 2025-11-28       Q4  0.379009     0.004268\n4 2025-11-28  Overall  0.639668     0.035555\n\n\n\n# score_df.to_json(\"score_df.json\", date_format = \"iso\")"
  },
  {
    "objectID": "posts/nns-r/index.html",
    "href": "posts/nns-r/index.html",
    "title": "Nonlinear nonparametric statistics",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\nrisk_sign &lt;- c(SP500 = 1, DTWEXAFEGS = 1, DGS10 = -1, BAMLH0A0HYM2 = -1)"
  },
  {
    "objectID": "posts/nns-r/index.html#partial-moments",
    "href": "posts/nns-r/index.html#partial-moments",
    "title": "Nonlinear nonparametric statistics",
    "section": "Partial moments",
    "text": "Partial moments\n\nhttps://cran.r-project.org/web/packages/NNS/vignettes/NNSvignette_Partial_Moments.html\n\n\nsigma &lt;- cov(returns_xts)\n\n\n# Registered S3 method overwritten by 'data.table': print.data.table\n# Registered S3 methods overwritten by 'htmltools': print.html (from=tools:rstudio), print.shiny.tag (from=tools:rstudio), print.shiny.tag.list (from=tools:rstudio)\n# Registered S3 method overwritten by 'htmlwidgets': print.htmlwidget (from=tools:rstudio)\npm &lt;- NNS::PM.matrix(LPM_degree = 1, UPM_degree = 1, target = \"mean\",\n                     variable = returns_xts, pop_adj = TRUE)\n\n\nall.equal(sigma, pm$cov.matrix)\n\n[1] TRUE\n\n\n\ncl_pm &lt;- pm$clpm\ncu_pm &lt;- pm$cupm\ndl_pm &lt;- pm$dlpm\ndu_pm &lt;- pm$dupm\n\n\nall.equal(sigma, (cl_pm + cu_pm) - (dl_pm + du_pm))\n\n[1] TRUE"
  },
  {
    "objectID": "posts/nns-r/index.html#implied-shocks",
    "href": "posts/nns-r/index.html#implied-shocks",
    "title": "Nonlinear nonparametric statistics",
    "section": "Implied shocks",
    "text": "Implied shocks\n\n# LV &lt;- eigen(cov(cl_pm - (dl_pm + du_pm)))\n# L &lt;- LV[[\"values\"]]\n# V &lt;- LV[[\"vectors\"]]\n# scale &lt;- shock / (sqrt(L[comp]) * V[ , comp])[1]\n# implied_shocks_pm &lt;- scale * (sqrt(L[comp]) * V[ , comp]) * risk_sign\n\ncov_pm &lt;- cov(cl_pm - (dl_pm + du_pm))\ncov_zz_pm &lt;- cov_pm[1, 1, drop = FALSE]\ncov_zx_pm &lt;- cov_pm[1, ]\n\nbeta &lt;- solve(cov_zz_pm, t(cov_zx_pm))\nimplied_shocks_pm &lt;- shock %*% beta * risk_sign\n\n\nround(data.frame(\n  implied_shocks = t(implied_shocks),\n  # implied_shocks_pc = implied_shocks_pc,\n  implied_shocks_pm = t(implied_shocks_pm),\n  row.names = c(\"SP500\", \"USD\", \"US10Y\", \"USHY\")\n) * 100, 2)\n\n      implied_shocks implied_shocks_pm\nSP500         -10.00            -10.00\nUSD             0.51              1.84\nUS10Y          -0.07             -0.17\nUSHY            0.56              0.59"
  },
  {
    "objectID": "posts/nns-r/index.html#bootstrapping",
    "href": "posts/nns-r/index.html#bootstrapping",
    "title": "Nonlinear nonparametric statistics",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nhttps://github.com/OVVO-Financial/Finance/blob/main/stress_test.md\nhttps://cran.r-project.org/web/packages/NNS/vignettes/NNSvignette_Sampling.html\n\n\ntarget &lt;- 0\nn_samples &lt;- 1e6\n\n\nidx &lt;- which(rowSums(returns_xts &gt; target) != length(factors)) # exclude cupm quadrant\n\n\nreturns_bs &lt;- apply(returns_xts[idx, ], 2, function(x) {\n  result &lt;- NNS::NNS.meboot(as.vector(x),\n                            reps = ceiling(n_samples / length(idx)),\n                            rho = 1,\n                            type = \"pearson\")[\"replicates\", ]$replicates\n  tail(as.vector(result), n_samples)\n})\n\n\nbs &lt;- NNS::PM.matrix(LPM_degree = 1, UPM_degree = 1, target = \"mean\",\n                     variable = returns_bs, pop_adj = TRUE)\n\n\ncl_bs &lt;- bs$clpm\ncu_bs &lt;- bs$cupm\ndl_bs &lt;- bs$dlpm\ndu_bs &lt;- bs$dupm\n\n\n# # LV &lt;- eigen(cov(returns_bs))\n# LV &lt;- eigen(cov(cl_pm - (dl_pm + du_pm)))\n# L &lt;- LV[[\"values\"]]\n# V &lt;- LV[[\"vectors\"]]\n# scale &lt;- shock / (sqrt(L[comp]) * V[ , comp])[1]\n# implied_shocks_bs &lt;- scale * (sqrt(L[comp]) * V[ , comp]) * risk_sign\n\ncov_bs &lt;- cov(cl_bs - (dl_bs + du_bs))\ncov_zz_bs &lt;- cov_bs[1, 1, drop = FALSE]\ncov_zx_bs &lt;- cov_bs[1, ]\n\nbeta &lt;- solve(cov_zz_bs, t(cov_zx_bs))\nimplied_shocks_bs &lt;- shock %*% beta * risk_sign\n\n\nround(data.frame(\n  implied_shocks = t(implied_shocks),\n  # implied_shocks_pc = implied_shocks_pc,\n  implied_shocks_pm = t(implied_shocks_pm),\n  implied_shocks_bs = t(implied_shocks_bs),\n  row.names = c(\"SP500\", \"USD\", \"US10Y\", \"USHY\")\n) * 100, 2)\n\n      implied_shocks implied_shocks_pm implied_shocks_bs\nSP500         -10.00            -10.00            -10.00\nUSD             0.51              1.84              1.91\nUS10Y          -0.07             -0.17             -0.17\nUSHY            0.56              0.59              0.58"
  }
]