[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jasonjfoster.github.io",
    "section": "",
    "text": "Markets\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nalgorithms\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nNov 7, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/optim-r/index.html",
    "href": "posts/optim-r/index.html",
    "title": "Optimization",
    "section": "",
    "text": "Need to generate uniformly distributed weights \\(\\mathbf{w}=(w_{1},w_{2},\\ldots,w_{N})\\) such that \\(\\sum_{j=1}^{N}w_{i}=1\\) and \\(w_{i}\\geq0\\):\n\nApproach 1: tempting to use \\(w_{i}=\\frac{u_{i}}{\\sum_{j=1}^{N}u_{i}}\\) where \\(u_{i}\\sim U(0,1)\\) but the distribution of \\(\\mathbf{w}\\) is not uniform\nApproach 2: instead, generate \\(\\text{Exp}(1)\\) and then normalize\n\nCan also scale random weights by \\(M\\), e.g. if sum of weights must be 10% then multiply weights by 10%.\n\nrand_weights1 &lt;- function(n_sim, n_assets) {\n    \n    rand_exp &lt;- matrix(runif(n_sim * n_assets), nrow = n_sim, ncol = n_assets)\n    result &lt;- rand_exp / rowSums(rand_exp)\n    \n    return(result)\n    \n}\n\n\nn_assets &lt;- 3\nn_sim &lt;- 10000\n\n\napproach1 &lt;- rand_weights1(n_sim, n_assets)\n\n\n\n\n\n\n\n\n\n\nApproach 2(a): uniform sample from the simplex (http://mathoverflow.net/a/76258) and then normalize\n\nIf \\(u\\sim U(0,1)\\) then \\(-\\ln(u)\\) is an \\(\\text{Exp}(1)\\) distribution\n\nThis is also known as generating a random vector from the symmetric Dirichlet distribution.\n\nrand_weights2a &lt;- function(n_sim, n_assets, lmbda) {\n    \n    # inverse transform sampling: https://en.wikipedia.org/wiki/Inverse_transform_sampling\n    rand_exp &lt;- matrix(-log(1 - runif(n_sim * n_assets)) / lmbda, nrow = n_sim, ncol = n_assets)\n    result &lt;- rand_exp / rowSums(rand_exp)\n    \n    return(result)\n    \n}\n\n\nlmbda &lt;- 1\n\n\napproach2a &lt;- rand_weights2a(n_sim, n_assets, lmbda)\n\n\n\n\n\n\n\n\n\n\nApproach 2(b): directly generate \\(\\text{Exp}(1)\\) and then normalize\n\nrand_weights2b &lt;- function(n_sim, n_assets) {\n    \n    rand_exp &lt;- matrix(rexp(n_sim * n_assets), nrow = n_sim, ncol = n_assets)\n    result &lt;- rand_exp / rowSums(rand_exp)\n    \n    return(result)\n    \n}\n\n\napproach2b &lt;- rand_weights2b(n_sim, n_assets)\n\n\n\n\n\n\n\n\n\n\n\n\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\nrand_turnover1 &lt;- function(n_sim, n_assets, lower, upper, target) {\n    \n    rng &lt;- upper - lower\n    \n    result &lt;- rand_weights2b(n_sim, n_assets) * rng\n    result &lt;- result - rng / n_assets\n    \n    return(result)\n    \n}\n\n\nlower &lt;- -0.05\nupper &lt;- 0.05\ntarget &lt;- 0\n\n\napproach1 &lt;- rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\n\n\n\n\nrand_iterative &lt;- function(n_assets, lower, upper, target) {\n    \n    result &lt;- runif(n_assets - 1, min = lower, max = upper)\n    temp &lt;- target - sum(result)\n    \n    while (!((temp &lt;= upper) && (temp &gt;= lower))) {\n        \n        result &lt;- runif(n_assets - 1, min = lower, max = upper)\n        temp &lt;- target - sum(result)\n        \n    }\n    \n    result &lt;- append(result, temp)\n    \n    return(result)\n    \n}\n\n\nrand_turnover2 &lt;- function(n_sim, n_assets, lower, upper, target) {\n  \n    result_ls &lt;- list()\n    \n    for (i in 1:n_sim) {\n        \n        result_sim &lt;- rand_iterative(n_assets, lower, upper, target)\n        result_ls &lt;- append(result_ls, list(result_sim))\n        \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    \n    return(result)\n    \n}\n\n\napproach2 &lt;- rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-r/index.html#random-turnover",
    "href": "posts/optim-r/index.html#random-turnover",
    "title": "Optimization",
    "section": "",
    "text": "How to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\nrand_turnover1 &lt;- function(n_sim, n_assets, lower, upper, target) {\n    \n    rng &lt;- upper - lower\n    \n    result &lt;- rand_weights2b(n_sim, n_assets) * rng\n    result &lt;- result - rng / n_assets\n    \n    return(result)\n    \n}\n\n\nlower &lt;- -0.05\nupper &lt;- 0.05\ntarget &lt;- 0\n\n\napproach1 &lt;- rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\n\n\n\n\nrand_iterative &lt;- function(n_assets, lower, upper, target) {\n    \n    result &lt;- runif(n_assets - 1, min = lower, max = upper)\n    temp &lt;- target - sum(result)\n    \n    while (!((temp &lt;= upper) && (temp &gt;= lower))) {\n        \n        result &lt;- runif(n_assets - 1, min = lower, max = upper)\n        temp &lt;- target - sum(result)\n        \n    }\n    \n    result &lt;- append(result, temp)\n    \n    return(result)\n    \n}\n\n\nrand_turnover2 &lt;- function(n_sim, n_assets, lower, upper, target) {\n  \n    result_ls &lt;- list()\n    \n    for (i in 1:n_sim) {\n        \n        result_sim &lt;- rand_iterative(n_assets, lower, upper, target)\n        result_ls &lt;- append(result_ls, list(result_sim))\n        \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    \n    return(result)\n    \n}\n\n\napproach2 &lt;- rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-means",
    "href": "posts/optim-r/index.html#maximize-means",
    "title": "Optimization",
    "section": "Maximize means",
    "text": "Maximize means\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ntarget &lt;- 0.06\n\n\nmax_mean_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Maximize(t(params) %*% mu)\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0,\n                 quad_form(params, sigma) &lt;= target ^ 2)\n    \n    prob &lt;- Problem(obj, cons)\n    \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams1 &lt;- t(max_mean_optim(mu, sigma, target))\nparams1\n\n         [,1]     [,2]         [,3]         [,4]\n[1,] 0.471109 0.528891 7.431385e-09 4.590119e-09\n\n\n\nparams1 %*% mu\n\n           [,1]\n[1,] 0.04179012\n\n\n\nsqrt(params1 %*% sigma %*% t(params1))\n\n     [,1]\n[1,] 0.06\n\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\") # roll (&gt;= 1.1.7)\n# library(rolloptim)\n# \n# mu &lt;- roll_mean(returns_x_xts, 5)\n# sigma &lt;- roll_cov(returns_x_xts, width = 5)\n# \n# xx &lt;- roll_crossprod(returns_x_xts, returns_x_xts, 5)\n# xy &lt;- roll_crossprod(returns_x_xts, y, 5) # TO DO\n# \n# roll_max_mean(mu)"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-variance",
    "href": "posts/optim-r/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ntarget &lt;- 0.03\n\n\nmin_var_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Minimize(quad_form(params, sigma))\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0,\n                 sum(mu * params) &gt;= target)\n    \n    prob &lt;- Problem(obj, cons)\n    \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams2 &lt;- t(min_var_optim(mu, sigma, target))\nparams2\n\n          [,1]      [,2]      [,3]         [,4]\n[1,] 0.3180424 0.5039842 0.1779734 2.454277e-21\n\n\n\nparams2 %*% mu\n\n     [,1]\n[1,] 0.03\n\n\n\nsqrt(params2 %*% sigma %*% t(params2))\n\n           [,1]\n[1,] 0.04216331\n\n\n\n# roll_min_var(sigma)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-utility",
    "href": "posts/optim-r/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nir &lt;- 0.5\ntarget &lt;- ir / 0.06 # ir / std (see Black-Litterman)\n\n\nmax_utility_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Minimize(0.5 * target * quad_form(params, sigma) - t(mu) %*% params)\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0)\n    \n    prob &lt;- Problem(obj, cons)\n        \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams3 &lt;- t(max_utility_optim(mu, sigma, target))\nparams3\n\n          [,1]      [,2]         [,3]         [,4]\n[1,] 0.4899514 0.5100486 8.639055e-24 5.886708e-23\n\n\n\nparams3 %*% mu \n\n           [,1]\n[1,] 0.04296222\n\n\n\nsqrt(params3 %*% sigma %*% t(params3))\n\n           [,1]\n[1,] 0.06221408\n\n\n\n# roll_max_utility(mu, sigma)"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nhttps://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html\n\n\n# roll_min_rss(xx, xy)\n\n\ndata.frame(\"max_pnl\" = t(params1),\n           \"min_risk\" = t(params2),\n           \"max_ratio\" = t(params3))\n\n       max_pnl     min_risk    max_ratio\n1 4.711090e-01 3.180424e-01 4.899514e-01\n2 5.288910e-01 5.039842e-01 5.100486e-01\n3 7.431385e-09 1.779734e-01 8.639055e-24\n4 4.590119e-09 2.454277e-21 5.886708e-23"
  },
  {
    "objectID": "posts/optim-py/index.html",
    "href": "posts/optim-py/index.html",
    "title": "Optimization",
    "section": "",
    "text": "Need to generate uniformly distributed weights \\(\\mathbf{w}=(w_{1},w_{2},\\ldots,w_{N})\\) such that \\(\\sum_{j=1}^{N}w_{i}=1\\) and \\(w_{i}\\geq0\\):\n\nApproach 1: tempting to use \\(w_{i}=\\frac{u_{i}}{\\sum_{j=1}^{N}u_{i}}\\) where \\(u_{i}\\sim U(0,1)\\) but the distribution of \\(\\mathbf{w}\\) is not uniform\nApproach 2: instead, generate \\(\\text{Exp}(1)\\) and then normalize\n\nCan also scale random weights by \\(M\\), e.g. if sum of weights must be 10% then multiply weights by 10%.\n\ndef rand_weights1(n_sim, n_assets):  \n    \n    rand_exp = np.matrix(np.random.uniform(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n\n\nn_assets = 3\nn_sim = 10000\n\n\napproach1 = rand_weights1(n_sim, n_assets)\n\n\n\n\n\n\n\n\n\n\nApproach 2(a): uniform sample from the simplex (http://mathoverflow.net/a/76258) and then normalize\n\nIf \\(u\\sim U(0,1)\\) then \\(-\\ln(u)\\) is an \\(\\text{Exp}(1)\\) distribution\n\nThis is also known as generating a random vector from the symmetric Dirichlet distribution.\n\ndef rand_weights2a(n_sim, n_assets, lmbda):   \n    \n    # inverse transform sampling: https://en.wikipedia.org/wiki/Inverse_transform_sampling\n    rand_exp = np.matrix(-np.log(1 - np.random.uniform(size = (n_sim, n_assets))) / lmbda)\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n\n\nlmbda = 1\n\n\napproach2a = rand_weights2a(n_sim, n_assets, lmbda)\n\n\n\n\n\n\n\n\n\n\nApproach 2(b): directly generate \\(\\text{Exp}(1)\\) and then normalize\n\ndef rand_weights2b(n_sim, n_assets):\n    \n    rand_exp = np.matrix(np.random.exponential(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n\n\napproach2b = rand_weights2b(n_sim, n_assets)\n\n\n\n\n\n\n\n\n\n\n\n\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights2b(n_sim, n_assets) * rng\n    result = result - rng / n_assets\n    \n    return result\n\n\nlower = -0.05\nupper = 0.05\ntarget = 0\n\n\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\n\n\n\n\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n    temp = target - sum(result)\n    \n    while not ((temp &lt;= upper) and (temp &gt;= lower)):\n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n    result = np.append(result, temp)\n    \n    return result\n\n\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n  \n    result_ls = []\n    \n    for i in range(n_sim):\n      \n      result_sim = rand_iterative(n_assets, lower, upper, target)\n      result_ls.append(result_sim)\n      \n    result = pd.DataFrame(result_ls)\n    \n    return result\n\n\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-py/index.html#random-turnover",
    "href": "posts/optim-py/index.html#random-turnover",
    "title": "Optimization",
    "section": "",
    "text": "How to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous period’s random weights.\n\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights2b(n_sim, n_assets) * rng\n    result = result - rng / n_assets\n    \n    return result\n\n\nlower = -0.05\nupper = 0.05\ntarget = 0\n\n\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\n\n\n\n\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n    temp = target - sum(result)\n    \n    while not ((temp &lt;= upper) and (temp &gt;= lower)):\n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n    result = np.append(result, temp)\n    \n    return result\n\n\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n  \n    result_ls = []\n    \n    for i in range(n_sim):\n      \n      result_sim = rand_iterative(n_assets, lower, upper, target)\n      result_ls.append(result_sim)\n      \n    result = pd.DataFrame(result_ls)\n    \n    return result\n\n\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-mean",
    "href": "posts/optim-py/index.html#maximize-mean",
    "title": "Optimization",
    "section": "Maximize mean",
    "text": "Maximize mean\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ntarget = 0.06\nstart = np.array([1] * len(factors))\nbnds = [(np.finfo(float).eps, 1) for i in range(len(factors))]\n\n\ndef max_mean_cons(params, sigma, target):\n    \n    var = np.dot(params, np.dot(sigma, params))\n    \n    result = target ** 2 - var\n    \n    return result\n\ndef max_mean_obj(params, mu):\n    \n    result = -np.dot(mu, params)\n    \n    return result\n\ndef max_mean_optim(params, mu, sigma, target):\n  \n    cons = [{\"type\": \"ineq\", \"fun\": max_mean_cons, \"args\": (sigma, target)},\n           {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n    \n    result = minimize(max_mean_obj, params, args = (mu), bounds = bnds, constraints = cons)\n    \n    return result.x\n\n\nparams1 = max_mean_optim(start, mu, sigma, target)\nparams1\n\narray([4.92048551e-01, 5.07951449e-01, 2.22044605e-16, 2.76986550e-16])\n\n\n\nnp.dot(mu, params1)\n\n0.05173107727195373\n\n\n\nnp.sqrt(np.dot(params1, np.dot(sigma, params1)))\n\n0.06000001529162513"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-variance",
    "href": "posts/optim-py/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ntarget = 0.03\n\n\ndef min_var_cons(params, mu, target):\n    \n    result = np.dot(mu, params) - target\n    \n    return result\n\ndef min_var_obj(params, sigma):\n    \n    result = np.dot(params, np.dot(sigma, params))\n    \n    return result\n\ndef min_var_optim(params, mu, sigma, target):\n  \n    cons = [{\"type\": \"ineq\", \"fun\": min_var_cons, \"args\": (mu, target)},\n            {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n    \n    result = minimize(min_var_obj, params, args = (sigma), bounds = bnds, constraints = cons)\n    \n    return result.x\n\n\nparams2 = min_var_optim(start, mu, sigma, target)\nparams2\n\narray([0.26444216, 0.45554317, 0.16964013, 0.11037455])\n\n\n\nnp.dot(mu, params2)\n\n0.029999999974819183\n\n\n\nnp.sqrt(np.dot(params2, np.dot(sigma, params2))) \n\n0.035144783508269446"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-utility",
    "href": "posts/optim-py/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\n\n\ndef max_utility_obj(params, mu, sigma, target):\n    \n    result = 0.5 * target * (np.dot(params, np.dot(sigma, params))) - np.dot(mu, params)\n    \n    return result\n\ndef max_utility_optim(params, mu, sigma, target):\n  \n    cons = [{\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n    \n    result = minimize(max_utility_obj, params, args = (mu, sigma, target), bounds = bnds,\n                      constraints = cons) \n    \n    return result.x\n\n\nparams3 = max_utility_optim(start, mu, sigma, target)\nparams3\n\narray([5.70840700e-01, 4.29159300e-01, 2.22044605e-16, 2.53703308e-16])\n\n\n\nnp.dot(mu, params3)\n\n0.05797878717218443\n\n\n\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n\n0.0701082063353154"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nhttps://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html\n\n\npd.DataFrame({\n  \"max_pnl\": params1,\n  \"min_risk\": params2,\n  \"max_ratio\": params3\n})\n\n        max_pnl  min_risk     max_ratio\n0  4.920486e-01  0.264442  5.708407e-01\n1  5.079514e-01  0.455543  4.291593e-01\n2  2.220446e-16  0.169640  2.220446e-16\n3  2.769866e-16  0.110375  2.537033e-16"
  },
  {
    "objectID": "posts/eigen-r/index.html",
    "href": "posts/eigen-r/index.html",
    "title": "Eigen",
    "section": "",
    "text": "Decomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\neigen_decomp &lt;- function(x, comps) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values[1:comps]\n    V &lt;- LV$vectors[ , 1:comps]\n    \n    result &lt;- V %*% sweep(t(V), 1, L, \"*\")\n    \n    return(result)\n    \n}\n\n\ncomps &lt;- 1\n\n\neigen_decomp(overlap_xts, comps) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  0.0300775965 -3.659506e-03 -1.572091e-04  2.578798e-03\n[2,] -0.0036595055  4.452477e-04  1.912744e-05 -3.137594e-04\n[3,] -0.0001572091  1.912744e-05  8.216978e-07 -1.347882e-05\n[4,]  0.0025787985 -3.137594e-04 -1.347882e-05  2.211015e-04\n\n\n\n# cov(overlap_xts) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\nvariance_explained &lt;- function(x) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values\n    \n    result &lt;- cumsum(L) / sum(L)\n    \n    return(result)\n    \n}\n\n\nvariance_explained(overlap_xts)\n\n[1] 0.8733494 0.9922194 0.9982806 1.0000000\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n\\]\n\nroll_eigen1 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        evec &lt;- eigen(cov(x[idx, ]))[[\"vectors\"]][ , comp]\n        result_ls &lt;- append(result_ls, list(evec))\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\ncomp &lt;- 1\n\n\nraw_df &lt;- roll_eigen1(overlap_xts, width, comp)\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jjf234/rolleigen\") # roll (&gt;= 1.1.7)\n# library(rolleigen)\n# raw_df &lt;- roll_eigen(overlap_xts, width)[[\"vectors\"]][ , comp, ]\n\n\n\n\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\nroll_eigen2 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        evecs &lt;- eigen(cov(x[idx, ]))[[\"vectors\"]]\n                \n        if (i &gt; width) {\n          \n            similarity &lt;- crossprod(evecs, result_ls[[length(result_ls)]])\n            order &lt;- which.max(abs(similarity))\n            evec &lt;- sign(similarity)[order] * evecs[ , order]\n            \n            result_ls &lt;- append(result_ls, list(evec))\n            \n        } else {\n            result_ls &lt;- append(result_ls, list(evecs[ , comp]))\n        }\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\nclean_df &lt;- roll_eigen2(overlap_xts, width, comp)"
  },
  {
    "objectID": "posts/eigen-py/index.html",
    "href": "posts/eigen-py/index.html",
    "title": "Eigen",
    "section": "",
    "text": "Decomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\ndef eigen(x):\n  \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    V = V[:, idx]\n    \n    result = {\n        \"values\": L,\n        \"vectors\": V\n    }\n    \n    return result\n\n\ndef eigen_decomp(x, comps):\n    \n    LV = eigen(x)\n    L = LV[\"values\"][:comps]\n    V = LV[\"vectors\"][:, :comps]\n    \n    result = np.dot(V, np.multiply(L, V.T))\n    \n    return result\n\n\ncomps = 1\n\n\neigen_decomp(overlap_df, comps) * scale[\"periods\"] * scale[\"overlap\"]\n\narray([[ 2.88335291e-02, -3.64429585e-03, -1.18217457e-04,\n         2.54224014e-03],\n       [-3.64429585e-03,  4.60605851e-04,  1.49416114e-05,\n        -3.21316032e-04],\n       [-1.18217457e-04,  1.49416114e-05,  4.84691521e-07,\n        -1.04231835e-05],\n       [ 2.54224014e-03, -3.21316032e-04, -1.04231835e-05,\n         2.24148244e-04]])\n\n\n\n# np.cov(overlap_df.T) * scale[\"periods\"] * scale[\"overlap\"]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\ndef variance_explained(x):\n    \n    LV = eigen(x)\n    L = LV[\"values\"]\n    \n    result = L.cumsum() / L.sum()\n    \n    return result\n\n\nvariance_explained(overlap_df)\n\narray([0.87489926, 0.99199136, 0.99831639, 1.        ])\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n\\]\n\ndef roll_eigen1(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evec = eigen(x.iloc[idx])[\"vectors\"][:, comp - 1]\n        result_ls.append(evec)\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df  \n\n\ncomp = 1\n\n\nraw_df = roll_eigen1(overlap_df, width, comp)\n\n\n\n\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\ndef roll_eigen2(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evecs = eigen(x.iloc[idx])[\"vectors\"]\n        \n        if i &gt; width - 1:\n            \n            similarity = np.dot(evecs.T, result_ls[-1])\n            order = np.argmax(np.abs(similarity))\n            evec = np.multiply(np.sign(similarity[order]), evecs[:, order])\n            \n            result_ls.append(evec)\n            \n        else:\n            result_ls.append(evecs[:, comp - 1])\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df\n\n\nclean_df = roll_eigen2(overlap_df, width, comp)"
  },
  {
    "objectID": "posts/markets-py/index.html",
    "href": "posts/markets-py/index.html",
    "title": "Markets",
    "section": "",
    "text": "One month reversal and 2-12 month momentum are two ends of the spectrum. The general trend indicates that positive acceleration leads to reversals or negative acceleration leads to rebounds. An unsustainable acceleration leading to reversal can reconcile the one-month reversal and 2-12 month momentum. The key is that it implies that acceleration is not sustainable.\n\n# \"Momentum, Acceleration, and Reversal\"\ndef pnl(x):\n    return np.nanprod(1 + x) - 1\n\n\norder = 20\n\n\nmomentum_df = returns_df[\"returns\"].shift(order).rolling(width - order, min_periods = 1).apply(pnl, raw = False).dropna()"
  },
  {
    "objectID": "posts/markets-py/index.html#expected-value",
    "href": "posts/markets-py/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-py/index.html#variance",
    "href": "posts/markets-py/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\ndef sd(x):\n    \n    n_rows = sum(~np.isnan(x))\n        \n    if n_rows &gt; 1:\n        result = np.sqrt(np.nansum(x ** 2) / (n_rows - 1))\n    else:\n        result = np.nan\n        \n    return result\n\n\n# volatility scale only\nscore_df = (momentum_df / momentum_df.rolling(width, min_periods = 1).apply(sd, raw = False)).dropna()\n\n\n# overall_df = score_df.mean(axis = 1)\n# overall_df = overall_df / overall_df.rolling(width, min_periods = 1).apply(risk, raw = False)\n\n\n# score_df.insert(loc = 0, column = \"Overall\", value = overall_df)\n# score_df = score_df.dropna()"
  },
  {
    "objectID": "posts/markets-py/index.html#interquartile-range",
    "href": "posts/markets-py/index.html#interquartile-range",
    "title": "Markets",
    "section": "Interquartile range",
    "text": "Interquartile range\nOutliers are defined as the regression residuals that fall below \\(Q_{1}−1.5\\times IQR\\) or above \\(Q_{3}+1.5\\times IQR\\):\n\nhttps://stats.stackexchange.com/a/1153\nhttps://stats.stackexchange.com/a/108951\nhttps://robjhyndman.com/hyndsight/tsoutliers/\n\n\ndef outliers(z):\n  \n    n_cols = z.shape[1]\n    result_ls = []\n\n    for j in range(n_cols):\n      \n        y = z.iloc[:, j]\n\n        if (n_cols == 0):\n            x = sm.add_constant(range(len(y)))\n        else:\n            x = sm.add_constant(z.drop(z.columns[j], axis = 1))\n\n        coef = sm.WLS(y, x).fit().params\n        predict = coef.iloc[0] + np.dot(x.iloc[:, 1:], coef[1:])\n        resid = y - predict\n\n        lower = resid.quantile(0.25)\n        upper = resid.quantile(0.75)\n        iqr = upper - lower\n\n        total = y[(resid &lt; lower - 1.5 * iqr) | (resid &gt; upper + 1.5 * iqr)]\n        \n        total = pd.DataFrame({\"date\": total.index, \"symbol\": total.name, \"values\": total})\n        result_ls.append(total)\n\n    result = pd.concat(result_ls, ignore_index = True)\n    result = result.pivot_table(index = \"date\", columns = \"symbol\", values = \"values\")\n\n    return result\n\n\noutliers_df = outliers(score_df)"
  },
  {
    "objectID": "posts/markets-py/index.html#contour-ellipsoid",
    "href": "posts/markets-py/index.html#contour-ellipsoid",
    "title": "Markets",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid"
  },
  {
    "objectID": "posts/risk-py/index.html",
    "href": "posts/risk-py/index.html",
    "title": "Risk",
    "section": "",
    "text": "intercept = True\n\n\n\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\ndef lm_coef(x, y, weights, intercept):\n    \n    if (intercept): x = sm.add_constant(x)\n        \n    result = np.dot(np.linalg.inv(np.dot(x.T, np.multiply(weights, x))),\n                    np.dot(x.T, np.multiply(weights, y)))\n    \n    return np.ravel(result)\n\n\nlm_coef(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([ 2.03859603e-05,  2.22498099e-01, -1.49187269e-01,  2.77910111e+00,\n        1.46549469e+00])\n\n\n\nif (intercept): overlap_x_df = sm.add_constant(overlap_x_df)\n    \nfit = sm.WLS(overlap_y_df, overlap_x_df, weights = weights).fit()\n\nif (intercept): overlap_x_df = overlap_x_df.iloc[:, 1:]\n\nnp.array(fit.params)\n\narray([ 2.03859603e-05,  2.22498099e-01, -1.49187269e-01,  2.77910111e+00,\n        1.46549469e+00])\n\n\n\n\n\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\ndef lm_rsq(x, y, weights, intercept):\n            \n    coef = np.matrix(lm_coef(x, y, weights, intercept))\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n    result = np.dot(coef, np.dot(np.dot(x.T, np.multiply(weights, x)), coef.T)) / \\\n        np.dot(y.T, np.multiply(weights, y))\n    \n    return result.item()\n\n\nlm_rsq(overlap_x_df, overlap_y_df, weights, intercept)\n\n0.8613601611976685\n\n\n\nfit.rsquared\n\n0.8613601611976677\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\ndef lm_se(x, y, weights, intercept):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n        df_resid = n_rows - n_cols - 1 \n        \n    else:\n        df_resid = n_rows - n_cols        \n    \n    var_y = np.dot(y.T, np.multiply(weights, y))\n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    result = np.sqrt(var_resid * np.linalg.inv(np.dot(x.T, np.multiply(weights, x))).diagonal())\n    \n    return np.ravel(result)\n\n\nlm_se(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([5.02568490e-05, 1.89876878e-02, 3.55755488e-02, 1.90505497e-01,\n       1.47305971e-01])\n\n\n\nnp.array(fit.bse)\n\narray([5.02568490e-05, 1.89876878e-02, 3.55755488e-02, 1.90505497e-01,\n       1.47305971e-01])\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\n\n\ncomps = 1\n\n\n\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\ndef pcr_coef(x, y, comps):\n    \n    x = x - np.average(x, axis = 0)\n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    V = V[:, idx]\n    \n    W = np.dot(x, V)\n    gamma = np.dot(np.dot(np.linalg.inv(np.dot(W.T, W)), W.T), y)\n    \n    result = np.dot(V[:, :comps], gamma[:comps])\n    \n    return np.ravel(result)\n\n\nscale_x_df = (overlap_x_df - np.average(overlap_x_df, axis = 0)) \\\n    / np.std(overlap_x_df, axis = 0, ddof = 1)\n\n\npcr_coef(scale_x_df, overlap_y_df, comps)\n\narray([ 0.00077506, -0.00068576,  0.00053957,  0.00058018])\n\n\n\npcr_coef(overlap_x_df, overlap_y_df, comps)\n\narray([ 0.42156346, -0.10053197,  0.01197475,  0.03201413])\n\n\n\npca = PCA(n_components = len(factors))\npca_x_df = pca.fit_transform(scale_x_df)\n\nfit = LinearRegression(fit_intercept = False).fit(pca_x_df, overlap_y_df)\n\ngamma = fit.coef_\nnp.dot(pca.components_.T[:, :comps], gamma.T[:comps]).ravel()\n\narray([ 0.00077506, -0.00068576,  0.00053957,  0.00058018])\n\n\n\n\n\n\ndef pcr_rsq(x, y, comps):\n    \n    coef = np.matrix(pcr_coef(x, y, comps))\n    \n    x = x - np.average(x, axis = 0)\n    y = y - np.average(y, axis = 0)\n    \n    result = np.dot(np.dot(coef, np.dot(x.T, x)), coef.T) / np.dot(y.T, y)\n    \n    return result.item()\n\n\npcr_rsq(scale_x_df, overlap_y_df, comps)\n\n0.8292106858280622\n\n\n\npcr_rsq(overlap_x_df, overlap_y_df, comps)\n\n0.6859945034291106\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\ndef pcr_se(x, y, comps):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = pcr_rsq(x, y, comps)\n    \n    y = y - np.average(y, axis = 0)\n    \n    df_resid = n_rows - n_cols - 1\n    \n    var_y = np.dot(y.T, y)   \n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    # uses statsmodels for illustrative purposes\n    pca = sm.multivariate.PCA(x, standardize = False, demean = True)\n    L = pca.eigenvals[:comps]\n    V = pca.eigenvecs.iloc[:, :comps]\n    \n    result = np.sqrt(var_resid * np.dot(V, np.dot(np.diag(1 / L), V.T)).diagonal())\n    \n    return np.ravel(result)\n\n\npcr_se(scale_x_df, overlap_y_df, comps)\n\narray([2.23813934e-05, 1.98024706e-05, 1.55811526e-05, 1.67537033e-05])\n\n\n\npcr_se(overlap_x_df, overlap_y_df, comps)\n\narray([0.01814775, 0.00432777, 0.0005155 , 0.00137817])"
  },
  {
    "objectID": "posts/risk-py/index.html#ordinary-least-squares",
    "href": "posts/risk-py/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "",
    "text": "intercept = True\n\n\n\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\ndef lm_coef(x, y, weights, intercept):\n    \n    if (intercept): x = sm.add_constant(x)\n        \n    result = np.dot(np.linalg.inv(np.dot(x.T, np.multiply(weights, x))),\n                    np.dot(x.T, np.multiply(weights, y)))\n    \n    return np.ravel(result)\n\n\nlm_coef(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([ 2.03859603e-05,  2.22498099e-01, -1.49187269e-01,  2.77910111e+00,\n        1.46549469e+00])\n\n\n\nif (intercept): overlap_x_df = sm.add_constant(overlap_x_df)\n    \nfit = sm.WLS(overlap_y_df, overlap_x_df, weights = weights).fit()\n\nif (intercept): overlap_x_df = overlap_x_df.iloc[:, 1:]\n\nnp.array(fit.params)\n\narray([ 2.03859603e-05,  2.22498099e-01, -1.49187269e-01,  2.77910111e+00,\n        1.46549469e+00])\n\n\n\n\n\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\ndef lm_rsq(x, y, weights, intercept):\n            \n    coef = np.matrix(lm_coef(x, y, weights, intercept))\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n    result = np.dot(coef, np.dot(np.dot(x.T, np.multiply(weights, x)), coef.T)) / \\\n        np.dot(y.T, np.multiply(weights, y))\n    \n    return result.item()\n\n\nlm_rsq(overlap_x_df, overlap_y_df, weights, intercept)\n\n0.8613601611976685\n\n\n\nfit.rsquared\n\n0.8613601611976677\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\ndef lm_se(x, y, weights, intercept):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n        df_resid = n_rows - n_cols - 1 \n        \n    else:\n        df_resid = n_rows - n_cols        \n    \n    var_y = np.dot(y.T, np.multiply(weights, y))\n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    result = np.sqrt(var_resid * np.linalg.inv(np.dot(x.T, np.multiply(weights, x))).diagonal())\n    \n    return np.ravel(result)\n\n\nlm_se(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([5.02568490e-05, 1.89876878e-02, 3.55755488e-02, 1.90505497e-01,\n       1.47305971e-01])\n\n\n\nnp.array(fit.bse)\n\narray([5.02568490e-05, 1.89876878e-02, 3.55755488e-02, 1.90505497e-01,\n       1.47305971e-01])"
  },
  {
    "objectID": "posts/risk-py/index.html#principal-component-regression",
    "href": "posts/risk-py/index.html#principal-component-regression",
    "title": "Risk",
    "section": "",
    "text": "from sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\n\n\ncomps = 1\n\n\n\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\ndef pcr_coef(x, y, comps):\n    \n    x = x - np.average(x, axis = 0)\n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    V = V[:, idx]\n    \n    W = np.dot(x, V)\n    gamma = np.dot(np.dot(np.linalg.inv(np.dot(W.T, W)), W.T), y)\n    \n    result = np.dot(V[:, :comps], gamma[:comps])\n    \n    return np.ravel(result)\n\n\nscale_x_df = (overlap_x_df - np.average(overlap_x_df, axis = 0)) \\\n    / np.std(overlap_x_df, axis = 0, ddof = 1)\n\n\npcr_coef(scale_x_df, overlap_y_df, comps)\n\narray([ 0.00077506, -0.00068576,  0.00053957,  0.00058018])\n\n\n\npcr_coef(overlap_x_df, overlap_y_df, comps)\n\narray([ 0.42156346, -0.10053197,  0.01197475,  0.03201413])\n\n\n\npca = PCA(n_components = len(factors))\npca_x_df = pca.fit_transform(scale_x_df)\n\nfit = LinearRegression(fit_intercept = False).fit(pca_x_df, overlap_y_df)\n\ngamma = fit.coef_\nnp.dot(pca.components_.T[:, :comps], gamma.T[:comps]).ravel()\n\narray([ 0.00077506, -0.00068576,  0.00053957,  0.00058018])\n\n\n\n\n\n\ndef pcr_rsq(x, y, comps):\n    \n    coef = np.matrix(pcr_coef(x, y, comps))\n    \n    x = x - np.average(x, axis = 0)\n    y = y - np.average(y, axis = 0)\n    \n    result = np.dot(np.dot(coef, np.dot(x.T, x)), coef.T) / np.dot(y.T, y)\n    \n    return result.item()\n\n\npcr_rsq(scale_x_df, overlap_y_df, comps)\n\n0.8292106858280622\n\n\n\npcr_rsq(overlap_x_df, overlap_y_df, comps)\n\n0.6859945034291106\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\ndef pcr_se(x, y, comps):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = pcr_rsq(x, y, comps)\n    \n    y = y - np.average(y, axis = 0)\n    \n    df_resid = n_rows - n_cols - 1\n    \n    var_y = np.dot(y.T, y)   \n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    # uses statsmodels for illustrative purposes\n    pca = sm.multivariate.PCA(x, standardize = False, demean = True)\n    L = pca.eigenvals[:comps]\n    V = pca.eigenvecs.iloc[:, :comps]\n    \n    result = np.sqrt(var_resid * np.dot(V, np.dot(np.diag(1 / L), V.T)).diagonal())\n    \n    return np.ravel(result)\n\n\npcr_se(scale_x_df, overlap_y_df, comps)\n\narray([2.23813934e-05, 1.98024706e-05, 1.55811526e-05, 1.67537033e-05])\n\n\n\npcr_se(overlap_x_df, overlap_y_df, comps)\n\narray([0.01814775, 0.00432777, 0.0005155 , 0.00137817])"
  },
  {
    "objectID": "posts/risk-py/index.html#partial-least-squares",
    "href": "posts/risk-py/index.html#partial-least-squares",
    "title": "Risk",
    "section": "Partial least squares",
    "text": "Partial least squares"
  },
  {
    "objectID": "posts/risk-py/index.html#standalone-risk",
    "href": "posts/risk-py/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\ndef cov_wt(x, weights, center):\n    \n    sum_w = sum(weights)\n    sumsq_w = sum(np.power(weights, 2))\n    \n    if (center):\n    \n        x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n    \n    result = np.dot(x.T, np.multiply(weights, x)) / (sum_w - sumsq_w / sum_w)\n    \n    return result\n\n\ndef lm_sar(x, y, weights, intercept):\n    \n    coef = lm_coef(x, y, weights, intercept)\n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept): x = sm.add_constant(x)\n    \n    # sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n    #                aweights = weights.reshape(-1))\n    sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n    sar = np.multiply(np.power(coef, 2).T, sigma[:-1, :-1].diagonal())\n    sar_eps = (1 - rsq) * sigma[-1, -1]\n    \n    result = np.sqrt(np.concatenate((np.matrix(sigma[-1, -1]),\n                                     np.matrix(sar),\n                                     np.matrix(sar_eps)), axis = 1))\n    \n    return np.ravel(result)\n\n\nlm_sar(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"]) * np.sqrt(scale[\"overlap\"])\n\narray([0.0734885 , 0.        , 0.03033462, 0.00916502, 0.03107055,\n       0.02367881, 0.02736298])"
  },
  {
    "objectID": "posts/risk-py/index.html#risk-contribution",
    "href": "posts/risk-py/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttps://bookdown.org/compfinezbook/introcompfinr/Portfolio-risk-reports.html\n\n\ndef lm_mcr(x, y, weights, intercept):\n    \n    coef = np.matrix(lm_coef(x, y, weights, intercept)).T\n    rsq = lm_rsq(x, y, weights, intercept)\n        \n    if (intercept): x = sm.add_constant(x)\n    \n#     sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n#                    aweights = weights.reshape(-1))\n    sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n    mcr = np.multiply(coef, np.dot(sigma[:-1, :-1], coef)) / np.sqrt(sigma[-1, -1])\n    mcr_eps = np.sqrt(sigma[-1, -1]) - sum(mcr)\n    \n    result = np.concatenate((np.sqrt(np.matrix(sigma[-1, -1])),\n                             np.matrix(mcr).T,\n                             np.matrix(mcr_eps)), axis = 1)\n    \n    return np.ravel(result)\n\n\nlm_mcr(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"]) * np.sqrt(scale[\"overlap\"])\n\narray([0.0734885 , 0.        , 0.02445131, 0.00547407, 0.01896858,\n       0.01440611, 0.01018843])"
  },
  {
    "objectID": "posts/risk-py/index.html#implied-shocks",
    "href": "posts/risk-py/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\ndef implied_shocks(shocks, x, z, weights):\n\n    beta = np.linalg.lstsq(np.multiply(weights, z), np.multiply(weights, x), rcond = None)[0]\n                     \n    result = np.dot(shocks, beta)\n    \n    return np.ravel(result)\n\n\nshocks = np.array([-0.1, 0.1])\noverlap_z_df = overlap_x_df.iloc[:, [0, 1]]\n\n\nimplied_shocks(shocks, overlap_x_df, overlap_z_df, weights)\n\narray([-0.1       ,  0.1       , -0.00942836, -0.00534669])"
  },
  {
    "objectID": "posts/risk-py/index.html#stress-pl",
    "href": "posts/risk-py/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\ndef pnl_stress(shocks, x, y, z, weights, intercept):\n    \n    coef = lm_coef(x, y, weights, intercept)\n    \n    if (intercept): x = sm.add_constant(x)\n    \n    result = np.multiply(coef.T, implied_shocks(shocks, x, z, weights))\n    \n    return np.ravel(result)\n\n\npnl_stress(shocks, overlap_x_df, overlap_y_df, overlap_z_df, weights, intercept)\n\narray([-9.69686889e-05, -2.22498099e-02, -1.49187269e-02, -2.62023739e-02,\n       -7.83554257e-03])"
  },
  {
    "objectID": "posts/markets-r/index.html",
    "href": "posts/markets-r/index.html",
    "title": "Markets",
    "section": "",
    "text": "One month reversal and 2-12 month momentum are two ends of the spectrum. The general trend indicates that positive acceleration leads to reversals or negative acceleration leads to rebounds. An unsustainable acceleration leading to reversal can reconcile the one-month reversal and 2-12 month momentum. The key is that it implies that acceleration is not sustainable.\n\norder &lt;- 20\n\n\n# \"Momentum, Acceleration, and Reversal\"\nmomentum_xts &lt;- na.omit(lag(roll_prod(1 + returns_xts, width - order, min_obs = 1) - 1, order))"
  },
  {
    "objectID": "posts/markets-r/index.html#expected-value",
    "href": "posts/markets-r/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-r/index.html#variance",
    "href": "posts/markets-r/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\n# volatility scale only\nscore_xts &lt;- na.omit(momentum_xts / roll_sd(momentum_xts, width, center = FALSE, min_obs = 1))\n\n\n# overall_xts &lt;- xts(rowMeans(score_xts), index(score_xts))\n# overall_xts &lt;- overall_xts / roll_sd(overall_xts, width, center = FALSE, min_obs = 1)\n# colnames(overall_xts) &lt;- \"Overall\"\n\n\n# score_xts &lt;- na.omit(merge(overall_xts, score_xts))"
  },
  {
    "objectID": "posts/markets-r/index.html#interquartile-range",
    "href": "posts/markets-r/index.html#interquartile-range",
    "title": "Markets",
    "section": "Interquartile range",
    "text": "Interquartile range\nOutliers are defined as the regression residuals that fall below \\(Q_{1}−1.5\\times IQR\\) or above \\(Q_{3}+1.5\\times IQR\\):\n\nhttps://stats.stackexchange.com/a/1153\nhttps://stats.stackexchange.com/a/108951\nhttps://robjhyndman.com/hyndsight/tsoutliers/\n\n\noutliers &lt;- function(z) {\n  \n  n_cols &lt;- ncol(z)\n  result_ls &lt;- list()\n  \n  for (j in 1:n_cols) {\n    \n    y &lt;- z[ , j]\n    \n    if (n_cols == 1) {\n      x &lt;- 1:length(y)\n    } else {\n      x &lt;- cbind(1:length(y), z[ , -j])\n    }\n    \n    coef &lt;- coef(lm(y ~ x))\n    predict &lt;- coef[1] + x %*% as.matrix(coef[-1])\n    resid &lt;- y - predict\n    \n    lower &lt;- quantile(resid, prob = 0.25)\n    upper &lt;- quantile(resid, prob = 0.75)\n    iqr &lt;- upper - lower\n    \n    total &lt;- y[(resid &lt; lower - 1.5 * iqr) | (resid &gt; upper + 1.5 * iqr)]\n    \n    result_ls &lt;- append(result_ls, list(total))\n    \n  }\n  \n  result &lt;- do.call(merge, result_ls)\n  \n  return(result)\n  \n}\n\n\noutliers_xts &lt;- outliers(score_xts)"
  },
  {
    "objectID": "posts/markets-r/index.html#contour-ellipsoid",
    "href": "posts/markets-r/index.html#contour-ellipsoid",
    "title": "Markets",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid"
  },
  {
    "objectID": "posts/risk-r/index.html",
    "href": "posts/risk-r/index.html",
    "title": "Risk",
    "section": "",
    "text": "intercept &lt;- TRUE\n\n\n\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\nlm_coef &lt;- function(x, y, weights, intercept) {\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    result &lt;- solve(crossprod(x, diag(weights)) %*% x) %*% crossprod(x, diag(weights) %*% y)\n    \n    return(result)\n    \n}\n\n\nt(lm_coef(overlap_x_xts, overlap_y_xts, weights, intercept))\n\n       (Intercept)    xSP500 xDTWEXAFEGS   xDGS10 xBAMLH0A0HYM2\nBAICX -1.60223e-05 0.2187729  -0.1447544 2.752537      1.599758\n\n\n\nif (intercept) {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts, weights = weights)\n} else {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts - 1, weights = weights)\n}\n    \ncoef(fit)\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n            -0.0000160223              0.2187728535             -0.1447543626 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             2.7525374459              1.5997581487 \n\n\n\n\n\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\nlm_rsq &lt;- function(x, y, weights, intercept) {\n        \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        x &lt;- sweep(x, 2, apply(x, 2, weighted.mean, w = weights), \"-\")\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n    }\n    \n    result &lt;- (t(coef) %*% (crossprod(x, diag(weights)) %*% x) %*% coef) / (crossprod(y, diag(weights)) %*% y)\n    \n    return(result)\n    \n}\n\n\nlm_rsq(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n          BAICX\nBAICX 0.8538352\n\n\n\nsummary(fit)$r.squared\n\n[1] 0.8538352\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\nlm_se &lt;- function(x, y, weights, intercept) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n        df_resid &lt;- n_rows - n_cols - 1\n        \n    } else {\n        df_resid &lt;- n_rows - n_cols\n    }\n    \n    var_y &lt;- crossprod(y, diag(weights)) %*% y\n    var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n    \n    result &lt;- sqrt(var_resid * diag(solve(crossprod(x, diag(weights)) %*% x)))\n    \n    return(result)\n}\n\n\nlm_se(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n  (Intercept)        xSP500   xDTWEXAFEGS        xDGS10 xBAMLH0A0HYM2 \n 5.155263e-05  1.933628e-02  3.600169e-02  1.892733e-01  1.552193e-01 \n\n\n\ncoef(summary(fit))[ , \"Std. Error\"]\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n             5.155263e-05              1.933628e-02              3.600169e-02 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             1.892733e-01              1.552193e-01 \n\n\n\n\n\n\n\nlibrary(pls)\n\n\ncomps &lt;- 1\n\n\n\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\npcr_coef &lt;- function(x, y, comps) {\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    LV &lt;- eigen(cov(x))\n    V &lt;- LV$vectors\n    \n    W &lt;- x %*% V\n    gamma &lt;- solve(crossprod(W)) %*% (crossprod(W, y))\n    \n    result &lt;- V[ , 1:comps] %*% as.matrix(gamma[1:comps])\n    \n    return(result)\n    \n}\n\n\nscale_x_xts &lt;- scale(overlap_x_xts)\n\n\nt(pcr_coef(scale_x_xts, overlap_y_xts, comps))\n\n            [,1]          [,2]         [,3]         [,4]\n[1,] 0.000811357 -0.0006897159 0.0005066415 0.0006189496\n\n\n\nt(pcr_coef(overlap_x_xts, overlap_y_xts, comps))\n\n          [,1]        [,2]        [,3]       [,4]\n[1,] 0.4155457 -0.08961212 0.009802575 0.03185193\n\n\n\nfit &lt;- pcr(reformulate(termlabels = \".\", response = tickers), \n           data = merge(scale_x_xts, overlap_y_xts), ncomp = comps)\ncoef(fit)[ , , 1]\n\n        SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 0.0008113570 -0.0006897159  0.0005066415  0.0006189496 \n\n\n\n\n\n\npcr_rsq &lt;- function(x, y, comps) {\n    \n    coef &lt;- pcr_coef(x, y, comps)\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    result &lt;- (t(coef) %*% crossprod(x) %*% coef) / crossprod(y)\n    \n    return(result)\n    \n}\n\n\npcr_rsq(scale_x_xts, overlap_y_xts, comps)\n\n          BAICX\nBAICX 0.8166682\n\n\n\npcr_rsq(overlap_x_xts, overlap_y_xts, comps)\n\n          BAICX\nBAICX 0.6689196\n\n\n\nR2(fit)$val[comps + 1]\n\n[1] 0.8166682\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\npcr_se &lt;- function(x, y, comps) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- pcr_rsq(x, y, comps)\n    \n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    df_resid &lt;- n_rows - n_cols - 1\n    \n    var_y &lt;- crossprod(y)\n    var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values[1:comps] * (n_rows - 1)\n    V &lt;- LV$vectors[ , 1:comps]\n    \n    result &lt;- sqrt(var_resid * diag(V %*% sweep(t(V), 1, 1 / L, \"*\")))\n    \n    return(result)\n    \n}\n\n\npcr_se(scale_x_xts, overlap_y_xts, comps)\n\n[1] 2.446018e-05 2.079303e-05 1.527384e-05 1.865962e-05\n\n\n\npcr_se(overlap_x_xts, overlap_y_xts, comps)\n\n[1] 0.0186015901 0.0040114188 0.0004388049 0.0014258278"
  },
  {
    "objectID": "posts/risk-r/index.html#ordinary-least-squares",
    "href": "posts/risk-r/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "",
    "text": "intercept &lt;- TRUE\n\n\n\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\nlm_coef &lt;- function(x, y, weights, intercept) {\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    result &lt;- solve(crossprod(x, diag(weights)) %*% x) %*% crossprod(x, diag(weights) %*% y)\n    \n    return(result)\n    \n}\n\n\nt(lm_coef(overlap_x_xts, overlap_y_xts, weights, intercept))\n\n       (Intercept)    xSP500 xDTWEXAFEGS   xDGS10 xBAMLH0A0HYM2\nBAICX -1.60223e-05 0.2187729  -0.1447544 2.752537      1.599758\n\n\n\nif (intercept) {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts, weights = weights)\n} else {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts - 1, weights = weights)\n}\n    \ncoef(fit)\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n            -0.0000160223              0.2187728535             -0.1447543626 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             2.7525374459              1.5997581487 \n\n\n\n\n\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\nlm_rsq &lt;- function(x, y, weights, intercept) {\n        \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        x &lt;- sweep(x, 2, apply(x, 2, weighted.mean, w = weights), \"-\")\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n    }\n    \n    result &lt;- (t(coef) %*% (crossprod(x, diag(weights)) %*% x) %*% coef) / (crossprod(y, diag(weights)) %*% y)\n    \n    return(result)\n    \n}\n\n\nlm_rsq(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n          BAICX\nBAICX 0.8538352\n\n\n\nsummary(fit)$r.squared\n\n[1] 0.8538352\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\nlm_se &lt;- function(x, y, weights, intercept) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n        df_resid &lt;- n_rows - n_cols - 1\n        \n    } else {\n        df_resid &lt;- n_rows - n_cols\n    }\n    \n    var_y &lt;- crossprod(y, diag(weights)) %*% y\n    var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n    \n    result &lt;- sqrt(var_resid * diag(solve(crossprod(x, diag(weights)) %*% x)))\n    \n    return(result)\n}\n\n\nlm_se(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n  (Intercept)        xSP500   xDTWEXAFEGS        xDGS10 xBAMLH0A0HYM2 \n 5.155263e-05  1.933628e-02  3.600169e-02  1.892733e-01  1.552193e-01 \n\n\n\ncoef(summary(fit))[ , \"Std. Error\"]\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n             5.155263e-05              1.933628e-02              3.600169e-02 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             1.892733e-01              1.552193e-01"
  },
  {
    "objectID": "posts/risk-r/index.html#principal-component-regression",
    "href": "posts/risk-r/index.html#principal-component-regression",
    "title": "Risk",
    "section": "",
    "text": "library(pls)\n\n\ncomps &lt;- 1\n\n\n\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\npcr_coef &lt;- function(x, y, comps) {\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    LV &lt;- eigen(cov(x))\n    V &lt;- LV$vectors\n    \n    W &lt;- x %*% V\n    gamma &lt;- solve(crossprod(W)) %*% (crossprod(W, y))\n    \n    result &lt;- V[ , 1:comps] %*% as.matrix(gamma[1:comps])\n    \n    return(result)\n    \n}\n\n\nscale_x_xts &lt;- scale(overlap_x_xts)\n\n\nt(pcr_coef(scale_x_xts, overlap_y_xts, comps))\n\n            [,1]          [,2]         [,3]         [,4]\n[1,] 0.000811357 -0.0006897159 0.0005066415 0.0006189496\n\n\n\nt(pcr_coef(overlap_x_xts, overlap_y_xts, comps))\n\n          [,1]        [,2]        [,3]       [,4]\n[1,] 0.4155457 -0.08961212 0.009802575 0.03185193\n\n\n\nfit &lt;- pcr(reformulate(termlabels = \".\", response = tickers), \n           data = merge(scale_x_xts, overlap_y_xts), ncomp = comps)\ncoef(fit)[ , , 1]\n\n        SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 0.0008113570 -0.0006897159  0.0005066415  0.0006189496 \n\n\n\n\n\n\npcr_rsq &lt;- function(x, y, comps) {\n    \n    coef &lt;- pcr_coef(x, y, comps)\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    result &lt;- (t(coef) %*% crossprod(x) %*% coef) / crossprod(y)\n    \n    return(result)\n    \n}\n\n\npcr_rsq(scale_x_xts, overlap_y_xts, comps)\n\n          BAICX\nBAICX 0.8166682\n\n\n\npcr_rsq(overlap_x_xts, overlap_y_xts, comps)\n\n          BAICX\nBAICX 0.6689196\n\n\n\nR2(fit)$val[comps + 1]\n\n[1] 0.8166682\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\npcr_se &lt;- function(x, y, comps) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- pcr_rsq(x, y, comps)\n    \n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    df_resid &lt;- n_rows - n_cols - 1\n    \n    var_y &lt;- crossprod(y)\n    var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values[1:comps] * (n_rows - 1)\n    V &lt;- LV$vectors[ , 1:comps]\n    \n    result &lt;- sqrt(var_resid * diag(V %*% sweep(t(V), 1, 1 / L, \"*\")))\n    \n    return(result)\n    \n}\n\n\npcr_se(scale_x_xts, overlap_y_xts, comps)\n\n[1] 2.446018e-05 2.079303e-05 1.527384e-05 1.865962e-05\n\n\n\npcr_se(overlap_x_xts, overlap_y_xts, comps)\n\n[1] 0.0186015901 0.0040114188 0.0004388049 0.0014258278"
  },
  {
    "objectID": "posts/risk-r/index.html#standalone-risk",
    "href": "posts/risk-r/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\nlm_sar &lt;- function(x, y, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n    sar &lt;- coef ^ 2 * diag(sigma[-ncol(sigma), -ncol(sigma)])\n    sar_eps &lt;- (1 - rsq) * sigma[ncol(sigma), ncol(sigma)]\n    \n    result &lt;- sqrt(c(sigma[ncol(sigma), ncol(sigma)],\n                     sar,\n                     sar_eps))\n    \n    return(result)\n    \n}\n\n\nlm_sar(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]]) * sqrt(scale[[\"overlap\"]])\n\n[1] 0.074128665 0.000000000 0.030419602 0.008921341 0.031630773 0.026017588\n[7] 0.028340502"
  },
  {
    "objectID": "posts/risk-r/index.html#risk-contribution",
    "href": "posts/risk-r/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttps://bookdown.org/compfinezbook/introcompfinr/Portfolio-risk-reports.html\n\n\nlm_mcr &lt;- function(x, y, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n    mcr &lt;- coef * sigma[-ncol(sigma), -ncol(sigma)] %*% coef / sqrt(sigma[ncol(sigma), ncol(sigma)])\n    mcr_eps &lt;- sqrt(sigma[ncol(sigma), ncol(sigma)]) - sum(mcr)\n    \n    result &lt;- c(sqrt(sigma[ncol(sigma), ncol(sigma)]),\n                mcr,\n                mcr_eps)\n    \n    return(result)\n    \n}\n\n\nlm_mcr(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]]) * sqrt(scale[[\"overlap\"]])\n\n[1] 0.074128665 0.000000000 0.024263394 0.005033437 0.017959933 0.016036900\n[7] 0.010835000"
  },
  {
    "objectID": "posts/risk-r/index.html#implied-shocks",
    "href": "posts/risk-r/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\nimplied_shocks &lt;- function(shocks, x, z, weights) {\n    \n    beta &lt;- solve(crossprod(z, diag(weights) %*% z)) %*% crossprod(z, diag(weights) %*% x)\n    \n    result &lt;- shocks %*% beta\n    \n    return(result)\n    \n}\n\n\nshocks &lt;- c(-0.1, 0.1)\noverlap_z_xts &lt;- overlap_x_xts[ , 1:2]\n\n\nimplied_shocks(shocks, overlap_x_xts, overlap_z_xts, weights)\n\n     SP500 DTWEXAFEGS        DGS10 BAMLH0A0HYM2\n[1,]  -0.1        0.1 -0.009343047  -0.00506424"
  },
  {
    "objectID": "posts/risk-r/index.html#stress-pl",
    "href": "posts/risk-r/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\npnl_stress &lt;- function(shocks, x, y, z, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    result &lt;- t(coef) * implied_shocks(shocks, x, z, weights)\n    \n    return(result)    \n    \n}\n\n\npnl_stress(shocks, overlap_x_xts, overlap_y_xts, overlap_z_xts, weights, intercept)\n\n       (Intercept)      xSP500 xDTWEXAFEGS      xDGS10 xBAMLH0A0HYM2\nBAICX 0.0001045179 -0.02187729 -0.01447544 -0.02571709  -0.008101559"
  },
  {
    "objectID": "posts/securities-r/index.html",
    "href": "posts/securities-r/index.html",
    "title": "Securities",
    "section": "",
    "text": "level_shock &lt;- function(shock, S, tau, sigma) {\n    \n    result &lt;- S * (1 + shock * sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\n\nhttps://en.wikipedia.org/wiki/Greeks_(finance)\nhttps://www.wolframalpha.com/input/?i=option+pricing+formula\n\n\nfactor &lt;- \"SP500\"\ntypes &lt;- c(\"call\", \"put\")\nS &lt;- coredata(na.locf(levels_xts[nrow(levels_xts), factor]))\nK &lt;- S\nr &lt;- 0 # use \"USD3MTD156N\"\nq &lt;- 0 # see https://stackoverflow.com/a/11286679 \ntau &lt;- 1 # = 252 / 252\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor]) # use \"VIXCLS\"\nshocks &lt;- seq(-3, 3, by = 0.5)\n\n\ngreeks_dt &lt;- CJ(type = types, shock = shocks)\ngreeks_dt[ , spot := level_shock(shock, S, tau, sigma), by = c(\"type\", \"shock\")]\n\n\n\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_value &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n    r_df &lt;- exp(-r * tau)\n    q_df &lt;- exp(-q * tau)\n    \n    call_value &lt;- S * q_df * Phi(d1) - r_df * K * Phi(d2)\n    put_value &lt;- r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n} \n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\nbs_d1 &lt;- function(S, K, r, q, tau, sigma) {\n    \n    result &lt;- (log(S / K) + (r - q + sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\nbs_d2 &lt;- function(S, K, r, q, tau, sigma) {\n    \n    result &lt;- (log(S / K) + (r - q - sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\nphi &lt;- function(x) {\n    \n    result &lt;- dnorm(x)\n    \n    return(result)\n    \n}\n\nPhi &lt;- function(x) {\n    \n    result &lt;- pnorm(x)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , d1 := bs_d1(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , d2 := bs_d2(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , value := bs_value(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_delta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    q_df &lt;- exp(-q * tau)\n  \n    call_value &lt;- q_df * Phi(d1)\n    put_value &lt;- -q_df * Phi(-d1)\n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n} \n\n\ngreeks_dt[ , delta := bs_delta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\n\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\nbs_delta_diff &lt;- function(type, S, K, r, q, tau, sigma, delta0) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    call_value &lt;- delta - delta0\n    put_value &lt;- delta0 - delta\n    \n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n        \n    return(result)\n    \n}\n\n\nbeta &lt;- 0.35\ntype &lt;- \"call\"\nn &lt;- 1\nmultiple &lt;- 100\ntotal &lt;- 1000000\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\nsec &lt;- list(\n    \"n\" = n,\n    \"multiple\" = multiple,\n    \"S\" = S,\n    \"delta\" = bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n    \"beta\" = 1\n)\n\n\nbeta_dt &lt;- CJ(type = type, shock = shocks)\nbeta_dt[ , spot := level_shock(shock, S, tau, sigma), by = c(\"type\", \"shock\")]\nbeta_dt[ , static := beta]\nbeta_dt[ , diff := bs_delta_diff(type, spot, K, r, q, tau, sigma, sec[[\"delta\"]])]\nbeta_dt[ , dynamic := beta + sec[[\"n\"]] * sec[[\"multiple\"]] * sec[[\"S\"]] * sec[[\"beta\"]] * diff / total, by = c(\"type\", \"shock\")]\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\nbs_vega &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    q_df &lt;- exp(-q * tau)\n    \n    result &lt;- S * q_df * phi(d1) * sqrt(tau)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , vega := bs_vega(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\n\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_theta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    r_df &lt;- exp(r * tau)\n    q_df &lt;- exp(q * tau)\n  \n    call_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) -\n      r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n    \n    put_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) +\n      r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n        \n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , theta := bs_theta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\nbs_gamma &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n    q_df &lt;- exp(-q * tau)\n    \n    result &lt;- q_df * phi(d1) / (S * sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , gamma := bs_gamma(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#value",
    "href": "posts/securities-r/index.html#value",
    "title": "Securities",
    "section": "",
    "text": "For a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_value &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n    r_df &lt;- exp(-r * tau)\n    q_df &lt;- exp(-q * tau)\n    \n    call_value &lt;- S * q_df * Phi(d1) - r_df * K * Phi(d2)\n    put_value &lt;- r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n} \n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\nbs_d1 &lt;- function(S, K, r, q, tau, sigma) {\n    \n    result &lt;- (log(S / K) + (r - q + sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\nbs_d2 &lt;- function(S, K, r, q, tau, sigma) {\n    \n    result &lt;- (log(S / K) + (r - q - sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\nphi &lt;- function(x) {\n    \n    result &lt;- dnorm(x)\n    \n    return(result)\n    \n}\n\nPhi &lt;- function(x) {\n    \n    result &lt;- pnorm(x)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , d1 := bs_d1(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , d2 := bs_d2(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , value := bs_value(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order",
    "href": "posts/securities-r/index.html#first-order",
    "title": "Securities",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_delta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    q_df &lt;- exp(-q * tau)\n  \n    call_value &lt;- q_df * Phi(d1)\n    put_value &lt;- -q_df * Phi(-d1)\n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n} \n\n\ngreeks_dt[ , delta := bs_delta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\n\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\nbs_delta_diff &lt;- function(type, S, K, r, q, tau, sigma, delta0) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    call_value &lt;- delta - delta0\n    put_value &lt;- delta0 - delta\n    \n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n        \n    return(result)\n    \n}\n\n\nbeta &lt;- 0.35\ntype &lt;- \"call\"\nn &lt;- 1\nmultiple &lt;- 100\ntotal &lt;- 1000000\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\nsec &lt;- list(\n    \"n\" = n,\n    \"multiple\" = multiple,\n    \"S\" = S,\n    \"delta\" = bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n    \"beta\" = 1\n)\n\n\nbeta_dt &lt;- CJ(type = type, shock = shocks)\nbeta_dt[ , spot := level_shock(shock, S, tau, sigma), by = c(\"type\", \"shock\")]\nbeta_dt[ , static := beta]\nbeta_dt[ , diff := bs_delta_diff(type, spot, K, r, q, tau, sigma, sec[[\"delta\"]])]\nbeta_dt[ , dynamic := beta + sec[[\"n\"]] * sec[[\"multiple\"]] * sec[[\"S\"]] * sec[[\"beta\"]] * diff / total, by = c(\"type\", \"shock\")]\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\nbs_vega &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    q_df &lt;- exp(-q * tau)\n    \n    result &lt;- S * q_df * phi(d1) * sqrt(tau)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , vega := bs_vega(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\n\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_theta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    r_df &lt;- exp(r * tau)\n    q_df &lt;- exp(q * tau)\n  \n    call_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) -\n      r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n    \n    put_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) +\n      r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n        \n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , theta := bs_theta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order",
    "href": "posts/securities-r/index.html#second-order",
    "title": "Securities",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\nbs_gamma &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n    q_df &lt;- exp(-q * tau)\n    \n    result &lt;- q_df * phi(d1) / (S * sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , gamma := bs_gamma(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order-1",
    "href": "posts/securities-r/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(P(y+\\Delta y)-P(y)\\approx -D\\Delta y +{\\frac{C\\times 100}{2!}}(\\Delta y)^{2}\\)\n\npnl_bond &lt;- function(duration, convexity, dy) {\n    \n    duration_pnl &lt;- -duration * dy\n    convexity_pnl &lt;- (convexity * 100 / 2) * dy ^ 2\n    income_pnl &lt;- dy\n    \n    result &lt;- list(\"total\" = duration_pnl + convexity_pnl + income_pnl,\n                   \"duration\" = duration_pnl,\n                   \"convexity\" = convexity_pnl,\n                   \"income\" = income_pnl)\n    \n    return(result)\n    \n} \n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor &lt;- \"DGS10\"\nduration &lt;- 6.5\nconvexity &lt;- 0.65\ny &lt;- coredata(tail(na.locf(levels_xts[ , factor]), width)[1])\n\n\nbonds_dt &lt;- data.table(index = index(tail(levels_xts, width)),\n                       duration = duration, convexity = convexity,\n                       dy = na.locf(tail(levels_xts[ , factor], width)))\nsetnames(bonds_dt, c(\"index\", \"duration\", \"convexity\", \"dy\"))\nbonds_dt[ , dy := (dy - y) / 100, by = index]\n\n\nattrib_dt &lt;- bonds_dt[ , as.list(unlist(pnl_bond(duration, convexity, dy))), by = index]\n\n\n\n\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=-\\frac{\\partial D}{\\partial y}\\\\\n&=\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}-\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=C-D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(C\\times 100-D^{2}\\right)=C-\\frac{D^{2}}{100}\\)\n\nyield_shock &lt;- function(shock, tau, sigma) {\n    \n    result &lt;- shock * sigma * sqrt(tau)\n    \n    return(result)\n    \n}\n\n\nduration_drift &lt;- function(duration, convexity, dy) {\n    \n    drift &lt;- convexity - duration ^ 2 / 100\n    change &lt;- -drift * dy * 100\n    \n    result &lt;- list(\"drift\" = drift,\n                   \"change\" = change)\n    \n    return(result)\n    \n}\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor &lt;- \"DGS10\"\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor])\n\n\nduration_dt &lt;- CJ(shock = shocks)\nduration_dt[ , spot := yield_shock(shock, tau, sigma), by = \"shock\"]\nduration_dt[ , static := duration]\nduration_dt[ , dynamic := duration + duration_drift(duration, convexity, spot)[[\"change\"]], by = \"shock\"]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order-1",
    "href": "posts/securities-r/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlack’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\npnl_option &lt;- function(type, S, K, r, q, tau, sigma, dS, dt, dsigma) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    value &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega &lt;- bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta &lt;- bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma &lt;- bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl &lt;- delta * dS / value\n    gamma_pnl &lt;- gamma / 2 * dS ^ 2 / value\n    vega_pnl &lt;- vega * dsigma / value\n    theta_pnl &lt;- theta * dt / value\n    \n    result &lt;- list(\"total\" = delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n                   \"delta\" = delta_pnl,\n                   \"gamma\" = gamma_pnl,\n                   \"vega\" = vega_pnl,\n                   \"theta\" = theta_pnl)\n    \n    return(result)    \n    \n}\n\n\nfactor &lt;- \"SP500\"\ntype &lt;- \"call\"\nS &lt;- coredata(tail(na.locf(levels_xts[ , factor]), width)[1])\nK &lt;- S # * (1 + 0.05)\ntau &lt;- 1 # = 252 / 252\nsigma &lt;- coredata(tail(sd_xts[ , factor], width)[1])\n\n\noptions_dt &lt;- data.table(index = index(tail(levels_xts, width)),\n                         spot = na.locf(tail(levels_xts[ , factor], width)),\n                         sigma = tail(sd_xts[ , factor], width))\nsetnames(options_dt, c(\"index\", \"spot\", \"sigma\"))\noptions_dt[ , dS := spot - S, by = index]\noptions_dt[ , dt_diff := as.numeric(index - index[1])]\noptions_dt[ , dt := dt_diff / tail(dt_diff, 1)]\noptions_dt[ , dsigma := sigma - ..sigma, by = index]\n\n\nattrib_dt &lt;- options_dt[ , as.list(unlist(pnl_option(type, S, K, r, q, tau, ..sigma,\n                                                     dS, dt, dsigma))), by = index]\n\n\n\n\n\n\n\n\n\n\n\n\nIto’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\nsim_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n    \n    result &lt;- S * exp(cumsum(sigma * sqrt(dt) * rnorm(n_sim)) +\n                      (mu - 0.5 * sigma ^ 2) * dt)\n    \n    return(result)\n    \n}\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\nsim_multi_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n    \n    n_cols &lt;- ncol(sigma)\n    \n    Z &lt;- matrix(rnorm(n_sim * n_cols), nrow = n_sim, ncol = n_cols)\n    X &lt;- sweep(sqrt(dt) * (Z %*% chol(sigma)), 2, (mu - 0.5 * diag(sigma)) * dt, \"+\")\n    \n    result &lt;- sweep(apply(X, 2, function(x) exp(cumsum(x))), 2, S, \"*\")\n    \n    return(result)\n    \n}\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS &lt;- rep(1, length(factors))\nsigma &lt;- cov(returns_xts, use = \"complete.obs\") * scale[[\"periods\"]]\nmu &lt;- colMeans(na.omit(returns_xts)) * scale[[\"periods\"]]\nmu &lt;- mu + diag(sigma) / 2 # drift\ndt &lt;- 1 / scale[[\"periods\"]]\n\n\nmu_ls &lt;- list()\nsigma_ls &lt;- list()\n\n\nfor (i in 1:1e4) {\n    \n    # assumes stock prices\n    levels_sim &lt;- sim_multi_gbm(width + 1, S, mu, sigma, dt)\n    returns_sim &lt;- diff(log(levels_sim))\n\n    mu_sim &lt;- colMeans(returns_sim) * scale[[\"periods\"]]\n    sigma_sim &lt;- apply(returns_sim, 2, sd) * sqrt(scale[[\"periods\"]])\n    \n    mu_ls &lt;- append(mu_ls, list(mu_sim))\n    sigma_ls &lt;- append(sigma_ls, list(sigma_sim))\n    \n}\n\n\ndata.frame(\"empirical\" = colMeans(na.omit(returns_xts)) * scale[[\"periods\"]],\n           \"theoretical\" = colMeans(do.call(rbind, mu_ls)))\n\n                 empirical   theoretical\nSP500         0.0883057229  0.0871990884\nDTWEXAFEGS    0.0143635035  0.0152920478\nDGS10        -0.0002281034 -0.0003471167\nBAMLH0A0HYM2  0.0009993103  0.0010436142\n\n\n\ndata.frame(\"empirical\" = sqrt(diag(sigma)),\n           \"theoretical\" = colMeans(do.call(rbind, sigma_ls)))\n\n               empirical theoretical\nSP500        0.179703273 0.179668698\nDTWEXAFEGS   0.062549582 0.062437886\nDGS10        0.008253365 0.008239269\nBAMLH0A0HYM2 0.016972925 0.016961714\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility",
    "href": "posts/securities-r/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\nimplied_vol_newton &lt;- function(params, type, S, K, r, q, tau) {\n    \n    target0 &lt;- 0\n    sigma &lt;- params[[\"sigma\"]]\n    sigma0 &lt;- sigma\n    \n    while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n        \n        d1 &lt;- bs_d1(S, K, r, q, tau, sigma0)\n        d2 &lt;- bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 &lt;- bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        vega0 &lt;- bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma &lt;- sigma0 - (target0 - params[[\"target\"]]) / vega0\n        sigma0 &lt;- sigma\n        \n    }\n    \n    return(sigma)\n    \n}\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS &lt;- coredata(na.locf(levels_xts)[nrow(levels_xts), factor])\nK &lt;- S # * (1 + 0.05)\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor]) # overrides matrix\nstart &lt;- 0.2\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\ntarget &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams &lt;- list(\n    \"target\" = target,\n    \"sigma\" = start,\n    \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nimplied_vol_newton(params, type, S, K, r, q, tau)\n\n         SP500\n[1,] 0.1317806"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity",
    "href": "posts/securities-r/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility-1",
    "href": "posts/securities-r/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\nimplied_vol_obj &lt;- function(param, type, S, K, r, q, tau, target) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, param)\n    d2 &lt;- bs_d2(S, K, r, q, tau, param)\n    target0 &lt;- bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result &lt;- abs(target0 - target)\n    \n    return(result)\n    \n}\n\nimplied_vol_optim &lt;- function(param, type, S, K, r, q, tau, target) {\n    \n    result &lt;- optim(param, implied_vol_obj, type = type, S = S, K = K, r = r, q = q,\n                    tau = tau, target = target, method = \"Brent\", lower = 0, upper = 1)\n    \n    return(result$par)\n    \n}\n\n\nimplied_vol_optim(start, type, S, K, r, q, tau, target)\n\n[1] 0.1317806"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity-1",
    "href": "posts/securities-r/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity"
  },
  {
    "objectID": "posts/securities-py/index.html",
    "href": "posts/securities-py/index.html",
    "title": "Securities",
    "section": "",
    "text": "def level_shock(shock, S, tau, sigma):\n    \n    result = S * (1 + shock * sigma * np.sqrt(tau))\n    \n    return result\n\n\nhttps://en.wikipedia.org/wiki/Greeks_(finance)\nhttps://www.wolframalpha.com/input/?i=option+pricing+formula\n\n\nfactor = \"SP500\"\ntypes = [\"call\", \"put\"]\nS = levels_df.ffill()[factor].iloc[-1]\nK = S\nr = 0 # use \"USD3MTD156N\"\nq = 0 # see https://stackoverflow.com/a/11286679 \ntau = 1 # = 252 / 252\nsigma = sd_df[factor].iloc[-1] # use \"VIXCLS\"\nshocks = [x / 2 for x in range(-6, 7)]\n\n\ngreeks_df = pd.DataFrame([(x, y) for x in types for y in shocks], \n    columns = [\"type\", \"shock\"])\ngreeks_df[\"spot\"] = level_shock(greeks_df[\"shock\"], S, tau, sigma)\n\n\n\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    if (type == \"call\"):\n        result =  S * np.exp(-q * tau) * Phi(d1) - np.exp(-r * tau) * K * Phi(d2)\n    elif (type == \"put\"):\n        result = np.exp(-r * tau) * K * Phi(-d2) - S * np.exp(-q * tau) * Phi(-d1)\n        \n    return result\n\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    r_df = np.exp(-r * tau)\n    q_df = np.exp(-q * tau)\n    \n    call_value = S * q_df * Phi(d1) - r_df * K * Phi(d2)\n    put_value = r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\ndef bs_d1(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q + sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n\ndef bs_d2(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q - sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n    \ndef phi(x):\n    \n    result = norm.pdf(x)\n    \n    return result\n\ndef Phi(x):\n    \n    result = norm.cdf(x)\n    \n    return result\n\n\ngreeks_df[\"d1\"] = bs_d1(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"d2\"] = bs_d2(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"value\"] = bs_value(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_delta(type, S, K, r, q, tau, sigma, d1, d2):\n  \n    q_df = np.exp(-q * tau)\n    \n    call_value = q_df * Phi(d1)\n    put_value = -q_df * Phi(-d1)\n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\n\ngreeks_df[\"delta\"] = bs_delta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\n\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\ndef bs_delta_diff(type, S, K, r, q, tau, sigma, delta0):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    call_value = delta - delta0\n    put_value = delta0 - delta\n    \n    result = np.where(type == \"call\", call_value, put_value)\n        \n    return result\n\n\nbeta = 0.35\ntype = \"call\"\nn = 1\nmultiple = 100\ntotal = 1000000\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\nsec = {\n    \"n\": n,\n    \"multiple\": multiple,\n    \"S\": S,\n    \"delta\": bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n    \"beta\": 1\n}\n\n\nbeta_df = pd.DataFrame([(x, y) for x in types for y in shocks], \n    columns = [\"type\", \"shock\"])\nbeta_df[\"spot\"] = level_shock(beta_df[\"shock\"], S, tau, sigma)\nbeta_df[\"static\"] = beta\nbeta_df[\"diff\"] = bs_delta_diff(type, beta_df[\"spot\"], K, r, q, tau, sigma, sec[\"delta\"])\nbeta_df[\"dynamic\"] = beta + sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * sec[\"beta\"] * beta_df[\"diff\"] / total\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\ndef bs_vega(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    q_df = np.exp(-q * tau)\n    result = S * q_df * phi(d1) * np.sqrt(tau)\n    \n    return result\n\n\ngreeks_df[\"vega\"] = bs_vega(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                            greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\n\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_theta(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    r_df = np.exp(-r * tau)\n    q_df = np.exp(-q * tau)\n    \n    call_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) - \\\n        r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n        \n    put_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) + \\\n        r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n        \n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\n\ngreeks_df[\"theta\"] = bs_theta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\ndef bs_gamma(type, S, K, r, q, tau, sigma, d1, d2):\n  \n    q_df = np.exp(-q * tau)\n    \n    result = q_df * phi(d1) / (S * sigma * np.sqrt(tau))\n    \n    return result\n\n\ngreeks_df[\"gamma\"] = bs_gamma(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#value",
    "href": "posts/securities-py/index.html#value",
    "title": "Securities",
    "section": "",
    "text": "For a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    if (type == \"call\"):\n        result =  S * np.exp(-q * tau) * Phi(d1) - np.exp(-r * tau) * K * Phi(d2)\n    elif (type == \"put\"):\n        result = np.exp(-r * tau) * K * Phi(-d2) - S * np.exp(-q * tau) * Phi(-d1)\n        \n    return result\n\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    r_df = np.exp(-r * tau)\n    q_df = np.exp(-q * tau)\n    \n    call_value = S * q_df * Phi(d1) - r_df * K * Phi(d2)\n    put_value = r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\ndef bs_d1(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q + sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n\ndef bs_d2(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q - sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n    \ndef phi(x):\n    \n    result = norm.pdf(x)\n    \n    return result\n\ndef Phi(x):\n    \n    result = norm.cdf(x)\n    \n    return result\n\n\ngreeks_df[\"d1\"] = bs_d1(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"d2\"] = bs_d2(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"value\"] = bs_value(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order",
    "href": "posts/securities-py/index.html#first-order",
    "title": "Securities",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_delta(type, S, K, r, q, tau, sigma, d1, d2):\n  \n    q_df = np.exp(-q * tau)\n    \n    call_value = q_df * Phi(d1)\n    put_value = -q_df * Phi(-d1)\n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\n\ngreeks_df[\"delta\"] = bs_delta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\n\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\ndef bs_delta_diff(type, S, K, r, q, tau, sigma, delta0):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    call_value = delta - delta0\n    put_value = delta0 - delta\n    \n    result = np.where(type == \"call\", call_value, put_value)\n        \n    return result\n\n\nbeta = 0.35\ntype = \"call\"\nn = 1\nmultiple = 100\ntotal = 1000000\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\nsec = {\n    \"n\": n,\n    \"multiple\": multiple,\n    \"S\": S,\n    \"delta\": bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n    \"beta\": 1\n}\n\n\nbeta_df = pd.DataFrame([(x, y) for x in types for y in shocks], \n    columns = [\"type\", \"shock\"])\nbeta_df[\"spot\"] = level_shock(beta_df[\"shock\"], S, tau, sigma)\nbeta_df[\"static\"] = beta\nbeta_df[\"diff\"] = bs_delta_diff(type, beta_df[\"spot\"], K, r, q, tau, sigma, sec[\"delta\"])\nbeta_df[\"dynamic\"] = beta + sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * sec[\"beta\"] * beta_df[\"diff\"] / total\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\ndef bs_vega(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    q_df = np.exp(-q * tau)\n    result = S * q_df * phi(d1) * np.sqrt(tau)\n    \n    return result\n\n\ngreeks_df[\"vega\"] = bs_vega(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                            greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\n\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_theta(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    r_df = np.exp(-r * tau)\n    q_df = np.exp(-q * tau)\n    \n    call_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) - \\\n        r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n        \n    put_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) + \\\n        r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n        \n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\n\ngreeks_df[\"theta\"] = bs_theta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order",
    "href": "posts/securities-py/index.html#second-order",
    "title": "Securities",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\ndef bs_gamma(type, S, K, r, q, tau, sigma, d1, d2):\n  \n    q_df = np.exp(-q * tau)\n    \n    result = q_df * phi(d1) / (S * sigma * np.sqrt(tau))\n    \n    return result\n\n\ngreeks_df[\"gamma\"] = bs_gamma(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order-1",
    "href": "posts/securities-py/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(P(y+\\Delta y)-P(y)\\approx -D\\Delta y +{\\frac{C\\times 100}{2!}}(\\Delta y)^{2}\\)\n\ndef pnl_bond(duration, convexity, dy):\n    \n    duration_pnl = -duration * dy\n    convexity_pnl = (convexity * 100 / 2) * dy ** 2\n    income_pnl = dy\n    \n    result = pd.DataFrame({\n        \"total\": duration_pnl + convexity_pnl + income_pnl,\n        \"duration\": duration_pnl,\n        \"convexity\": convexity_pnl,\n        \"income\": income_pnl\n    })\n    \n    return result\n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor = \"DGS10\"\nduration = 6.5\nconvexity = 0.65\ny = levels_df.ffill()[factor].iloc[-width]\n\n\nbond_df = pd.DataFrame({\n    \"duration\": duration,\n    \"convexity\": convexity,\n    \"dy\": (levels_df.ffill()[factor].iloc[-width:] - y) / 100\n})\n\n\nattrib_df = pnl_bond(bond_df[\"duration\"], bond_df[\"convexity\"], bond_df[\"dy\"])\n\n\n\n\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=-\\frac{\\partial D}{\\partial y}\\\\\n&=\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}-\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=C-D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(C\\times 100-D^{2}\\right)=C-\\frac{D^{2}}{100}\\)\n\ndef yield_shock(shock, tau, sigma):\n    \n    result = shock * sigma * np.sqrt(tau)\n    \n    return result\n\n\ndef duration_drift(duration, convexity, dy):\n    \n    drift = convexity - duration ** 2 / 100\n    change = -drift * dy * 100\n    \n    result = {\n        \"drift\": drift,\n        \"change\": change\n    }\n    \n    return result\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor = \"DGS10\"\nsigma = sd_df[factor].iloc[-1]\n\n\nduration_df = pd.DataFrame(shocks).rename(columns = {0: \"shock\"})\nduration_df[\"spot\"] = yield_shock(duration_df[\"shock\"], tau, sigma)\nduration_df[\"static\"] = duration\nduration_df[\"dynamic\"] = duration + \\\n    duration_drift(duration, convexity, duration_df[\"spot\"])[\"change\"]"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order-1",
    "href": "posts/securities-py/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlack’s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Black’s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\ndef pnl_option(type, S, K, r, q, tau, sigma, dS, dt, dsigma):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    value = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega = bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta = bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma = bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl = delta * dS / value\n    gamma_pnl = gamma / 2 * dS ** 2 / value\n    vega_pnl = vega * dsigma / value\n    theta_pnl = theta * dt / value\n    \n    result = pd.DataFrame({\n        \"total\": delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n        \"delta\": delta_pnl,\n        \"gamma\": gamma_pnl,\n        \"vega\": vega_pnl,\n        \"theta\": theta_pnl\n    })\n    \n    return result\n\n\nfactor = \"SP500\"\ntype = \"call\"\nS = levels_df.ffill()[factor].iloc[-width]\nK = S # * (1 + 0.05)\ntau = 1 # = 252 / 252\nsigma = sd_df[factor].iloc[-width]\n\n\noptions_df = pd.DataFrame({\n    \"spot\": levels_df.ffill()[factor].iloc[-width:],\n    \"sigma\": sd_df[factor].iloc[-width:]\n})\noptions_df[\"dS\"] = options_df[\"spot\"] - S\noptions_df[\"dt_diff\"] = (options_df.index - options_df.index[0]).days\noptions_df[\"dt\"] = options_df[\"dt_diff\"] / options_df[\"dt_diff\"][-1]\n\n&lt;string&gt;:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\noptions_df[\"dsigma\"] = options_df[\"sigma\"] - sigma\n\n\nattrib_df = pnl_option(type, S, K, r, q, tau, sigma,\n                       options_df[\"dS\"], options_df[\"dt\"], options_df[\"dsigma\"])\n\n\n\n\n\n\n\n\n\n\n\n\nIto’s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Ito’s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Ito’s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\ndef sim_gbm(n_sim, S, mu, sigma, dt):\n    \n    result = S * np.exp(np.cumsum(sigma * np.sqrt(dt) * np.random.normal(size = n_sim)) + \\\n                        (mu - 0.5 * sigma ** 2) * dt)\n    \n    return result\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e. \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\ndef sim_multi_gbm(n_sim, S, mu, sigma, dt):\n    \n    n_cols = sigma.shape[1]\n    \n    Z = np.random.normal(size = n_sim * n_cols).reshape((n_sim, n_cols))\n    X = np.sqrt(dt) * Z @ np.linalg.cholesky(sigma).T + (mu - 0.5 * np.diag(sigma)) * dt\n    \n    result = S * np.exp(X.cumsum(axis = 0))\n    \n    return np.asmatrix(result)\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS = [1] * len(factors)\nsigma = np.cov(returns_df[\"returns\"].dropna().T, ddof = 1) * scale[\"periods\"]\nmu = np.array(returns_df[\"returns\"].dropna().mean()) * scale[\"periods\"]\nmu = mu + np.diag(sigma) / 2 # drift\ndt = 1 / scale[\"periods\"]\n\n\nmu_ls = []\nsigma_ls = []\n\n\nfor i in range(10000): # \"TypeError: 'float' object cannot be interpreted as an integer\"\n  \n    # assumes underlying stock price follows geometric Brownian motion with constant volatility\n    levels_sim = pd.DataFrame(sim_multi_gbm(width + 1, S, mu, sigma, dt))\n    returns_sim = np.log(levels_sim).diff()\n\n    mu_sim = returns_sim.mean() * scale[\"periods\"]\n    sigma_sim = returns_sim.std() * np.sqrt(scale[\"periods\"])\n\n    mu_ls.append(mu_sim)\n    sigma_ls.append(sigma_sim)\n\n\nmu_df = pd.DataFrame(mu_ls)\nsigma_df = pd.DataFrame(sigma_ls)\n\n\npd.DataFrame({\n    \"empirical\": np.array(returns_df[\"returns\"].dropna().mean()) * scale[\"periods\"],\n    \"theoretical\": mu_df.mean()\n})\n\n   empirical  theoretical\n0   0.088306     0.088227\n1   0.014364     0.014682\n2  -0.000228    -0.000221\n3   0.000999     0.000994\n\n\n\npd.DataFrame({\n    \"empirical\": np.sqrt(np.diag(sigma)),\n    \"theoretical\": sigma_df.mean()\n})\n\n   empirical  theoretical\n0   0.179703     0.179287\n1   0.062550     0.062478\n2   0.008253     0.008250\n3   0.016973     0.016944\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility",
    "href": "posts/securities-py/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewton’s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newton’s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\ndef implied_vol_newton(params, type, S, K, r, q, tau):\n    \n    target0 = 0\n    sigma = params[\"sigma\"]\n    sigma0 = sigma\n    \n    while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n        \n        d1 = bs_d1(S, K, r, q, tau, sigma0)\n        d2 = bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 = bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        vega0 = bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma = sigma0 - (target0 - params[\"target\"]) / vega0\n        sigma0 = sigma\n        \n    return sigma\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS = levels_df.ffill()[factor].iloc[-1]\nK = S # * (1 + 0.05)\nsigma = sd_df[factor].iloc[-1] # overrides matrix\nstart = 0.2\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\ntarget = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams = {\n    \"target\": target,\n    \"sigma\": start,\n    \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nimplied_vol_newton(params, type, S, K, r, q, tau) \n\n0.12957318298553508"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity",
    "href": "posts/securities-py/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility-1",
    "href": "posts/securities-py/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\ndef implied_vol_obj(param, type, S, K, r, q, tau, target):\n    \n    d1 = bs_d1(S, K, r, q, tau, param)\n    d2 = bs_d2(S, K, r, q, tau, param)\n    target0 = bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result = abs(target0 - target)\n    \n    return result\n\ndef implied_vol_optim(param, type, S, K, r, q, tau, target):\n    \n    result = minimize(implied_vol_obj, param, args = (type, S, K, r, q, tau, target))\n    \n    return result.x.item()\n\n\nimplied_vol_optim(start, type, S, K, r, q, tau, target)\n\n0.12957317553495462"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity-1",
    "href": "posts/securities-py/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity"
  }
]