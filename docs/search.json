[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jasonjfoster.github.io",
    "section": "",
    "text": "Markets\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nMar 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMarkets\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nMar 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nMar 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSecurities\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nMar 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nMar 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRisk\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nMar 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nMar 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEigen\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nMar 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\n\nfinance\n\n\npython\n\n\n\n\n\n\n\nJason Foster\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization\n\n\n\nanalysis\n\n\nfinance\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nalgorithms\n\n\nr\n\n\n\n\n\n\n\nJason Foster\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/optim-r/index.html",
    "href": "posts/optim-r/index.html",
    "title": "Optimization",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/optim-r/index.html#random-turnover",
    "href": "posts/optim-r/index.html#random-turnover",
    "title": "Optimization",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous periodâ€™s random weights.\n\nrand_turnover1 &lt;- function(n_sim, n_assets, lower, upper, target) {\n    \n    rng &lt;- upper - lower\n    \n    result &lt;- rand_weights2b(n_sim, n_assets) * rng\n    result &lt;- result - rng / n_assets\n    \n    return(result)\n    \n}\n\n\nlower &lt;- -0.05\nupper &lt;- 0.05\ntarget &lt;- 0\n\n\napproach1 &lt;- rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\n\n\n\n\nrand_iterative &lt;- function(n_assets, lower, upper, target) {\n    \n    result &lt;- runif(n_assets - 1, min = lower, max = upper)\n    temp &lt;- target - sum(result)\n    \n    while (!((temp &lt;= upper) && (temp &gt;= lower))) {\n        \n        result &lt;- runif(n_assets - 1, min = lower, max = upper)\n        temp &lt;- target - sum(result)\n        \n    }\n    \n    result &lt;- append(result, temp)\n    \n    return(result)\n    \n}\n\n\nrand_turnover2 &lt;- function(n_sim, n_assets, lower, upper, target) {\n  \n    result_ls &lt;- list()\n    \n    for (i in 1:n_sim) {\n        \n        result_sim &lt;- rand_iterative(n_assets, lower, upper, target)\n        result_ls &lt;- append(result_ls, list(result_sim))\n        \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    \n    return(result)\n    \n}\n\n\napproach2 &lt;- rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-means",
    "href": "posts/optim-r/index.html#maximize-means",
    "title": "Optimization",
    "section": "Maximize means",
    "text": "Maximize means\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmax_mean_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Maximize(t(params) %*% mu)\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0,\n                 quad_form(params, sigma) &lt;= target ^ 2)\n    \n    prob &lt;- Problem(obj, cons)\n    \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\ntarget &lt;- 0.06\n\n\nparams1 &lt;- t(max_mean_optim(mu, sigma, target))\nparams1\n\n          [,1]      [,2]         [,3]         [,4]\n[1,] 0.4961023 0.5038977 7.919319e-09 4.929859e-09\n\n\n\nparams1 %*% mu\n\n          [,1]\n[1,] 0.0472976\n\n\n\nsqrt(params1 %*% sigma %*% t(params1))\n\n     [,1]\n[1,] 0.06\n\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jasonjfoster/rolloptim\") # roll (&gt;= 1.1.7)\n# library(rolloptim)\n# \n# mu &lt;- roll_mean(returns_x_xts, 5)\n# sigma &lt;- roll_cov(returns_x_xts, width = 5)\n# \n# xx &lt;- roll_crossprod(returns_x_xts, returns_x_xts, 5)\n# xy &lt;- roll_crossprod(returns_x_xts, returns_y_xts, 5)\n# \n# roll_max_mean(mu)"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-variance",
    "href": "posts/optim-r/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmin_var_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Minimize(quad_form(params, sigma))\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0,\n                 sum(mu * params) &gt;= target)\n    \n    prob &lt;- Problem(obj, cons)\n    \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\ntarget &lt;- 0.03\n\n\nparams2 &lt;- t(min_var_optim(mu, sigma, target))\nparams2\n\n        [,1]      [,2]      [,3]         [,4]\n[1,] 0.29584 0.4463629 0.2577972 1.877369e-21\n\n\n\nparams2 %*% mu\n\n     [,1]\n[1,] 0.03\n\n\n\nsqrt(params2 %*% sigma %*% t(params2))\n\n          [,1]\n[1,] 0.0372178\n\n\n\n# roll_min_var(sigma)"
  },
  {
    "objectID": "posts/optim-r/index.html#maximize-utility",
    "href": "posts/optim-r/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nmax_utility_optim &lt;- function(mu, sigma, target) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Minimize(0.5 * target * quad_form(params, sigma) - t(mu) %*% params)\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0)\n    \n    prob &lt;- Problem(obj, cons)\n        \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\nir &lt;- 0.5\ntarget &lt;- ir / 0.06 # ir / std (see Black-Litterman)\n\n\nparams3 &lt;- t(max_utility_optim(mu, sigma, target))\nparams3\n\n          [,1]      [,2]         [,3]         [,4]\n[1,] 0.5584661 0.4415339 9.327449e-23 4.233534e-23\n\n\n\nparams3 %*% mu \n\n           [,1]\n[1,] 0.05158278\n\n\n\nsqrt(params3 %*% sigma %*% t(params3))\n\n          [,1]\n[1,] 0.0672845\n\n\n\n# roll_max_utility(mu, sigma)"
  },
  {
    "objectID": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-r/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nhttps://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html\n\n\nmin_rss_optim1 &lt;- function(mu, sigma) {\n    \n    params &lt;- Variable(length(mu))\n    \n    obj &lt;- Minimize(0.5 * quad_form(params, sigma) - t(mu) %*% params)\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0)\n    \n    prob &lt;- Problem(obj, cons)\n        \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams4 &lt;- t(min_rss_optim1(crossprod(overlap_x_xts, overlap_y_xts), crossprod(overlap_x_xts)))\nparams4\n\n         [,1]         [,2]     [,3]         [,4]\n[1,] 0.374955 7.191451e-23 0.625045 2.959672e-23\n\n\n\nparams4 %*% mu \n\n           [,1]\n[1,] 0.03039745\n\n\n\nsqrt(params4 %*% sigma %*% t(params4))\n\n           [,1]\n[1,] 0.05050206\n\n\n\n# roll_min_rss(xx, xy)\n\n\nmin_rss_optim2 &lt;- function(x, y) {\n    \n    params &lt;- Variable(ncol(x))\n    \n    obj &lt;- Minimize(sum_squares(y - x %*% params))\n    \n    cons &lt;- list(sum(params) == 1, params &gt;= 0)\n    \n    prob &lt;- Problem(obj, cons)\n        \n    result &lt;- solve(prob)$getValue(params)\n    \n    return(result)\n\n}\n\n\nparams5 &lt;- t(min_rss_optim2(coredata(overlap_x_xts), coredata(overlap_y_xts)))\nparams5\n\n         [,1]          [,2]     [,3]          [,4]\n[1,] 0.374955 -1.364843e-20 0.625045 -8.927092e-21\n\n\n\nparams5 %*% mu \n\n           [,1]\n[1,] 0.03039745\n\n\n\nsqrt(params5 %*% sigma %*% t(params5))\n\n           [,1]\n[1,] 0.05050206\n\n\n\nround(data.frame(\"max_pnl\" = t(params1) * 100,\n                 \"min_risk\" = t(params2) * 100,\n                 \"max_utility\" = t(params3) * 100,\n                 \"min_rss1\" = t(params4) * 100,\n                 \"min_rss2\" = t(params5) * 100), 2)\n\n  max_pnl min_risk max_utility min_rss1 min_rss2\n1   49.61    29.58       55.85     37.5     37.5\n2   50.39    44.64       44.15      0.0      0.0\n3    0.00    25.78        0.00     62.5     62.5\n4    0.00     0.00        0.00      0.0      0.0"
  },
  {
    "objectID": "posts/optim-py/index.html",
    "href": "posts/optim-py/index.html",
    "title": "Optimization",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/optim-py/index.html#random-turnover",
    "href": "posts/optim-py/index.html#random-turnover",
    "title": "Optimization",
    "section": "Random turnover",
    "text": "Random turnover\nHow to generate random weights between lower bound \\(a\\) and upper bound \\(b\\) that sum to zero?\n\nApproach 1: tempting to multiply random weights by \\(M\\) and then subtract by \\(\\frac{M}{N}\\) but the distribution is not between \\(a\\) and \\(b\\)\nApproach 2: instead, use an iterative approach for random turnover:\n\nGenerate \\(N-1\\) uniformly distributed weights between \\(a\\) and \\(b\\)\nFor \\(u_{N}\\) compute sum of values and subtract from \\(M\\)\nIf \\(u_{N}\\) is between \\(a\\) and \\(b\\), then keep; otherwise, discard\n\n\nThen add random turnover to previous periodâ€™s random weights.\n\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights2b(n_sim, n_assets) * rng\n    result = result - rng / n_assets\n    \n    return result\n\n\nlower = -0.05\nupper = 0.05\ntarget = 0\n\n\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\n\n\n\n\n\n\n\n\n\n\n\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n    temp = target - sum(result)\n    \n    while not ((temp &lt;= upper) and (temp &gt;= lower)):\n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n    result = np.append(result, temp)\n    \n    return result\n\n\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n  \n    result_ls = []\n    \n    for i in range(n_sim):\n      \n      result_sim = rand_iterative(n_assets, lower, upper, target)\n      result_ls.append(result_sim)\n      \n    result = pd.DataFrame(result_ls)\n    \n    return result\n\n\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-mean",
    "href": "posts/optim-py/index.html#maximize-mean",
    "title": "Optimization",
    "section": "Maximize mean",
    "text": "Maximize mean\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ndef max_mean_optim(mu, sigma, target):\n    \n    params = cp.Variable(len(mu))\n    \n    obj = cp.Maximize(params.T @ mu)\n    \n    cons = [cp.sum(params) == 1, params &gt;= 0,\n            cp.quad_form(params, sigma) &lt;= target ** 2]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n\n\ntarget = 0.06\n\n\nparams1 = max_mean_optim(mu, sigma, target)\n\nC:\\Users\\jason\\AppData\\Local\\Programs\\Python\\PYTHON~1\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:336: FutureWarning: \n    Your problem is being solved with the ECOS solver by default. Starting in \n    CVXPY 1.5.0, Clarabel will be used as the default solver instead. To continue \n    using ECOS, specify the ECOS solver explicitly using the ``solver=cp.ECOS`` \n    argument to the ``problem.solve`` method.\n    \n  warnings.warn(ECOS_DEPRECATION_MSG, FutureWarning)\n\nparams1\n\narray([4.96102333e-01, 5.03897654e-01, 7.91928306e-09, 4.92983589e-09])\n\n\n\nnp.dot(mu, params1)\n\n0.047297601235280304\n\n\n\nnp.sqrt(np.dot(params1, np.dot(sigma, params1)))\n\n0.05999999990321796"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-variance",
    "href": "posts/optim-py/index.html#minimize-variance",
    "title": "Optimization",
    "section": "Minimize variance",
    "text": "Minimize variance\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ndef min_var_optim(mu, sigma, target):\n    \n    params = cp.Variable(len(mu))\n    \n    obj = cp.Minimize(cp.quad_form(params, sigma))\n    \n    cons = [cp.sum(params) == 1, params &gt;= 0,\n            params.T @ mu &gt;= target]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n\n\ntarget = 0.03\n\n\nparams2 = min_var_optim(mu, sigma, target)\nparams2\n\narray([2.95839964e-01, 4.46362866e-01, 2.57797170e-01, 1.87729677e-21])\n\n\n\nnp.dot(mu, params2)\n\n0.030000000000000002\n\n\n\nnp.sqrt(np.dot(params2, np.dot(sigma, params2))) \n\n0.03721780158177454"
  },
  {
    "objectID": "posts/optim-py/index.html#maximize-utility",
    "href": "posts/optim-py/index.html#maximize-utility",
    "title": "Optimization",
    "section": "Maximize utility",
    "text": "Maximize utility\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\ndef max_utility_optim(mu, sigma, target):\n    \n    params = cp.Variable(len(mu))\n    \n    obj = cp.Minimize(0.5 * target * cp.quad_form(params, sigma) - params.T @ mu)\n    \n    cons = [cp.sum(params) == 1, params &gt;= 0]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n\n\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\n\n\nparams3 = max_utility_optim(mu, sigma, target)\nparams3\n\narray([5.58466107e-01, 4.41533893e-01, 4.54119747e-23, 4.45163334e-23])\n\n\n\nnp.dot(mu, params3)\n\n0.05158277899969651\n\n\n\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n\n0.06728449966244048"
  },
  {
    "objectID": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "href": "posts/optim-py/index.html#minimize-residual-sum-of-squares",
    "title": "Optimization",
    "section": "Minimize residual sum of squares",
    "text": "Minimize residual sum of squares\n\\[\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n\\]\nTo incorporate these conditions into one equation, introduce new variables \\(\\lambda_{i}\\) that are the Lagrange multipliers and define a new function \\(\\mathcal{L}\\) as follows:\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n\\]\nThen, to minimize this function, take derivatives with respect to \\(w\\) and Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n\\]\nSimplify the equations above in matrix form and solve for the Lagrange multipliers \\(\\lambda_{i}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nhttps://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html\n\n\ndef min_rss_optim1(mu, sigma):\n    \n    params = cp.Variable(len(mu))\n    \n    obj = cp.Minimize(0.5 * cp.quad_form(params, sigma) - params.T @ mu)\n    \n    cons = [cp.sum(params) == 1, params &gt;= 0]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n\n\nparams4 = min_rss_optim1(np.dot(overlap_x_df.T.values, overlap_y_df.values),\n                         np.dot(overlap_x_df.T.values, overlap_x_df.values))\nparams4\n\narray([ 3.74954959e-01, -8.42653692e-23,  6.25045041e-01,  2.43889802e-23])\n\n\n\nnp.dot(mu, params4)\n\n0.03039745091951785\n\n\n\nnp.sqrt(np.matmul(np.transpose(params4), np.matmul(sigma, params4)))\n\n0.05050206095994644\n\n\n\ndef min_rss_optim2(x, y):\n    \n    params = cp.Variable(x.shape[1])\n    \n    obj = cp.Minimize(cp.sum_squares(y - x @ params))\n    \n    cons = [cp.sum(params) == 1, params &gt;= 0]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n\n\nparams5 = min_rss_optim2(overlap_x_df.values, overlap_y_df.iloc[:, 0].values)\nparams5\n\narray([ 3.74954959e-01, -1.36488119e-20,  6.25045041e-01, -8.92709642e-21])\n\n\n\nnp.dot(mu, params5)\n\n0.030397450919517933\n\n\n\nnp.sqrt(np.matmul(np.transpose(params5), np.matmul(sigma, params5)))\n\n0.05050206095994657\n\n\n\npd.DataFrame({\n  \"max_pnl\": params1 * 100,\n  \"min_risk\": params2 * 100,\n  \"max_utility\": params3 * 100,\n  \"min_rss1\": params4 * 100,\n  \"min_rss2\": params5 * 100\n}).round(2)\n\n   max_pnl  min_risk  max_utility  min_rss1  min_rss2\n0    49.61     29.58        55.85      37.5      37.5\n1    50.39     44.64        44.15      -0.0      -0.0\n2     0.00     25.78         0.00      62.5      62.5\n3     0.00      0.00         0.00       0.0      -0.0"
  },
  {
    "objectID": "posts/eigen-r/index.html",
    "href": "posts/eigen-r/index.html",
    "title": "Eigen",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\n\n\nDecomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\neigen_decomp &lt;- function(x, comps) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV[[\"values\"]][1:comps]\n    V &lt;- LV[[\"vectors\"]][ , 1:comps]\n    \n    result &lt;- V %*% sweep(t(V), 1, L, \"*\")\n    \n    return(result)\n    \n}\n\n\ncomps &lt;- 1\n\n\neigen_decomp(overlap_xts, comps) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  0.0298380602 -3.659333e-03 -1.478789e-04  2.555579e-03\n[2,] -0.0036593330  4.487798e-04  1.813583e-05 -3.134157e-04\n[3,] -0.0001478789  1.813583e-05  7.328947e-07 -1.266557e-05\n[4,]  0.0025555794 -3.134157e-04 -1.266557e-05  2.188810e-04\n\n\n\n# cov(overlap_xts) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\nvariance_explained &lt;- function(x) {\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV[[\"values\"]]\n    \n    result &lt;- cumsum(L) / sum(L)\n    \n    return(result)\n    \n}\n\n\nvariance_explained(overlap_xts)\n\n[1] 0.8736404 0.9921508 0.9982652 1.0000000\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n\\]\n\nroll_eigen1 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        \n        LV &lt;- eigen(cov(x[idx, ]))\n        V &lt;- LV[[\"vectors\"]]\n        \n        result_ls &lt;- append(result_ls, list(V[ , comp]))\n        \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\ncomp &lt;- 1\n\n\nraw_df &lt;- roll_eigen1(overlap_xts, width, comp)\n\n\n# # install.packages(\"devtools\")\n# devtools::install_github(\"jjf234/rolleigen\") # roll (&gt;= 1.1.7)\n# library(rolleigen)\n# raw_df &lt;- roll_eigen(overlap_xts, width)[[\"vectors\"]][ , comp, ]\n\n\n\n\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\nroll_eigen2 &lt;- function(x, width, comp) {\n    \n    n_rows &lt;- nrow(x)\n    V_ls &lt;- list()\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        \n        LV &lt;- eigen(cov(x[idx, ]))\n        V &lt;- LV[[\"vectors\"]]\n                \n        if (i &gt; width) {\n          \n            similarity &lt;- crossprod(V, V_ls[[length(V_ls)]])\n            order &lt;- apply(abs(similarity), 1, which.max)\n            V &lt;- t(sign(diag(similarity[ , order])) * t(V[ , order]))\n            \n        }\n        \n        V_ls &lt;- append(V_ls, list(V))\n        result_ls &lt;- append(result_ls, list(V[ , comp]))\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\nclean_df &lt;- roll_eigen2(overlap_xts, width, comp)\n\n\n\n\n\n\n\n\n\n\n\n\nImplied shocks\nProduct of the \\(n\\)th eigenvector and square root of the \\(n\\)th eigenvalue:\n\nroll_shocks &lt;- function(x, width, comp) {\n  \n    n_rows &lt;- nrow(x)\n    V_ls &lt;- list()\n    result_ls &lt;- list()\n    \n    for (i in width:n_rows) {\n        \n        idx &lt;- max(i - width + 1, 1):i\n        \n        LV &lt;- eigen(cov(x[idx, ]))\n        L &lt;- LV[[\"values\"]]\n        V &lt;- LV[[\"vectors\"]]\n        \n        if (length(V_ls) &gt; 1) {\n          \n            similarity &lt;- crossprod(V, V_ls[[length(V_ls)]])\n            order &lt;- apply(abs(similarity), 1, which.max)\n            L &lt;- L[order]\n            V &lt;- t(sign(diag(similarity[ , order])) * t(V[ , order]))\n          \n        }\n        \n        shocks &lt;- sqrt(L[comp]) * V[ , comp]\n        V_ls &lt;- append(V_ls, list(V))\n        result_ls &lt;- append(result_ls, list(shocks))\n                \n    }\n    \n    result &lt;- do.call(rbind, result_ls)\n    result &lt;- xts(result, index(x)[width:n_rows])\n    colnames(result) &lt;- colnames(x)\n    \n    return(result)\n    \n}\n\n\nshocks_xts &lt;- roll_shocks(overlap_xts, width, comp) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])"
  },
  {
    "objectID": "posts/eigen-py/index.html",
    "href": "posts/eigen-py/index.html",
    "title": "Eigen",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\n\n\nDecomposition\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n\\]\n\nhttps://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\n\n\ndef eigen(x):\n  \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    V = V[:, idx]\n    \n    result = {\n        \"values\": L,\n        \"vectors\": V\n    }\n    \n    return result\n\n\ndef eigen_decomp(x, comps):\n    \n    LV = eigen(x)\n    L = LV[\"values\"][:comps]\n    V = LV[\"vectors\"][:, :comps]\n    \n    result = np.dot(V, np.multiply(L, V.T))\n    \n    return result\n\n\ncomps = 1\n\n\neigen_decomp(overlap_df, comps) * scale[\"periods\"] * scale[\"overlap\"]\n\narray([[ 2.98380602e-02, -3.65933302e-03, -1.47878851e-04,\n         2.55557936e-03],\n       [-3.65933302e-03,  4.48779783e-04,  1.81358292e-05,\n        -3.13415681e-04],\n       [-1.47878851e-04,  1.81358292e-05,  7.32894651e-07,\n        -1.26655734e-05],\n       [ 2.55557936e-03, -3.13415681e-04, -1.26655734e-05,\n         2.18881048e-04]])\n\n\n\n# np.cov(overlap_df.T) * scale[\"periods\"] * scale[\"overlap\"]\n\n\n\nVariance\nWe often look at the proportion of variance explained by the first \\(i\\) principal components as an indication of how many components are needed.\n\\[\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\ndef variance_explained(x):\n    \n    LV = eigen(x)\n    L = LV[\"values\"]\n    \n    result = L.cumsum() / L.sum()\n    \n    return result\n\n\nvariance_explained(overlap_df)\n\narray([0.87364043, 0.99215076, 0.99826523, 1.        ])\n\n\n\n\nSimilarity\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\\[\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n\\]\n\ndef roll_eigen1(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        \n        LV = eigen(x.iloc[idx])\n        V = LV[\"vectors\"]\n        \n        result_ls.append(V[:, comp - 1])\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df  \n\n\ncomp = 1\n\n\nraw_df = roll_eigen1(overlap_df, width, comp)\n\n\n\n\n\n\n\n\n\n\n\nhttps://quant.stackexchange.com/a/3095\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158\n\n\ndef roll_eigen2(x, width, comp):\n    \n    n_rows = len(x)\n    V_ls = []\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        \n        LV = eigen(x.iloc[idx])\n        V = LV[\"vectors\"]\n        \n        if i &gt; width - 1:\n            \n            similarity = np.dot(V, V_ls[-1])\n            order = np.argmax(np.abs(similarity), axis = 1)\n            V = np.sign(np.diag(similarity[:, order])) * V[:, order]\n            \n        V_ls.append(V)\n        result_ls.append(V[:, comp - 1])\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df\n\n\nclean_df = roll_eigen2(overlap_df, width, comp)\n\n\n\n\n\n\n\n\n\n\n\n\nImplied shocks\nProduct of the \\(n\\)th eigenvector and square root of the \\(n\\)th eigenvalue:\n\ndef roll_shocks(x, width, comp):\n    \n    n_rows = len(x)\n    V_ls = []\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        \n        LV = eigen(x.iloc[idx])\n        L = LV[\"values\"]\n        V = LV[\"vectors\"]\n        \n        if len(V_ls) &gt; 1:\n            \n            similarity = np.dot(V.T, V_ls[-1])\n            order = np.argmax(np.abs(similarity), axis = 1)\n            L = L[order]\n            V = np.sign(np.diag(similarity[:, order])) * V[:, order]\n        \n        shocks = np.sqrt(L[comp - 1]) * V[:, comp - 1]\n        V_ls.append(V)\n        result_ls.append(shocks)\n        \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df\n\n\nshocks_df = roll_shocks(overlap_df, width, comp) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])"
  },
  {
    "objectID": "posts/markets-py/index.html",
    "href": "posts/markets-py/index.html",
    "title": "Markets",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/markets-py/index.html#expected-value",
    "href": "posts/markets-py/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-py/index.html#variance",
    "href": "posts/markets-py/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\ndef sd(x):\n    \n    n_rows = sum(~np.isnan(x))\n        \n    if n_rows &gt; 1:\n        result = np.sqrt(np.nansum(x ** 2) / (n_rows - 1))\n    else:\n        result = np.nan\n        \n    return result\n\n\n# volatility scale only\nscore_df = (momentum_df / momentum_df.rolling(width, min_periods = 1).apply(sd, raw = False)).dropna()\n\n\n# overall_df = score_df.mean(axis = 1)\n# overall_df = overall_df / overall_df.rolling(width, min_periods = 1).apply(risk, raw = False)\n\n\n# score_df.insert(loc = 0, column = \"Overall\", value = overall_df)\n# score_df = score_df.dropna()"
  },
  {
    "objectID": "posts/markets-py/index.html#interquartile-range",
    "href": "posts/markets-py/index.html#interquartile-range",
    "title": "Markets",
    "section": "Interquartile range",
    "text": "Interquartile range\nOutliers are defined as the regression residuals that fall below \\(Q_{1}âˆ’1.5\\times IQR\\) or above \\(Q_{3}+1.5\\times IQR\\):\n\nhttps://stats.stackexchange.com/a/1153\nhttps://stats.stackexchange.com/a/108951\nhttps://robjhyndman.com/hyndsight/tsoutliers/\n\n\ndef outliers(z):\n  \n    n_cols = z.shape[1]\n    result_ls = []\n\n    for j in range(n_cols):\n      \n        y = z.iloc[:, j]\n\n        if (n_cols == 0):\n            x = sm.add_constant(range(len(y)))\n        else:\n            x = sm.add_constant(z.drop(z.columns[j], axis = 1))\n\n        coef = sm.WLS(y, x).fit().params\n        predict = coef.iloc[0] + np.dot(x.iloc[:, 1:], coef[1:])\n        resid = y - predict\n\n        lower = resid.quantile(0.25)\n        upper = resid.quantile(0.75)\n        iqr = upper - lower\n\n        total = y[(resid &lt; lower - 1.5 * iqr) | (resid &gt; upper + 1.5 * iqr)]\n        \n        total = pd.DataFrame({\"date\": total.index, \"symbol\": total.name, \"values\": total})\n        result_ls.append(total)\n\n    result = pd.concat(result_ls, ignore_index = True)\n    result = result.pivot_table(index = \"date\", columns = \"symbol\", values = \"values\")\n\n    return result\n\n\noutliers_df = outliers(score_df)"
  },
  {
    "objectID": "posts/markets-py/index.html#contour-ellipsoid",
    "href": "posts/markets-py/index.html#contour-ellipsoid",
    "title": "Markets",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid"
  },
  {
    "objectID": "posts/risk-py/index.html",
    "href": "posts/risk-py/index.html",
    "title": "Risk",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\nintercept = True"
  },
  {
    "objectID": "posts/risk-py/index.html#ordinary-least-squares",
    "href": "posts/risk-py/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\ndef lm_coef(x, y, weights, intercept):\n    \n    if (intercept): x = sm.add_constant(x)\n        \n    result = np.dot(np.linalg.inv(np.dot(x.T, np.multiply(weights, x))),\n                    np.dot(x.T, np.multiply(weights, y)))\n    \n    return np.ravel(result)\n\n\nlm_coef(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([-7.13334150e-05,  1.92994136e-01, -1.27443013e-01,  3.05671159e+00,\n        1.82896910e+00])\n\n\n\nif (intercept): overlap_x_df = sm.add_constant(overlap_x_df)\n    \nfit = sm.WLS(overlap_y_df, overlap_x_df, weights = weights).fit()\n\nif (intercept): overlap_x_df = overlap_x_df.iloc[:, 1:]\n\nnp.array(fit.params)\n\narray([-7.13334150e-05,  1.92994136e-01, -1.27443013e-01,  3.05671159e+00,\n        1.82896910e+00])\n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\ndef lm_rsq(x, y, weights, intercept):\n            \n    coef = np.matrix(lm_coef(x, y, weights, intercept))\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n    result = np.dot(coef, np.dot(np.dot(x.T, np.multiply(weights, x)), coef.T)) / \\\n        np.dot(y.T, np.multiply(weights, y))\n    \n    return result.item()\n\n\nlm_rsq(overlap_x_df, overlap_y_df, weights, intercept)\n\n0.811030165499897\n\n\n\nfit.rsquared\n\n0.8110301654998973\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\ndef lm_se(x, y, weights, intercept):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n        df_resid = n_rows - n_cols - 1 \n        \n    else:\n        df_resid = n_rows - n_cols        \n    \n    var_y = np.dot(y.T, np.multiply(weights, y))\n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    result = np.sqrt(var_resid * np.linalg.inv(np.dot(x.T, np.multiply(weights, x))).diagonal())\n    \n    return np.ravel(result)\n\n\nlm_se(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([5.50814345e-05, 2.20868727e-02, 3.96741594e-02, 2.14137999e-01,\n       1.85252611e-01])\n\n\n\nnp.array(fit.bse)\n\narray([5.50814345e-05, 2.20868727e-02, 3.96741594e-02, 2.14137999e-01,\n       1.85252611e-01])\n\n\n\n\nShapley values\n\\[\n{\\displaystyle \\varphi _{i}(v)=\\sum _{S\\subseteq N\\setminus \\{i\\}}{\\frac {|S|!\\;(n-|S|-1)!}{n!}}(v(S\\cup \\{i\\})-v(S))}\n\\]\n\nhttps://real-statistics.com/multiple-regression/shapley-owen-decomposition/\n\n\ndef lm_shap(x, y, weights, intercept):\n  \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    n_combn = 2 ** n_cols\n    n_vec = np.zeros(n_combn)\n    ix_mat = np.zeros((n_cols, n_combn))\n    rsq = np.zeros(n_combn)\n    result = np.zeros(n_cols)\n    \n    # number of binary combinations\n    for k in range(n_combn):\n      \n        n = 0\n        n_size = k\n        \n        # find the binary combination\n        for j in range(n_cols):\n          \n            if n_size % 2 == 0:\n              \n                n += 1\n                \n                ix_mat[j, k] = j + 1\n                \n            n_size //= 2\n        \n        n_vec[k] = n\n        \n        if n &gt; 0:\n          \n            ix_subset = np.where(ix_mat[:, k] != 0)[0]\n            x_subset = x.iloc[:, ix_subset]\n            \n            rsq[k] = lm_rsq(x_subset, y, weights, intercept)\n\n    # calculate the exact Shapley value for r-squared\n    for j in range(n_cols):\n      \n        ix_pos = np.where(ix_mat[j, :] != 0)[0]\n        ix_neg = np.where(ix_mat[j, :] == 0)[0]\n        ix_n = n_vec[ix_neg]\n        rsq_diff = rsq[ix_pos] - rsq[ix_neg]\n\n        for k in range(int(n_combn / 2)):\n          \n            s = int(ix_n[k])\n            weight = math.factorial(s) * math.factorial((n_cols - s - 1)) \\\n                / math.factorial(n_cols)\n            result[j] += weight * rsq_diff[k]\n\n    return result\n\n\nlm_shap(overlap_x_df, overlap_y_df, weights, intercept)\n\narray([0.29339758, 0.11561117, 0.2110997 , 0.19092172])"
  },
  {
    "objectID": "posts/risk-py/index.html#principal-component-regression",
    "href": "posts/risk-py/index.html#principal-component-regression",
    "title": "Risk",
    "section": "Principal component regression",
    "text": "Principal component regression\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\n\n\ncomps = 1\n\n\nCoefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\ndef pcr_coef(x, y, comps):\n    \n    x = x - np.average(x, axis = 0)\n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    V = V[:, idx]\n    \n    W = np.dot(x, V)\n    gamma = np.dot(np.dot(np.linalg.inv(np.dot(W.T, W)), W.T), y)\n    \n    result = np.dot(V[:, :comps], gamma[:comps])\n    \n    return np.ravel(result)\n\n\nscale_x_df = (overlap_x_df - np.average(overlap_x_df, axis = 0)) \\\n    / np.std(overlap_x_df, axis = 0, ddof = 1)\n\n\npcr_coef(scale_x_df, overlap_y_df, comps)\n\narray([ 0.0007779 , -0.00061924,  0.00035   ,  0.0005932 ])\n\n\n\npcr_coef(overlap_x_df, overlap_y_df, comps)\n\narray([ 0.39567789, -0.08163777,  0.00731631,  0.030101  ])\n\n\n\npca = PCA(n_components = len(factors))\npca_x_df = pca.fit_transform(scale_x_df)\n\nfit = LinearRegression(fit_intercept = False).fit(pca_x_df, overlap_y_df)\n\ngamma = fit.coef_\nnp.dot(pca.components_.T[:, :comps], gamma.T[:comps]).ravel()\n\narray([ 0.0007779 , -0.00061924,  0.00035   ,  0.0005932 ])\n\n\n\n\nR-squared\n\ndef pcr_rsq(x, y, comps):\n    \n    coef = np.matrix(pcr_coef(x, y, comps))\n    \n    x = x - np.average(x, axis = 0)\n    y = y - np.average(y, axis = 0)\n    \n    result = np.dot(np.dot(coef, np.dot(x.T, x)), coef.T) / np.dot(y.T, y)\n    \n    return result.item()\n\n\npcr_rsq(scale_x_df, overlap_y_df, comps)\n\n0.74161702658949\n\n\n\npcr_rsq(overlap_x_df, overlap_y_df, comps)\n\n0.6007758995627772\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\ndef pcr_se(x, y, comps):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = pcr_rsq(x, y, comps)\n    \n    y = y - np.average(y, axis = 0)\n    \n    df_resid = n_rows - n_cols - 1\n    \n    var_y = np.dot(y.T, y)   \n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    # uses statsmodels for illustrative purposes\n    pca = sm.multivariate.PCA(x, standardize = False, demean = True)\n    L = pca.eigenvals[:comps]\n    V = pca.eigenvecs.iloc[:, :comps]\n    \n    result = np.sqrt(var_resid * np.dot(V, np.dot(np.diag(1 / L), V.T)).diagonal())\n    \n    return np.ravel(result)\n\n\npcr_se(scale_x_df, overlap_y_df, comps)\n\narray([2.92157444e-05, 2.32569820e-05, 1.31451818e-05, 2.22789536e-05])\n\n\n\npcr_se(overlap_x_df, overlap_y_df, comps)\n\narray([0.02052322, 0.00423443, 0.00037949, 0.00156129])"
  },
  {
    "objectID": "posts/risk-py/index.html#partial-least-squares",
    "href": "posts/risk-py/index.html#partial-least-squares",
    "title": "Risk",
    "section": "Partial least squares",
    "text": "Partial least squares"
  },
  {
    "objectID": "posts/risk-py/index.html#standalone-risk",
    "href": "posts/risk-py/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\ndef cov_wt(x, weights, center):\n    \n    sum_w = sum(weights)\n    sumsq_w = sum(np.power(weights, 2))\n    \n    if (center):\n    \n        x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n    \n    result = np.dot(x.T, np.multiply(weights, x)) / (sum_w - sumsq_w / sum_w)\n    \n    return result\n\n\ndef lm_sar(x, y, weights, intercept):\n    \n    coef = lm_coef(x, y, weights, intercept)\n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept): x = sm.add_constant(x)\n    \n    # sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n    #                aweights = weights.reshape(-1))\n    sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n    sar = np.multiply(np.power(coef, 2).T, sigma[:-1, :-1].diagonal())\n    sar_eps = (1 - rsq) * sigma[-1, -1]\n    \n    result = np.sqrt(np.concatenate((np.matrix(sigma[-1, -1]),\n                                     np.matrix(sar),\n                                     np.matrix(sar_eps)), axis = 1))\n    \n    return np.ravel(result)\n\n\nlm_sar(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])\n\narray([0.06844763, 0.        , 0.0247587 , 0.00735331, 0.03371399,\n       0.0275071 , 0.02975464])"
  },
  {
    "objectID": "posts/risk-py/index.html#risk-contribution",
    "href": "posts/risk-py/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttps://bookdown.org/compfinezbook/introcompfinr/Portfolio-risk-reports.html\n\n\ndef lm_mcr(x, y, weights, intercept):\n    \n    coef = np.matrix(lm_coef(x, y, weights, intercept)).T\n    rsq = lm_rsq(x, y, weights, intercept)\n        \n    if (intercept): x = sm.add_constant(x)\n    \n#     sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n#                    aweights = weights.reshape(-1))\n    sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n    mcr = np.multiply(coef, np.dot(sigma[:-1, :-1], coef)) / np.sqrt(sigma[-1, -1])\n    mcr_eps = np.sqrt(sigma[-1, -1]) - sum(mcr)\n    \n    result = np.concatenate((np.sqrt(np.matrix(sigma[-1, -1])),\n                             np.matrix(mcr).T,\n                             np.matrix(mcr_eps)), axis = 1)\n    \n    return np.ravel(result)\n\n\nlm_mcr(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])\n\narray([ 0.06844763, -0.        ,  0.01868365,  0.00394039,  0.01770996,\n        0.0151791 ,  0.01293454])"
  },
  {
    "objectID": "posts/risk-py/index.html#implied-shocks",
    "href": "posts/risk-py/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\ndef implied_shocks(shocks, x, z, weights):\n\n    beta = np.linalg.lstsq(np.multiply(weights, z), np.multiply(weights, x), rcond = None)[0]\n                     \n    result = np.dot(shocks, beta)\n    \n    return np.ravel(result)\n\n\nshocks = np.array([-0.1, 0.1])\noverlap_z_df = overlap_x_df.iloc[:, [0, 1]]\n\n\nimplied_shocks(shocks, overlap_x_df, overlap_z_df, weights)\n\narray([-0.1       ,  0.1       , -0.00900019, -0.00479809])"
  },
  {
    "objectID": "posts/risk-py/index.html#stress-pl",
    "href": "posts/risk-py/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\ndef pnl_stress(shocks, x, y, z, weights, intercept):\n    \n    coef = lm_coef(x, y, weights, intercept)\n    \n    if (intercept): x = sm.add_constant(x)\n    \n    result = np.multiply(coef.T, implied_shocks(shocks, x, z, weights))\n    \n    return np.ravel(result)\n\n\npnl_stress(shocks, overlap_x_df, overlap_y_df, overlap_z_df, weights, intercept)\n\narray([ 0.00039124, -0.01929941, -0.0127443 , -0.02751097, -0.00877557])"
  },
  {
    "objectID": "posts/markets-r/index.html",
    "href": "posts/markets-r/index.html",
    "title": "Markets",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/markets-r/index.html#expected-value",
    "href": "posts/markets-r/index.html#expected-value",
    "title": "Markets",
    "section": "Expected value",
    "text": "Expected value\n\\[\n\\begin{aligned}\n\\mathrm{E}\\left(\\bar{X}\\right)&=\\mathrm{E}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)\\mathrm{E}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)\\left(n\\mu\\right)\\\\\n&=\\mu\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/markets-r/index.html#variance",
    "href": "posts/markets-r/index.html#variance",
    "title": "Markets",
    "section": "Variance",
    "text": "Variance\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left(\\bar{X}\\right)&=\\mathrm{Var}\\left[\\left(\\frac{1}{n}\\right)\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\right]\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\mathrm{Var}\\left(X_{1}+X_{2}+\\cdots+X_{n}\\right)\\\\\n&=\\left(\\frac{1}{n}\\right)^{2}\\left(n\\sigma^{2}\\right)\\\\\n&=\\frac{\\sigma^{2}}{n}\n\\end{aligned}\n\\]\n\nhttp://scipp.ucsc.edu/~haber/ph116C/iid.pdf\n\n\n# volatility scale only\nscore_xts &lt;- na.omit(momentum_xts / roll_sd(momentum_xts, width, center = FALSE, min_obs = 1))\n\n\n# overall_xts &lt;- xts(rowMeans(score_xts), index(score_xts))\n# overall_xts &lt;- overall_xts / roll_sd(overall_xts, width, center = FALSE, min_obs = 1)\n# colnames(overall_xts) &lt;- \"Overall\"\n\n\n# score_xts &lt;- na.omit(merge(overall_xts, score_xts))"
  },
  {
    "objectID": "posts/markets-r/index.html#interquartile-range",
    "href": "posts/markets-r/index.html#interquartile-range",
    "title": "Markets",
    "section": "Interquartile range",
    "text": "Interquartile range\nOutliers are defined as the regression residuals that fall below \\(Q_{1}âˆ’1.5\\times IQR\\) or above \\(Q_{3}+1.5\\times IQR\\):\n\nhttps://stats.stackexchange.com/a/1153\nhttps://stats.stackexchange.com/a/108951\nhttps://robjhyndman.com/hyndsight/tsoutliers/\n\n\noutliers &lt;- function(z) {\n  \n  n_cols &lt;- ncol(z)\n  result_ls &lt;- list()\n  \n  for (j in 1:n_cols) {\n    \n    y &lt;- z[ , j]\n    \n    if (n_cols == 1) {\n      x &lt;- 1:length(y)\n    } else {\n      x &lt;- cbind(1:length(y), z[ , -j])\n    }\n    \n    coef &lt;- coef(lm(y ~ x))\n    predict &lt;- coef[1] + x %*% as.matrix(coef[-1])\n    resid &lt;- y - predict\n    \n    lower &lt;- quantile(resid, prob = 0.25)\n    upper &lt;- quantile(resid, prob = 0.75)\n    iqr &lt;- upper - lower\n    \n    total &lt;- y[(resid &lt; lower - 1.5 * iqr) | (resid &gt; upper + 1.5 * iqr)]\n    \n    result_ls &lt;- append(result_ls, list(total))\n    \n  }\n  \n  result &lt;- do.call(merge, result_ls)\n  \n  return(result)\n  \n}\n\n\noutliers_xts &lt;- outliers(score_xts)"
  },
  {
    "objectID": "posts/markets-r/index.html#contour-ellipsoid",
    "href": "posts/markets-r/index.html#contour-ellipsoid",
    "title": "Markets",
    "section": "Contour ellipsoid",
    "text": "Contour ellipsoid"
  },
  {
    "objectID": "posts/risk-r/index.html",
    "href": "posts/risk-r/index.html",
    "title": "Risk",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")\ntickers &lt;- \"BAICX\" # fund inception date is \"2011-11-28\"\nintercept &lt;- TRUE"
  },
  {
    "objectID": "posts/risk-r/index.html#ordinary-least-squares",
    "href": "posts/risk-r/index.html#ordinary-least-squares",
    "title": "Risk",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\n\nCoefficients\n\\[\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n\\]\n\nhttps://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf\n\n\nlm_coef &lt;- function(x, y, weights, intercept) {\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    result &lt;- solve(crossprod(x, diag(weights)) %*% x) %*% crossprod(x, diag(weights) %*% y)\n    \n    return(result)\n    \n}\n\n\nt(lm_coef(overlap_x_xts, overlap_y_xts, weights, intercept))\n\n        (Intercept)    xSP500 xDTWEXAFEGS   xDGS10 xBAMLH0A0HYM2\nBAICX -7.133341e-05 0.1929941   -0.127443 3.056712      1.828969\n\n\n\nif (intercept) {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts, weights = weights)\n} else {\n    fit &lt;- lm(overlap_y_xts ~ overlap_x_xts - 1, weights = weights)\n}\n    \ncoef(fit)\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n            -7.133341e-05              1.929941e-01             -1.274430e-01 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             3.056712e+00              1.828969e+00 \n\n\n\n\nR-squared\n\\[\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n\\]\n\nlm_rsq &lt;- function(x, y, weights, intercept) {\n        \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        x &lt;- sweep(x, 2, apply(x, 2, weighted.mean, w = weights), \"-\")\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n    }\n    \n    result &lt;- (t(coef) %*% (crossprod(x, diag(weights)) %*% x) %*% coef) / (crossprod(y, diag(weights)) %*% y)\n    \n    return(result)\n    \n}\n\n\nlm_rsq(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n          BAICX\nBAICX 0.8110302\n\n\n\nsummary(fit)$r.squared\n\n[1] 0.8110302\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n\\]\n\nhttp://people.duke.edu/~rnau/mathreg.htm\n\n\nlm_se &lt;- function(x, y, weights, intercept) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) {\n        \n        x &lt;- model.matrix(~ x)\n        y &lt;- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\")\n        \n        df_resid &lt;- n_rows - n_cols - 1\n        \n    } else {\n        df_resid &lt;- n_rows - n_cols\n    }\n    \n    var_y &lt;- crossprod(y, diag(weights)) %*% y\n    var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n    \n    result &lt;- sqrt(var_resid * diag(solve(crossprod(x, diag(weights)) %*% x)))\n    \n    return(result)\n}\n\n\nlm_se(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n  (Intercept)        xSP500   xDTWEXAFEGS        xDGS10 xBAMLH0A0HYM2 \n 5.508143e-05  2.208687e-02  3.967416e-02  2.141380e-01  1.852526e-01 \n\n\n\ncoef(summary(fit))[ , \"Std. Error\"]\n\n              (Intercept)        overlap_x_xtsSP500   overlap_x_xtsDTWEXAFEGS \n             5.508143e-05              2.208687e-02              3.967416e-02 \n       overlap_x_xtsDGS10 overlap_x_xtsBAMLH0A0HYM2 \n             2.141380e-01              1.852526e-01 \n\n\n\n\nShapley values\n\\[\n{\\displaystyle \\varphi _{i}(v)=\\sum _{S\\subseteq N\\setminus \\{i\\}}{\\frac {|S|!\\;(n-|S|-1)!}{n!}}(v(S\\cup \\{i\\})-v(S))}\n\\]\n\nhttps://real-statistics.com/multiple-regression/shapley-owen-decomposition/\n\n\nlm_shap &lt;- function(x, y, weights, intercept) {\n  \n  n_rows &lt;- nrow(x)\n  n_cols &lt;- ncol(x)\n  n_combn &lt;- 2 ^ n_cols\n  n_vec &lt;- array(0, n_combn)\n  ix_mat &lt;- matrix(0, nrow = n_cols, ncol = n_combn)\n  rsq &lt;- array(0, n_combn)\n  result &lt;- array(0, n_cols)\n  \n  # number of binary combinations\n  for (k in 1:n_combn) {\n    \n    n &lt;- 0\n    n_size &lt;- k - 1\n    \n    # find the binary combination\n    for (j in 1:n_cols) {\n      \n      if (n_size %% 2 == 0) {\n        \n        n &lt;- n + 1\n        \n        ix_mat[j, k] = j - 1 + 1\n        \n      }\n      \n      n_size &lt;- n_size %/% 2\n      \n    }\n    \n    n_vec[k] &lt;- n\n    \n    if (n &gt; 0) {\n      \n      ix_subset&lt;- which(ix_mat[ , k] != 0)\n      x_subset &lt;- x[ , ix_subset]\n      \n      rsq[k] &lt;- lm_rsq(x_subset, y, weights, intercept)\n\n    }\n    \n  }\n\n  # calculate the exact Shapley value for r-squared\n  for (j in 1:n_cols) {\n\n    ix_pos &lt;- which(ix_mat[j, ] != 0)\n    ix_neg &lt;- which(ix_mat[j, ] == 0)\n    ix_n &lt;- n_vec[ix_neg]\n    rsq_diff &lt;- rsq[ix_pos] - rsq[ix_neg]\n\n    for (k in 1:(n_combn / 2)) {\n\n      s &lt;- ix_n[k]\n      weight &lt;- factorial(s) * factorial((n_cols - s - 1)) / factorial(n_cols)\n      result[j] &lt;- result[j] + weight * rsq_diff[k]\n\n    }\n    \n  }\n\n  return(result)\n  \n}\n\n\nlm_shap(overlap_x_xts, overlap_y_xts, weights, intercept)\n\n[1] 0.2933976 0.1156112 0.2110997 0.1909217"
  },
  {
    "objectID": "posts/risk-r/index.html#principal-component-regression",
    "href": "posts/risk-r/index.html#principal-component-regression",
    "title": "Risk",
    "section": "Principal component regression",
    "text": "Principal component regression\n\nlibrary(pls)\n\n\ncomps &lt;- 1\n\n\nCoefficients\n\\[\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Principal_component_regression\n\n\npcr_coef &lt;- function(x, y, comps) {\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    LV &lt;- eigen(cov(x))\n    V &lt;- LV[[\"vectors\"]]\n    \n    W &lt;- x %*% V\n    gamma &lt;- solve(crossprod(W)) %*% (crossprod(W, y))\n    \n    result &lt;- V[ , 1:comps] %*% as.matrix(gamma[1:comps])\n    \n    return(result)\n    \n}\n\n\nscale_x_xts &lt;- scale(overlap_x_xts)\n\n\nt(pcr_coef(scale_x_xts, overlap_y_xts, comps))\n\n             [,1]         [,2]        [,3]      [,4]\n[1,] 0.0007778992 -0.000619241 0.000350004 0.0005932\n\n\n\nt(pcr_coef(overlap_x_xts, overlap_y_xts, comps))\n\n          [,1]        [,2]        [,3]     [,4]\n[1,] 0.3956779 -0.08163777 0.007316309 0.030101\n\n\n\nfit &lt;- pcr(reformulate(termlabels = \".\", response = tickers), \n           data = merge(scale_x_xts, overlap_y_xts), ncomp = comps)\ncoef(fit)[ , , 1]\n\n        SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 0.0007778992 -0.0006192410  0.0003500040  0.0005932000 \n\n\n\n\nR-squared\n\npcr_rsq &lt;- function(x, y, comps) {\n    \n    coef &lt;- pcr_coef(x, y, comps)\n    \n    x &lt;- sweep(x, 2, colMeans(x), \"-\")\n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    result &lt;- (t(coef) %*% crossprod(x) %*% coef) / crossprod(y)\n    \n    return(result)\n    \n}\n\n\npcr_rsq(scale_x_xts, overlap_y_xts, comps)\n\n         BAICX\nBAICX 0.741617\n\n\n\npcr_rsq(overlap_x_xts, overlap_y_xts, comps)\n\n          BAICX\nBAICX 0.6007759\n\n\n\nR2(fit)$val[comps + 1]\n\n[1] 0.741617\n\n\n\n\nStandard errors\n\\[\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n\\]\n\n# unable to verify the result\npcr_se &lt;- function(x, y, comps) {\n    \n    n_rows &lt;- nrow(x)\n    n_cols &lt;- ncol(x)\n    \n    rsq &lt;- pcr_rsq(x, y, comps)\n    \n    y &lt;- sweep(y, 2, colMeans(y), \"-\")\n    \n    df_resid &lt;- n_rows - n_cols - 1\n    \n    var_y &lt;- crossprod(y)\n    var_resid &lt;- as.numeric((1 - rsq) * var_y / df_resid)\n    \n    LV &lt;- eigen(cov(x))\n    L &lt;- LV$values[1:comps] * (n_rows - 1)\n    V &lt;- LV$vectors[ , 1:comps]\n    \n    result &lt;- sqrt(var_resid * diag(V %*% sweep(t(V), 1, 1 / L, \"*\")))\n    \n    return(result)\n    \n}\n\n\npcr_se(scale_x_xts, overlap_y_xts, comps)\n\n[1] 2.921574e-05 2.325698e-05 1.314518e-05 2.227895e-05\n\n\n\npcr_se(overlap_x_xts, overlap_y_xts, comps)\n\n[1] 0.0205232166 0.0042344282 0.0003794859 0.0015612934"
  },
  {
    "objectID": "posts/risk-r/index.html#standalone-risk",
    "href": "posts/risk-r/index.html#standalone-risk",
    "title": "Risk",
    "section": "Standalone risk",
    "text": "Standalone risk\n\\[\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n\\]\n\nlm_sar &lt;- function(x, y, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n    sar &lt;- coef ^ 2 * diag(sigma[-ncol(sigma), -ncol(sigma)])\n    sar_eps &lt;- (1 - rsq) * sigma[ncol(sigma), ncol(sigma)]\n    \n    result &lt;- sqrt(c(sigma[ncol(sigma), ncol(sigma)],\n                     sar,\n                     sar_eps))\n    \n    return(result)\n    \n}\n\n\nlm_sar(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])\n\n[1] 0.068447632 0.000000000 0.024758695 0.007353314 0.033713991 0.027507099\n[7] 0.029754638"
  },
  {
    "objectID": "posts/risk-r/index.html#risk-contribution",
    "href": "posts/risk-r/index.html#risk-contribution",
    "title": "Risk",
    "section": "Risk contribution",
    "text": "Risk contribution\n\\[\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n\\]\n\nhttps://bookdown.org/compfinezbook/introcompfinr/Portfolio-risk-reports.html\n\n\nlm_mcr &lt;- function(x, y, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    rsq &lt;- lm_rsq(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    sigma &lt;- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov\n    mcr &lt;- coef * sigma[-ncol(sigma), -ncol(sigma)] %*% coef / sqrt(sigma[ncol(sigma), ncol(sigma)])\n    mcr_eps &lt;- sqrt(sigma[ncol(sigma), ncol(sigma)]) - sum(mcr)\n    \n    result &lt;- c(sqrt(sigma[ncol(sigma), ncol(sigma)]),\n                mcr,\n                mcr_eps)\n    \n    return(result)\n    \n}\n\n\nlm_mcr(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]] * scale[[\"overlap\"]])\n\n[1] 0.068447632 0.000000000 0.018683647 0.003940388 0.017709964 0.015179097\n[7] 0.012934538"
  },
  {
    "objectID": "posts/risk-r/index.html#implied-shocks",
    "href": "posts/risk-r/index.html#implied-shocks",
    "title": "Risk",
    "section": "Implied shocks",
    "text": "Implied shocks\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n\\]\n\nimplied_shocks &lt;- function(shocks, x, z, weights) {\n    \n    beta &lt;- solve(crossprod(z, diag(weights) %*% z)) %*% crossprod(z, diag(weights) %*% x)\n    \n    result &lt;- shocks %*% beta\n    \n    return(result)\n    \n}\n\n\nshocks &lt;- c(-0.1, 0.1)\noverlap_z_xts &lt;- overlap_x_xts[ , 1:2]\n\n\nimplied_shocks(shocks, overlap_x_xts, overlap_z_xts, weights)\n\n     SP500 DTWEXAFEGS        DGS10 BAMLH0A0HYM2\n[1,]  -0.1        0.1 -0.009000186 -0.004798094"
  },
  {
    "objectID": "posts/risk-r/index.html#stress-pl",
    "href": "posts/risk-r/index.html#stress-pl",
    "title": "Risk",
    "section": "Stress P&L",
    "text": "Stress P&L\n\npnl_stress &lt;- function(shocks, x, y, z, weights, intercept) {\n    \n    coef &lt;- lm_coef(x, y, weights, intercept)\n    \n    if (intercept) x &lt;- model.matrix(~ x)\n    \n    result &lt;- t(coef) * implied_shocks(shocks, x, z, weights)\n    \n    return(result)    \n    \n}\n\n\npnl_stress(shocks, overlap_x_xts, overlap_y_xts, overlap_z_xts, weights, intercept)\n\n       (Intercept)      xSP500 xDTWEXAFEGS      xDGS10 xBAMLH0A0HYM2\nBAICX 0.0003912373 -0.01929941  -0.0127443 -0.02751097  -0.008775565"
  },
  {
    "objectID": "posts/securities-r/index.html",
    "href": "posts/securities-r/index.html",
    "title": "Securities",
    "section": "",
    "text": "factors_r &lt;- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d &lt;- c(\"DGS10\", \"BAMLH0A0HYM2\")"
  },
  {
    "objectID": "posts/securities-r/index.html#value",
    "href": "posts/securities-r/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_value &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n    r_df &lt;- exp(-r * tau)\n    q_df &lt;- exp(-q * tau)\n    \n    call_value &lt;- S * q_df * Phi(d1) - r_df * K * Phi(d2)\n    put_value &lt;- r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n} \n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\nbs_d1 &lt;- function(S, K, r, q, tau, sigma) {\n    \n    result &lt;- (log(S / K) + (r - q + sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\nbs_d2 &lt;- function(S, K, r, q, tau, sigma) {\n    \n    result &lt;- (log(S / K) + (r - q - sigma ^ 2 / 2) * tau) / (sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\nphi &lt;- function(x) {\n    \n    result &lt;- dnorm(x)\n    \n    return(result)\n    \n}\n\nPhi &lt;- function(x) {\n    \n    result &lt;- pnorm(x)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , d1 := bs_d1(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , d2 := bs_d2(spot, K, r, q, tau, sigma), by = c(\"type\", \"shock\")]\ngreeks_dt[ , value := bs_value(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order",
    "href": "posts/securities-r/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_delta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    q_df &lt;- exp(-q * tau)\n  \n    call_value &lt;- q_df * Phi(d1)\n    put_value &lt;- -q_df * Phi(-d1)\n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n} \n\n\ngreeks_dt[ , delta := bs_delta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nDelta-beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\nbs_delta_diff &lt;- function(type, S, K, r, q, tau, sigma, delta0) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    call_value &lt;- delta - delta0\n    put_value &lt;- delta0 - delta\n    \n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n        \n    return(result)\n    \n}\n\n\nbeta &lt;- 0.35\ntype &lt;- \"call\"\nn &lt;- 1\nmultiple &lt;- 100\ntotal &lt;- 1000000\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\nsec &lt;- list(\n    \"n\" = n,\n    \"multiple\" = multiple,\n    \"S\" = S,\n    \"delta\" = bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n    \"beta\" = 1\n)\n\n\nbeta_dt &lt;- CJ(type = type, shock = shocks)\nbeta_dt[ , spot := level_shock(shock, S, tau, sigma), by = c(\"type\", \"shock\")]\nbeta_dt[ , static := beta]\nbeta_dt[ , diff := bs_delta_diff(type, spot, K, r, q, tau, sigma, sec[[\"delta\"]])]\nbeta_dt[ , dynamic := beta + sec[[\"n\"]] * sec[[\"multiple\"]] * sec[[\"S\"]] * sec[[\"beta\"]] * diff / total, by = c(\"type\", \"shock\")]\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\nbs_vega &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    q_df &lt;- exp(-q * tau)\n    \n    result &lt;- S * q_df * phi(d1) * sqrt(tau)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , vega := bs_vega(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]\n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\nbs_theta &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n    \n    r_df &lt;- exp(r * tau)\n    q_df &lt;- exp(q * tau)\n  \n    call_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) -\n      r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n    \n    put_value &lt;- -q_df * S * phi(d1) * sigma / (2 * sqrt(tau)) +\n      r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n        \n    result &lt;- ifelse(type == \"call\", call_value, put_value)\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , theta := bs_theta(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order",
    "href": "posts/securities-r/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\nbs_gamma &lt;- function(type, S, K, r, q, tau, sigma, d1, d2) {\n  \n    q_df &lt;- exp(-q * tau)\n    \n    result &lt;- q_df * phi(d1) / (S * sigma * sqrt(tau))\n    \n    return(result)\n    \n}\n\n\ngreeks_dt[ , gamma := bs_gamma(type, spot, K, r, q, tau, sigma, d1, d2), by = c(\"type\", \"shock\")]"
  },
  {
    "objectID": "posts/securities-r/index.html#first-order-1",
    "href": "posts/securities-r/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(P(y+\\Delta y)-P(y)\\approx -D\\Delta y +{\\frac{C\\times 100}{2!}}(\\Delta y)^{2}\\)\n\npnl_bond &lt;- function(duration, convexity, dy) {\n    \n    duration_pnl &lt;- -duration * dy\n    convexity_pnl &lt;- (convexity * 100 / 2) * dy ^ 2\n    income_pnl &lt;- dy\n    \n    result &lt;- list(\"total\" = duration_pnl + convexity_pnl + income_pnl,\n                   \"duration\" = duration_pnl,\n                   \"convexity\" = convexity_pnl,\n                   \"income\" = income_pnl)\n    \n    return(result)\n    \n} \n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor &lt;- \"DGS10\"\nduration &lt;- 6.5\nconvexity &lt;- 0.65\ny &lt;- coredata(tail(na.locf(levels_xts[ , factor]), width)[1])\n\n\nbonds_dt &lt;- data.table(index = index(tail(levels_xts, width)),\n                       duration = duration, convexity = convexity,\n                       dy = na.locf(tail(levels_xts[ , factor], width)))\nsetnames(bonds_dt, c(\"index\", \"duration\", \"convexity\", \"dy\"))\nbonds_dt[ , dy := (dy - y) / 100, by = index]\n\n\nattrib_dt &lt;- bonds_dt[ , as.list(unlist(pnl_bond(duration, convexity, dy))), by = index]\n\n\n\n\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=-\\frac{\\partial D}{\\partial y}\\\\\n&=\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}-\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=C-D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(C\\times 100-D^{2}\\right)=C-\\frac{D^{2}}{100}\\)\n\nyield_shock &lt;- function(shock, tau, sigma) {\n    \n    result &lt;- shock * sigma * sqrt(tau)\n    \n    return(result)\n    \n}\n\n\nduration_drift &lt;- function(duration, convexity, dy) {\n    \n    drift &lt;- convexity - duration ^ 2 / 100\n    change &lt;- -drift * dy * 100\n    \n    result &lt;- list(\"drift\" = drift,\n                   \"change\" = change)\n    \n    return(result)\n    \n}\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor &lt;- \"DGS10\"\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor])\n\n\nduration_dt &lt;- CJ(shock = shocks)\nduration_dt[ , spot := yield_shock(shock, tau, sigma), by = \"shock\"]\nduration_dt[ , static := duration]\nduration_dt[ , dynamic := duration + duration_drift(duration, convexity, spot)[[\"change\"]], by = \"shock\"]"
  },
  {
    "objectID": "posts/securities-r/index.html#second-order-1",
    "href": "posts/securities-r/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlackâ€™s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Blackâ€™s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\npnl_option &lt;- function(type, S, K, r, q, tau, sigma, dS, dt, dsigma) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, sigma)\n    d2 &lt;- bs_d2(S, K, r, q, tau, sigma)\n    value &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta &lt;- bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega &lt;- bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta &lt;- bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma &lt;- bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl &lt;- delta * dS / value\n    gamma_pnl &lt;- gamma / 2 * dS ^ 2 / value\n    vega_pnl &lt;- vega * dsigma / value\n    theta_pnl &lt;- theta * dt / value\n    \n    result &lt;- list(\"total\" = delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n                   \"delta\" = delta_pnl,\n                   \"gamma\" = gamma_pnl,\n                   \"vega\" = vega_pnl,\n                   \"theta\" = theta_pnl)\n    \n    return(result)    \n    \n}\n\n\nfactor &lt;- \"SP500\"\ntype &lt;- \"call\"\nS &lt;- coredata(tail(na.locf(levels_xts[ , factor]), width)[1])\nK &lt;- S # * (1 + 0.05)\ntau &lt;- 1 # = 252 / 252\nsigma &lt;- coredata(tail(sd_xts[ , factor], width)[1])\n\n\noptions_dt &lt;- data.table(index = index(tail(levels_xts, width)),\n                         spot = na.locf(tail(levels_xts[ , factor], width)),\n                         sigma = tail(sd_xts[ , factor], width))\nsetnames(options_dt, c(\"index\", \"spot\", \"sigma\"))\noptions_dt[ , dS := spot - S, by = index]\noptions_dt[ , dt_diff := as.numeric(index - index[1])]\noptions_dt[ , dt := dt_diff / tail(dt_diff, 1)]\noptions_dt[ , dsigma := sigma - ..sigma, by = index]\n\n\nattrib_dt &lt;- options_dt[ , as.list(unlist(pnl_option(type, S, K, r, q, tau, ..sigma,\n                                                     dS, dt, dsigma))), by = index]\n\n\n\n\n\n\n\n\n\n\n\n\nItoâ€™s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Itoâ€™s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Itoâ€™s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\nsim_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n    \n    result &lt;- S * exp(cumsum(sigma * sqrt(dt) * rnorm(n_sim)) +\n                      (mu - 0.5 * sigma ^ 2) * dt)\n    \n    return(result)\n    \n}\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e.Â \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\nsim_multi_gbm &lt;- function(n_sim, S, mu, sigma, dt) {\n    \n    n_cols &lt;- ncol(sigma)\n    \n    Z &lt;- matrix(rnorm(n_sim * n_cols), nrow = n_sim, ncol = n_cols)\n    X &lt;- sweep(sqrt(dt) * (Z %*% chol(sigma)), 2, (mu - 0.5 * diag(sigma)) * dt, \"+\")\n    \n    result &lt;- sweep(apply(X, 2, function(x) exp(cumsum(x))), 2, S, \"*\")\n    \n    return(result)\n    \n}\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS &lt;- rep(1, length(factors))\nsigma &lt;- cov(returns_xts, use = \"complete.obs\") * scale[[\"periods\"]]\nmu &lt;- colMeans(na.omit(returns_xts)) * scale[[\"periods\"]]\nmu &lt;- mu + diag(sigma) / 2 # drift\ndt &lt;- 1 / scale[[\"periods\"]]\n\n\nmu_ls &lt;- list()\nsigma_ls &lt;- list()\n\n\nfor (i in 1:1e4) {\n    \n    # assumes stock prices\n    levels_sim &lt;- sim_multi_gbm(width + 1, S, mu, sigma, dt)\n    returns_sim &lt;- diff(log(levels_sim))\n\n    mu_sim &lt;- colMeans(returns_sim) * scale[[\"periods\"]]\n    sigma_sim &lt;- apply(returns_sim, 2, sd) * sqrt(scale[[\"periods\"]])\n    \n    mu_ls &lt;- append(mu_ls, list(mu_sim))\n    sigma_ls &lt;- append(sigma_ls, list(sigma_sim))\n    \n}\n\n\ndata.frame(\"empirical\" = colMeans(na.omit(returns_xts)) * scale[[\"periods\"]],\n           \"theoretical\" = colMeans(do.call(rbind, mu_ls)))\n\n                 empirical   theoretical\nSP500         0.0999783055  0.0993400580\nDTWEXAFEGS    0.0147926206  0.0143617071\nDGS10        -0.0006064461 -0.0006006238\nBAMLH0A0HYM2  0.0015594327  0.0015578659\n\n\n\ndata.frame(\"empirical\" = sqrt(diag(sigma)),\n           \"theoretical\" = colMeans(do.call(rbind, sigma_ls)))\n\n               empirical theoretical\nSP500        0.179423413 0.179339270\nDTWEXAFEGS   0.062497519 0.062433448\nDGS10        0.008300136 0.008287537\nBAMLH0A0HYM2 0.016945919 0.016921373\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility",
    "href": "posts/securities-r/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewtonâ€™s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newtonâ€™s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\nimplied_vol_newton &lt;- function(params, type, S, K, r, q, tau) {\n    \n    target0 &lt;- 0\n    sigma &lt;- params[[\"sigma\"]]\n    sigma0 &lt;- sigma\n    \n    while (abs(target0 - params[[\"target\"]]) &gt; params[[\"tol\"]]) {\n        \n        d1 &lt;- bs_d1(S, K, r, q, tau, sigma0)\n        d2 &lt;- bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 &lt;- bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        vega0 &lt;- bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma &lt;- sigma0 - (target0 - params[[\"target\"]]) / vega0\n        sigma0 &lt;- sigma\n        \n    }\n    \n    return(sigma)\n    \n}\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS &lt;- coredata(na.locf(levels_xts)[nrow(levels_xts), factor])\nK &lt;- S # * (1 + 0.05)\nsigma &lt;- coredata(sd_xts[nrow(sd_xts), factor]) # overrides matrix\nstart &lt;- 0.2\n\n\nd1 &lt;- bs_d1(S, K, r, q, tau, sigma)\nd2 &lt;- bs_d2(S, K, r, q, tau, sigma)\ntarget &lt;- bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams &lt;- list(\n    \"target\" = target,\n    \"sigma\" = start,\n    \"tol\" = 1e-4 # .Machine$double.eps\n)\n\n\nimplied_vol_newton(params, type, S, K, r, q, tau)\n\n         SP500\n[1,] 0.1233483"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity",
    "href": "posts/securities-r/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity"
  },
  {
    "objectID": "posts/securities-r/index.html#implied-volatility-1",
    "href": "posts/securities-r/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\nimplied_vol_obj &lt;- function(param, type, S, K, r, q, tau, target) {\n    \n    d1 &lt;- bs_d1(S, K, r, q, tau, param)\n    d2 &lt;- bs_d2(S, K, r, q, tau, param)\n    target0 &lt;- bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result &lt;- abs(target0 - target)\n    \n    return(result)\n    \n}\n\nimplied_vol_optim &lt;- function(param, type, S, K, r, q, tau, target) {\n    \n    result &lt;- optim(param, implied_vol_obj, type = type, S = S, K = K, r = r, q = q,\n                    tau = tau, target = target, method = \"Brent\", lower = 0, upper = 1)\n    \n    return(result$par)\n    \n}\n\n\nimplied_vol_optim(start, type, S, K, r, q, tau, target)\n\n[1] 0.1233483"
  },
  {
    "objectID": "posts/securities-r/index.html#yield-to-maturity-1",
    "href": "posts/securities-r/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity"
  },
  {
    "objectID": "posts/securities-py/index.html",
    "href": "posts/securities-py/index.html",
    "title": "Securities",
    "section": "",
    "text": "factors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]"
  },
  {
    "objectID": "posts/securities-py/index.html#value",
    "href": "posts/securities-py/index.html#value",
    "title": "Securities",
    "section": "Value",
    "text": "Value\nFor a given spot price \\(S\\), strike price \\(K\\), risk-free rate \\(r\\), annual dividend yield \\(q\\), time-to-maturity \\(\\tau = T - t\\), and volatility \\(\\sigma\\):\n\\[\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    if (type == \"call\"):\n        result =  S * np.exp(-q * tau) * Phi(d1) - np.exp(-r * tau) * K * Phi(d2)\n    elif (type == \"put\"):\n        result = np.exp(-r * tau) * K * Phi(-d2) - S * np.exp(-q * tau) * Phi(-d1)\n        \n    return result\n\n\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    r_df = np.exp(-r * tau)\n    q_df = np.exp(-q * tau)\n    \n    call_value = S * q_df * Phi(d1) - r_df * K * Phi(d2)\n    put_value = r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\nwhere\n\\[\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n\\]\n\ndef bs_d1(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q + sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n\ndef bs_d2(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q - sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n    \ndef phi(x):\n    \n    result = norm.pdf(x)\n    \n    return result\n\ndef Phi(x):\n    \n    result = norm.cdf(x)\n    \n    return result\n\n\ngreeks_df[\"d1\"] = bs_d1(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"d2\"] = bs_d2(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"value\"] = bs_value(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order",
    "href": "posts/securities-py/index.html#first-order",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nDelta\n\\[\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_delta(type, S, K, r, q, tau, sigma, d1, d2):\n  \n    q_df = np.exp(-q * tau)\n    \n    call_value = q_df * Phi(d1)\n    put_value = -q_df * Phi(-d1)\n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\n\ngreeks_df[\"delta\"] = bs_delta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\nDelta-beta\nNotional market value is the market value of a leveraged position:\n\\[\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n\\]\n\nhttps://en.wikipedia.org/wiki/Notional_amount\n\n\ndef bs_delta_diff(type, S, K, r, q, tau, sigma, delta0):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    call_value = delta - delta0\n    put_value = delta0 - delta\n    \n    result = np.where(type == \"call\", call_value, put_value)\n        \n    return result\n\n\nbeta = 0.35\ntype = \"call\"\nn = 1\nmultiple = 100\ntotal = 1000000\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\nsec = {\n    \"n\": n,\n    \"multiple\": multiple,\n    \"S\": S,\n    \"delta\": bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n    \"beta\": 1\n}\n\n\nbeta_df = pd.DataFrame([(x, y) for x in types for y in shocks], \n    columns = [\"type\", \"shock\"])\nbeta_df[\"spot\"] = level_shock(beta_df[\"shock\"], S, tau, sigma)\nbeta_df[\"static\"] = beta\nbeta_df[\"diff\"] = bs_delta_diff(type, beta_df[\"spot\"], K, r, q, tau, sigma, sec[\"delta\"])\nbeta_df[\"dynamic\"] = beta + sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * sec[\"beta\"] * beta_df[\"diff\"] / total\n\n\n\n\n\n\n\n\n\n\nFor completeness, duration equivalent is defined as:\n\\[\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n\\]\n\n\nVega\n\\[\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n\\]\n\ndef bs_vega(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    q_df = np.exp(-q * tau)\n    result = S * q_df * phi(d1) * np.sqrt(tau)\n    \n    return result\n\n\ngreeks_df[\"vega\"] = bs_vega(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                            greeks_df[\"d1\"], greeks_df[\"d2\"]) \n\n\n\nTheta\n\\[\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n\\]\n\ndef bs_theta(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    r_df = np.exp(-r * tau)\n    q_df = np.exp(-q * tau)\n    \n    call_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) - \\\n        r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n        \n    put_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) + \\\n        r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n        \n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n\n\ngreeks_df[\"theta\"] = bs_theta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order",
    "href": "posts/securities-py/index.html#second-order",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nGamma\n\\[\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n\\]\n\ndef bs_gamma(type, S, K, r, q, tau, sigma, d1, d2):\n  \n    q_df = np.exp(-q * tau)\n    \n    result = q_df * phi(d1) / (S * sigma * np.sqrt(tau))\n    \n    return result\n\n\ngreeks_df[\"gamma\"] = bs_gamma(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"])"
  },
  {
    "objectID": "posts/securities-py/index.html#first-order-1",
    "href": "posts/securities-py/index.html#first-order-1",
    "title": "Securities",
    "section": "First-order",
    "text": "First-order\n\nPrice-yield formula\nFor a function of one variable, \\(f(x)\\), the Taylor series formula is:\n\\[\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n\\]\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\\[\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(P(y+\\Delta y)-P(y)\\approx -D\\Delta y +{\\frac{C\\times 100}{2!}}(\\Delta y)^{2}\\)\n\ndef pnl_bond(duration, convexity, dy):\n    \n    duration_pnl = -duration * dy\n    convexity_pnl = (convexity * 100 / 2) * dy ** 2\n    income_pnl = dy\n    \n    result = pd.DataFrame({\n        \"total\": duration_pnl + convexity_pnl + income_pnl,\n        \"duration\": duration_pnl,\n        \"convexity\": convexity_pnl,\n        \"income\": income_pnl\n    })\n    \n    return result\n\n\nhttps://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\nhttps://www.investopedia.com/terms/c/convexity-adjustment.asp\n\n\nfactor = \"DGS10\"\nduration = 6.5\nconvexity = 0.65\ny = levels_df.ffill()[factor].iloc[-width]\n\n\nbond_df = pd.DataFrame({\n    \"duration\": duration,\n    \"convexity\": convexity,\n    \"dy\": (levels_df.ffill()[factor].iloc[-width:] - y) / 100\n})\n\n\nattrib_df = pnl_bond(bond_df[\"duration\"], bond_df[\"convexity\"], bond_df[\"dy\"])\n\n\n\n\n\n\n\n\n\n\n\n\nDuration-yield formula\nThe derivative of duration with respect to interest rates gives:\n\\[\n\\begin{aligned}\n\\text{Drift}&=-\\frac{\\partial D}{\\partial y}\\\\\n&=\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}-\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=C-D^{2}\n\\end{aligned}\n\\]\nBecause of market conventions, use the following formula: \\(\\text{Drift}=\\frac{1}{100}\\left(C\\times 100-D^{2}\\right)=C-\\frac{D^{2}}{100}\\)\n\ndef yield_shock(shock, tau, sigma):\n    \n    result = shock * sigma * np.sqrt(tau)\n    \n    return result\n\n\ndef duration_drift(duration, convexity, dy):\n    \n    drift = convexity - duration ** 2 / 100\n    change = -drift * dy * 100\n    \n    result = {\n        \"drift\": drift,\n        \"change\": change\n    }\n    \n    return result\n\n\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor = \"DGS10\"\nsigma = sd_df[factor].iloc[-1]\n\n\nduration_df = pd.DataFrame(shocks).rename(columns = {0: \"shock\"})\nduration_df[\"spot\"] = yield_shock(duration_df[\"shock\"], tau, sigma)\nduration_df[\"static\"] = duration\nduration_df[\"dynamic\"] = duration + \\\n    duration_drift(duration, convexity, duration_df[\"spot\"])[\"change\"]"
  },
  {
    "objectID": "posts/securities-py/index.html#second-order-1",
    "href": "posts/securities-py/index.html#second-order-1",
    "title": "Securities",
    "section": "Second-order",
    "text": "Second-order\n\nBlackâ€™s formula\nA similar formula holds for functions of several variables \\(f(x_{1},\\ldots,x_{n})\\). This is usually written as:\n\\[\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n\\]\nUsing Blackâ€™s formula, the estimated change of an option price is:\n\\[\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n\\]\n\nhttps://quant-next.com/option-greeks-and-pl-decomposition-part-1/\n\n\ndef pnl_option(type, S, K, r, q, tau, sigma, dS, dt, dsigma):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    value = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega = bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta = bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma = bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl = delta * dS / value\n    gamma_pnl = gamma / 2 * dS ** 2 / value\n    vega_pnl = vega * dsigma / value\n    theta_pnl = theta * dt / value\n    \n    result = pd.DataFrame({\n        \"total\": delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n        \"delta\": delta_pnl,\n        \"gamma\": gamma_pnl,\n        \"vega\": vega_pnl,\n        \"theta\": theta_pnl\n    })\n    \n    return result\n\n\nfactor = \"SP500\"\ntype = \"call\"\nS = levels_df.ffill()[factor].iloc[-width]\nK = S # * (1 + 0.05)\ntau = 1 # = 252 / 252\nsigma = sd_df[factor].iloc[-width]\n\n\noptions_df = pd.DataFrame({\n    \"spot\": levels_df.ffill()[factor].iloc[-width:],\n    \"sigma\": sd_df[factor].iloc[-width:]\n})\noptions_df[\"dS\"] = options_df[\"spot\"] - S\noptions_df[\"dt_diff\"] = (options_df.index - options_df.index[0]).days\noptions_df[\"dt\"] = options_df[\"dt_diff\"] / options_df[\"dt_diff\"].iloc[-1]\noptions_df[\"dsigma\"] = options_df[\"sigma\"] - sigma\n\n\nattrib_df = pnl_option(type, S, K, r, q, tau, sigma,\n                       options_df[\"dS\"], options_df[\"dt\"], options_df[\"dsigma\"])\n\n\n\n\n\n\n\n\n\n\n\n\nItoâ€™s lemma\nFor a given diffiusion \\(X(t, w)\\) driven by:\n\\[\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen proceed with the Taylor series for a function of two variables \\(f(t,x)\\):\n\\[\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n\\]\nNote: set the \\(dt^{2}\\) and \\(dtdB_{t}\\) terms to zero and substitute \\(dt\\) for \\(dB^{2}\\).\n\n\nGeometric Brownian motion\nThe most common application of Itoâ€™s lemma in finance is to start with the percent change of an asset:\n\\[\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n\\]\nThen apply Itoâ€™s lemma with \\(f(S)=log(S)\\):\n\\[\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nIt follows that:\n\\[\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n\\]\nExponentiating gives the expression for \\(S\\):\n\\[\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n\\]\nThis provides a recursive procedure for simulating values of \\(S\\) at \\(t_{0}&lt;t_{1}&lt;\\cdots&lt;t_{n}\\):\n\\[\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(Z_{1},Z_{2},\\ldots,Z_{n}\\) are independent standard normals.\n\ndef sim_gbm(n_sim, S, mu, sigma, dt):\n    \n    result = S * np.exp(np.cumsum(sigma * np.sqrt(dt) * np.random.normal(size = n_sim)) + \\\n                        (mu - 0.5 * sigma ** 2) * dt)\n    \n    return result\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\\[\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the Cholesky factor of \\(\\Sigma\\), i.e.Â \\(A\\) is any matrix for which \\(AA^\\mathrm{T}=\\Sigma\\).\n\ndef sim_multi_gbm(n_sim, S, mu, sigma, dt):\n    \n    n_cols = sigma.shape[1]\n    \n    Z = np.random.normal(size = n_sim * n_cols).reshape((n_sim, n_cols))\n    X = np.sqrt(dt) * Z @ np.linalg.cholesky(sigma).T + (mu - 0.5 * np.diag(sigma)) * dt\n    \n    result = S * np.exp(X.cumsum(axis = 0))\n    \n    return np.asmatrix(result)\n\n\nhttps://arxiv.org/pdf/0812.4210.pdf\nhttps://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\nhttps://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\nhttps://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\nhttps://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\n\n\nS = [1] * len(factors)\nsigma = np.cov(returns_df[\"returns\"].dropna().T, ddof = 1) * scale[\"periods\"]\nmu = np.array(returns_df[\"returns\"].dropna().mean()) * scale[\"periods\"]\nmu = mu + np.diag(sigma) / 2 # drift\ndt = 1 / scale[\"periods\"]\n\n\nmu_ls = []\nsigma_ls = []\n\n\nfor i in range(10000): # \"TypeError: 'float' object cannot be interpreted as an integer\"\n  \n    # assumes underlying stock price follows geometric Brownian motion with constant volatility\n    levels_sim = pd.DataFrame(sim_multi_gbm(width + 1, S, mu, sigma, dt))\n    returns_sim = np.log(levels_sim).diff()\n\n    mu_sim = returns_sim.mean() * scale[\"periods\"]\n    sigma_sim = returns_sim.std() * np.sqrt(scale[\"periods\"])\n\n    mu_ls.append(mu_sim)\n    sigma_ls.append(sigma_sim)\n\n\nmu_df = pd.DataFrame(mu_ls)\nsigma_df = pd.DataFrame(sigma_ls)\n\n\npd.DataFrame({\n    \"empirical\": np.array(returns_df[\"returns\"].dropna().mean()) * scale[\"periods\"],\n    \"theoretical\": mu_df.mean()\n})\n\n   empirical  theoretical\n0   0.099978     0.099587\n1   0.014793     0.015007\n2  -0.000606    -0.000502\n3   0.001559     0.001614\n\n\n\npd.DataFrame({\n    \"empirical\": np.sqrt(np.diag(sigma)),\n    \"theoretical\": sigma_df.mean()\n})\n\n   empirical  theoretical\n0   0.179423     0.179104\n1   0.062498     0.062422\n2   0.008300     0.008293\n3   0.016946     0.016916\n\n\n\n\nVasicek model\n\n# assumes interest rates follow mean-reverting process with stochastic volatility"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility",
    "href": "posts/securities-py/index.html#implied-volatility",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nNewtonâ€™s method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function \\(f(x)\\):\n\\[\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n\\]\nTo solve \\(V(\\sigma_{n})-V=0\\) for \\(\\sigma_{n}\\), use Newtonâ€™s method and repeat until \\(\\left|\\sigma_{n+1}-\\sigma_{n}\\right|&lt;\\varepsilon\\):\n\\[\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n\\]\n\ndef implied_vol_newton(params, type, S, K, r, q, tau):\n    \n    target0 = 0\n    sigma = params[\"sigma\"]\n    sigma0 = sigma\n    \n    while (abs(target0 - params[\"target\"]) &gt; params[\"tol\"]):\n        \n        d1 = bs_d1(S, K, r, q, tau, sigma0)\n        d2 = bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 = bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        vega0 = bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma = sigma0 - (target0 - params[\"target\"]) / vega0\n        sigma0 = sigma\n        \n    return sigma\n\n\nhttp://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\nhttps://books.google.com/books?id=VLi61POD61IC&pg=PA104\n\n\nS = levels_df.ffill()[factor].iloc[-1]\nK = S # * (1 + 0.05)\nsigma = sd_df[factor].iloc[-1] # overrides matrix\nstart = 0.2\n\n\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\ntarget = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams = {\n    \"target\": target,\n    \"sigma\": start,\n    \"tol\": 1e-4 # np.finfo(float).eps\n}\n\n\nimplied_vol_newton(params, type, S, K, r, q, tau) \n\n0.12102159931820798"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity",
    "href": "posts/securities-py/index.html#yield-to-maturity",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity"
  },
  {
    "objectID": "posts/securities-py/index.html#implied-volatility-1",
    "href": "posts/securities-py/index.html#implied-volatility-1",
    "title": "Securities",
    "section": "Implied volatility",
    "text": "Implied volatility\nIf the derivative is unknown, try optimization:\n\ndef implied_vol_obj(param, type, S, K, r, q, tau, target):\n    \n    d1 = bs_d1(S, K, r, q, tau, param)\n    d2 = bs_d2(S, K, r, q, tau, param)\n    target0 = bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result = abs(target0 - target)\n    \n    return result\n\ndef implied_vol_optim(param, type, S, K, r, q, tau, target):\n    \n    result = minimize(implied_vol_obj, param, args = (type, S, K, r, q, tau, target))\n    \n    return result.x.item()\n\n\nimplied_vol_optim(start, type, S, K, r, q, tau, target)\n\n0.1210215937669012"
  },
  {
    "objectID": "posts/securities-py/index.html#yield-to-maturity-1",
    "href": "posts/securities-py/index.html#yield-to-maturity-1",
    "title": "Securities",
    "section": "Yield-to-maturity",
    "text": "Yield-to-maturity"
  },
  {
    "objectID": "posts/statistics/index.html",
    "href": "posts/statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Usage\n\nlibrary(roll)\nlibrary(microbenchmark)\noptions(microbenchmark.unit = \"us\")\n\n\nn_vars &lt;- 10\nn_obs &lt;- 1000\nweights &lt;- 0.9 ^ (n_obs:1)\n\nx &lt;- matrix(rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)\ny &lt;- matrix(rnorm(n_obs), nrow = n_obs, ncol = 1)\nx_lgl &lt;- x &lt; 0\n\n\n\nRolling any\n\nresult &lt;- microbenchmark(\"125\" = roll_any(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_any(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_any(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_any(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 110.7 124.85 143.675 137.20 158.85 260.7   100\n  250 109.8 123.15 143.010 139.35 153.85 257.6   100\n  500 107.4 126.25 152.564 141.50 157.45 941.5   100\n 1000  89.2 117.90 137.008 131.90 146.00 268.6   100\n\n\n\n\nRolling all\n\nresult &lt;- microbenchmark(\"125\" = roll_all(x_lgl, width = 125, min_obs = 1),\n                         \"250\" = roll_all(x_lgl, width = 250, min_obs = 1),\n                         \"500\" = roll_all(x_lgl, width = 500, min_obs = 1),\n                         \"1000\" = roll_all(x_lgl, width = 1000, min_obs = 1))\nprint(result)\n\nUnit: microseconds\n expr  min     lq    mean median     uq   max neval\n  125 94.8 119.35 140.786 133.45 148.45 281.8   100\n  250 96.4 118.65 136.826 130.45 144.90 355.6   100\n  500 90.7 120.35 139.965 132.65 151.10 235.2   100\n 1000 89.3 117.90 131.160 128.70 140.75 225.4   100\n\n\n\n\nRolling sums\n\\[\n\\begin{aligned}\n&\\text{Expanding window} \\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_sum(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sum(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sum(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sum(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr  min     lq    mean median     uq   max neval\n  125 97.8 110.95 124.658 118.75 126.95 370.9   100\n  250 94.5 112.90 127.183 120.50 136.10 215.9   100\n  500 92.4 111.25 122.203 116.05 127.80 237.6   100\n 1000 90.0 108.10 122.633 113.15 119.75 359.1   100\n\n\n\n\nRolling products\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{prod}_{w}\\leftarrow\\text{prod}_{w}\\times\\text{w}_{new}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{prod}_{x}\\leftarrow\\text{prod}_{x}\\times\\text{x}_{new}/\\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_prod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_prod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_prod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_prod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 217.1 225.85 251.520 242.10 265.35 353.1   100\n  250 216.4 225.95 264.452 244.75 293.95 565.8   100\n  500 150.1 162.05 181.242 175.05 190.15 298.2   100\n 1000 145.4 155.30 175.632 165.00 189.20 359.3   100\n\n\n\n\nRolling means\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sum}_{x}\\leftarrow\\lambda\\times\\text{sum}_{x}+\\text{w}_{new}\\times\\text{x}_{new}-\\lambda\\times\\text{w}_{old}\\times \\text{x}_{old}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_mean(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_mean(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_mean(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_mean(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 104.1 119.85 135.770  128.1 142.55 300.8   100\n  250 106.0 115.55 127.126  120.8 134.90 201.8   100\n  500 100.0 113.40 125.763  118.7 130.50 332.4   100\n 1000 102.0 112.25 121.243  116.1 127.40 204.2   100\n\n\n\n\nRolling minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_min(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_min(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_min(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_min(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 111.0 126.25 146.571  139.1 157.75 265.2   100\n  250 107.9 124.45 145.026  138.0 159.50 417.1   100\n  500 103.4 124.20 144.345  141.3 157.25 267.8   100\n 1000 112.9 130.00 149.460  144.8 160.20 230.9   100\n\n\n\n\nRolling maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_max(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_max(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_max(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_max(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 107.6 122.40 137.499 130.45 148.85 221.8   100\n  250 105.2 120.15 143.777 129.65 150.90 424.3   100\n  500 104.2 122.60 137.575 129.85 149.10 259.6   100\n 1000 108.0 126.00 150.147 136.10 165.75 242.4   100\n\n\n\n\nRolling index of minimums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmin(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmin(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmin(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmin(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125  97.3 113.20 124.298 119.00 126.25 270.0   100\n  250 101.6 116.65 128.360 120.20 126.45 352.6   100\n  500  99.4 112.75 125.474 120.75 126.25 271.1   100\n 1000 100.3 119.00 130.169 123.80 132.00 247.4   100\n\n\n\n\nRolling index of maximums\n\nresult &lt;- microbenchmark(\"125\" = roll_idxmax(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_idxmax(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_idxmax(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_idxmax(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr  min     lq    mean median     uq   max neval\n  125 90.9 114.95 129.031 122.40 130.30 244.6   100\n  250 93.6 115.20 131.451 119.95 127.05 387.1   100\n  500 93.9 114.05 126.224 122.25 134.95 222.2   100\n 1000 98.2 118.45 136.672 123.75 134.50 283.1   100\n\n\n\n\nRolling medians\n\nresult &lt;- microbenchmark(\"125\" = roll_median(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_median(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_median(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_median(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr     min       lq      mean   median      uq     max neval\n  125  4505.5  4966.95  6208.672  5133.85  5701.0 17053.8   100\n  250  8748.7  9601.00 11443.724  9717.95 10260.0 27160.0   100\n  500 16142.0 17381.50 19502.497 17575.25 17922.0 38869.2   100\n 1000 22792.7 24166.55 27190.942 24435.65 24990.4 48654.0   100\n\n\n\n\nRolling quantiles\n\nresult &lt;- microbenchmark(\"125\" = roll_quantile(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_quantile(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_quantile(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_quantile(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr     min       lq      mean   median      uq     max neval\n  125  4506.4  4999.85  7553.648  5394.50  9763.8 17091.9   100\n  250  8757.3  9606.15 12263.851  9753.20 13165.4 26560.4   100\n  500 16277.9 17400.65 20567.089 17688.95 20462.4 39632.4   100\n 1000 22711.3 24108.10 28492.525 24400.95 30554.0 48352.3   100\n\n\n\n\nRolling variances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{x}\\leftarrow\\lambda\\times\\text{sumsq}_{x}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{x}_{new}-\\text{mean}_{prev_x})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{x}_{old}-\\text{mean}_{prev_x})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_var(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_var(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_var(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_var(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 145.2 157.95 177.536 165.55 179.75 457.0   100\n  250 141.3 159.90 174.021 166.40 178.10 422.1   100\n  500 133.7 150.65 166.272 161.05 172.40 256.3   100\n 1000 120.8 144.55 158.313 150.50 165.65 233.8   100\n\n\n\n\nRolling standard deviations\n\nresult &lt;- microbenchmark(\"125\" = roll_sd(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_sd(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_sd(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_sd(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 144.9 156.15 166.379 162.05 173.65 283.9   100\n  250 139.4 152.05 169.034 161.15 171.10 471.4   100\n  500 122.0 152.25 163.314 158.10 168.00 288.5   100\n 1000 128.0 147.00 158.308 153.95 162.50 253.2   100\n\n\n\n\nRolling scaling and centering\n\nresult &lt;- microbenchmark(\"125\" = roll_scale(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_scale(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_scale(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_scale(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq    mean median     uq   max neval\n  125 155.1 162.75 188.563 179.10 194.05 473.2   100\n  250 152.3 163.65 181.431 176.10 189.85 264.9   100\n  500 146.1 161.35 179.196 171.65 185.10 273.7   100\n 1000 143.8 153.20 174.091 168.80 188.40 278.8   100\n\n\n\n\nRolling covariances\n\\[\n\\begin{aligned}\n&\\text{Expanding window}\\\\\n&\\bullet\\text{sum}_{w}\\leftarrow\\text{sum}_{w}+\\text{w}_{new}\\\\\n&\\bullet\\text{sumsq}_{w}\\leftarrow\\text{sumsq}_{w}+\\text{w}_{new}^{2}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})\\\\\n&\\text{Rolling window}\\\\\n&\\bullet\\text{sumsq}_{xy}\\leftarrow\\lambda\\times\\text{sumsq}_{xy}+\\text{w}_{new}\\times (\\text{x}_{new}-\\text{mean}_{x})(\\text{y}_{new}-\\text{mean}_{prev_y})-\\\\\n&\\lambda\\times\\text{w}_{old}\\times (\\text{x}_{old}-\\text{mean}_{x})(\\text{y}_{old}-\\text{mean}_{prev_y})\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_cov(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cov(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cov(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cov(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min      lq     mean  median      uq    max neval\n  125 987.7 1148.45 1407.242 1211.30 1292.90 6102.7   100\n  250 987.9 1105.45 1174.584 1153.65 1227.40 1668.2   100\n  500 949.4 1034.70 1160.050 1107.70 1193.55 5641.6   100\n 1000 731.8  948.50 1024.970  996.00 1067.05 1752.4   100\n\n\n\n\nRolling correlations\n\nresult &lt;- microbenchmark(\"125\" = roll_cor(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_cor(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_cor(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_cor(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq     max neval\n  125 1181.4 1364.65 1664.987 1443.25 1533.95 10174.8   100\n  250 1179.0 1314.35 1667.719 1397.25 1522.05  7523.0   100\n  500 1011.0 1273.75 1559.954 1351.65 1429.25  6186.2   100\n 1000  929.6 1150.00 1341.007 1200.40 1265.80  4747.9   100\n\n\n\n\nRolling crossproducts\n\nresult &lt;- microbenchmark(\"125\" = roll_crossprod(x, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_crossprod(x, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_crossprod(x, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_crossprod(x, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr   min     lq     mean  median      uq    max neval\n  125 815.9 980.55 1068.121 1019.85 1060.80 5642.4   100\n  250 873.1 949.55 1090.666  989.15 1050.95 5738.1   100\n  500 833.7 901.25 1005.190  945.10  998.25 5618.4   100\n 1000 772.1 817.80  918.126  842.00  883.40 5421.0   100\n\n\n\n\nRolling linear models\n\\[\n\\begin{aligned}\n&\\text{coef}=\\text{cov}_{xx}^{-1}\\times\\text{cov}_{xy}\\\\\n&\\text{intercept}=\\text{mean}_{y}-\\text{coef}\\times\\text{mean}_{x}\\\\\n&\\text{rsq}=\\frac{\\text{coef}^{T}\\times\\text{cov}_{xx}\\times\\text{coef}}{\\text{var}_{y}}\\\\\n&\\text{var}_{resid}=\\frac{(1-\\text{rsq})(\\text{var}_{y})(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})}{\\text{n}_{rows}-\\text{n}_{cols}}\\\\\n&\\text{xx}=\\text{cov}_{xx}\\times(\\text{sum}_{w}-\\text{sumsq}_{w}/\\text{sum}_{w})\\\\\n&\\text{se}_{coef}=\\sqrt{\\text{var}_{resid}\\times\\text{diag}(\\text{xx}^{-1})}\\\\\n&\\text{se}_{intercept}=\\sqrt{\\text{var}_{resid}\\left(1/\\text{sum}_{w}+\\text{mean}_{x}^{T}\\text{xx}^{-1}\\text{mean}_{x}\\right)}\n\\end{aligned}\n\\]\n\nresult &lt;- microbenchmark(\"125\" = roll_lm(x, y, width = 125, min_obs = 1, weights = weights),\n                         \"250\" = roll_lm(x, y, width = 250, min_obs = 1, weights = weights),\n                         \"500\" = roll_lm(x, y, width = 500, min_obs = 1, weights = weights),\n                         \"1000\" = roll_lm(x, y, width = 1000, min_obs = 1, weights = weights))\nprint(result)\n\nUnit: microseconds\n expr    min      lq     mean  median      uq     max neval\n  125 2457.4 3197.85 3836.952 3321.10 4063.95 11342.9   100\n  250 2883.3 3220.45 3756.470 3435.10 4058.35  6946.9   100\n  500 2482.4 3152.90 3559.144 3336.35 3787.80  7575.4   100\n 1000 2322.4 2934.10 3658.411 3079.65 3842.20 12159.1   100\n\n\n\n\nReferences\n\nhttps://stackoverflow.com/a/9933794\nhttps://stackoverflow.com/a/11316626\nhttps://stackoverflow.com/a/34363187\nhttps://stackoverflow.com/a/243342\nhttps://stats.stackexchange.com/a/64217\nhttps://stackoverflow.com/a/51992954\nhttps://stackoverflow.com/a/25921772\nhttps://stackoverflow.com/a/40416506\nhttps://stackoverflow.com/a/5970314\nhttps://gist.github.com/ashelly/5665911\nhttps://stackoverflow.com/a/51992954"
  },
  {
    "objectID": "posts/crowds-py/crowds-py.html",
    "href": "posts/crowds-py/crowds-py.html",
    "title": "Crowds",
    "section": "",
    "text": "import requests\nfrom lxml import html\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\n\n\nfactors_r = [\"SP500\"] # \"SP500\" does not contain dividends\nfactors_d = [\"SOFR\"]\nfactors = factors_r + factors_d\nwidth = 20 * 3\nscale = {\"periods\": 252, \"overlap\": 5}\n\n\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\n\n\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\n\n\nimport os\nimport cvxpy as cp\n\n\ndef get_nth(x, n, offset = 0):\n    \n    result = x[offset::n]\n    \n    return result\n\n\ndef get_text(x, n = 0):\n    \n    result_ls = []\n    \n    for i in x:\n      \n        if (len(i) == 0):\n            result_ls.append(i.text_content()) # types\n        else:\n            result_ls.append(i[n].text_content()) # names and tickers\n    \n    return result_ls\n\n\ndef get_mstar():\n    \n    i = 0\n    status = True\n    names_ls = []\n    tickers_ls = []\n    types_ls = []\n\n    while status:\n\n        i += 1\n\n        url = \"https://www.morningstar.com/asset-allocation-funds?page=\" + str(i)\n        response = requests.get(url)\n        tree = html.fromstring(response.content)\n\n        table = tree.xpath(\"//div[@class='topic__table-container']\")\n\n        if (len(table) == 0):\n            status = False\n        else:\n\n            names_tickers = tree.xpath(\"//a[@class='mdc-link mds-link mds-link--data-table mdc-link--no-visited']\")\n            types = tree.xpath(\"//span[@class='mdc-data-point mdc-data-point--string mdc-string']\")\n            \n        names_ls.extend(get_text(get_nth(names_tickers, 2)))\n        tickers_ls.extend(get_text(get_nth(names_tickers, 2, 1)))\n        types_ls.extend(get_text(get_nth(types, 5, 2)))\n\n    result = pd.DataFrame({\n      \"name\": names_ls,\n      \"ticker\": tickers_ls,\n      \"type\": types_ls\n    })\n    \n    return result\n\n\nmstar_df = get_mstar()\n\n\ntickers = mstar_df.loc[mstar_df[\"type\"] == \"Tactical Allocation\", \"ticker\"].tolist()\nprices_df = pdr.get_data_tiingo(tickers, start = \"1900-01-01\", api_key = os.getenv(\"TIINGO_API_KEY\"))\nprices_df = prices_df.pivot_table(index = \"date\", columns = \"symbol\", values = \"adjClose\") \\\n    .tz_localize(None)\nprices_df.sort_index(axis = 0, inplace = True)\ntickers = prices_df.columns\n\n\nreturns_cols = [(\"returns\", i) for i in tickers]\noverlap_cols = [(\"overlap\", i) for i in tickers]\nreturns_df[returns_cols] = np.log(prices_df).diff()\nreturns_df[overlap_cols] = returns_df[returns_cols].rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df.sort_index(axis = 1, inplace = True)\n\n\noverlap_df = returns_df.dropna()[\"overlap\"]\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors]\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers]\n\n\ndef pnl(x):\n    return np.nanprod(1 + x) - 1\n\n\nOptimization\n\ndef min_rss_optim(x, y):\n    \n    w = cp.Variable(x.shape[1])\n    \n    objective = cp.Minimize(cp.sum_squares(y - x @ w))\n    \n    constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= 1]\n    \n    problem = cp.Problem(objective, constraints)\n    problem.solve()\n    \n    return w.value\n\n\nn_rows = overlap_df.shape[0]\nresult_ls = []\n\nfor i in range(width - 1, n_rows):\n  \n  idx = range(max(i - width + 1, 0), i + 1)\n  x_subset = overlap_x_df.iloc[idx]\n  y_subset = overlap_y_df.iloc[idx]\n  params_ls = []\n  \n  for j in tickers:\n  \n    params = min_rss_optim(x_subset.values, y_subset.loc[:, j].values)\n    params_ls.append(params)\n  \n  result_ls.append(np.mean(params_ls, axis = 0))\n\n\nposition_df = pd.DataFrame(result_ls, index = overlap_df.index[(width - 1):],\n                           columns = factors)\nposition_df.tail()\n\n\n\n\n\n\n\n\nSP500\nSOFR\n\n\nDATE\n\n\n\n\n\n\n2024-02-06\n0.648071\n0.351929\n\n\n2024-02-07\n0.650647\n0.349353\n\n\n2024-02-08\n0.653656\n0.346344\n\n\n2024-02-09\n0.659915\n0.340085\n\n\n2024-02-12\n0.671116\n0.328884"
  },
  {
    "objectID": "posts/risk-r/index.html#partial-least-squares",
    "href": "posts/risk-r/index.html#partial-least-squares",
    "title": "Risk",
    "section": "Partial least squares",
    "text": "Partial least squares"
  }
]