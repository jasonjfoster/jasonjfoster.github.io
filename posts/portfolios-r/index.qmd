---
title: "Portfolios"
author: "[Jason Foster](mailto:jason.j.foster@gmail.com)"
date: last-modified
categories:
  - analysis
  - finance
  - r
---

```{r, echo = FALSE, message = FALSE}
source("../plot/theme_jjf.R")
```

```{r, message = FALSE}
library(quantmod)
library(roll)
library(data.table)
```

```{r, echo = FALSE}
options("getSymbols.warning4.0" = FALSE)
```

```{r}
factors_r <- c("SP500", "DTWEXAFEGS") # "SP500" does not contain dividends; note: "DTWEXM" discontinued as of Jan 2020
factors_d <- c("DGS10", "BAMLH0A0HYM2")
factors <- c(factors_r, factors_d)
width <- 252
scale <- list("periods" = 252, "overlap" = 5)
```

```{r, echo = FALSE}
palette <- c("black", palette_jjf(length(factors)))
names(palette) <- c("Overall", factors)
```

```{r, results = "hide"}
getSymbols(factors, src = "FRED")
levels_xts <- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))
```

```{r}
returns_xts <- do.call(merge, lapply(factors, function(i) {
    if (i %in% factors_r) {
        diff(log((levels_xts[ , i])))
    } else if (i %in% factors_d) {
        -diff(levels_xts[ , i]) / 100
    }    
}))
overlap_xts <- roll_mean(returns_xts, scale[["overlap"]], min_obs = 1, na_restore = TRUE)
```

```{r, message = FALSE}
library(pls)
library(CVXR)
```

```{r}
tickers <- "BAICX" # fund inception date is "2011-11-28" 
invisible(getSymbols(tickers, src = "tiingo", api.key = Sys.getenv("TIINGO_API_KEY"), adjust = TRUE))
prices_xts <- do.call(merge, c(lapply(tickers, function(i) Cl(get(i))), all = TRUE))
colnames(prices_xts) <- tickers
index(prices_xts) <- as.Date(index(prices_xts))
```

```{r}
returns_xts <- merge(returns_xts, diff(log(prices_xts)))
overlap_xts <- merge(overlap_xts, roll_mean(returns_xts[ , tickers], scale[["overlap"]], min_obs = 1))
```

```{r}
# weights <- 0.9 ^ ((width - 1):0)
weights <- rep(1, width)
```

```{r}
# overlap_df <- na.omit(overlap_xts)
overlap_x_df <- na.omit(overlap_xts)[ , factors]
overlap_y_df <- na.omit(overlap_xts)[ , tickers]
overlap_x_xts <- tail(overlap_x_df, width)
overlap_y_xts <- tail(overlap_y_df, width)
```

<!-- # Factor models -->

<!-- ## Ordinary least squares -->

<!-- ### Coefficients -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \hat{\beta}=(X^\mathrm{T}WX)^{-1}X^\mathrm{T}Wy -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- -   <https://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf> -->

<!-- ```{r} -->
<!-- lm_coef <- function(x, y, weights, intercept) { -->

<!--     if (intercept) x <- model.matrix(~ x) -->

<!--     # result <- solve(t(x) %*% sweep(x, 1, weights, "*")) %*% t(x) %*% sweep(y, 1, weights, "*") -->
<!--     result <- solve(crossprod(x, diag(weights)) %*% x) %*% crossprod(x, diag(weights) %*% y) -->

<!--     return(result) -->

<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- intercept <- TRUE -->
<!-- ``` -->

<!-- ```{r} -->
<!-- lm_coef(overlap_x_xts, overlap_y_xts, weights, intercept) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # START HERE -->
<!-- if (intercept) { -->
<!--     fit <- lm(overlap_y_xts ~ overlap_x_xts, weights = weights) -->
<!-- } else { -->
<!--     fit <- lm(overlap_y_xts ~ overlap_x_xts - 1, weights = weights) -->
<!-- } -->

<!-- coef(fit) -->
<!-- ``` -->

<!-- ### R-squared -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- R^{2}=\frac{\hat{\beta}^\mathrm{T}(X^\mathrm{T}WX)\hat{\beta}}{y^\mathrm{T}Wy} -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- ```{r} -->
<!-- lm_rsq <- function(x, y, weights, intercept) { -->

<!--     coef <- lm_coef(x, y, weights, intercept) -->

<!--     if (intercept) { -->

<!--         x <- model.matrix(~ x) -->
<!--         x <- sweep(x, 2, apply(x, 2, weighted.mean, w = weights), "-") -->
<!--         y <- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), "-") -->

<!--     } -->

<!--     result <- (t(coef) %*% (t(x) %*% sweep(x, 1, weights, "*")) %*% coef) / (t(y) %*% sweep(y, 1, weights, "*")) -->

<!--     return(result) -->

<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- lm_rsq(overlap_x_xts, overlap_y_xts, weights, intercept) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- summary(fit)$r.squared -->
<!-- ``` -->

<!-- ### Standard errors -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \sigma_{\hat{\beta}}^{2}&=\sigma_{\varepsilon}^{2}(X^\mathrm{T}WX)^{-1}\\ -->
<!-- &=\frac{(1-R^{2})}{n-p}(X^\mathrm{T}WX)^{-1}\\ -->
<!-- &=\frac{SSE}{df_{E}}(X^\mathrm{T}WX)^{-1}\\ -->
<!-- \sigma_{\hat{\alpha}}^{2}&=\sigma_{\varepsilon}^{2}\left(\frac{1}{n}+\mu^\mathrm{T}(X^\mathrm{T}WX)^{-1}\mu\right) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- ```{r} -->
<!-- # http://people.duke.edu/~rnau/mathreg.htm -->
<!-- lm_se <- function(x, y, weights, intercept) { -->

<!--     n_rows <- nrow(x) -->
<!--     n_cols <- ncol(x) -->

<!--     rsq <- lm_rsq(x, y, weights, intercept) -->

<!--     if (intercept) { -->

<!--         x <- model.matrix(~ x) -->
<!--         y <- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), "-") -->

<!--         df_resid <- n_rows - n_cols - 1 -->

<!--     } else { -->
<!--         df_resid <- n_rows - n_cols -->
<!--     } -->

<!--     var_y <- t(y) %*% sweep(y, 1, weights, "*") -->
<!--     var_resid <- as.vector((1 - rsq) * var_y / df_resid) -->

<!--     result <- sqrt(var_resid * diag(solve(t(x) %*% sweep(x, 1, weights, "*")))) -->

<!--     return(result) -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- lm_se(overlap_x_xts, overlap_y_xts, weights, intercept) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- coef(summary(fit))[ , "Std. Error"] -->
<!-- ``` -->

<!-- ## Standalone risk -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \text{SAR}_{k}&=\sqrt{w_{k}^{2}\sigma_{k}^{2}}\\ -->
<!-- \text{SAR}_{\varepsilon}&=\sqrt{(1-R^{2})\sigma_{y}^{2}} -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- ```{r} -->
<!-- lm_sar <- function(x, y, weights, intercept) { -->

<!--     coef <- lm_coef(x, y, weights, intercept) -->
<!--     rsq <- lm_rsq(x, y, weights, intercept) -->

<!--     if (intercept) x <- model.matrix(~ x) -->

<!--     sigma <- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov -->
<!--     sar <- coef ^ 2 * diag(sigma[-ncol(sigma), -ncol(sigma)]) -->
<!--     sar_eps <- (1 - rsq) * sigma[ncol(sigma), ncol(sigma)] -->

<!--     result <- sqrt(c(sigma[ncol(sigma), ncol(sigma)], -->
<!--                      sar, -->
<!--                      sar_eps)) -->

<!--     return(result) -->

<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- lm_sar(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[["periods"]]) * sqrt(scale[["overlap"]]) -->
<!-- ``` -->

<!-- ## Risk contribution -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \text{MCR}&=w^\mathrm{T}\frac{\partial\sigma_{y}}{\partial w}\\ -->
<!-- &=w^\mathrm{T}\frac{\Sigma w}{\sigma_{y}}\\ -->
<!-- \text{MCR}_{\varepsilon}&=\sigma_{y}-\sum_{k=1}^{n}\text{MCR}_{k} -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- ```{r} -->
<!-- # http://faculty.washington.edu/ezivot/research/factormodelrisklecture_handout.pdf -->
<!-- lm_mcr <- function(x, y, weights, intercept) { -->

<!--     coef <- lm_coef(x, y, weights, intercept) -->
<!--     rsq <- lm_rsq(x, y, weights, intercept) -->

<!--     if (intercept) x <- model.matrix(~ x) -->

<!--     sigma <- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov -->
<!--     mcr <- coef * sigma[-ncol(sigma), -ncol(sigma)] %*% coef / sqrt(sigma[ncol(sigma), ncol(sigma)]) -->
<!--     mcr_eps <- sqrt(sigma[ncol(sigma), ncol(sigma)]) - sum(mcr) -->

<!--     result <- c(sqrt(sigma[ncol(sigma), ncol(sigma)]), -->
<!--                 mcr, -->
<!--                 mcr_eps) -->

<!--     return(result) -->

<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- lm_mcr(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[["periods"]]) * sqrt(scale[["overlap"]]) -->
<!-- ``` -->

<!-- ## Implied shocks -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \hat{\beta}&=(Z^\mathrm{T}WZ)^{-1}Z^\mathrm{T}WX -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- ```{r} -->
<!-- implied_shocks <- function(shocks, x, z, weights) { -->

<!--     beta <- solve(t(z) %*% sweep(z, 1, weights, "*")) %*% t(z) %*% sweep(x, 1, weights, "*") -->

<!--     result <- shocks %*% beta -->

<!--     return(result) -->

<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- shocks <- c(-0.1, 0.1) -->
<!-- overlap_z_xts <- overlap_x_xts[ , 1:2] -->
<!-- ``` -->

<!-- ```{r} -->
<!-- implied_shocks(shocks, overlap_x_xts, overlap_z_xts, weights) -->
<!-- ``` -->

<!-- ## Stress P&L -->

<!-- ```{r} -->
<!-- pnl_stress <- function(shocks, x, y, z, weights, intercept) { -->

<!--     coef <- lm_coef(x, y, weights, intercept) -->

<!--     if (intercept) x <- model.matrix(~ x) -->

<!--     result <- t(coef) * implied_shocks(shocks, x, z, weights) -->

<!--     return(result)     -->

<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- pnl_stress(shocks, overlap_x_xts, overlap_y_xts, overlap_z_xts, weights, intercept) -->
<!-- ``` -->

# Principal component analysis

Underlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (*note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales*) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.

## Eigendecomposition

$$
\begin{aligned}
\boldsymbol{\Sigma}&=\lambda_{1}\mathbf{v}_{1}\mathbf{v}_{1}^\mathrm{T}+\lambda_{2}\mathbf{v}_{2}\mathbf{v}_{2}^\mathrm{T}+\cdots+\lambda_{k}\mathbf{v}_{k}\mathbf{v}_{k}^\mathrm{T}\\
&=V\Lambda V^{\mathrm{T}}
\end{aligned}
$$

```{r}
# https://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/
eigen_decomp <- function(x, comps) {
    
    LV <- eigen(cov(x))
    L <- LV$values[1:comps]
    V <- LV$vectors[ , 1:comps]
    
    result <- V %*% sweep(t(V), 1, L, "*")
    
    return(result)
    
}
```

```{r}
comps <- 1
```

```{r}
eigen_decomp(overlap_x_xts, comps) * scale[["periods"]] * scale[["overlap"]]
```

```{r}
# cov(overlap_x_xts) * scale[["periods"]] * scale[["overlap"]]
```

## Variance explained

We often look at the proportion of variance explained by the first $i$ principal components as an indication of how many components are needed.

$$
\begin{aligned}
\frac{\sum_{j=1}^{i}{\lambda_{j}}}{\sum_{j=1}^{k}{\lambda_{j}}}
\end{aligned}
$$

```{r}
variance_explained <- function(x) {
    
    LV <- eigen(cov(x))
    L <- LV$values
    
    result <- cumsum(L) / sum(L)
    
    return(result)
    
}
```

```{r}
variance_explained(overlap_x_xts)
```

## Cosine similarity

Also, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.

$$
\begin{aligned}
\text{similarity}=\frac{\mathbf{A}\cdot\mathbf{B}}{\|\mathbf{A}\|\|\mathbf{B}\|}
\end{aligned}
$$

```{r}
eigen_vals <- function(x) {
    
    LV <- eigen(cov(x))
    L <- LV$values
    
    return(L)    
}

eigen_vecs <- function(x) {
    
    LV <- eigen(cov(x))
    V <- LV$vectors
    
    return(V)    
}
```

```{r}
roll_eigen1 <- function(x, width, comp) {
    
    n_rows <- nrow(x)
    result_ls <- list()
    
    for (i in width:n_rows) {
        
        idx <- max(i - width + 1, 1):i
        evec <- eigen_vecs(x[idx, ])[ , comp]
        result_ls <- append(result_ls, list(evec))
                
    }
    
    result <- do.call(rbind, result_ls)
    result <- xts(result, index(x)[width:n_rows])
    colnames(result) <- colnames(x)
    
    return(result)
    
}
```

```{r}
comp <- 1
```

```{r}
raw_df <- roll_eigen1(overlap_x_df, width, comp)
```

```{r, fig.width = 6, fig.height = 3}
raw_mlt <- melt(as.data.table(raw_df), id.vars = "index")
raw_plt <- plot_ts(raw_mlt, title = "Eigenvector 1Y")
print(raw_plt)
```

```{r}
# https://quant.stackexchange.com/a/3095
roll_eigen2 <- function(x, width, comp) {
    
    n_rows <- nrow(x)
    result_ls <- list()
    
    for (i in width:n_rows) {
        
        idx <- max(i - width + 1, 1):i
        evec <- eigen_vecs(x[idx, ])[ , comp]
                
        if (i > width) {
            
            similarity <- evec %*% result_ls[[length(result_ls)]]
            evec <- as.vector(sign(similarity)) * evec
            result_ls <- append(result_ls, list(evec))
            
        } else {
            result_ls <- append(result_ls, list(evec))
        }
                
    }
    
    result <- do.call(rbind, result_ls)
    result <- xts(result, index(x)[width:n_rows])
    colnames(result) <- colnames(x)
    
    return(result)
    
}
```

```{r}
clean_df <- roll_eigen2(overlap_x_df, width, comp)
```

```{r, fig.width = 6, fig.height = 3}
clean_mlt <- melt(as.data.table(clean_df), id.vars = "index")
clean_plt <- plot_ts(clean_mlt, title = "Eigenvector 1Y")
print(clean_plt)
```

## Contour ellipsoid

The contours of a multivariate normal (MVN) distribution are ellipsoids centered at the mean. The directions of the axes are given by the eigenvectors of the covariance matrix and squared lengths are given by the eigenvalues:

$$
\begin{aligned}
({\mathbf{x}}-{\boldsymbol{\mu}})^{\mathrm{T}}{\boldsymbol{\Sigma}}^{-1}({\mathbf{x}}-{\boldsymbol{\mu}})=c^{2}
\end{aligned}
$$

Or, in general parametric form:

$$
\begin{aligned}
X(t)&=X_{c}+a\,\cos t\,\cos \varphi -b\,\sin t\,\sin \varphi\\
Y(t)&=Y_{c}+a\,\cos t\,\sin \varphi +b\,\sin t\,\cos \varphi
\end{aligned}
$$ where $t$ varies from $0,\ldots,2\pi$. Here $(X_{c},Y_{c})$ is the center of the ellipse and $\varphi$ is the angle between the x-axis and the major axis of the ellipse.

Specifically:

$$
\begin{aligned}
&\text{Center: }\boldsymbol{\mu}=(X_{c},Y_{c})\\
&\text{Radius: }c^{2}= \chi_{\alpha}^{2}(df)\\
&\text{Length: }a=c\sqrt{\lambda_{k}}\\
&\text{Angle of rotation: }\varphi=\text{atan2}\left(\frac{V_{k}(2)}{V_{k}(1)}\right)
\end{aligned}
$$

```{r}
# https://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/
# https://maitra.public.iastate.edu/stat501/lectures/MultivariateNormalDistribution-I.pdf
# https://en.wikipedia.org/wiki/Multivariate_normal_distribution
# https://en.wikipedia.org/wiki/Ellipse#General_parametric_form
ellipse <- function(n_sim, x, y, sigma) {
    
    data <- cbind(x, y)
    LV <- eigen(cov(data))
    L <- LV$values
    V <- LV$vectors
    
    c <- sqrt(qchisq(pnorm(sigma), 2))
    t <- seq(0, 2 * pi, len = n_sim)
    phi <- atan2(V[2, 1], V[1, 1])
    a <- c * sqrt(L[1]) * cos(t)
    b <- c * sqrt(L[2]) * sin(t)
    R <- matrix(c(cos(phi), -sin(phi), sin(phi), cos(phi)), nrow = 2, ncol = 2)
    r <- t(rbind(a, b)) %*% R
    
    result <- sweep(r, 2, colMeans(data), "+") # 2D only
    
    return(result)
    
}
```

```{r}
returns_x_xts <- na.omit(returns_xts)[ , factors] # extended history
ellipse_x_xts <- ellipse(1000, returns_x_xts[ , 1], returns_x_xts[ , 3], 1)
```

```{r, fig.width = 3, fig.height = 3}
ellipse_plt <- plot_scatter(data.table(returns_x_xts[ , c(1, 3)]), x = "SP500", y = "DGS10",
                            title = "Return 1D (%)") +
  geom_point(data = data.table(ellipse_x_xts), aes(x = V1 * 100, y = V2 * 100))
print(ellipse_plt)
```

## Principal component regression

### Coefficients

$$
\begin{aligned}
W_{k}&=\mathbf{X}V_{k}=[\mathbf{X}\mathbf{v}_{1},\ldots,\mathbf{X}\mathbf{v}_{k}]\\
{\widehat{\gamma}}_{k}&=\left(W_{k}^\mathrm{T}W_{k}\right)^{-1}W_{k}^\mathrm{T}\mathbf{Y}\\
{\widehat{\boldsymbol{\beta}}}_{k}&=V_{k}{\widehat{\gamma}}_{k}
\end{aligned}
$$

```{r}
# https://en.wikipedia.org/wiki/Principal_component_regression
pcr_coef <- function(x, y, comps) {
    
    x <- sweep(x, 2, colMeans(x), "-")
    LV <- eigen(cov(x))
    V <- LV$vectors
    
    W <- x %*% V
    gamma <- solve(t(W) %*% W) %*% (t(W) %*% y)
    
    result <- V[ , 1:comps] %*% as.matrix(gamma[1:comps])
    
    return(result)
    
}
```

```{r}
scale_x_xts <- scale(overlap_x_xts)
```

```{r}
pcr_coef(scale_x_xts, overlap_y_xts, comps)
```

```{r}
pcr_coef(overlap_x_xts, overlap_y_xts, comps)
```

```{r}
fit <- pcr(reformulate(termlabels = ".", response = tickers), 
           data = merge(scale_x_xts, overlap_y_xts), ncomp = comps)
coef(fit)
```

### R-squared

```{r}
pcr_rsq <- function(x, y, comps) {
    
    coef <- pcr_coef(x, y, comps)
    
    x <- sweep(x, 2, colMeans(x), "-")
    y <- sweep(y, 2, colMeans(y), "-")
    
    result <- (t(coef) %*% (t(x) %*% x) %*% coef) / (t(y) %*% y)
    
    return(result)
    
}
```

```{r}
pcr_rsq(scale_x_xts, overlap_y_xts, comps)
```

```{r}
pcr_rsq(overlap_x_xts, overlap_y_xts, comps)
```

```{r}
R2(fit)$val[comps + 1]
```

### Standard errors

$$
\begin{aligned}
\text{Var}({\widehat{\boldsymbol{\beta}}}_{k})&=\sigma^{2}V_{k}(W_{k}^\mathrm{T}W_{k})^{-1}V_{k}^\mathrm{T}\\
&=\sigma^{2}V_{k}\text{diag}\left(\lambda_{1}^{-1},\ldots,\lambda_{k}^{-1}\right)V_{k}^\mathrm{T}\\
&=\sigma^{2}\sum_{j=1}^{k}{\frac{\mathbf{v}_{j}\mathbf{v}_{j}^\mathrm{T}}{\lambda_{j}}}
\end{aligned}
$$

```{r}
# unable to verify the result
pcr_se <- function(x, y, comps) {
    
    n_rows <- nrow(x)
    n_cols <- ncol(x)
    
    rsq <- pcr_rsq(x, y, comps)
    
    y <- sweep(y, 2, colMeans(y), "-")
    
    df_resid <- n_rows - n_cols - 1
    
    var_y <- t(y) %*% y
    var_resid <- as.vector((1 - rsq) * var_y / df_resid)
    
    LV <- eigen(cov(x))
    L <- LV$values[1:comps] * (n_rows - 1)
    V <- LV$vectors[ , 1:comps]
    
    result <- sqrt(var_resid * diag(V %*% sweep(t(V), 1, 1 / L, "*")))
    
    return(result)
    
}
```

```{r}
pcr_se(scale_x_xts, overlap_y_xts, comps)
```

```{r}
pcr_se(overlap_x_xts, overlap_y_xts, comps)
```

## Marchenko--Pastur distribution

Marchenko--Pastur distribution is the limiting distribution of eigenvalues of Wishart matrices as the matrix dimension $m$ and degrees of freedom $n$ both tend to infinity with ratio $m/n\,\to \,\lambda\in(0,+\infty)$:

$$
\begin{aligned}
d\nu(x)&={\frac {1}{2\pi\sigma ^{2}}}{\frac{\sqrt{(\lambda_{+}-x)(x-\lambda_{-})}}{\lambda x}}\,\mathbf{1}_{x\in[\lambda_{-},\lambda _{+}]}\,dx
\end{aligned}
$$

with

$$
\begin{aligned}
\lambda_{\pm}&=\sigma^{2}(1\pm{\sqrt{\lambda }})^{2}
\end{aligned}
$$

```{r}
# https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution
# https://faculty.baruch.cuny.edu/jgatheral/RandomMatrixCovariance2008.pdf
dmp <- function(x, sigma = 1) {
  
  LV <- eigen(cov(x))
  L <- LV$values
  
  lmbda <- ncol(x) / nrow(x)
  lower <- sigma * (1 - sqrt(lmbda)) ^ 2
  upper <- sigma * (1 + sqrt(lmbda)) ^ 2
  
  d <- ifelse((L <= lower) | (L >= upper), 0,
              1 / (2 * pi * sigma * lmbda * L) * sqrt((upper - L) * (L - lower)))
  
  return(d)
  
}
```

```{r}
n_sim <- 5000
n_cols <- 1000
```

```{r}
data_sim <- matrix(rnorm(n_sim * n_cols), nrow = n_sim, ncol = n_cols)
```

```{r, warning = FALSE}
dmp_dt <- data.table(evals = eigen(cov(data_sim))$values,
                     dmp = dmp(data_sim))
```

```{r, fig.width = 4, fig.height = 3}
dmp_plt <- plot_density(dmp_dt, x = "evals", y = "dmp",
                        title = "Marchenko-Pastur distribution", xlab = "Eigenvalues", ylab = "Density")
print(dmp_plt)
```

# Random portfolios

Need to generate uniformly distributed weights $\mathbf{w}=(w_{1},w_{2},\ldots,w_{N})$ such that $\sum_{j=1}^{N}w_{i}=1$ and $w_{i}\geq0$:

-   **Approach 1**: tempting to use $w_{i}=\frac{u_{i}}{\sum_{j=1}^{N}u_{i}}$ where $u_{i}\sim U(0,1)$ but the distribution of $\mathbf{w}$ is not uniform

-   **Approach 2**: instead, generate $\text{Exp}(1)$ and then normalize

Can also scale random weights by $M$, e.g. if sum of weights must be 10% then multiply weights by 10%.

```{r}
rand_weights1 <- function(n_sim, n_assets, lmbda) {
    
    rand_exp <- matrix(runif(n_sim * n_assets), nrow = n_sim, ncol = n_assets)
    result <- sweep(rand_exp, 1, rowSums(rand_exp), "/")
    
    return(result)
    
}
```

```{r}
# Methodology: uniform sampling from the simplex (http://mathoverflow.net/a/76258)
# z ~ U(0, 1) then -ln(z) is an exponential(1) distribution
# This is also known as generating a random vector from the symmetric Dirichlet distribution
rand_weights2 <- function(n_sim, n_assets, lmbda) {
    
    rand_exp <- matrix(-log(1 - runif(n_sim * n_assets)) / lmbda, nrow = n_sim, ncol = n_assets)
    result <- sweep(rand_exp, 1, rowSums(rand_exp), "/")
    
    return(result)
    
}
```

```{r}
# 1. Generate n exponential(1) random variables x_1, x_2, ..., x_n
# 2. Let y_i = x_i / (sum_{i = 1}^{n} x_i)
rand_weights3 <- function(n_sim, n_assets, lmbda) {
    
    rand_exp <- matrix(rexp(n_sim * n_assets), nrow = n_sim, ncol = n_assets)
    result <- sweep(rand_exp, 1, rowSums(rand_exp), "/")
    
    return(result)
    
}
```

```{r}
lmbda <- 1
n_assets <- 3
n_sim <- 10000
```

```{r}
approach1 <- rand_weights1(n_sim, n_assets, lmbda)
approach2 <- rand_weights2(n_sim, n_assets, lmbda)
approach3 <- rand_weights2(n_sim, n_assets, lmbda)
```

```{r, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(approach1), title = "Weight (%)")
```

```{r, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(approach2), title = "Weight (%)")
```

```{r, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(approach3), title = "Weight (%)")
```

## Random turnover

How to generate random weights between lower bound $a$ and upper bound $b$ that sum to zero?

-   **Approach 1**: tempting to multiply random weights by $M$ and then subtract by $\frac{M}{N}$ but the distribution is not between $a$ and $b$

-   **Approach 2**: instead, use an iterative approach for random turnover:

    1.  Generate $N-1$ uniformly distributed weights between $a$ and $b$
    2.  For $u_{N}$ compute sum of values and subtract from $M$
    3.  If $u_{N}$ is between $a$ and $b$, then keep; otherwise, discard

Then add random turnover to previous period's random weights.

```{r}
rand_iterative <- function(n_assets, lower, upper, target) {
    
    plug <- FALSE
    
    while (!plug) {
        
        result <- as.matrix(runif(n_assets - 1, min = lower, max = upper))
        temp <- target - sum(result)
        
        if ((temp <= upper) && (temp >= lower)) {
            plug <- TRUE            
        }
        
    }
    
    result <- append(result, temp)
    
    return(result)
    
}
```

```{r}
rand_turnover1 <- function(n_sim, n_assets, lower, upper, target) {
    
    rng <- upper - lower
    
    result <- rand_weights3(n_sim, n_assets, lmbda) * rng
    result <- result - rng / n_assets
    
    return(result)
    
}
```

```{r}
rand_turnover2 <- function(n_sim, n_assets, lower, upper, target) {
    
    result <- matrix(rand_iterative(n_assets, lower, upper, target), nrow = 1, ncol = n_assets)
    
    while (nrow(result) < n_sim) {
        
        temp <- matrix(rand_iterative(n_assets, lower, upper, target), nrow = 1, ncol = n_assets)
        result <- rbind(result, temp)
        
    }
    
    return(result)
    
}
```

```{r}
lower <- -0.05
upper <- 0.05
target <- 0
```

```{r}
approach1 <- rand_turnover1(n_sim, n_assets, lower, upper, target)
approach2 <- rand_turnover2(n_sim, n_assets, lower, upper, target)
```

```{r, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(approach1), title = "Weight (%)")
```

```{r, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(approach2), title = "Weight (%)")
```

# Mean-variance

```{r}
geometric_mean <- function(x, scale) {
    
    result <- prod(1 + x) ^ (scale / length(x)) - 1
    
    return(result)
    
}
```

```{r}
mu <- apply(returns_x_xts, 2, geometric_mean, scale = scale[["periods"]])
sigma <- cov(overlap_x_xts) * scale[["periods"]] * scale[["overlap"]]
```

## Maximum return

$$
\begin{aligned}
\begin{array}{rrcl}
\displaystyle\max_{x}&\mu^{T}\mathbf{w}\\
\textrm{s.t.}&\mathbf{w}^T\Sigma\mathbf{w}&\leq&\sigma^{2}\\
&e^T\mathbf{w}&=&1
\end{array}
\end{aligned}
$$

```{r}
target <- 0.06
```

```{r}
# https://palomar.home.ece.ust.hk/MAFS6010R_lectures/slides_robust_portfolio.html
max_pnl_optim <- function(mu, sigma, target) {
    
    params <- Variable(length(mu))
    
    cons <- list(params >= 0, sum(params) == 1,
                 quad_form(params, sigma) <= target ^ 2)
    
    obj <- Maximize(t(params) %*% mu)
        
    result <- solve(Problem(obj, cons))$getValue(params)
    
    return(result)

}
```

```{r}
params1 <- max_pnl_optim(mu, sigma, target)
params1
```

```{r}
mu %*% params1
```

```{r}
sqrt(t(params1) %*% sigma %*% params1)
```

## Minimum variance

$$
\begin{aligned}
\begin{array}{rrcl}
\displaystyle\min_{x}&\mathbf{w}^T\Sigma\mathbf{w}\\
\textrm{s.t.}&\mu^{T}\mathbf{w}&\geq&M\\
&e^T\mathbf{w}&=&1
\end{array}
\end{aligned}
$$

```{r}
target <- 0.03
```

```{r}
min_risk_optim <- function(mu, sigma, target) {
    
    params <- Variable(length(mu))
    
    cons <- list(params >= 0, sum(params) == 1,
                 sum(mu * params) >= target)
    
    obj <- Minimize(quad_form(params, sigma))
        
    result <- solve(Problem(obj, cons))$getValue(params)
    
    return(result)

}
```

```{r}
params2 <- min_risk_optim(mu, sigma, target)
params2
```

```{r}
mu %*% params2
```

```{r}
sqrt(t(params2) %*% sigma %*% params2)
```

## Maximum ratio

$$
\begin{aligned}
\begin{array}{rrcl}
\displaystyle\max_{x}&\mu^{T}\mathbf{w}-\frac{1}{2}\delta(\mathbf{w}^T\Sigma\mathbf{w})\\
\textrm{s.t.}&e^T\mathbf{w}&=&1
\end{array}
\end{aligned}
$$

```{r}
ir <- 0.5
target <- ir / 0.06 # ir / std (see Black-Litterman)
```

```{r}
max_ratio_optim <- function(mu, sigma, target) {
    
    params <- Variable(length(mu))
    
    cons <- list(params >= 0, sum(params) == 1)
    
    obj <- Maximize(t(mu) %*% params - 0.5 * target * quad_form(params, sigma))
        
    result <- solve(Problem(obj, cons))$getValue(params)
    
    return(result)

}
```

```{r}
params3 <- max_ratio_optim(mu, sigma, target)
params3
```

```{r}
mu %*% params3
```

```{r}
sqrt(t(params3) %*% sigma %*% params3)
```

<!-- # Black-Litterman -->

<!-- ## Prior distribution -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \text{Risk aversion: } &\lambda=\frac{E(r)-r_{f}}{\sigma^{2}}=\frac{IR}{\sigma}\\ -->
<!-- \text{Implied returns: } &\Pi=\lambda\Sigma w\\ -->
<!-- \text{Distribution: } &N\sim(\Pi,\tau\Sigma) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- ```{r} -->
<!-- implied_pnl <- function(params, ir, sigma) { -->

<!--     lmbda <- as.numeric(ir / sqrt(t(params) %*% sigma %*% params)) -->

<!--     result <- lmbda * sigma %*% params -->

<!--     return(result)     -->

<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- implied_pnl(params3, ir, sigma) -->
<!-- ``` -->

<!-- ## Conditional distribution -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \text{Prior mean variance: } &\tau\in(0.01, 0.05)\approx(0.025)\\ -->
<!-- \text{Asset views: } &\mathbf{P}={\begin{bmatrix} -->
<!-- p_{11}&\cdots&p_{1n}\\ -->
<!-- \vdots&\ddots&\vdots\\ -->
<!-- p_{k1}&\cdots&p_{kn} -->
<!-- \end{bmatrix}}= -->
<!-- {\begin{bmatrix} -->
<!-- 0&0&0&0&0&0&1&0\\ -->
<!-- -1&1&0&0&0&0&0&0\\ -->
<!-- 0&0&0.5&-0.5&0.5&-0.5&0&0 -->
<!-- \end{bmatrix}}\\ -->
<!-- \text{View returns: } &\mathbf{Q}={\begin{bmatrix} -->
<!-- q_{1}\\ -->
<!-- \vdots\\ -->
<!-- q_{k} -->
<!-- \end{bmatrix}}= -->
<!-- {\begin{bmatrix} -->
<!-- 0.0525\\ -->
<!-- 0.0025\\ -->
<!-- 0.0200 -->
<!-- \end{bmatrix}}\\ -->
<!-- \text{View confidence: } &\mathbf{C}={\begin{bmatrix} -->
<!-- c_{1}\\ -->
<!-- \vdots\\ -->
<!-- c_{k} -->
<!-- \end{bmatrix}}= -->
<!-- {\begin{bmatrix} -->
<!-- 0.2500\\ -->
<!-- 0.5000\\ -->
<!-- 0.6500 -->
<!-- \end{bmatrix}}\\ -->
<!-- \text{View covariance: } &\mathbf{\Omega}={\begin{bmatrix} -->
<!-- \tau\left(\frac{1-c_{1}}{c_{1}}\right)\left(p_{1}\Sigma p_{1}^{T}\right)&0&0\\ -->
<!-- 0&\ddots&0\\ -->
<!-- 0&0&\tau\left(\frac{1-c_{k}}{c_{k}}\right)\left(p_{k}\Sigma p_{k}^{T}\right) -->
<!-- \end{bmatrix}}\\ -->
<!-- \text{Distribution: } &N\sim(\mathbf{Q}, \mathbf{\Omega}) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- ## Posterior distribution -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \text{Implied returns: } &\hat{\Pi}=\Pi+\tau\Sigma \mathbf{P}^{T}\left(\tau \mathbf{P}\Sigma \mathbf{P}^{T}+\mathbf{\Omega}\right)^{-1}\left(\mathbf{Q}-\mathbf{P}\Pi^{T}\right)\\ -->
<!-- \text{Covariance: } &\hat{\Sigma}=\Sigma+\tau\left[\Sigma-\Sigma\mathbf{P}^{T}\left(\tau\mathbf{P}\Sigma\mathbf{P}^{T}+\mathbf{\Omega}\right)^{-1}\tau\mathbf{P}\Sigma\right]\\ -->
<!-- \text{Weights: } &\hat{w}=\hat{\Pi}\left(\lambda\Sigma\right)^{-1}\\ -->
<!-- \text{Distribution: } &N\sim\left(\left[\left(\tau\Sigma\right)^{-1}+\mathbf{P}^{T}\Omega^{-1}\mathbf{P}\right]^{-1}\left[\left(\tau\Sigma\right)^{-1}\Pi+\mathbf{P}^{T}\Omega^{-1}\mathbf{Q}\right],\left[\left(\tau\Sigma\right)^{-1}+\mathbf{P}^{T}\Omega^{-1}\mathbf{P}\right]^{-1}\right) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- ```{r} -->
<!-- black_litterman <- function(params, ir, sigma, views) { -->

<!--     # prior distribution -->
<!--     weights_prior <- params -->
<!--     sigma_prior <- sigma     -->
<!--     lmbda <- as.numeric(ir / sqrt(t(weights_prior) %*% sigma %*% weights_prior)) -->
<!--     pi_prior <- lmbda * sigma_prior %*% weights_prior -->

<!--     # matrix calculations -->
<!--     matmul_left <- views[["tau"]] * sigma_prior %*% t(views[["P"]]) -->
<!--     matmul_mid <- views[["tau"]] * views[["P"]] %*% sigma_prior %*% t(views[["P"]]) -->
<!--     matmul_right <- views[["Q"]] - views[["P"]] %*% pi_prior -->

<!--     # conditional distribution -->
<!--     omega <- diag(diag(diag((1 - views[["C"]]) / views[["C"]]) %*% matmul_mid)) -->

<!--     # posterior distribution -->
<!--     pi_posterior <- pi_prior + matmul_left %*% solve(matmul_mid + omega) %*% matmul_right -->

<!--     sigma_posterior <- sigma_prior +  views[["tau"]] * sigma_prior - -->
<!--         matmul_left %*% solve(matmul_mid + omega) %*% (tau * views[["P"]] %*% sigma_prior) -->

<!--     weights_posterior <- t(pi_posterior) %*% solve(lmbda * sigma_prior) -->

<!--     # implied confidence -->
<!--     pi_posterior_100 <- pi_prior + matmul_left %*% solve(matmul_mid) %*% matmul_right -->

<!--     weights_posterior_100 <- t(pi_posterior_100) %*% solve(lmbda * sigma_prior) -->

<!--     implied_confidence <- (weights_posterior - weights_prior) / (weights_posterior_100 - weights_prior) -->

<!--     result <- list("implied_confidence" = implied_confidence, -->
<!--                    "weights_prior" = t(as.matrix(weights_prior)), -->
<!--                    "weights_posterior" = weights_posterior, -->
<!--                    "pi_prior" = t(pi_prior), -->
<!--                    "pi_posterior" = t(pi_posterior), -->
<!--                    "sigma_prior" = sigma_prior, -->
<!--                    "sigma_posterior" = sigma_posterior) -->

<!--     return(result)    -->

<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- tau <- 0.025 -->
<!-- P <- diag(length(factors)) -->
<!-- Q <- t(implied_shocks(0.1, overlap_x_xts, overlap_x_xts[ , 1], 1)) -->
<!-- C <- rep(0.95, length(factors)) -->
<!-- views <- list("tau" = tau, "P" = P, "Q" = Q, "C" = C) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- bl <- black_litterman(as.vector(params3), ir, sigma, views) -->
<!-- bl -->
<!-- ``` -->

<!-- ```{r} -->
<!-- params4 <- as.vector(bl[["weights_posterior"]]) -->
<!-- params4 <- params4 / sum(params4) # no leverage -->
<!-- params4 -->
<!-- ``` -->

<!-- ```{r} -->
<!-- mu %*% params4 -->
<!-- ``` -->

<!-- ```{r} -->
<!-- sqrt(t(params4) %*% sigma %*% params4) -->
<!-- ``` -->

# Risk parity

Risk parity is an approach to portfolio management that focuses on allocation of risk rather than allocation of capital. In a risk parity strategy, the asset allocations are leveraged, or deleveraged, to have equal risk contributions. Suppose that $\mathbf{R}$ is a $T \times N$ matrix of asset returns where the return of the $i^{th}$ asset is $R_{i,t}$ at time $t$. Define $\Sigma$ to be the covariance matrix of $\mathbf{R}$ and let $\mathbf{w}=(w_{1},\dots,w_{N})$ be a vector of asset weights. Then the volatility of the return of the strategy is $\sigma_{P}=\sqrt{\mathbf{w}^T\Sigma\mathbf{w}}$ and, by Euler's Theorem, satisfies:

$$
\begin{aligned}
\sigma_{P}&=\sum_{i=1}^{N}w_{i}\frac{\partial\sigma_{P}}{\partial w_{i}}\\
&=w_{1}\frac{\partial\sigma_{P}}{\partial w_{1}}+\dots+w_{N}\frac{\partial\sigma_{P}}{\partial w_{N}}
\end{aligned}
$$

where each element is the risk contribution of the $i^{th}$ risky asset. The risk parity objective solves for weights such that each asset contributes equal risk using the following nonlinear constrained optimization problem:

$$
\begin{aligned}
\begin{array}{rrcl}
\displaystyle\max_{x}&\displaystyle\sum_{i=1}^{N}\log(w_{i})\\
\textrm{s.t.}&\sqrt{\mathbf{w}^T\Sigma\mathbf{w}}&\leq&\sigma 
\end{array}
\end{aligned}
$$

To incorporate these conditions into one equation, introduce a new variable $\lambda$ that is the Lagrange multiplier and define a new function $\mathcal{L}$ as follows:

$$
\begin{aligned}
\mathcal{L}(\mathbf{w},\lambda)&=\sum_{i=1}^{N}\log(w_{i})-\lambda(\sqrt{\mathbf{w}^T\Sigma\mathbf{w}}-\sigma)
\end{aligned}
$$

Then set the partial derivatives of $\mathcal{L}$ equal to zero for each asset $i$:

$$
\begin{aligned}
\frac{\partial\mathcal{L}(\mathbf{w},\lambda)}{\partial w_{i}}&=\frac{1}{w_{i}}-\lambda\frac{\partial\sigma_{P}}{\partial w_{i}}=0
\Leftrightarrow
w_{i}\frac{\partial\sigma_{P}}{\partial w_{i}}=\frac{1}{\lambda}
\end{aligned}
$$

Notice that $1/\lambda$ is the risk contribution of the $i^{th}$ asset. Now use `R` to maximize the Lagrangian numerically:

```{r}
# http://faculty.washington.edu/ezivot/econ424/riskbudgetingslides.pdf
# https://systematicinvestor.wordpress.com/2011/11/16/black-litterman-model/
# https://cran.r-project.org/web/packages/BLCOP/vignettes/BLCOP.pdf
# http://math.stackexchange.com/questions/17776/inverse-of-the-sum-of-matrices
risk_parity_optim <- function(sigma, target) {
    
    params <- Variable(nrow(sigma))
    
    risk <- quad_form(params, sigma)
    risk_contrib <- target ^ 2 / nrow(sigma)
            
    obj <- Maximize((sum(log(params)) - (1 / risk_contrib) * (risk - target ^ 2)))
        
    result <- solve(Problem(obj))$getValue(params)
    result <- result / sum(result) # no leverage
    
    return(result)

}
```

```{r}
target <- 1
```

```{r}
params5 <- risk_parity_optim(sigma, target)
params5
```

```{r}
risk <- as.numeric(sqrt(t(params5) %*% sigma %*% params5))
risk_contrib <- params5 * sigma %*% params5 / risk
risk_contrib
```

```{r}
mu %*% params5
```

```{r}
sqrt(t(params5) %*% sigma %*% params5)
```

```{r}
data.frame("max_pnl" = params1,
           "min_risk" = params2,
           "max_ratio" = params3,
           # "black_litterman" = params4,
           "risk_parity" = params5)
```

# Portfolio attribution

## Single-period

The arithmetic active return is commonly decomposed using the Brinson-Fachler method:

$$
\begin{aligned}
\text{Allocation: } &r_{a}=\sum_{k=1}^{n}(w_{p,k}-w_{b,k})(r_{b,k}-r_{b})\\
\text{Selection: } &r_{s}=\sum_{k=1}^{n}w_{p,k}(r_{p,k}-r_{b,k})\\
\end{aligned}
$$

where $k=1,\ldots,n$ is each sector or factor.

## Multi-period

Arithmetic attributes add to the active return of a single period; however, they cannot be summed or compounded to explain the active return over multiple periods. To solve this problem, the original arithmetic attribute is multiplied by a single scaling coefficient for that period. After all single-period original attributes have been transformed, the adjusted attributes sum to the active return over the periods.

$$
\begin{aligned}
\text{Carino scaling coefficient: } &c_{t}=\frac{[\ln(1+r_{p,t})-\ln(1+r_{b,t})]/(r_{p,t}-r_{b,t})}{[\ln(1+r_{p})-\ln(1+r_{b})]/(r_{p}-r_{b})}
\end{aligned}
$$

where $t=1,\ldots,n$ is each period.

```{r}
# http://www.frongello.com/support/Works/Chap20RiskBook.pdf
# https://github.com/R-Finance/PortfolioAttribution/blob/master/R/Carino.R
pnl_attrib <- function(params, x) {
    
    total_i <- rowSums(x)
    total <- prod(1 + total_i) - 1
    
    coef <- (log(1 + total_i) / total_i) / (log(1 + total) / total)
    
    result <- colSums(x * coef)
    
    return(result)
    
}
```

```{r}
attrib_mat <- sweep(tail(na.omit(returns_xts)[ , factors], width), 2, params1, "*")
```

```{r}
pnl_attrib(params1, attrib_mat)
```
