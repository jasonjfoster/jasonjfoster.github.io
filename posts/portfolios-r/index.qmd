---
title: "Portfolios"
author: "[Jason Foster](mailto:jason.j.foster@gmail.com)"
date: last-modified
categories:
  - analysis
  - finance
  - r
---

```{r, echo = FALSE, message = FALSE}
source("../plot/theme_jjf.R")
```

```{r, message = FALSE}
library(quantmod)
library(roll)
library(data.table)
```

```{r, echo = FALSE}
options("getSymbols.warning4.0" = FALSE)
```

```{r}
factors_r <- c("SP500", "DTWEXAFEGS") # "SP500" does not contain dividends; note: "DTWEXM" discontinued as of Jan 2020
factors_d <- c("DGS10", "BAMLH0A0HYM2")
factors <- c(factors_r, factors_d)
width <- 252
scale <- list("periods" = 252, "overlap" = 5)
```

```{r, echo = FALSE}
palette <- c("black", palette_jjf(length(factors)))
names(palette) <- c("Overall", factors)
```

```{r, results = "hide"}
getSymbols(factors, src = "FRED")
levels_xts <- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))
```

```{r}
returns_xts <- do.call(merge, lapply(factors, function(i) {
    if (i %in% factors_r) {
        diff(log((levels_xts[ , i])))
    } else if (i %in% factors_d) {
        -diff(levels_xts[ , i]) / 100
    }    
}))
overlap_xts <- roll_mean(returns_xts, scale[["overlap"]], min_obs = 1, na_restore = TRUE)
```

```{r, message = FALSE}
library(pls)
library(CVXR)
```

```{r}
tickers <- "BAICX" # fund inception date is "2011-11-28" 
invisible(getSymbols(tickers, src = "tiingo", api.key = Sys.getenv("TIINGO_API_KEY"), adjust = TRUE))
prices_xts <- do.call(merge, c(lapply(tickers, function(i) Cl(get(i))), all = TRUE))
colnames(prices_xts) <- tickers
index(prices_xts) <- as.Date(index(prices_xts))
```

```{r}
returns_xts <- merge(returns_xts, diff(log(prices_xts)))
overlap_xts <- merge(overlap_xts, roll_mean(returns_xts[ , tickers], scale[["overlap"]], min_obs = 1))
```

```{r}
# weights <- 0.9 ^ ((width - 1):0)
weights <- rep(1, width)
```

```{r}
# overlap_df <- na.omit(overlap_xts)
overlap_x_df <- na.omit(overlap_xts)[ , factors]
overlap_y_df <- na.omit(overlap_xts)[ , tickers]
overlap_x_xts <- tail(overlap_x_df, width)
overlap_y_xts <- tail(overlap_y_df, width)
```

# Principal component analysis

Underlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (*note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales*) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.

## Eigendecomposition

$$
\begin{aligned}
\boldsymbol{\Sigma}&=\lambda_{1}\mathbf{v}_{1}\mathbf{v}_{1}^\mathrm{T}+\lambda_{2}\mathbf{v}_{2}\mathbf{v}_{2}^\mathrm{T}+\cdots+\lambda_{k}\mathbf{v}_{k}\mathbf{v}_{k}^\mathrm{T}\\
&=V\Lambda V^{\mathrm{T}}
\end{aligned}
$$

```{r}
# https://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/
eigen_decomp <- function(x, comps) {
    
    LV <- eigen(cov(x))
    L <- LV$values[1:comps]
    V <- LV$vectors[ , 1:comps]
    
    result <- V %*% sweep(t(V), 1, L, "*")
    
    return(result)
    
}
```

```{r}
comps <- 1
```

```{r}
eigen_decomp(overlap_x_xts, comps) * scale[["periods"]] * scale[["overlap"]]
```

```{r}
# cov(overlap_x_xts) * scale[["periods"]] * scale[["overlap"]]
```

## Variance explained

We often look at the proportion of variance explained by the first $i$ principal components as an indication of how many components are needed.

$$
\begin{aligned}
\frac{\sum_{j=1}^{i}{\lambda_{j}}}{\sum_{j=1}^{k}{\lambda_{j}}}
\end{aligned}
$$

```{r}
variance_explained <- function(x) {
    
    LV <- eigen(cov(x))
    L <- LV$values
    
    result <- cumsum(L) / sum(L)
    
    return(result)
    
}
```

```{r}
variance_explained(overlap_x_xts)
```

## Cosine similarity

Also, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.

$$
\begin{aligned}
\text{similarity}=\frac{\mathbf{A}\cdot\mathbf{B}}{\|\mathbf{A}\|\|\mathbf{B}\|}
\end{aligned}
$$

```{r}
eigen_vals <- function(x) {
    
    LV <- eigen(cov(x))
    L <- LV$values
    
    return(L)    
}

eigen_vecs <- function(x) {
    
    LV <- eigen(cov(x))
    V <- LV$vectors
    
    return(V)    
}
```

```{r}
roll_eigen1 <- function(x, width, comp) {
    
    n_rows <- nrow(x)
    result_ls <- list()
    
    for (i in width:n_rows) {
        
        idx <- max(i - width + 1, 1):i
        evec <- eigen_vecs(x[idx, ])[ , comp]
        result_ls <- append(result_ls, list(evec))
                
    }
    
    result <- do.call(rbind, result_ls)
    result <- xts(result, index(x)[width:n_rows])
    colnames(result) <- colnames(x)
    
    return(result)
    
}
```

```{r}
comp <- 1
```

```{r}
raw_df <- roll_eigen1(overlap_x_df, width, comp)
```

```{r, fig.width = 6, fig.height = 3}
raw_mlt <- melt(as.data.table(raw_df), id.vars = "index")
raw_plt <- plot_ts(raw_mlt, title = "Eigenvector 1Y")
print(raw_plt)
```

```{r}
# https://quant.stackexchange.com/a/3095
roll_eigen2 <- function(x, width, comp) {
    
    n_rows <- nrow(x)
    result_ls <- list()
    
    for (i in width:n_rows) {
        
        idx <- max(i - width + 1, 1):i
        evec <- eigen_vecs(x[idx, ])[ , comp]
                
        if (i > width) {
            
            similarity <- evec %*% result_ls[[length(result_ls)]]
            evec <- as.vector(sign(similarity)) * evec
            result_ls <- append(result_ls, list(evec))
            
        } else {
            result_ls <- append(result_ls, list(evec))
        }
                
    }
    
    result <- do.call(rbind, result_ls)
    result <- xts(result, index(x)[width:n_rows])
    colnames(result) <- colnames(x)
    
    return(result)
    
}
```

```{r}
clean_df <- roll_eigen2(overlap_x_df, width, comp)
```

```{r, fig.width = 6, fig.height = 3}
clean_mlt <- melt(as.data.table(clean_df), id.vars = "index")
clean_plt <- plot_ts(clean_mlt, title = "Eigenvector 1Y")
print(clean_plt)
```

# Random portfolios

Need to generate uniformly distributed weights $\mathbf{w}=(w_{1},w_{2},\ldots,w_{N})$ such that $\sum_{j=1}^{N}w_{i}=1$ and $w_{i}\geq0$:

-   **Approach 1**: tempting to use $w_{i}=\frac{u_{i}}{\sum_{j=1}^{N}u_{i}}$ where $u_{i}\sim U(0,1)$ but the distribution of $\mathbf{w}$ is not uniform

-   **Approach 2**: instead, generate $\text{Exp}(1)$ and then normalize

Can also scale random weights by $M$, e.g. if sum of weights must be 10% then multiply weights by 10%.

```{r}
rand_weights1 <- function(n_sim, n_assets, lmbda) {
    
    rand_exp <- matrix(runif(n_sim * n_assets), nrow = n_sim, ncol = n_assets)
    result <- sweep(rand_exp, 1, rowSums(rand_exp), "/")
    
    return(result)
    
}
```

```{r}
# Methodology: uniform sampling from the simplex (http://mathoverflow.net/a/76258)
# z ~ U(0, 1) then -ln(z) is an exponential(1) distribution
# This is also known as generating a random vector from the symmetric Dirichlet distribution
rand_weights2 <- function(n_sim, n_assets, lmbda) {
    
    rand_exp <- matrix(-log(1 - runif(n_sim * n_assets)) / lmbda, nrow = n_sim, ncol = n_assets)
    result <- sweep(rand_exp, 1, rowSums(rand_exp), "/")
    
    return(result)
    
}
```

```{r}
# 1. Generate n exponential(1) random variables x_1, x_2, ..., x_n
# 2. Let y_i = x_i / (sum_{i = 1}^{n} x_i)
rand_weights3 <- function(n_sim, n_assets, lmbda) {
    
    rand_exp <- matrix(rexp(n_sim * n_assets), nrow = n_sim, ncol = n_assets)
    result <- sweep(rand_exp, 1, rowSums(rand_exp), "/")
    
    return(result)
    
}
```

```{r}
lmbda <- 1
n_assets <- 3
n_sim <- 10000
```

```{r}
approach1 <- rand_weights1(n_sim, n_assets, lmbda)
approach2 <- rand_weights2(n_sim, n_assets, lmbda)
approach3 <- rand_weights2(n_sim, n_assets, lmbda)
```

```{r, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(approach1), title = "Weight (%)")
```

```{r, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(approach2), title = "Weight (%)")
```

```{r, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(approach3), title = "Weight (%)")
```

## Random turnover

How to generate random weights between lower bound $a$ and upper bound $b$ that sum to zero?

-   **Approach 1**: tempting to multiply random weights by $M$ and then subtract by $\frac{M}{N}$ but the distribution is not between $a$ and $b$

-   **Approach 2**: instead, use an iterative approach for random turnover:

    1.  Generate $N-1$ uniformly distributed weights between $a$ and $b$
    2.  For $u_{N}$ compute sum of values and subtract from $M$
    3.  If $u_{N}$ is between $a$ and $b$, then keep; otherwise, discard

Then add random turnover to previous period's random weights.

```{r}
rand_iterative <- function(n_assets, lower, upper, target) {
    
    plug <- FALSE
    
    while (!plug) {
        
        result <- as.matrix(runif(n_assets - 1, min = lower, max = upper))
        temp <- target - sum(result)
        
        if ((temp <= upper) && (temp >= lower)) {
            plug <- TRUE            
        }
        
    }
    
    result <- append(result, temp)
    
    return(result)
    
}
```

```{r}
rand_turnover1 <- function(n_sim, n_assets, lower, upper, target) {
    
    rng <- upper - lower
    
    result <- rand_weights3(n_sim, n_assets, lmbda) * rng
    result <- result - rng / n_assets
    
    return(result)
    
}
```

```{r}
rand_turnover2 <- function(n_sim, n_assets, lower, upper, target) {
    
    result <- matrix(rand_iterative(n_assets, lower, upper, target), nrow = 1, ncol = n_assets)
    
    while (nrow(result) < n_sim) {
        
        temp <- matrix(rand_iterative(n_assets, lower, upper, target), nrow = 1, ncol = n_assets)
        result <- rbind(result, temp)
        
    }
    
    return(result)
    
}
```

```{r}
lower <- -0.05
upper <- 0.05
target <- 0
```

```{r}
approach1 <- rand_turnover1(n_sim, n_assets, lower, upper, target)
approach2 <- rand_turnover2(n_sim, n_assets, lower, upper, target)
```

```{r, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(approach1), title = "Weight (%)")
```

```{r, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(approach2), title = "Weight (%)")
```

# Mean-variance

```{r}
geometric_mean <- function(x, scale) {
    
    result <- prod(1 + x) ^ (scale / length(x)) - 1
    
    return(result)
    
}
```

```{r}
returns_x_xts <- na.omit(returns_xts)[ , factors] # extended history # REMOVE LATER
mu <- apply(returns_x_xts, 2, geometric_mean, scale = scale[["periods"]])
sigma <- cov(overlap_x_xts) * scale[["periods"]] * scale[["overlap"]]
```

## Maximum return

$$
\begin{aligned}
\begin{array}{rrcl}
\displaystyle\max_{x}&\mu^{T}\mathbf{w}\\
\textrm{s.t.}&\mathbf{w}^T\Sigma\mathbf{w}&\leq&\sigma^{2}\\
&e^T\mathbf{w}&=&1
\end{array}
\end{aligned}
$$

```{r}
target <- 0.06
```

```{r}
# https://palomar.home.ece.ust.hk/MAFS6010R_lectures/slides_robust_portfolio.html
max_pnl_optim <- function(mu, sigma, target) {
    
    params <- Variable(length(mu))
    
    cons <- list(params >= 0, sum(params) == 1,
                 quad_form(params, sigma) <= target ^ 2)
    
    obj <- Maximize(t(params) %*% mu)
        
    result <- solve(Problem(obj, cons))$getValue(params)
    
    return(result)

}
```

```{r}
params1 <- max_pnl_optim(mu, sigma, target)
params1
```

```{r}
mu %*% params1
```

```{r}
sqrt(t(params1) %*% sigma %*% params1)
```

## Minimum variance

$$
\begin{aligned}
\begin{array}{rrcl}
\displaystyle\min_{x}&\mathbf{w}^T\Sigma\mathbf{w}\\
\textrm{s.t.}&\mu^{T}\mathbf{w}&\geq&M\\
&e^T\mathbf{w}&=&1
\end{array}
\end{aligned}
$$

```{r}
target <- 0.03
```

```{r}
min_risk_optim <- function(mu, sigma, target) {
    
    params <- Variable(length(mu))
    
    cons <- list(params >= 0, sum(params) == 1,
                 sum(mu * params) >= target)
    
    obj <- Minimize(quad_form(params, sigma))
        
    result <- solve(Problem(obj, cons))$getValue(params)
    
    return(result)

}
```

```{r}
params2 <- min_risk_optim(mu, sigma, target)
params2
```

```{r}
mu %*% params2
```

```{r}
sqrt(t(params2) %*% sigma %*% params2)
```

## Maximum ratio

$$
\begin{aligned}
\begin{array}{rrcl}
\displaystyle\max_{x}&\mu^{T}\mathbf{w}-\frac{1}{2}\delta(\mathbf{w}^T\Sigma\mathbf{w})\\
\textrm{s.t.}&e^T\mathbf{w}&=&1
\end{array}
\end{aligned}
$$

```{r}
ir <- 0.5
target <- ir / 0.06 # ir / std (see Black-Litterman)
```

```{r}
max_ratio_optim <- function(mu, sigma, target) {
    
    params <- Variable(length(mu))
    
    cons <- list(params >= 0, sum(params) == 1)
    
    obj <- Maximize(t(mu) %*% params - 0.5 * target * quad_form(params, sigma))
        
    result <- solve(Problem(obj, cons))$getValue(params)
    
    return(result)

}
```

```{r}
params3 <- max_ratio_optim(mu, sigma, target)
params3
```

```{r}
mu %*% params3
```

```{r}
sqrt(t(params3) %*% sigma %*% params3)
```

<!-- # Black-Litterman -->

<!-- ## Prior distribution -->

<!-- $$ -->

<!-- \begin{aligned} -->

<!-- \text{Risk aversion: } &\lambda=\frac{E(r)-r_{f}}{\sigma^{2}}=\frac{IR}{\sigma}\\ -->

<!-- \text{Implied returns: } &\Pi=\lambda\Sigma w\\ -->

<!-- \text{Distribution: } &N\sim(\Pi,\tau\Sigma) -->

<!-- \end{aligned} -->

<!-- $$ -->

<!-- ```{r} -->

<!-- implied_pnl <- function(params, ir, sigma) { -->

<!--     lmbda <- as.numeric(ir / sqrt(t(params) %*% sigma %*% params)) -->

<!--     result <- lmbda * sigma %*% params -->

<!--     return(result)     -->

<!-- } -->

<!-- ``` -->

<!-- ```{r} -->

<!-- implied_pnl(params3, ir, sigma) -->

<!-- ``` -->

<!-- ## Conditional distribution -->

<!-- $$ -->

<!-- \begin{aligned} -->

<!-- \text{Prior mean variance: } &\tau\in(0.01, 0.05)\approx(0.025)\\ -->

<!-- \text{Asset views: } &\mathbf{P}={\begin{bmatrix} -->

<!-- p_{11}&\cdots&p_{1n}\\ -->

<!-- \vdots&\ddots&\vdots\\ -->

<!-- p_{k1}&\cdots&p_{kn} -->

<!-- \end{bmatrix}}= -->

<!-- {\begin{bmatrix} -->

<!-- 0&0&0&0&0&0&1&0\\ -->

<!-- -1&1&0&0&0&0&0&0\\ -->

<!-- 0&0&0.5&-0.5&0.5&-0.5&0&0 -->

<!-- \end{bmatrix}}\\ -->

<!-- \text{View returns: } &\mathbf{Q}={\begin{bmatrix} -->

<!-- q_{1}\\ -->

<!-- \vdots\\ -->

<!-- q_{k} -->

<!-- \end{bmatrix}}= -->

<!-- {\begin{bmatrix} -->

<!-- 0.0525\\ -->

<!-- 0.0025\\ -->

<!-- 0.0200 -->

<!-- \end{bmatrix}}\\ -->

<!-- \text{View confidence: } &\mathbf{C}={\begin{bmatrix} -->

<!-- c_{1}\\ -->

<!-- \vdots\\ -->

<!-- c_{k} -->

<!-- \end{bmatrix}}= -->

<!-- {\begin{bmatrix} -->

<!-- 0.2500\\ -->

<!-- 0.5000\\ -->

<!-- 0.6500 -->

<!-- \end{bmatrix}}\\ -->

<!-- \text{View covariance: } &\mathbf{\Omega}={\begin{bmatrix} -->

<!-- \tau\left(\frac{1-c_{1}}{c_{1}}\right)\left(p_{1}\Sigma p_{1}^{T}\right)&0&0\\ -->

<!-- 0&\ddots&0\\ -->

<!-- 0&0&\tau\left(\frac{1-c_{k}}{c_{k}}\right)\left(p_{k}\Sigma p_{k}^{T}\right) -->

<!-- \end{bmatrix}}\\ -->

<!-- \text{Distribution: } &N\sim(\mathbf{Q}, \mathbf{\Omega}) -->

<!-- \end{aligned} -->

<!-- $$ -->

<!-- ## Posterior distribution -->

<!-- $$ -->

<!-- \begin{aligned} -->

<!-- \text{Implied returns: } &\hat{\Pi}=\Pi+\tau\Sigma \mathbf{P}^{T}\left(\tau \mathbf{P}\Sigma \mathbf{P}^{T}+\mathbf{\Omega}\right)^{-1}\left(\mathbf{Q}-\mathbf{P}\Pi^{T}\right)\\ -->

<!-- \text{Covariance: } &\hat{\Sigma}=\Sigma+\tau\left[\Sigma-\Sigma\mathbf{P}^{T}\left(\tau\mathbf{P}\Sigma\mathbf{P}^{T}+\mathbf{\Omega}\right)^{-1}\tau\mathbf{P}\Sigma\right]\\ -->

<!-- \text{Weights: } &\hat{w}=\hat{\Pi}\left(\lambda\Sigma\right)^{-1}\\ -->

<!-- \text{Distribution: } &N\sim\left(\left[\left(\tau\Sigma\right)^{-1}+\mathbf{P}^{T}\Omega^{-1}\mathbf{P}\right]^{-1}\left[\left(\tau\Sigma\right)^{-1}\Pi+\mathbf{P}^{T}\Omega^{-1}\mathbf{Q}\right],\left[\left(\tau\Sigma\right)^{-1}+\mathbf{P}^{T}\Omega^{-1}\mathbf{P}\right]^{-1}\right) -->

<!-- \end{aligned} -->

<!-- $$ -->

<!-- ```{r} -->

<!-- black_litterman <- function(params, ir, sigma, views) { -->

<!--     # prior distribution -->

<!--     weights_prior <- params -->

<!--     sigma_prior <- sigma     -->

<!--     lmbda <- as.numeric(ir / sqrt(t(weights_prior) %*% sigma %*% weights_prior)) -->

<!--     pi_prior <- lmbda * sigma_prior %*% weights_prior -->

<!--     # matrix calculations -->

<!--     matmul_left <- views[["tau"]] * sigma_prior %*% t(views[["P"]]) -->

<!--     matmul_mid <- views[["tau"]] * views[["P"]] %*% sigma_prior %*% t(views[["P"]]) -->

<!--     matmul_right <- views[["Q"]] - views[["P"]] %*% pi_prior -->

<!--     # conditional distribution -->

<!--     omega <- diag(diag(diag((1 - views[["C"]]) / views[["C"]]) %*% matmul_mid)) -->

<!--     # posterior distribution -->

<!--     pi_posterior <- pi_prior + matmul_left %*% solve(matmul_mid + omega) %*% matmul_right -->

<!--     sigma_posterior <- sigma_prior +  views[["tau"]] * sigma_prior - -->

<!--         matmul_left %*% solve(matmul_mid + omega) %*% (tau * views[["P"]] %*% sigma_prior) -->

<!--     weights_posterior <- t(pi_posterior) %*% solve(lmbda * sigma_prior) -->

<!--     # implied confidence -->

<!--     pi_posterior_100 <- pi_prior + matmul_left %*% solve(matmul_mid) %*% matmul_right -->

<!--     weights_posterior_100 <- t(pi_posterior_100) %*% solve(lmbda * sigma_prior) -->

<!--     implied_confidence <- (weights_posterior - weights_prior) / (weights_posterior_100 - weights_prior) -->

<!--     result <- list("implied_confidence" = implied_confidence, -->

<!--                    "weights_prior" = t(as.matrix(weights_prior)), -->

<!--                    "weights_posterior" = weights_posterior, -->

<!--                    "pi_prior" = t(pi_prior), -->

<!--                    "pi_posterior" = t(pi_posterior), -->

<!--                    "sigma_prior" = sigma_prior, -->

<!--                    "sigma_posterior" = sigma_posterior) -->

<!--     return(result)    -->

<!-- } -->

<!-- ``` -->

<!-- ```{r} -->

<!-- tau <- 0.025 -->

<!-- P <- diag(length(factors)) -->

<!-- Q <- t(implied_shocks(0.1, overlap_x_xts, overlap_x_xts[ , 1], 1)) -->

<!-- C <- rep(0.95, length(factors)) -->

<!-- views <- list("tau" = tau, "P" = P, "Q" = Q, "C" = C) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- bl <- black_litterman(as.vector(params3), ir, sigma, views) -->

<!-- bl -->

<!-- ``` -->

<!-- ```{r} -->

<!-- params4 <- as.vector(bl[["weights_posterior"]]) -->

<!-- params4 <- params4 / sum(params4) # no leverage -->

<!-- params4 -->

<!-- ``` -->

<!-- ```{r} -->

<!-- mu %*% params4 -->

<!-- ``` -->

<!-- ```{r} -->

<!-- sqrt(t(params4) %*% sigma %*% params4) -->

<!-- ``` -->

# Risk parity

Risk parity is an approach to portfolio management that focuses on allocation of risk rather than allocation of capital. In a risk parity strategy, the asset allocations are leveraged, or deleveraged, to have equal risk contributions. Suppose that $\mathbf{R}$ is a $T \times N$ matrix of asset returns where the return of the $i^{th}$ asset is $R_{i,t}$ at time $t$. Define $\Sigma$ to be the covariance matrix of $\mathbf{R}$ and let $\mathbf{w}=(w_{1},\dots,w_{N})$ be a vector of asset weights. Then the volatility of the return of the strategy is $\sigma_{P}=\sqrt{\mathbf{w}^T\Sigma\mathbf{w}}$ and, by Euler's Theorem, satisfies:

$$
\begin{aligned}
\sigma_{P}&=\sum_{i=1}^{N}w_{i}\frac{\partial\sigma_{P}}{\partial w_{i}}\\
&=w_{1}\frac{\partial\sigma_{P}}{\partial w_{1}}+\dots+w_{N}\frac{\partial\sigma_{P}}{\partial w_{N}}
\end{aligned}
$$

where each element is the risk contribution of the $i^{th}$ risky asset. The risk parity objective solves for weights such that each asset contributes equal risk using the following nonlinear constrained optimization problem:

$$
\begin{aligned}
\begin{array}{rrcl}
\displaystyle\max_{x}&\displaystyle\sum_{i=1}^{N}\log(w_{i})\\
\textrm{s.t.}&\sqrt{\mathbf{w}^T\Sigma\mathbf{w}}&\leq&\sigma 
\end{array}
\end{aligned}
$$

To incorporate these conditions into one equation, introduce a new variable $\lambda$ that is the Lagrange multiplier and define a new function $\mathcal{L}$ as follows:

$$
\begin{aligned}
\mathcal{L}(\mathbf{w},\lambda)&=\sum_{i=1}^{N}\log(w_{i})-\lambda(\sqrt{\mathbf{w}^T\Sigma\mathbf{w}}-\sigma)
\end{aligned}
$$

Then set the partial derivatives of $\mathcal{L}$ equal to zero for each asset $i$:

$$
\begin{aligned}
\frac{\partial\mathcal{L}(\mathbf{w},\lambda)}{\partial w_{i}}&=\frac{1}{w_{i}}-\lambda\frac{\partial\sigma_{P}}{\partial w_{i}}=0
\Leftrightarrow
w_{i}\frac{\partial\sigma_{P}}{\partial w_{i}}=\frac{1}{\lambda}
\end{aligned}
$$

Notice that $1/\lambda$ is the risk contribution of the $i^{th}$ asset. Now use `R` to maximize the Lagrangian numerically:

```{r}
# http://faculty.washington.edu/ezivot/econ424/riskbudgetingslides.pdf
# https://systematicinvestor.wordpress.com/2011/11/16/black-litterman-model/
# https://cran.r-project.org/web/packages/BLCOP/vignettes/BLCOP.pdf
# http://math.stackexchange.com/questions/17776/inverse-of-the-sum-of-matrices
risk_parity_optim <- function(sigma, target) {
    
    params <- Variable(nrow(sigma))
    
    risk <- quad_form(params, sigma)
    risk_contrib <- target ^ 2 / nrow(sigma)
            
    obj <- Maximize((sum(log(params)) - (1 / risk_contrib) * (risk - target ^ 2)))
        
    result <- solve(Problem(obj))$getValue(params)
    result <- result / sum(result) # no leverage
    
    return(result)

}
```

```{r}
target <- 1
```

```{r}
params5 <- risk_parity_optim(sigma, target)
params5
```

```{r}
risk <- as.numeric(sqrt(t(params5) %*% sigma %*% params5))
risk_contrib <- params5 * sigma %*% params5 / risk
risk_contrib
```

```{r}
mu %*% params5
```

```{r}
sqrt(t(params5) %*% sigma %*% params5)
```

```{r}
data.frame("max_pnl" = params1,
           "min_risk" = params2,
           "max_ratio" = params3,
           # "black_litterman" = params4,
           "risk_parity" = params5)
```

# Portfolio attribution

## Single-period

The arithmetic active return is commonly decomposed using the Brinson-Fachler method:

$$
\begin{aligned}
\text{Allocation: } &r_{a}=\sum_{k=1}^{n}(w_{p,k}-w_{b,k})(r_{b,k}-r_{b})\\
\text{Selection: } &r_{s}=\sum_{k=1}^{n}w_{p,k}(r_{p,k}-r_{b,k})\\
\end{aligned}
$$

where $k=1,\ldots,n$ is each sector or factor.

## Multi-period

Arithmetic attributes add to the active return of a single period; however, they cannot be summed or compounded to explain the active return over multiple periods. To solve this problem, the original arithmetic attribute is multiplied by a single scaling coefficient for that period. After all single-period original attributes have been transformed, the adjusted attributes sum to the active return over the periods.

$$
\begin{aligned}
\text{Carino scaling coefficient: } &c_{t}=\frac{[\ln(1+r_{p,t})-\ln(1+r_{b,t})]/(r_{p,t}-r_{b,t})}{[\ln(1+r_{p})-\ln(1+r_{b})]/(r_{p}-r_{b})}
\end{aligned}
$$

where $t=1,\ldots,n$ is each period.

```{r}
# http://www.frongello.com/support/Works/Chap20RiskBook.pdf
# https://github.com/R-Finance/PortfolioAttribution/blob/master/R/Carino.R
pnl_attrib <- function(params, x) {
    
    total_i <- rowSums(x)
    total <- prod(1 + total_i) - 1
    
    coef <- (log(1 + total_i) / total_i) / (log(1 + total) / total)
    
    result <- colSums(x * coef)
    
    return(result)
    
}
```

```{r}
attrib_mat <- sweep(tail(na.omit(returns_xts)[ , factors], width), 2, params1, "*")
```

```{r}
pnl_attrib(params1, attrib_mat)
```
