---
title: "Optimization"
author: "[Jason Foster](mailto:jason.j.foster@gmail.com)"
date: last-modified
categories:
  - analysis
  - finance
  - python
---

```{python, echo = FALSE}
# closed: https://github.com/rstudio/rstudio/issues/5945
# closed: https://github.com/rstudio/reticulate/issues/808
# closed: https://github.com/rstudio/reticulate/issues/847
# open: "run all chunks"
```

```{r, echo = FALSE, message = FALSE}
library(reticulate)
library(data.table)
source("../plot/theme_jjf.R")
```

```{python}
import pandas as pd
import numpy as np
import statsmodels.api as sm
import pandas_datareader as pdr
from scipy.stats import norm, chi2
```

```{python}
factors_r = ["SP500", "DTWEXAFEGS"] # "SP500" does not contain dividends; note: "DTWEXM" discontinued as of Jan 2020
factors_d = ["DGS10", "BAMLH0A0HYM2"]
factors = factors_r + factors_d
width = 252
scale = {"periods": 252, "overlap": 5}
```

```{r, echo = FALSE}
palette <- c("black", palette_jjf(length(py$factors)))
names(palette) <- c("Overall", py$factors)
```

-   <https://pandas-datareader.readthedocs.io/en/latest/remote_data.html>

```{python}
levels_df = pdr.get_data_fred(factors, start = "1900-01-01")
```

```{python}
returns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)
overlap_df = returns_df.rolling(scale["overlap"], min_periods = 1).mean()
returns_df = pd.concat([returns_df, overlap_df], keys = ["returns", "overlap"], axis = 1)
```

```{python}
import os
from scipy.optimize import minimize
```

-   Open: <https://github.com/pydata/pandas-datareader/issues/965>

```{python}
tickers = ["BAICX"] # fund inception date is "2011-11-28"
prices_df = pdr.get_data_tiingo(tickers, start = "1900-01-01", api_key = os.getenv("TIINGO_API_KEY"))
prices_df = prices_df.pivot_table(index = "date", columns = "symbol", values = "adjClose") \
    .tz_localize(None)
```

```{python}
returns_cols = list(zip(["returns"], tickers))
overlap_cols = list(zip(["overlap"], tickers))
returns_df[returns_cols] = np.log(prices_df).diff()
returns_df[overlap_cols] = returns_df[returns_cols].rolling(scale["overlap"], min_periods = 1).mean()
returns_df.sort_index(axis = 1, inplace = True)
```

```{python}
# weights = np.array([0.9 ** i for i in range(width - 1, -1, -1)]).reshape((width, 1))
weights = np.array([1] * width).reshape((width, 1))
```

```{python}
overlap_df = returns_df.dropna()["overlap"]
overlap_x_df = returns_df.dropna()["overlap"][factors][-width:] # same dimension as `weights`
overlap_y_df = returns_df.dropna()["overlap"][tickers][-width:]
```

# Random weights

Need to generate uniformly distributed weights $\mathbf{w}=(w_{1},w_{2},\ldots,w_{N})$ such that $\sum_{j=1}^{N}w_{i}=1$ and $w_{i}\geq0$:

-   **Approach 1**: tempting to use $w_{i}=\frac{u_{i}}{\sum_{j=1}^{N}u_{i}}$ where $u_{i}\sim U(0,1)$ but the distribution of $\mathbf{w}$ is not uniform

-   **Approach 2**: instead, generate $\text{Exp}(1)$ and then normalize

Can also scale random weights by $M$, e.g. if sum of weights must be 10% then multiply weights by 10%.

```{python}
def rand_weights1(n_sim, n_assets):  
    
    rand_exp = np.matrix(np.random.uniform(size = (n_sim, n_assets)))
    rand_exp_sum = np.sum(rand_exp, axis = 1)
    
    result = rand_exp / rand_exp_sum
    
    return result
```

```{python}
n_assets = 3
n_sim = 10000
```

```{python}
approach1 = rand_weights1(n_sim, n_assets)
```

```{r, echo = FALSE, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(py$approach1), title = "Weight (%)")
```

**Approach 2(a)**: uniform sample from the simplex (<http://mathoverflow.net/a/76258>) and then normalize

-   If $u\sim U(0,1)$ then $-\ln(u)$ is an $\text{Exp}(1)$ distribution

This is also known as generating a random vector from the symmetric Dirichlet distribution.

```{python}
def rand_weights2a(n_sim, n_assets, lmbda):   
    
    # inverse transform sampling: https://en.wikipedia.org/wiki/Inverse_transform_sampling
    rand_exp = np.matrix(-np.log(1 - np.random.uniform(size = (n_sim, n_assets))) / lmbda)
    rand_exp_sum = np.sum(rand_exp, axis = 1)
    
    result = rand_exp / rand_exp_sum
    
    return result
```

```{python}
lmbda = 1
```

```{python}
approach2a = rand_weights2a(n_sim, n_assets, lmbda)
```

```{r, echo = FALSE, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(py$approach2a), title = "Weight (%)")
```

**Approach 2(b)**: directly generate $\text{Exp}(1)$ and then normalize

```{python}
def rand_weights2b(n_sim, n_assets):
    
    rand_exp = np.matrix(np.random.exponential(size = (n_sim, n_assets)))
    rand_exp_sum = np.sum(rand_exp, axis = 1)
    
    result = rand_exp / rand_exp_sum
    
    return result
```

```{python}
approach2b = rand_weights2b(n_sim, n_assets)
```

```{r, echo = FALSE, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(py$approach2b), title = "Weight (%)")
```

## Random turnover

How to generate random weights between lower bound $a$ and upper bound $b$ that sum to zero?

-   **Approach 1**: tempting to multiply random weights by $M$ and then subtract by $\frac{M}{N}$ but the distribution is not between $a$ and $b$

-   **Approach 2**: instead, use an iterative approach for random turnover:

    1.  Generate $N-1$ uniformly distributed weights between $a$ and $b$
    2.  For $u_{N}$ compute sum of values and subtract from $M$
    3.  If $u_{N}$ is between $a$ and $b$, then keep; otherwise, discard

Then add random turnover to previous period's random weights.

```{python}
def rand_turnover1(n_sim, n_assets, lower, upper, target):
    
    rng = upper - lower
    
    result = rand_weights2b(n_sim, n_assets) * rng
    result = result - rng / n_assets
    
    return result
```

```{python}
lower = -0.05
upper = 0.05
target = 0
```

```{python}
approach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)
```

```{r, echo = FALSE, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(py$approach1), title = "Weight (%)")
```

```{python}
def rand_iterative(n_assets, lower, upper, target):
    
    result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)
    temp = target - sum(result)
    
    while not ((temp <= upper) and (temp >= lower)):
        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)
        temp = target - sum(result)
        
    result = np.append(result, temp)
    
    return result
```

```{python}
def rand_turnover2(n_sim, n_assets, lower, upper, target):
  
    result_ls = []
    
    for i in range(n_sim):
      
      result_sim = rand_iterative(n_assets, lower, upper, target)
      result_ls.append(result_sim)
      
    result = pd.DataFrame(result_ls)
    
    return result
```

```{python}
approach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)
```

```{r, echo = FALSE, fig.width = 4, fig.height = 4}
plot_pairs(as.data.table(py$approach2), title = "Weight (%)")
```

# Mean-variance

```{python}
def geometric_mean(x, scale):
    
    result = np.prod(1 + x) ** (scale/ len(x)) - 1
    
    return result
```

-   <https://www.adrian.idv.hk/2021-06-22-kkt/>
-   <https://or.stackexchange.com/a/3738>
-   <https://bookdown.org/compfinezbook/introFinRbook/Portfolio-Theory-with-Matrix-Algebra.html#algorithm-for-computing-efficient-frontier>
-   <https://palomar.home.ece.ust.hk/MAFS6010R_lectures/slides_robust_portfolio.html>

```{python}
returns_x_df = returns_df.dropna()["returns"][factors]
mu = returns_x_df.apply(geometric_mean, axis = 0, scale = scale["periods"])
sigma = np.cov(overlap_x_df.T, ddof = 1) * scale["periods"] * scale["overlap"]
```

## Maximize mean

$$
\begin{aligned}
\begin{array}{rrcl}
\displaystyle\min&-\mathbf{w}^{T}\mu\\
\textrm{s.t.}&\mathbf{w}^{T}e&=&1\\
&\mathbf{w}^T\Sigma\mathbf{w}&\leq&\sigma^{2}\\
\end{array}
\end{aligned}
$$

To incorporate these conditions into one equation, introduce new variables $\lambda_{i}$ that are the Lagrange multipliers and define a new function $\mathcal{L}$ as follows:

$$
\begin{aligned}
\mathcal{L}(\mathbf{w},\lambda)&=-\mathbf{w}^{T}\mu-\lambda_{1}(\mathbf{w}^{T}e-1)
\end{aligned}
$$

Then, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\lambda_{i}$:

$$
\begin{aligned}
\frac{\partial\mathcal{L}(\mathbf{w},\lambda)}{\partial w}&=-\mu-\lambda_{1}e=0\\
\frac{\partial\mathcal{L}(\mathbf{w},\lambda)}{\partial \lambda_{1}}&=\mathbf{w}e^T-1=0
\end{aligned}
$$

Simplify the equations above in matrix form and solve for the Lagrange multipliers $\lambda_{i}$:

$$
\begin{aligned}
\begin{bmatrix}
-\mu & e \\
e^{T} & 0
\end{bmatrix}
\begin{bmatrix}
\mathbf{w} \\
-\lambda_{1}
\end{bmatrix}
&=
\begin{bmatrix}
0 \\
1
\end{bmatrix}
\\
\begin{bmatrix}
\mathbf{w} \\
-\lambda_{1}
\end{bmatrix}
&=
\begin{bmatrix}
-\mu & e \\
e^{T} & 0
\end{bmatrix}^{-1}
\begin{bmatrix}
0 \\
1
\end{bmatrix}
\end{aligned}
$$

```{python}
target = 0.06
start = np.array([1] * len(factors))
bnds = [(np.finfo(float).eps, 1) for i in range(len(factors))]
```

```{python}
def max_mean_cons(params, sigma, target):
    
    var = np.dot(params, np.dot(sigma, params))
    
    result = target ** 2 - var
    
    return result

def max_mean_obj(params, mu):
    
    result = -np.dot(mu, params)
    
    return result

def max_mean_optim(params, mu, sigma, target):
  
    cons = [{"type": "ineq", "fun": max_mean_cons, "args": (sigma, target)},
           {"type": "eq", "fun": lambda params: np.sum(params) - 1}]
    
    result = minimize(max_mean_obj, params, args = (mu), bounds = bnds, constraints = cons)
    
    return result.x
```

```{python}
params1 = max_mean_optim(start, mu, sigma, target)
params1
```

```{python}
np.dot(mu, params1)
```

```{python}
np.sqrt(np.dot(params1, np.dot(sigma, params1)))
```

## Minimize variance

$$
\begin{aligned}
\begin{array}{rrcl}
\displaystyle\min&\frac{1}{2}\mathbf{w}^T\Sigma\mathbf{w}\\
\textrm{s.t.}&\mathbf{w}^{T}e&=&1\\
&\mu^{T}\mathbf{w}&\geq&M\\
\end{array}
\end{aligned}
$$

To incorporate these conditions into one equation, introduce new variables $\lambda_{i}$ that are the Lagrange multipliers and define a new function $\mathcal{L}$ as follows:

$$
\begin{aligned}
\mathcal{L}(\mathbf{w},\lambda)&=\frac{1}{2}\mathbf{w}^{T}\Sigma\mathbf{w}-\lambda_{1}(\mathbf{w}^{T}e-1)
\end{aligned}
$$

Then, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\lambda_{i}$:

$$
\begin{aligned}
\frac{\partial\mathcal{L}(\mathbf{w},\lambda)}{\partial w}&=\mathbf{w}\Sigma-\lambda_{1}e=0\\
\frac{\partial\mathcal{L}(\mathbf{w},\lambda)}{\partial \lambda_{1}}&=\mathbf{w}e^T-1=0
\end{aligned}
$$

Simplify the equations above in matrix form and solve for the Lagrange multipliers $\lambda_{i}$:

$$
\begin{aligned}
\begin{bmatrix}
\Sigma & e \\
e^{T} & 0
\end{bmatrix}
\begin{bmatrix}
\mathbf{w} \\
-\lambda_{1}
\end{bmatrix}
&=
\begin{bmatrix}
0 \\
1
\end{bmatrix}
\\
\begin{bmatrix}
\mathbf{w} \\
-\lambda_{1}
\end{bmatrix}
&=
\begin{bmatrix}
\Sigma & e \\
e^{T} & 0
\end{bmatrix}^{-1}
\begin{bmatrix}
0 \\
1
\end{bmatrix}
\end{aligned}
$$

```{python}
target = 0.03
```

```{python}
def min_var_cons(params, mu, target):
    
    result = np.dot(mu, params) - target
    
    return result

def min_var_obj(params, sigma):
    
    result = np.dot(params, np.dot(sigma, params))
    
    return result

def min_var_optim(params, mu, sigma, target):
  
    cons = [{"type": "ineq", "fun": min_var_cons, "args": (mu, target)},
            {"type": "eq", "fun": lambda params: np.sum(params) - 1}]
    
    result = minimize(min_var_obj, params, args = (sigma), bounds = bnds, constraints = cons)
    
    return result.x
```

```{python}
params2 = min_var_optim(start, mu, sigma, target)
params2
```

```{python}
np.dot(mu, params2)
```

```{python}
np.sqrt(np.dot(params2, np.dot(sigma, params2))) 
```

## Maximize utility

$$
\begin{aligned}
\begin{array}{rrcl}
\displaystyle\min&\frac{1}{2}\delta(\mathbf{w}^{T}\Sigma\mathbf{w})-\mu^{T}\mathbf{w}\\
\textrm{s.t.}&\mathbf{w}^{T}e&=&1\\
\end{array}
\end{aligned}
$$

To incorporate these conditions into one equation, introduce new variables $\lambda_{i}$ that are the Lagrange multipliers and define a new function $\mathcal{L}$ as follows:

$$
\begin{aligned}
\mathcal{L}(\mathbf{w},\lambda)&=\frac{1}{2}\mathbf{w}^{T}\Sigma\mathbf{w}-\mu^{T}\mathbf{w}-\lambda_{1}(\mathbf{w}^{T}e-1)
\end{aligned}
$$

Then, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\lambda_{i}$:

$$
\begin{aligned}
\frac{\partial\mathcal{L}(\mathbf{w},\lambda)}{\partial w}&=\mathbf{w}\Sigma-\mu^{T}-\lambda_{1}e=0\\
\frac{\partial\mathcal{L}(\mathbf{w},\lambda)}{\partial \lambda_{1}}&=\mathbf{w}e^T-1=0
\end{aligned}
$$

Simplify the equations above in matrix form and solve for the Lagrange multipliers $\lambda_{i}$:

$$
\begin{aligned}
\begin{bmatrix}
\Sigma & e \\
e^{T} & 0
\end{bmatrix}
\begin{bmatrix}
\mathbf{w} \\
-\lambda_{1}
\end{bmatrix}
&=
\begin{bmatrix}
\mu^{T} \\
1
\end{bmatrix}
\\
\begin{bmatrix}
\mathbf{w} \\
-\lambda_{1}
\end{bmatrix}
&=
\begin{bmatrix}
\Sigma & e \\
e^{T} & 0
\end{bmatrix}^{-1}
\begin{bmatrix}
\mu^{T} \\
1
\end{bmatrix}
\end{aligned}
$$

```{python}
ir = 0.5
target = ir / 0.06 # ir / std (see Black-Litterman)
```

```{python}
def max_utility_obj(params, mu, sigma, target):
    
    result = 0.5 * target * (np.dot(params, np.dot(sigma, params))) - np.dot(mu, params)
    
    return result

def max_utility_optim(params, mu, sigma, target):
  
    cons = [{"type": "eq", "fun": lambda params: np.sum(params) - 1}]
    
    result = minimize(max_utility_obj, params, args = (mu, sigma, target), bounds = bnds,
                      constraints = cons) 
    
    return result.x
```

```{python}
params3 = max_utility_optim(start, mu, sigma, target)
params3
```

```{python}
np.dot(mu, params3)
```

```{python}
np.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))
```

## Minimize residual sum of squares

$$
\begin{aligned}
\begin{array}{rrcl}
\displaystyle\min&\frac{1}{2}\delta(\mathbf{w}^{T}X^{T}X\mathbf{w})-X^{T}y\mathbf{w}\\
\textrm{s.t.}&\mathbf{w}^{T}e&=&1\\
\end{array}
\end{aligned}
$$

To incorporate these conditions into one equation, introduce new variables $\lambda_{i}$ that are the Lagrange multipliers and define a new function $\mathcal{L}$ as follows:

$$
\begin{aligned}
\mathcal{L}(\mathbf{w},\lambda)&=\frac{1}{2}\mathbf{w}^{T}X^{T}X\mathbf{w}-X^{T}y\mathbf{w}-\lambda_{1}(\mathbf{w}^{T}e-1)
\end{aligned}
$$

Then, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\lambda_{i}$:

$$
\begin{aligned}
\frac{\partial\mathcal{L}(\mathbf{w},\lambda)}{\partial w}&=\mathbf{w}X^{T}X-X^{T}y-\lambda_{1}e=0\\
\frac{\partial\mathcal{L}(\mathbf{w},\lambda)}{\partial \lambda_{1}}&=\mathbf{w}e^T-1=0
\end{aligned}
$$

Simplify the equations above in matrix form and solve for the Lagrange multipliers $\lambda_{i}$:

$$
\begin{aligned}
\begin{bmatrix}
X^{T}X & e \\
e^{T} & 0
\end{bmatrix}
\begin{bmatrix}
\mathbf{w} \\
-\lambda_{1}
\end{bmatrix}
&=
\begin{bmatrix}
X^{T}y \\
1
\end{bmatrix}
\\
\begin{bmatrix}
\mathbf{w} \\
-\lambda_{1}
\end{bmatrix}
&=
\begin{bmatrix}
X^{T}X & e \\
e^{T} & 0
\end{bmatrix}^{-1}
\begin{bmatrix}
X^{T}y \\
1
\end{bmatrix}
\end{aligned}
$$

-   <https://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html>

```{python}
pd.DataFrame({
  "max_pnl": params1,
  "min_risk": params2,
  "max_ratio": params3
})
```
