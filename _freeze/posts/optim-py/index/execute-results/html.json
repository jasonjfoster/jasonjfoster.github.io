{
  "hash": "3be56ec8a96ecda24df0372ffd4f114d",
  "result": {
    "markdown": "---\ntitle: \"Optimization\"\nauthor: \"[Jason Foster](mailto:jason.j.foster@gmail.com)\"\ndate: last-modified\ncategories:\n  - analysis\n  - finance\n  - python\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n-   <https://pandas-datareader.readthedocs.io/en/latest/remote_data.html>\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nfrom scipy.optimize import minimize\n```\n:::\n\n\n-   Open: <https://github.com/pydata/pandas-datareader/issues/965>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\nprices_df = pdr.get_data_tiingo(tickers, start = \"1900-01-01\", api_key = os.getenv(\"TIINGO_API_KEY\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nC:\\Users\\jason\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas_datareader\\tiingo.py:234: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  return pd.concat(dfs, self._concat_axis)\n```\n:::\n\n```{.python .cell-code}\nprices_df = prices_df.pivot_table(index = \"date\", columns = \"symbol\", values = \"adjClose\") \\\n    .tz_localize(None)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_cols = list(zip([\"returns\"], tickers))\noverlap_cols = list(zip([\"overlap\"], tickers))\nreturns_df[returns_cols] = np.log(prices_df).diff()\nreturns_df[overlap_cols] = returns_df[returns_cols].rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df.sort_index(axis = 1, inplace = True)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# weights = np.array([0.9 ** i for i in range(width - 1, -1, -1)]).reshape((width, 1))\nweights = np.array([1] * width).reshape((width, 1))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\noverlap_df = returns_df.dropna()[\"overlap\"]\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors][-width:] # same dimension as `weights`\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers][-width:]\n```\n:::\n\n\n# Random weights\n\nNeed to generate uniformly distributed weights $\\mathbf{w}=(w_{1},w_{2},\\ldots,w_{N})$ such that $\\sum_{j=1}^{N}w_{i}=1$ and $w_{i}\\geq0$:\n\n-   **Approach 1**: tempting to use $w_{i}=\\frac{u_{i}}{\\sum_{j=1}^{N}u_{i}}$ where $u_{i}\\sim U(0,1)$ but the distribution of $\\mathbf{w}$ is not uniform\n\n-   **Approach 2**: instead, generate $\\text{Exp}(1)$ and then normalize\n\nCan also scale random weights by $M$, e.g. if sum of weights must be 10% then multiply weights by 10%.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights1(n_sim, n_assets):  \n    \n    rand_exp = np.matrix(np.random.uniform(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nn_assets = 3\nn_sim = 10000\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach1 = rand_weights1(n_sim, n_assets)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=384}\n:::\n:::\n\n\n**Approach 2**: uniform sample from the simplex (<http://mathoverflow.net/a/76258>) and then normalize\n\n-   If $z\\sim U(0,1)$ then $-\\ln(z)$ is an $\\text{Exp}(1)$ distribution\n\nThis is also known as generating a random vector from the symmetric Dirichlet distribution.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights2(n_sim, n_assets, lmbda):   \n    \n    # uses 'inverse transform sampling' method: <https://en.wikipedia.org/wiki/Inverse_transform_sampling>\n    rand_exp = np.matrix(-np.log(1 - np.random.uniform(size = (n_sim, n_assets))) / lmbda)\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlmbda = 1\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach2 = rand_weights2(n_sim, n_assets, lmbda)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=384}\n:::\n:::\n\n\n**Approach 3**: directly generate $\\text{Exp}(1)$ and then normalize\n\n-   If $x\\sim\\text{Exp}(1)$ then let $y_{i}=\\frac{x_{i}}{\\sum_{i=1}^{N}x_{i}}$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights3(n_sim, n_assets):\n    \n    rand_exp = np.matrix(np.random.exponential(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach3 = rand_weights3(n_sim, n_assets)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=384}\n:::\n:::\n\n\n## Random turnover\n\nHow to generate random weights between lower bound $a$ and upper bound $b$ that sum to zero?\n\n-   **Approach 1**: tempting to multiply random weights by $M$ and then subtract by $\\frac{M}{N}$ but the distribution is not between $a$ and $b$\n\n-   **Approach 2**: instead, use an iterative approach for random turnover:\n\n    1.  Generate $N-1$ uniformly distributed weights between $a$ and $b$\n    2.  For $u_{N}$ compute sum of values and subtract from $M$\n    3.  If $u_{N}$ is between $a$ and $b$, then keep; otherwise, discard\n\nThen add random turnover to previous period's random weights.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    plug = False\n    \n    while not plug:\n        \n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n        if ((temp <= upper) and (temp >= lower)):\n            plug = True\n        \n    result = np.append(result, temp)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights3(n_sim, n_assets) * rng\n    result = result - rng / n_assets\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n    \n    result = np.matrix(rand_iterative(n_assets, lower, upper, target))\n    \n    while result.shape[0] < n_sim:\n    \n        temp = np.matrix(rand_iterative(n_assets, lower, upper, target))\n        result = np.concatenate((result, temp), axis = 0)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlower = -0.05\nupper = 0.05\ntarget = 0\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-30-1.png){width=384}\n:::\n:::\n\n\n# Mean-variance\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef geometric_mean(x, scale):\n    \n    result = np.prod(1 + x) ** (scale / x.shape[1]) - 1\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_x_df = returns_df.dropna()[\"returns\"][factors] # REMOVE LATER\nreturns_x_mat = np.matrix(returns_x_df) # extended history # REMOVE LATER\nmu = np.apply_along_axis(geometric_mean, 0, returns_x_mat, scale[\"periods\"])\nsigma = np.cov(overlap_x_df.T, ddof = 1) * scale[\"periods\"] * scale[\"overlap\"]\n```\n:::\n\n\n## Maximize mean\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 0.06\nstart = np.array([1] * len(factors))\nbnds = [(np.finfo(float).eps, 1) for i in range(len(factors))]\ncons = [{\"type\": \"ineq\", \"fun\": lambda params, sigma, target: max_pnl_cons(params, sigma, target),\n         \"args\": (sigma, target)},\n        {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef max_pnl_cons(params, sigma, target):\n    \n    var = np.matmul(np.transpose(params), np.matmul(sigma, params))\n    \n    result = target ** 2 - var\n    \n    return result\n\ndef max_pnl_obj(params, mu):\n    \n    result = np.matmul(mu, params)\n    \n    return -result\n\ndef max_pnl_optim(params, mu):\n    \n    result = minimize(max_pnl_obj, params, args = (mu), bounds = bnds, constraints = cons)\n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams1 = max_pnl_optim(start, mu)\nparams1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([4.58738017e-01, 5.41261983e-01, 2.22044605e-16, 2.32859522e-16])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.matmul(mu, params1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.04181332070485769\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params1), np.matmul(sigma, params1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.06000001956736139\n```\n:::\n:::\n\n\n## Minimize variance\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min_{x}&\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 0.03\nstart = np.array([1] * len(factors))\ncons = [{\"type\": \"ineq\", \"fun\": lambda params, mu, target: min_risk_cons(params, mu, target),\n         \"args\": (mu, target)},\n        {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef min_risk_cons(params, mu, target):\n    \n    result = np.matmul(mu, params) - target\n    \n    return result\n\ndef min_risk_obj(params, sigma):\n    \n    result = np.matmul(np.transpose(params), np.matmul(sigma, params))\n    \n    return result\n\ndef min_risk_optim(params, sigma):\n    \n    result = minimize(min_risk_obj, params, args = (sigma), bounds = bnds, constraints = cons)\n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams2 = min_risk_optim(start, sigma)\nparams2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0.30240864, 0.52942142, 0.12462501, 0.04354492])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.matmul(mu, params2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.030000000044080797\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params2), np.matmul(sigma, params2))) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.04257030014439174\n```\n:::\n:::\n\n\n## Maximize utility\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}-\\frac{1}{2}\\delta(\\mathbf{w}^T\\Sigma\\mathbf{w})\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\nstart = np.array([1] * len(factors))\ncons = [{\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef max_ratio_obj(params, mu, sigma, target):\n    \n    result = np.matmul(mu, params) - 0.5 * target * (np.matmul(np.transpose(params),\n                                                               np.matmul(sigma, params)))\n#     result = np.matmul(mu, params) / np.sqrt(np.matmul(np.transpose(params), np.matmul(sigma, params)))\n    \n    return -result\n\ndef max_ratio_optim(params, mu, sigma, target):\n    \n    result = minimize(max_ratio_obj, params, args = (mu, sigma, target), bounds = bnds,\n                      constraints = cons) \n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams3 = max_ratio_optim(start, mu, sigma, target)\nparams3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([4.78449628e-01, 5.21550372e-01, 2.46005473e-16, 2.38090797e-16])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.matmul(mu, params3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.0430094200561693\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.06225230173496182\n```\n:::\n:::\n\n\n## Minimize residual sum of squares\n\n\n::: {.cell}\n\n```{.python .cell-code}\npd.DataFrame.from_dict({\"max_pnl\": params1,\n                        \"min_risk\": params2,\n                        \"max_ratio\": params3})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        max_pnl  min_risk     max_ratio\n0  4.587380e-01  0.302409  4.784496e-01\n1  5.412620e-01  0.529421  5.215504e-01\n2  2.220446e-16  0.124625  2.460055e-16\n3  2.328595e-16  0.043545  2.380908e-16\n```\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}