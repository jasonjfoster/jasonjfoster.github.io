{
  "hash": "cc9715994aff4ddbfa4917f16e14b2c2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Optimization\"\nauthor: \"[Jason Foster](mailto:jason.j.foster@gmail.com)\"\ndate: last-modified\ncategories:\n  - analysis\n  - finance\n  - python\n---\n\n::: {.cell}\n\n```{.python .cell-code}\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n# Random weights\n\nNeed to generate uniformly distributed weights $\\mathbf{w}=(w_{1},w_{2},\\ldots,w_{N})$ such that $\\sum_{j=1}^{N}w_{i}=1$ and $w_{i}\\geq0$:\n\n-   **Approach 1**: tempting to use $w_{i}=\\frac{u_{i}}{\\sum_{j=1}^{N}u_{i}}$ where $u_{i}\\sim U(0,1)$ but the distribution of $\\mathbf{w}$ is not uniform\n\n-   **Approach 2**: instead, generate $\\text{Exp}(1)$ and then normalize\n\nCan also scale random weights by $M$, e.g. if sum of weights must be 10% then multiply weights by 10%.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights1(n_sim, n_assets):  \n    \n    rand_exp = np.matrix(np.random.uniform(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nn_assets = 3\nn_sim = 10000\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach1 = rand_weights1(n_sim, n_assets)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=384}\n:::\n:::\n\n\n**Approach 2(a)**: uniform sample from the simplex (<http://mathoverflow.net/a/76258>) and then normalize\n\n-   If $u\\sim U(0,1)$ then $-\\ln(u)$ is an $\\text{Exp}(1)$ distribution\n\nThis is also known as generating a random vector from the symmetric Dirichlet distribution.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights2a(n_sim, n_assets, lmbda):   \n    \n    # inverse transform sampling: https://en.wikipedia.org/wiki/Inverse_transform_sampling\n    rand_exp = np.matrix(-np.log(1 - np.random.uniform(size = (n_sim, n_assets))) / lmbda)\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlmbda = 1\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach2a = rand_weights2a(n_sim, n_assets, lmbda)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=384}\n:::\n:::\n\n\n**Approach 2(b)**: directly generate $\\text{Exp}(1)$ and then normalize\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights2b(n_sim, n_assets):\n    \n    rand_exp = np.matrix(np.random.exponential(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach2b = rand_weights2b(n_sim, n_assets)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=384}\n:::\n:::\n\n\n## Random turnover\n\nHow to generate random weights between lower bound $a$ and upper bound $b$ that sum to zero?\n\n-   **Approach 1**: tempting to multiply random weights by $M$ and then subtract by $\\frac{M}{N}$ but the distribution is not between $a$ and $b$\n\n-   **Approach 2**: instead, use an iterative approach for random turnover:\n\n    1.  Generate $N-1$ uniformly distributed weights between $a$ and $b$\n    2.  For $u_{N}$ compute sum of values and subtract from $M$\n    3.  If $u_{N}$ is between $a$ and $b$, then keep; otherwise, discard\n\nThen add random turnover to previous period's random weights.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights2b(n_sim, n_assets) * rng\n    result = result - rng / n_assets\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlower = -0.05\nupper = 0.05\ntarget = 0\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n    temp = target - sum(result)\n    \n    while not ((temp <= upper) and (temp >= lower)):\n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n    result = np.append(result, temp)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n  \n    result_ls = []\n    \n    for i in range(n_sim):\n      \n      result_sim = rand_iterative(n_assets, lower, upper, target)\n      result_ls.append(result_sim)\n      \n    result = pd.DataFrame(result_ls)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=384}\n:::\n:::\n\n\n# Mean-variance\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport cvxpy as cp\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef geometric_mean(x, scale):\n    \n    result = np.prod(1 + x) ** (scale/ len(x)) - 1\n    \n    return result\n```\n:::\n\n\n-   <https://www.adrian.idv.hk/2021-06-22-kkt/>\n-   <https://or.stackexchange.com/a/3738>\n-   <https://bookdown.org/compfinezbook/introFinRbook/Portfolio-Theory-with-Matrix-Algebra.html#algorithm-for-computing-efficient-frontier>\n-   <https://palomar.home.ece.ust.hk/MAFS6010R_lectures/slides_robust_portfolio.html>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_x_df = returns_df.dropna()[\"returns\"][factors]\nmu = returns_x_df.apply(geometric_mean, axis = 0, scale = scale[\"periods\"])\nsigma = np.cov(overlap_x_df.T, ddof = 1) * scale[\"periods\"] * scale[\"overlap\"]\n```\n:::\n\n\n## Maximize mean\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce new variables $\\lambda_{i}$ that are the Lagrange multipliers and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n$$\n\nThen, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n$$\n\nSimplify the equations above in matrix form and solve for the Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 0.06\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef max_mean_optim(mu, sigma, target):\n    \n    params = cp.Variable(len(mu))\n    \n    obj = cp.Maximize(params.T @ mu)\n    \n    cons = [cp.sum(params) == 1, params >= 0,\n            cp.quad_form(params, sigma) <= target ** 2]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams1 = max_mean_optim(mu, sigma, target)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nC:\\Users\\jason\\AppData\\Local\\Programs\\Python\\PYTHON~1\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:336: FutureWarning: \n    Your problem is being solved with the ECOS solver by default. Starting in \n    CVXPY 1.5.0, Clarabel will be used as the default solver instead. To continue \n    using ECOS, specify the ECOS solver explicitly using the ``solver=cp.ECOS`` \n    argument to the ``problem.solve`` method.\n    \n  warnings.warn(ECOS_DEPRECATION_MSG, FutureWarning)\n```\n\n\n:::\n\n```{.python .cell-code}\nparams1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([4.96102333e-01, 5.03897655e-01, 7.45737846e-09, 5.14666729e-09])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(mu, params1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.04760141696513001\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.dot(params1, np.dot(sigma, params1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.05999999988605831\n```\n\n\n:::\n:::\n\n\n## Minimize variance\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce new variables $\\lambda_{i}$ that are the Lagrange multipliers and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n$$\n\nThen, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n$$\n\nSimplify the equations above in matrix form and solve for the Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 0.03\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef min_var_optim(mu, sigma, target):\n    \n    params = cp.Variable(len(mu))\n    \n    obj = cp.Minimize(cp.quad_form(params, sigma))\n    \n    cons = [cp.sum(params) == 1, params >= 0,\n            params.T @ mu >= target]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams2 = min_var_optim(mu, sigma, target)\nparams2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([2.91908162e-01, 4.49693442e-01, 2.58398396e-01, 1.80164000e-21])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(mu, params2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.030000000000000006\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.dot(params2, np.dot(sigma, params2))) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.03687458859894496\n```\n\n\n:::\n:::\n\n\n## Maximize utility\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce new variables $\\lambda_{i}$ that are the Lagrange multipliers and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n$$\n\nThen, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n$$\n\nSimplify the equations above in matrix form and solve for the Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef max_utility_optim(mu, sigma, target):\n    \n    params = cp.Variable(len(mu))\n    \n    obj = cp.Minimize(0.5 * target * cp.quad_form(params, sigma) - params.T @ mu)\n    \n    cons = [cp.sum(params) == 1, params >= 0]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams3 = max_utility_optim(mu, sigma, target)\nparams3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([ 5.54403648e-01,  4.45596352e-01, -7.37694377e-23, -1.40391711e-23])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(mu, params3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.05155607568648578\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.06678801818795001\n```\n\n\n:::\n:::\n\n\n## Minimize residual sum of squares\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce new variables $\\lambda_{i}$ that are the Lagrange multipliers and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n$$\n\nThen, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n$$\n\nSimplify the equations above in matrix form and solve for the Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n-   <https://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef min_rss_optim1(mu, sigma):\n    \n    params = cp.Variable(len(mu))\n    \n    obj = cp.Minimize(0.5 * cp.quad_form(params, sigma) - params.T @ mu)\n    \n    cons = [cp.sum(params) == 1, params >= 0]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams4 = min_rss_optim1(np.dot(overlap_x_df.T.values, overlap_y_df.values),\n                         np.dot(overlap_x_df.T.values, overlap_x_df.values))\nparams4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([ 3.74954959e-01, -8.42653692e-23,  6.25045041e-01,  2.43889802e-23])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(mu, params4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.030311313405483105\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params4), np.matmul(sigma, params4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.05050206095994644\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef min_rss_optim2(x, y):\n    \n    params = cp.Variable(x.shape[1])\n    \n    obj = cp.Minimize(cp.sum_squares(y - x @ params))\n    \n    cons = [cp.sum(params) == 1, params >= 0]\n    \n    prob = cp.Problem(obj, cons)\n    \n    prob.solve()\n    \n    return params.value\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams5 = min_rss_optim2(overlap_x_df.values, overlap_y_df.iloc[:, 0].values)\nparams5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([ 3.74954959e-01, -1.36488119e-20,  6.25045041e-01, -8.92709642e-21])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(mu, params5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.030311313405483188\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params5), np.matmul(sigma, params5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.05050206095994657\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npd.DataFrame({\n  \"max_pnl\": params1 * 100,\n  \"min_risk\": params2 * 100,\n  \"max_utility\": params3 * 100,\n  \"min_rss1\": params4 * 100,\n  \"min_rss2\": params5 * 100\n}).round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   max_pnl  min_risk  max_utility  min_rss1  min_rss2\n0    49.61     29.19        55.44      37.5      37.5\n1    50.39     44.97        44.56      -0.0      -0.0\n2     0.00     25.84        -0.00      62.5      62.5\n3     0.00      0.00        -0.00       0.0      -0.0\n```\n\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}