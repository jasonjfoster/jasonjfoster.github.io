{
  "hash": "a1bc0e3400c8baf8fd1ebe524094e5a4",
  "result": {
    "markdown": "---\ntitle: \"Optimization\"\nauthor: \"[Jason Foster](mailto:jason.j.foster@gmail.com)\"\ndate: last-modified\ncategories:\n  - analysis\n  - finance\n  - python\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n-   <https://pandas-datareader.readthedocs.io/en/latest/remote_data.html>\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nfrom scipy.optimize import minimize\n```\n:::\n\n\n-   Open: <https://github.com/pydata/pandas-datareader/issues/965>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\nprices_df = pdr.get_data_tiingo(tickers, start = \"1900-01-01\", api_key = os.getenv(\"TIINGO_API_KEY\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nC:\\Users\\jason\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas_datareader\\tiingo.py:234: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  return pd.concat(dfs, self._concat_axis)\n```\n:::\n\n```{.python .cell-code}\nprices_df = prices_df.pivot_table(index = \"date\", columns = \"symbol\", values = \"adjClose\") \\\n    .tz_localize(None)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_cols = list(zip([\"returns\"], tickers))\noverlap_cols = list(zip([\"overlap\"], tickers))\nreturns_df[returns_cols] = np.log(prices_df).diff()\nreturns_df[overlap_cols] = returns_df[returns_cols].rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df.sort_index(axis = 1, inplace = True)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# weights = np.array([0.9 ** i for i in range(width - 1, -1, -1)]).reshape((width, 1))\nweights = np.array([1] * width).reshape((width, 1))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\noverlap_df = returns_df.dropna()[\"overlap\"]\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors][-width:] # same dimension as `weights`\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers][-width:]\n```\n:::\n\n\n# Random weights\n\nNeed to generate uniformly distributed weights $\\mathbf{w}=(w_{1},w_{2},\\ldots,w_{N})$ such that $\\sum_{j=1}^{N}w_{i}=1$ and $w_{i}\\geq0$:\n\n-   **Approach 1**: tempting to use $w_{i}=\\frac{u_{i}}{\\sum_{j=1}^{N}u_{i}}$ where $u_{i}\\sim U(0,1)$ but the distribution of $\\mathbf{w}$ is not uniform\n\n-   **Approach 2**: instead, generate $\\text{Exp}(1)$ and then normalize\n\nCan also scale random weights by $M$, e.g. if sum of weights must be 10% then multiply weights by 10%.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights1(n_sim, n_assets):  \n    \n    rand_exp = np.matrix(np.random.uniform(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nn_assets = 3\nn_sim = 10000\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach1 = rand_weights1(n_sim, n_assets)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=384}\n:::\n:::\n\n\n**Approach 2(a)**: uniform sample from the simplex (<http://mathoverflow.net/a/76258>) and then normalize\n\n-   If $u\\sim U(0,1)$ then $-\\ln(u)$ is an $\\text{Exp}(1)$ distribution\n\nThis is also known as generating a random vector from the symmetric Dirichlet distribution.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights2a(n_sim, n_assets, lmbda):   \n    \n    # use 'inverse transform sampling' method: https://en.wikipedia.org/wiki/Inverse_transform_sampling\n    rand_exp = np.matrix(-np.log(1 - np.random.uniform(size = (n_sim, n_assets))) / lmbda)\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlmbda = 1\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach2a = rand_weights2a(n_sim, n_assets, lmbda)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=384}\n:::\n:::\n\n\n**Approach 2(b)**: directly generate $\\text{Exp}(1)$ and then normalize\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights2b(n_sim, n_assets):\n    \n    rand_exp = np.matrix(np.random.exponential(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach2b = rand_weights2b(n_sim, n_assets)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=384}\n:::\n:::\n\n\n## Random turnover\n\nHow to generate random weights between lower bound $a$ and upper bound $b$ that sum to zero?\n\n-   **Approach 1**: tempting to multiply random weights by $M$ and then subtract by $\\frac{M}{N}$ but the distribution is not between $a$ and $b$\n\n-   **Approach 2**: instead, use an iterative approach for random turnover:\n\n    1.  Generate $N-1$ uniformly distributed weights between $a$ and $b$\n    2.  For $u_{N}$ compute sum of values and subtract from $M$\n    3.  If $u_{N}$ is between $a$ and $b$, then keep; otherwise, discard\n\nThen add random turnover to previous period's random weights.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights2b(n_sim, n_assets) * rng\n    result = result - rng / n_assets\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlower = -0.05\nupper = 0.05\ntarget = 0\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n    temp = target - sum(result)\n    \n    while not ((temp <= upper) and (temp >= lower)):\n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n    result = np.append(result, temp)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n  \n    result_ls = []\n    \n    for i in range(n_sim):\n      \n      result_sim = rand_iterative(n_assets, lower, upper, target)\n      result_ls.append(result_sim)\n      \n    result = pd.DataFrame(result_ls)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-31-1.png){width=384}\n:::\n:::\n\n\n# Mean-variance\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef geometric_mean(x, scale):\n    \n    result = np.prod(1 + x) ** (scale/ len(x)) - 1\n    \n    return result\n```\n:::\n\n\n-   <https://www.adrian.idv.hk/2021-06-22-kkt/>\n-   <https://or.stackexchange.com/a/3738>\n-   <https://bookdown.org/compfinezbook/introFinRbook/Portfolio-Theory-with-Matrix-Algebra.html#algorithm-for-computing-efficient-frontier>\n-   <https://palomar.home.ece.ust.hk/MAFS6010R_lectures/slides_robust_portfolio.html>\n\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_x_df = returns_df.dropna()[\"returns\"][factors]\nmu = returns_x_df.apply(geometric_mean, axis = 0, scale = scale[\"periods\"])\nsigma = np.cov(overlap_x_df.T, ddof = 1) * scale[\"periods\"] * scale[\"overlap\"]\n```\n:::\n\n\n## Maximize mean\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce new variables $\\lambda_{i}$ that are the Lagrange multipliers and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n$$\n\nThen, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n$$\n\nSimplify the equations above in matrix form and solve for the Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 0.06\nstart = np.array([1] * len(factors))\nbnds = [(np.finfo(float).eps, 1) for i in range(len(factors))]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef max_mean_cons(params, sigma, target):\n    \n    var = np.dot(params, np.dot(sigma, params))\n    \n    result = target ** 2 - var\n    \n    return result\n\ndef max_mean_obj(params, mu):\n    \n    result = -np.dot(mu, params)\n    \n    return result\n\ndef max_mean_optim(params, mu):\n  \n    cons = [{\"type\": \"ineq\", \"fun\": max_mean_cons, \"args\": (sigma, target)},\n           {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n    \n    result = minimize(max_mean_obj, params, args = (mu), bounds = bnds, constraints = cons)\n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams1 = max_mean_optim(start, mu)\nparams1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([4.62578958e-01, 5.37421042e-01, 2.22044605e-16, 2.22044605e-16])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(mu, params1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.041570397509656645\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.dot(params1, np.dot(sigma, params1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.060000021858548906\n```\n:::\n:::\n\n\n## Minimize variance\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce new variables $\\lambda_{i}$ that are the Lagrange multipliers and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n$$\n\nThen, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n$$\n\nSimplify the equations above in matrix form and solve for the Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 0.03\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef min_var_cons(params, mu, target):\n    \n    result = np.dot(mu, params) - target\n    \n    return result\n\ndef min_var_obj(params, sigma):\n    \n    result = np.dot(params, np.dot(sigma, params))\n    \n    return result\n\ndef min_var_optim(params, sigma):\n  \n    cons = [{\"type\": \"ineq\", \"fun\": min_var_cons, \"args\": (mu, target)},\n            {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n    \n    result = minimize(min_var_obj, params, args = (sigma), bounds = bnds, constraints = cons)\n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams2 = min_var_optim(start, sigma)\nparams2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0.3075824 , 0.53278519, 0.12181692, 0.03781549])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(mu, params2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.030000000046054496\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.dot(params2, np.dot(sigma, params2))) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.04274960927285742\n```\n:::\n:::\n\n\n## Maximize utility\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce new variables $\\lambda_{i}$ that are the Lagrange multipliers and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n$$\n\nThen, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n$$\n\nSimplify the equations above in matrix form and solve for the Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef max_utility_obj(params, mu, sigma, target):\n    \n    result = 0.5 * target * (np.dot(params, np.dot(sigma, params))) - np.dot(mu, params)\n    \n    return result\n\ndef max_utility_optim(params, mu, sigma, target):\n  \n    cons = [{\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n    \n    result = minimize(max_utility_obj, params, args = (mu, sigma, target), bounds = bnds,\n                      constraints = cons) \n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams3 = max_utility_optim(start, mu, sigma, target)\nparams3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([4.80909376e-01, 5.19090624e-01, 4.00070602e-16, 3.48896259e-16])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(mu, params3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.04269044980275753\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.062117917640854114\n```\n:::\n:::\n\n\n## Minimize residual sum of squares\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce new variables $\\lambda_{i}$ that are the Lagrange multipliers and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n$$\n\nThen, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n$$\n\nSimplify the equations above in matrix form and solve for the Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\npd.DataFrame({\n  \"max_pnl\": params1,\n  \"min_risk\": params2,\n  \"max_ratio\": params3\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        max_pnl  min_risk     max_ratio\n0  4.625790e-01  0.307582  4.809094e-01\n1  5.374210e-01  0.532785  5.190906e-01\n2  2.220446e-16  0.121817  4.000706e-16\n3  2.220446e-16  0.037815  3.488963e-16\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}