{
  "hash": "10e7da5f7caa42063ab9b9990eebb710",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Optimization\"\nauthor: \"[Jason Foster](mailto:jason.j.foster@gmail.com)\"\ndate: last-modified\ncategories:\n  - analysis\n  - finance\n  - python\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n# Random weights\n\nNeed to generate uniformly distributed weights $\\mathbf{w}=(w_{1},w_{2},\\ldots,w_{N})$ such that $\\sum_{j=1}^{N}w_{i}=1$ and $w_{i}\\geq0$:\n\n-   **Approach 1**: tempting to use $w_{i}=\\frac{u_{i}}{\\sum_{j=1}^{N}u_{i}}$ where $u_{i}\\sim U(0,1)$ but the distribution of $\\mathbf{w}$ is not uniform\n\n-   **Approach 2**: instead, generate $\\text{Exp}(1)$ and then normalize\n\nCan also scale random weights by $M$, e.g. if sum of weights must be 10% then multiply weights by 10%.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights1(n_sim, n_assets):  \n    \n    rand_exp = np.matrix(np.random.uniform(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nn_assets = 3\nn_sim = 10000\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach1 = rand_weights1(n_sim, n_assets)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=384}\n:::\n:::\n\n\n**Approach 2(a)**: uniform sample from the simplex (<http://mathoverflow.net/a/76258>) and then normalize\n\n-   If $u\\sim U(0,1)$ then $-\\ln(u)$ is an $\\text{Exp}(1)$ distribution\n\nThis is also known as generating a random vector from the symmetric Dirichlet distribution.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights2a(n_sim, n_assets, lmbda):   \n    \n    # inverse transform sampling: https://en.wikipedia.org/wiki/Inverse_transform_sampling\n    rand_exp = np.matrix(-np.log(1 - np.random.uniform(size = (n_sim, n_assets))) / lmbda)\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlmbda = 1\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach2a = rand_weights2a(n_sim, n_assets, lmbda)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=384}\n:::\n:::\n\n\n**Approach 2(b)**: directly generate $\\text{Exp}(1)$ and then normalize\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights2b(n_sim, n_assets):\n    \n    rand_exp = np.matrix(np.random.exponential(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = rand_exp / rand_exp_sum\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach2b = rand_weights2b(n_sim, n_assets)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=384}\n:::\n:::\n\n\n## Random turnover\n\nHow to generate random weights between lower bound $a$ and upper bound $b$ that sum to zero?\n\n-   **Approach 1**: tempting to multiply random weights by $M$ and then subtract by $\\frac{M}{N}$ but the distribution is not between $a$ and $b$\n\n-   **Approach 2**: instead, use an iterative approach for random turnover:\n\n    1.  Generate $N-1$ uniformly distributed weights between $a$ and $b$\n    2.  For $u_{N}$ compute sum of values and subtract from $M$\n    3.  If $u_{N}$ is between $a$ and $b$, then keep; otherwise, discard\n\nThen add random turnover to previous period's random weights.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights2b(n_sim, n_assets) * rng\n    result = result - rng / n_assets\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlower = -0.05\nupper = 0.05\ntarget = 0\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n    temp = target - sum(result)\n    \n    while not ((temp <= upper) and (temp >= lower)):\n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n    result = np.append(result, temp)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n  \n    result_ls = []\n    \n    for i in range(n_sim):\n      \n      result_sim = rand_iterative(n_assets, lower, upper, target)\n      result_ls.append(result_sim)\n      \n    result = pd.DataFrame(result_ls)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=384}\n:::\n:::\n\n\n# Mean-variance\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.optimize import minimize\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef geometric_mean(x, scale):\n    \n    result = np.prod(1 + x) ** (scale/ len(x)) - 1\n    \n    return result\n```\n:::\n\n\n-   <https://www.adrian.idv.hk/2021-06-22-kkt/>\n-   <https://or.stackexchange.com/a/3738>\n-   <https://bookdown.org/compfinezbook/introFinRbook/Portfolio-Theory-with-Matrix-Algebra.html#algorithm-for-computing-efficient-frontier>\n-   <https://palomar.home.ece.ust.hk/MAFS6010R_lectures/slides_robust_portfolio.html>\n\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_x_df = returns_df.dropna()[\"returns\"][factors]\nmu = returns_x_df.apply(geometric_mean, axis = 0, scale = scale[\"periods\"])\nsigma = np.cov(overlap_x_df.T, ddof = 1) * scale[\"periods\"] * scale[\"overlap\"]\n```\n:::\n\n\n## Maximize mean\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&-\\mathbf{w}^{T}\\mu\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce new variables $\\lambda_{i}$ that are the Lagrange multipliers and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=-\\mathbf{w}^{T}\\mu-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n$$\n\nThen, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=-\\mu-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n$$\n\nSimplify the equations above in matrix form and solve for the Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n-\\mu & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 0.06\nstart = np.array([1] * len(factors))\nbnds = [(np.finfo(float).eps, 1) for i in range(len(factors))]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef max_mean_cons(params, sigma, target):\n    \n    var = np.dot(params, np.dot(sigma, params))\n    \n    result = target ** 2 - var\n    \n    return result\n\ndef max_mean_obj(params, mu):\n    \n    result = -np.dot(mu, params)\n    \n    return result\n\ndef max_mean_optim(params, mu, sigma, target):\n  \n    cons = [{\"type\": \"ineq\", \"fun\": max_mean_cons, \"args\": (sigma, target)},\n           {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n    \n    result = minimize(max_mean_obj, params, args = (mu), bounds = bnds, constraints = cons)\n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams1 = max_mean_optim(start, mu, sigma, target)\nparams1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([4.92048551e-01, 5.07951449e-01, 2.22044605e-16, 2.76986550e-16])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(mu, params1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.05173107727195373\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.dot(params1, np.dot(sigma, params1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.06000001529162513\n```\n\n\n:::\n:::\n\n\n## Minimize variance\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce new variables $\\lambda_{i}$ that are the Lagrange multipliers and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n$$\n\nThen, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n$$\n\nSimplify the equations above in matrix form and solve for the Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 0.03\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef min_var_cons(params, mu, target):\n    \n    result = np.dot(mu, params) - target\n    \n    return result\n\ndef min_var_obj(params, sigma):\n    \n    result = np.dot(params, np.dot(sigma, params))\n    \n    return result\n\ndef min_var_optim(params, mu, sigma, target):\n  \n    cons = [{\"type\": \"ineq\", \"fun\": min_var_cons, \"args\": (mu, target)},\n            {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n    \n    result = minimize(min_var_obj, params, args = (sigma), bounds = bnds, constraints = cons)\n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams2 = min_var_optim(start, mu, sigma, target)\nparams2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([0.26444216, 0.45554317, 0.16964013, 0.11037455])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(mu, params2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.029999999974819183\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.dot(params2, np.dot(sigma, params2))) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.035144783508269446\n```\n\n\n:::\n:::\n\n\n## Maximize utility\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}\\Sigma\\mathbf{w})-\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce new variables $\\lambda_{i}$ that are the Lagrange multipliers and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}\\Sigma\\mathbf{w}-\\mu^{T}\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n$$\n\nThen, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}\\Sigma-\\mu^{T}-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n$$\n\nSimplify the equations above in matrix form and solve for the Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\Sigma & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\mu^{T} \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef max_utility_obj(params, mu, sigma, target):\n    \n    result = 0.5 * target * (np.dot(params, np.dot(sigma, params))) - np.dot(mu, params)\n    \n    return result\n\ndef max_utility_optim(params, mu, sigma, target):\n  \n    cons = [{\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n    \n    result = minimize(max_utility_obj, params, args = (mu, sigma, target), bounds = bnds,\n                      constraints = cons) \n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams3 = max_utility_optim(start, mu, sigma, target)\nparams3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([5.70840700e-01, 4.29159300e-01, 2.22044605e-16, 2.53703308e-16])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.dot(mu, params3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.05797878717218443\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.0701082063353154\n```\n\n\n:::\n:::\n\n\n## Minimize residual sum of squares\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min&\\frac{1}{2}\\delta(\\mathbf{w}^{T}X^{T}X\\mathbf{w})-X^{T}y\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^{T}e&=&1\\\\\n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce new variables $\\lambda_{i}$ that are the Lagrange multipliers and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\frac{1}{2}\\mathbf{w}^{T}X^{T}X\\mathbf{w}-X^{T}y\\mathbf{w}-\\lambda_{1}(\\mathbf{w}^{T}e-1)\n\\end{aligned}\n$$\n\nThen, to minimize this function, take derivatives with respect to $w$ and Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w}&=\\mathbf{w}X^{T}X-X^{T}y-\\lambda_{1}e=0\\\\\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial \\lambda_{1}}&=\\mathbf{w}e^T-1=0\n\\end{aligned}\n$$\n\nSimplify the equations above in matrix form and solve for the Lagrange multipliers $\\lambda_{i}$:\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n\\mathbf{w} \\\\\n-\\lambda_{1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nX^{T}X & e \\\\\ne^{T} & 0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\nX^{T}y \\\\\n1\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n-   <https://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html>\n\n\n::: {.cell}\n\n```{.python .cell-code}\npd.DataFrame({\n  \"max_pnl\": params1,\n  \"min_risk\": params2,\n  \"max_ratio\": params3\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        max_pnl  min_risk     max_ratio\n0  4.920486e-01  0.264442  5.708407e-01\n1  5.079514e-01  0.455543  4.291593e-01\n2  2.220446e-16  0.169640  2.220446e-16\n3  2.769866e-16  0.110375  2.537033e-16\n```\n\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}