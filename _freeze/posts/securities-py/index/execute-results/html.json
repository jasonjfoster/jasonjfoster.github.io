{
  "hash": "540cb783610ce188ddbb9b2980322a83",
  "result": {
    "markdown": "---\ntitle: \"Securities\"\nauthor: \"[Jason Foster](mailto:jason.j.foster@gmail.com)\"\ndate: last-modified\ncategories:\n  - analysis\n  - finance\n  - python\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n-   <https://pandas-datareader.readthedocs.io/en/latest/remote_data.html>\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport datetime\nfrom scipy.optimize import minimize\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nsd_df = returns_df[\"overlap\"].rolling(width, min_periods = 1).std() * \\\n    np.sqrt(scale[\"periods\"]) * np.sqrt(scale[\"overlap\"])\n```\n:::\n\n\n# Black-Scholes model\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef level_shock(shock, S, tau, sigma):\n    \n    result = S * (1 + shock * sigma * np.sqrt(tau))\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# https://en.wikipedia.org/wiki/Greeks_(finance)\n# https://www.wolframalpha.com/input/?i=option+pricing+formula\nfactor = \"SP500\"\ntypes = [\"call\", \"put\"]\nS = levels_df.fillna(method = \"ffill\")[factor][-1]\nK = S\nr = 0 # use \"USD3MTD156N\"\nq = 0 # see https://stackoverflow.com/a/11286679 \ntau = 1 # = 252 / 252\nsigma = sd_df[factor][-1] # use \"VIXCLS\"\nshocks = [x / 2 for x in range(-6, 7)]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngreeks_df = pd.DataFrame([(x, y) for x in types for y in shocks], \n    columns = [\"type\", \"shock\"])\ngreeks_df[\"spot\"] = level_shock(greeks_df[\"shock\"], S, tau, sigma)\n```\n:::\n\n\n## Value\n\nFor a given spot price $S$, strike price $K$, risk-free rate $r$, annual dividend yield $q$, time-to-maturity $\\tau = T - t$, and volatility $\\sigma$:\n\n$$\n\\begin{aligned}\nV_{c}&=Se^{-q\\tau}\\Phi(d_{1})-e^{-r\\tau}K\\Phi(d_{2}) \\\\\nV_{p}&=e^{-r\\tau}K\\Phi(-d_{2})-Se^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    if (type == \"call\"):\n        result =  S * np.exp(-q * tau) * Phi(d1) - np.exp(-r * tau) * K * Phi(d2)\n    elif (type == \"put\"):\n        result = np.exp(-r * tau) * K * Phi(-d2) - S * np.exp(-q * tau) * Phi(-d1)\n        \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef bs_value(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    r_df = np.exp(-r * tau)\n    q_df = np.exp(-q * tau)\n    \n    call_value = S * q_df * Phi(d1) - r_df * K * Phi(d2)\n    put_value = r_df * K * Phi(-d2) - S * q_df * Phi(-d1)\n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n```\n:::\n\n\nwhere\n\n$$\n\\begin{aligned}\nd_{1}&={\\frac{\\ln(S/K)+(r-q+\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}} \\\\\nd_{2}&={\\frac{\\ln(S/K)+(r-q-\\sigma^{2}/2)\\tau}{\\sigma{\\sqrt{\\tau}}}}=d_{1}-\\sigma{\\sqrt{\\tau}} \\\\\n\\phi(x)&={\\frac{e^{-{\\frac {x^{2}}{2}}}}{\\sqrt{2\\pi}}} \\\\\n\\Phi(x)&={\\frac{1}{\\sqrt{2\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{y^{2}}{2}}}dy=1-{\\frac{1}{\\sqrt{2\\pi}}}\\int_{x}^{\\infty}e^{-{\\frac{y^{2}}{2}}dy}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef bs_d1(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q + sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n\ndef bs_d2(S, K, r, q, tau, sigma):\n    \n    result = (np.log(S / K) + (r - q - sigma ** 2 / 2) * tau) / (sigma * np.sqrt(tau))\n    \n    return result\n    \ndef phi(x):\n    \n    result = norm.pdf(x)\n    \n    return result\n\ndef Phi(x):\n    \n    result = norm.cdf(x)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngreeks_df[\"d1\"] = bs_d1(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"d2\"] = bs_d2(greeks_df[\"spot\"], K, r, q, tau, sigma)\ngreeks_df[\"value\"] = bs_value(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n```\n:::\n\n\n## First-order\n\n### Delta\n\n$$\n\\begin{aligned}\n\\Delta_{c}&={\\frac{\\partial V_{c}}{\\partial S}}=e^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Delta_{p}&={\\frac{\\partial V_{p}}{\\partial S}}=-e^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef bs_delta(type, S, K, r, q, tau, sigma, d1, d2):\n  \n    q_df = np.exp(-q * tau)\n    \n    call_value = q_df * Phi(d1)\n    put_value = -q_df * Phi(-d1)\n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngreeks_df[\"delta\"] = bs_delta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n```\n:::\n\n\n### Delta-beta\n\nNotional market value is the market value of a leveraged position:\n\n$$\n\\begin{aligned}\n\\text{Equity options }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\\\\n\\text{Delta-adjusted }=&\\,\\#\\text{ contracts}\\times\\text{multiple}\\times\\text{spot price}\\times\\text{delta}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# https://en.wikipedia.org/wiki/Notional_amount\ndef bs_delta_diff(type, S, K, r, q, tau, sigma, delta0):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    call_value = delta - delta0\n    put_value = delta0 - delta\n    \n    result = np.where(type == \"call\", call_value, put_value)\n        \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nbeta = 0.35\ntype = \"call\"\nn = 1\nmultiple = 100\ntotal = 1000000\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\nsec = {\n    \"n\": n,\n    \"multiple\": multiple,\n    \"S\": S,\n    \"delta\": bs_delta(type, S, K, r, q, tau, sigma, d1, d2),\n    \"beta\": 1\n}\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nbeta_df = pd.DataFrame([(x, y) for x in types for y in shocks], \n    columns = [\"type\", \"shock\"])\nbeta_df[\"spot\"] = level_shock(beta_df[\"shock\"], S, tau, sigma)\nbeta_df[\"static\"] = beta\nbeta_df[\"diff\"] = bs_delta_diff(type, beta_df[\"spot\"], K, r, q, tau, sigma, sec[\"delta\"])\nbeta_df[\"dynamic\"] = beta + sec[\"n\"] * sec[\"multiple\"] * sec[\"S\"] * sec[\"beta\"] * beta_df[\"diff\"] / total\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=384}\n:::\n:::\n\n\nFor completeness, duration equivalent is defined as:\n\n$$\n\\begin{aligned}\n\\text{10-year equivalent }=\\,&\\frac{\\text{security duration}}{\\text{10-year OTR duration}}\n\\end{aligned}\n$$\n\n### Vega\n\n$$\n\\begin{aligned}\n\\nu_{c,p}&={\\frac{\\partial V_{c,p}}{\\partial\\sigma}}=Se^{-q\\tau}\\phi(d_{1}){\\sqrt{\\tau}}=Ke^{-r\\tau}\\phi(d_{2}){\\sqrt{\\tau}}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef bs_vega(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    q_df = np.exp(-q * tau)\n    result = S * q_df * phi(d1) * np.sqrt(tau)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngreeks_df[\"vega\"] = bs_vega(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                            greeks_df[\"d1\"], greeks_df[\"d2\"]) \n```\n:::\n\n\n### Theta\n\n$$\n\\begin{aligned}\n\\Theta_{c}&=-{\\frac{\\partial V_{c}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}-rKe^{-r\\tau}\\Phi(d_{2})+qSe^{-q\\tau}\\Phi(d_{1}) \\\\\n\\Theta_{p}&=-{\\frac{\\partial V_{p}}{\\partial \\tau}}=-e^{-q\\tau}{\\frac{S\\phi(d_{1})\\sigma}{2{\\sqrt{\\tau}}}}+rKe^{-r\\tau}\\Phi(-d_{2})-qSe^{-q\\tau}\\Phi(-d_{1})\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef bs_theta(type, S, K, r, q, tau, sigma, d1, d2):\n    \n    r_df = np.exp(-r * tau)\n    q_df = np.exp(-q * tau)\n    \n    call_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) - \\\n        r * K * r_df * Phi(d2) + q * S * q_df * Phi(d1)\n        \n    put_value = -q_df * S * phi(d1) * sigma / (2 * np.sqrt(tau)) + \\\n        r * K * r_df * Phi(-d2) - q * S * q_df * Phi(-d1)\n        \n    result = np.where(type == \"call\", call_value, put_value)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngreeks_df[\"theta\"] = bs_theta(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n```\n:::\n\n\n## Second-order\n\n### Gamma\n\n$$\n\\begin{aligned}\n\\Gamma_{c,p}&={\\frac{\\partial\\Delta_{c,p}}{\\partial S}}={\\frac{\\partial^{2}V_{c,p}}{\\partial S^{2}}}=e^{-q\\tau}{\\frac{\\phi(d_{1})}{S\\sigma{\\sqrt{\\tau}}}}=Ke^{-r\\tau}{\\frac{\\phi(d_{2})}{S^{2}\\sigma{\\sqrt{\\tau}}}}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef bs_gamma(type, S, K, r, q, tau, sigma, d1, d2):\n  \n    q_df = np.exp(-q * tau)\n    \n    result = q_df * phi(d1) / (S * sigma * np.sqrt(tau))\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngreeks_df[\"gamma\"] = bs_gamma(greeks_df[\"type\"], greeks_df[\"spot\"], K, r, q, tau, sigma,\n                              greeks_df[\"d1\"], greeks_df[\"d2\"]) \n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-30-1.png){width=960}\n:::\n:::\n\n\n# Taylor series\n\n## First-order\n\n### Price-yield formula\n\nFor a function of one variable, $f(x)$, the Taylor series formula is:\n\n$$\n\\begin{aligned}\nf(x+\\Delta x)&=f(x)+{\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\\\\\nf(x+\\Delta x)-f(x)&={\\frac{f'(x)}{1!}}\\Delta x+{\\frac{f''(x)}{2!}}(\\Delta x)^{2}+{\\frac{f^{(3)}(x)}{3!}}(\\Delta x)^{3}+\\cdots+{\\frac{f^{(n)}(x)}{n!}}(\\Delta x)^{n}+\\cdots\n\\end{aligned}\n$$\n\nUsing the price-yield formula, the estimated percentage change in price for a change in yield is:\n\n$$\n\\begin{aligned}\nP(y+\\Delta y)-P(y)&\\approx{\\frac{P'(y)}{1!}}\\Delta y+{\\frac{P''(y)}{2!}}(\\Delta y)^{2}\\\\\n&\\approx -D\\Delta y +{\\frac{C}{2!}}(\\Delta y)^{2}\n\\end{aligned}\n$$\n\nBecause of market conventions, use the following formula: $P(y+\\Delta y)-P(y)\\approx -D\\Delta y +{\\frac{C\\times 100}{2!}}(\\Delta y)^{2}$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef pnl_bond(duration, convexity, dy):\n    \n    duration_pnl = -duration * dy\n    convexity_pnl = (convexity * 100 / 2) * dy ** 2\n    income_pnl = dy\n    \n    result = pd.DataFrame({\"total\": duration_pnl + convexity_pnl + income_pnl,\n                           \"duration\": duration_pnl,\n                           \"convexity\": convexity_pnl,\n                           \"income\": income_pnl})\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# https://engineering.nyu.edu/sites/default/files/2021-07/CarWuRF2021.pdf\n# https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118267967.app1\n# https://www.investopedia.com/terms/c/convexity-adjustment.asp\nfactor = \"DGS10\"\nduration = 6.5\nconvexity = 0.65\ny = levels_df.fillna(method = \"ffill\")[factor][-width]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nbond_df = pd.DataFrame({\"duration\": duration, \"convexity\": convexity,\n    \"dy\": (levels_df.fillna(method = \"ffill\")[factor][-width:] - y) / 100})\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nattrib_df = pnl_bond(bond_df[\"duration\"], bond_df[\"convexity\"], bond_df[\"dy\"])\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-35-1.png){width=576}\n:::\n:::\n\n\n### Duration-yield formula\n\nThe derivative of duration with respect to interest rates gives:\n\n$$\n\\begin{aligned}\n\\text{Drift}&=-\\frac{\\partial D}{\\partial y}\\\\\n&=\\frac{1}{P}\\frac{\\partial^{2}P}{\\partial y^{2}}-\\frac{1}{P^{2}}\\frac{\\partial P}{\\partial y}\\frac{\\partial P}{\\partial y}\\\\\n&=C-D^{2}\n\\end{aligned}\n$$\n\nBecause of market conventions, use the following formula: $\\text{Drift}=\\frac{1}{100}\\left(C\\times 100-D^{2}\\right)=C-\\frac{D^{2}}{100}$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef yield_shock(shock, tau, sigma):\n    \n    result = shock * sigma * np.sqrt(tau)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef duration_drift(duration, convexity, dy):\n    \n    drift = convexity - duration ** 2 / 100\n    change = -drift * dy * 100\n    \n    result = {\"drift\": drift,\n              \"change\": change}\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# \"Risk Management: Approaches for Fixed Income Markets\" (page 45)\nfactor = \"DGS10\"\nsigma = sd_df[factor][-1]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nduration_df = pd.DataFrame(shocks).rename(columns = {0: \"shock\"})\nduration_df[\"spot\"] = duration_df[\"shock\"].apply(yield_shock, args = (tau, sigma))\nduration_df[\"static\"] = duration\nduration_df[\"dynamic\"] = duration_df \\\n    .apply(lambda x: duration +\n           duration_drift(duration, convexity, x[\"spot\"])[\"change\"], axis = 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nduration_mlt <- as.data.table(py$duration_df)[ , \"spot\" := NULL]\nduration_mlt <- melt(duration_mlt, id.vars = \"shock\")\nduration_plt <- plot_scen(duration_mlt, title = \"Duration\", xlab = \"Shock\")\nprint(duration_plt)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-40-1.png){width=384}\n:::\n:::\n\n\n## Second-order\n\n### Black's formula\n\nA similar formula holds for functions of several variables $f(x_{1},\\ldots,x_{n})$. This is usually written as:\n\n$$\n\\begin{aligned}\nf(x_{1}+\\Delta x_{1},\\ldots,x_{n}+\\Delta x_{n})&=f(x_{1},\\ldots, x_{n})+ \\sum _{j=1}^{n}{\\frac{\\partial f(x_{1},\\ldots,x_{n})}{\\partial x_{j}}}(\\Delta x_{j})\\\\\n&+{\\frac {1}{2!}}\\sum_{j=1}^{n}\\sum_{k=1}^{n}{\\frac{\\partial^{2}f(x_{1},\\ldots,x_{d})}{\\partial x_{j}\\partial x_{k}}}(\\Delta x_{j})(\\Delta x_{k})+\\cdots\n\\end{aligned}\n$$\n\nUsing Black's formula, the estimated change of an option price is:\n\n$$\n\\begin{aligned}\nV(S+\\Delta S,\\sigma+\\Delta\\sigma,t+\\Delta t)-V(S,\\sigma,t)&\\approx{\\frac{\\partial V}{\\partial S}}\\Delta S+{\\frac{1}{2!}}{\\frac{\\partial^{2}V}{\\partial S^{2}}}(\\Delta S)^{2}+{\\frac{\\partial V}{\\partial \\sigma}}\\Delta\\sigma+{\\frac{\\partial V}{\\partial t}}\\Delta t\\\\\n&\\approx \\Delta_{c,p}\\Delta S+{\\frac{1}{2!}}\\Gamma_{c,p}(\\Delta S)^{2}+\\nu_{c,p}\\Delta\\sigma+\\Theta_{c,p}\\Delta t\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# https://quant-next.com/option-greeks-and-pl-decomposition-part-1/\ndef pnl_option(type, S, K, r, q, tau, sigma, dS, dt, dsigma):\n    \n    d1 = bs_d1(S, K, r, q, tau, sigma)\n    d2 = bs_d2(S, K, r, q, tau, sigma)\n    value = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\n    delta = bs_delta(type, S, K, r, q, tau, sigma, d1, d2)\n    vega = bs_vega(type, S, K, r, q, tau, sigma, d1, d2)\n    theta = bs_theta(type, S, K, r, q, tau, sigma, d1, d2)\n    gamma = bs_gamma(type, S, K, r, q, tau, sigma, d1, d2)\n    \n    delta_pnl = delta * dS / value\n    gamma_pnl = gamma / 2 * dS ** 2 / value\n    vega_pnl = vega * dsigma / value\n    theta_pnl = theta * dt / value\n    \n    result = {\"total\": delta_pnl + gamma_pnl + vega_pnl + theta_pnl,\n              \"delta\": delta_pnl,\n              \"gamma\": gamma_pnl,\n              \"vega\": vega_pnl,\n              \"theta\": theta_pnl}\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfactor = \"SP500\"\ntype = \"call\"\nS = levels_df.fillna(method = \"ffill\")[factor][-width]\nK = S # * (1 + 0.05)\ntau = 1 # = 252 / 252\nsigma = sd_df[factor][-width]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\noptions_df = pd.concat(dict(spot = levels_df.fillna(method = \"ffill\")[factor][-width:],\n                            sigma = sd_df[factor][-width:]), axis = 1)\noptions_df[\"dS\"] = options_df[\"spot\"] - S\noptions_df[\"dt_diff\"] = (options_df.index - options_df.index[0]).days\noptions_df[\"dt\"] = options_df[\"dt_diff\"] / options_df[\"dt_diff\"][-1]\noptions_df[\"dsigma\"] = options_df[\"sigma\"] - sigma\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nattrib_cols = [\"total\", \"delta\", \"gamma\", \"vega\", \"theta\"]\nattrib_df = options_df.apply(lambda x: pnl_option(type, S, K, r, q, tau, sigma,\n                                                  x[\"dS\"], x[\"dt\"], x[\"dsigma\"]), axis = 1)\nattrib_df = pd.DataFrame.from_records(attrib_df, index = attrib_df.index)\nattrib_df = attrib_df[attrib_cols]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nattrib_mlt <- melt(as.data.table(py$attrib_df, keep.rownames = \"index\"), id.vars = \"index\")\nattrib_mlt[ , index := as.Date(index)]\nattrib_plt <- plot_ts_decomp(attrib_mlt, decomp = \"Total\", title = \"Attribution 1Y (%)\")\nprint(attrib_plt)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-45-1.png){width=576}\n:::\n:::\n\n\n### Ito's lemma\n\nFor a given diffiusion $X(t, w)$ driven by:\n\n$$\n\\begin{aligned}\ndX_{t}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n$$\n\nThen proceed with the Taylor series for a function of two variables $f(t,x)$:\n\n$$\n\\begin{aligned}\ndf&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}dx+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}dx^{2}\\\\\n&={\\frac{\\partial f}{\\partial t}}dt+{\\frac{\\partial f}{\\partial x}}(\\mu_{t}dt+\\sigma_{t}dB_{t})+{\\frac{1}{2}}{\\frac{\\partial^{2}f}{\\partial x^{2}}}\\left(\\mu_{t}^{2}dt^{2}+2\\mu_{t}\\sigma _{t}dtdB_{t}+\\sigma_{t}^{2}dB_{t}^{2}\\right)\\\\\n&=\\left({\\frac{\\partial f}{\\partial t}}+\\mu_{t}{\\frac{\\partial f}{\\partial x}}+{\\frac{\\sigma _{t}^{2}}{2}}{\\frac{\\partial ^{2}f}{\\partial x^{2}}}\\right)dt+\\sigma_{t}{\\frac{\\partial f}{\\partial x}}dB_{t}\n\\end{aligned}\n$$\n\nNote: set the $dt^{2}$ and $dtdB_{t}$ terms to zero and substitute $dt$ for $dB^{2}$.\n\n### Geometric Brownian motion\n\nThe most common application of Ito's lemma in finance is to start with the percent change of an asset:\n\n$$\n\\begin{aligned}\n\\frac{dS}{S}&=\\mu_{t}dt+\\sigma_{t}dB_{t}\n\\end{aligned}\n$$\n\nThen apply Ito's lemma with $f(S)=log(S)$:\n\n$$\n\\begin{aligned}\nd\\log(S)&=f^{\\prime}(S)dS+{\\frac{1}{2}}f^{\\prime\\prime}(S)S^{2}\\sigma^{2}dt\\\\\n&={\\frac {1}{S}}\\left(\\sigma SdB+\\mu Sdt\\right)-{\\frac{1}{2}}\\sigma^{2}dt\\\\\n&=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n$$\n\nIt follows that:\n\n$$\n\\begin{aligned}\n\\log(S_{t})-\\log(S_{0})=\\sigma dB+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)dt\n\\end{aligned}\n$$\n\nExponentiating gives the expression for $S$:\n\n$$\n\\begin{aligned}\nS_{t}=S_{0}\\exp\\left(\\sigma B_{t}+\\left(\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right)t\\right)\n\\end{aligned}\n$$\n\nThis provides a recursive procedure for simulating values of $S$ at $t_{0}<t_{1}<\\cdots<t_{n}$:\n\n$$\n\\begin{aligned}\nS(t_{i+1})&=S(t_{i})\\exp\\left(\\sigma\\sqrt{t_{i+1}-t_{i}}Z_{i+1}+\\left[\\mu-{\\tfrac{\\sigma^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n$$\n\nwhere $Z_{1},Z_{2},\\ldots,Z_{n}$ are independent standard normals.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef sim_gbm(n_sim, S, mu, sigma, dt):\n    \n    result = S * np.exp(np.cumsum(sigma * np.sqrt(dt) * np.random.normal(size = n_sim)) + \\\n                        (mu - 0.5 * sigma ** 2) * dt)\n    \n    return result\n```\n:::\n\n\nThis leads to an algorithm for simulating a multidimensional geometric Brownian motion:\n\n$$\n\\begin{aligned}\nS_{k}(t_{i+1})&=S_{k}(t_{i})\\exp\\left(\\sqrt{t_{i+1}-t_{i}}\\sum_{j=1}^{d}{A_{kj}Z_{i+1,j}}+\\left[\\mu_{k}-{\\tfrac{\\sigma_{k}^{2}}{2}}\\right]\\left(t_{i+1}-t_{i}\\right)\\right)\n\\end{aligned}\n$$\n\nwhere $A$ is the Cholesky factor of $\\Sigma$, i.e. $A$ is any matrix for which $AA^\\mathrm{T}=\\Sigma$.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef sim_multi_gbm(n_sim, S, mu, sigma, dt):\n    \n    n_cols = sigma.shape[1]\n    \n    Z = np.random.normal(size = n_sim * n_cols).reshape((n_sim, n_cols))\n    X = np.add(np.sqrt(dt) * np.matmul(np.matrix(Z), np.linalg.cholesky(sigma).T),\n               ((mu - 0.5 * np.diag(sigma)) * dt))\n    \n    result = np.multiply(S, np.exp(X.cumsum(axis = 0)))\n    \n    return np.asmatrix(result)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# https://arxiv.org/pdf/0812.4210.pdf\n# https://quant.stackexchange.com/questions/15219/calibration-of-a-gbm-what-should-dt-be\n# https://stackoverflow.com/questions/36463227/geometrical-brownian-motion-simulation-in-r\n# https://quant.stackexchange.com/questions/25219/simulate-correlated-geometric-brownian-motion-in-the-r-programming-language\n# https://quant.stackexchange.com/questions/35194/estimating-the-historical-drift-and-volatility/\nS = [1] * len(factors)\nsigma = np.cov(returns_df[\"returns\"].dropna().T, ddof = 1) * scale[\"periods\"]\nmu = returns_df[\"returns\"].dropna().mean().tolist()\nmu = [x * scale[\"periods\"] for x in mu]\nmu = mu + np.diag(sigma) / 2 # drift\ndt = 1 / scale[\"periods\"]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmu_df = pd.DataFrame()\nsigma_df = pd.DataFrame()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfor i in range(1000):\n    \n    # assumes stock prices\n    levels_sim = pd.DataFrame(sim_multi_gbm(width + 1, S, mu, sigma, dt))\n    returns_sim = np.log(levels_sim).diff().dropna()\n    \n    mu_sim = returns_sim.mean() * scale[\"periods\"]\n    sigma_sim = returns_sim.std() * np.sqrt(scale[\"periods\"])\n    \n    mu_df = mu_df.append(pd.DataFrame(mu_sim).T)\n    sigma_df = sigma_df.append(pd.DataFrame(sigma_sim).T)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npd.DataFrame.from_dict({\"empirical\": np.array(returns_df[\"returns\"].dropna().mean()) * scale[\"periods\"],\n                        \"theoretical\": np.array(mu_df.mean())})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   empirical  theoretical\n0   0.086633     0.081153\n1   0.017836     0.020305\n2  -0.000936    -0.001171\n3   0.000762     0.000566\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npd.DataFrame.from_dict({\"empirical\": np.sqrt(np.diag(sigma)),\n                        \"theoretical\": np.array(sigma_df.mean())})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   empirical  theoretical\n0   0.179790     0.179268\n1   0.062190     0.062060\n2   0.008188     0.008199\n3   0.016939     0.016916\n```\n:::\n:::\n\n\n# Newton's method\n\n## Implied volatility\n\nNewton's method (main idea is also from a Taylor series) is a method for finding approximations to the roots of a function $f(x)$:\n\n$$\n\\begin{aligned}\nx_{n+1}=x_{n}-{\\frac{f(x_{n})}{f'(x_{n})}}\n\\end{aligned}\n$$\n\nTo solve $V(\\sigma_{n})-V=0$ for $\\sigma_{n}$, use Newton's method and repeat until $\\left|\\sigma_{n+1}-\\sigma_{n}\\right|<\\varepsilon$:\n\n$$\n\\begin{aligned}\n\\sigma_{n+1}=\\sigma_{n}-{\\frac{V(\\sigma_{n})-V}{V'(\\sigma_{n})}}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef implied_vol_newton(params, type, S, K, r, q, tau):\n    \n    target0 = 0\n    sigma0 = params[\"sigma\"]\n    \n    while (abs(target0 - params[\"target\"]) > params[\"tol\"]):\n        \n        d1 = bs_d1(S, K, r, q, tau, sigma0)\n        d2 = bs_d2(S, K, r, q, tau, sigma0)\n        \n        target0 = bs_value(type, S, K, r, q, tau, sigma0, d1, d2)\n        vega0 = bs_vega(type, S, K, r, q, tau, sigma0, d1, d2)\n        \n        sigma = sigma0 - (target0 - params[\"target\"]) / vega0\n        sigma0 = sigma\n        \n    return sigma\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# http://www.aspenres.com/documents/help/userguide/help/bopthelp/bopt2Implied_Volatility_Formula.html\n# https://books.google.com/books?id=VLi61POD61IC&pg=PA104\nS = levels_df.fillna(method = \"ffill\")[factor][-1]\nK = S * (1 + 0.05)\nsigma = sd_df[factor][-1] # overrides matrix\nstart = 0.2\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nd1 = bs_d1(S, K, r, q, tau, sigma)\nd2 = bs_d2(S, K, r, q, tau, sigma)\ntarget = bs_value(type, S, K, r, q, tau, sigma, d1, d2)\nparams = {\n    \"target\": target,\n    \"sigma\": start,\n    \"tol\": 1e-4 # np.finfo(float).eps\n}\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimplied_vol_newton(params, type, S, K, r, q, tau) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.141198999288776\n```\n:::\n:::\n\n\n# Optimization\n\n## Implied volatility\n\nIf the derivative is unknown, try optimization:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef implied_vol_obj(param, type, S, K, r, q, tau, target):\n    \n    d1 = bs_d1(S, K, r, q, tau, param)\n    d2 = bs_d2(S, K, r, q, tau, param)\n    target0 = bs_value(type, S, K, r, q, tau, param, d1, d2)\n    \n    result = abs(target0 - target)\n    \n    return result\n\ndef implied_vol_optim(param, type, S, K, r, q, tau, target):\n    \n    result = minimize(implied_vol_obj, param, args = (type, S, K, r, q, tau, target))\n    \n    return result.x.item()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimplied_vol_optim(start, type, S, K, r, q, tau, target)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.14119899687501564\n```\n:::\n:::\n\n\n<!-- ## Variance swaps -->\n\n<!-- A variance swap can be written as: -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- K_{var}={\\frac{2e^{rT}}{T}}\\left(\\int\\limits_{0}^{F_{0}}{\\frac{1}{K^{2}}}P(K)dK+\\int\\limits_{F_{0}}^{\\infty}{\\frac{1}{K^{2}}} C(K)dK\\right) -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- The CBOE Volatility Index (VIX) is calculated as a variance swap on the 30-day variance of the S&P 500 with an adjustment term: -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\sigma^{2}={\\frac{2e^{rT}}{T}}\\left(\\sum_{i=0}^{K_{0}}\\frac{\\Delta K_{i}}{K_{i}^{2}}P(K_{i})+\\sum_{i=K_{0}}^{\\infty}\\frac{\\Delta K_{i}}{K_{i}^{2}}C(K_{i})\\right)-\\frac{1}{T}\\left(\\frac{F}{K_{0}}-1\\right)^{2} -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ```{python} -->\n<!-- def rle(x): -->\n\n<!--     n = len(x) -->\n<!--     y = x.iloc[1:].reset_index(drop = True) != x.iloc[:-1].reset_index(drop = True) -->\n<!--     i = y[y].index.tolist() + [n - 1] -->\n\n<!--     result = {\"lengths\": np.diff([0] + i), -->\n<!--               \"values\": x.reset_index(drop = True)[i]} -->\n\n<!--     return result -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- # https://cdn.cboe.com/resources/vix/vixwhite.pdf -->\n<!-- # https://en.wikipedia.org/wiki/VIX -->\n<!-- # https://en.wikipedia.org/wiki/Variance_swap -->\n<!-- # https://www.ivolatility.com/doc/VarianceSwaps.pdf -->\n<!-- def implied_vol_vix(calls_df, puts_df, r, tau): -->\n\n<!--     # time to expiration  -->\n<!--     t = ((tau[\"exp_time\"] - tau[\"sys_time\"]).total_seconds() / 60) / (365 * 24 * 60) -->\n\n<!--     # midpoint of bid and ask -->\n<!--     calls_df[\"Mid\"] = calls_df[[\"Bid\", \"Ask\"]].mean(axis = 1) -->\n<!--     puts_df[\"Mid\"] = puts_df[[\"Bid\", \"Ask\"]].mean(axis = 1) -->\n\n<!--     options_df = calls_df[[\"Strike\", \"Mid\"]] \\ -->\n<!--         .merge(puts_df[[\"Strike\", \"Mid\"]], on = \"Strike\") \\ -->\n<!--         .rename(columns = {\"Mid_x\": \"Call\", \"Mid_y\": \"Put\"}) -->\n\n<!--     options_df[\"Diff\"] = abs(options_df[\"Call\"] - options_df[\"Put\"]) -->\n\n<!--     # minimum absolute difference is forward index level -->\n<!--     forward_df = options_df.loc[options_df[\"Diff\"] == min(options_df[\"Diff\"])] -->\n<!--     k = forward_df[\"Strike\"] -->\n<!--     c = forward_df[\"Call\"] -->\n<!--     p = forward_df[\"Put\"] -->\n<!--     f = k + np.exp(r * t) * (c - p) -->\n\n<!--     # strike price equal or below forward index level -->\n<!--     k0 = options_df.loc[options_df[\"Strike\"] <= int(f), \"Strike\"].iloc[-1] -->\n\n<!--     # out-of-the-money options -->\n<!--     puts_otm_df = puts_df.loc[puts_df[\"Strike\"] < k0] -->\n<!--     calls_otm_df = calls_df.loc[calls_df[\"Strike\"] > k0] -->\n\n<!--     # stop after two consecutive strike prices with zero bid prices -->\n<!--     # https://stackoverflow.com/a/50311890 -->\n<!--     puts_otm_rle = rle(puts_otm_df[\"Bid\"]) -->\n<!--     idx = puts_otm_rle[\"lengths\"].cumsum() # end -->\n<!--     try: -->\n<!--         idx = idx[(puts_otm_rle[\"lengths\"] > 1) & (puts_otm_rle[\"values\"] == 0)].max() -->\n<!--         puts_otm_df = puts_otm_df.iloc[(idx + 1):] -->\n<!--     except: -->\n<!--         pass -->\n\n<!--     calls_otm_rle = rle(calls_otm_df[\"Bid\"]) -->\n<!--     idx = calls_otm_rle[\"lengths\"].cumsum() # end -->\n<!--     idx = idx - calls_otm_rle[\"lengths\"] + 1 # start -->\n<!--     try: -->\n<!--         idx = idx[(calls_otm_rle[\"lengths\"] > 1) & (calls_otm_rle[\"values\"] == 0)].max() -->\n<!--         calls_otm_df = calls_otm_df.iloc[:idx] -->\n<!--     except: -->\n<!--         pass -->\n\n<!--     # average put and call prices for k0 -->\n<!--     # note: exclude options with zero bid price -->\n<!--     result_df = puts_otm_df.loc[puts_otm_df[\"Bid\"] != 0, [\"Strike\", \"Mid\"]] \\ -->\n<!--         .append(pd.DataFrame(puts_df.loc[puts_df[\"Strike\"] == k0, [\"Strike\", \"Mid\"]] \\ -->\n<!--                              .append(calls_df.loc[calls_df[\"Strike\"] == k0, -->\n<!--                                                   [\"Strike\", \"Mid\"]]).mean(axis = 0)).transpose() \\ -->\n<!--                 .append(calls_otm_df.loc[calls_otm_df[\"Bid\"] != 0, -->\n<!--                                          [\"Strike\", \"Mid\"]])).reset_index(drop = True) -->\n\n<!--     # differences between strike prices -->\n<!--     # note: create new column -->\n<!--     result_df.loc[0, \"Diff\"] = result_df.loc[1, \"Strike\"] - result_df.loc[0, \"Strike\"] -->\n<!--     result_df[\"Diff\"].iloc[-1] = result_df[\"Strike\"].iloc[-1] - result_df[\"Strike\"].iloc[-2] -->\n<!--     result_df[\"Diff\"].iloc[1:-1] = (result_df[\"Strike\"].iloc[2:].values - -->\n<!--                                     result_df[\"Strike\"].iloc[:-2].values) / 2 -->\n\n<!--     # variance -->\n<!--     v = sum(result_df[\"Diff\"] / result_df[\"Strike\"] ** 2 * np.exp(r * t) * -->\n<!--             result_df[\"Mid\"]) * (2 / t) - (1 / t) * (f / k0 - 1) ** 2 -->\n\n<!--     result = {\"t\": t, -->\n<!--               \"v\": v} -->\n\n<!--     return result -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- sys_time = datetime.datetime.now().replace(hour = 9, minute = 46, second = 0) -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- v1 = implied_vol_vix(pd.read_csv(\"../securities/near_calls.csv\"), pd.read_csv(\"../securities/near_puts.csv\"), 0.000305, -->\n<!--                      {\"sys_time\": sys_time, -->\n<!--                       \"exp_time\": sys_time.replace(hour = 8, minute = 30, second = 0) + -->\n<!--                                   datetime.timedelta(days = 25)}) -->\n<!-- v2 = implied_vol_vix(pd.read_csv(\"../securities/next_calls.csv\"), pd.read_csv(\"../securities/next_puts.csv\"), 0.000286, -->\n<!--                      {\"sys_time\": sys_time, -->\n<!--                       \"exp_time\": sys_time.replace(hour = 15, minute = 0, second = 0) + -->\n<!--                                   datetime.timedelta(days = 32)}) -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- nt1 = v1[\"t\"] * (365 * 24 * 60) -->\n<!-- nt2 = v2[\"t\"] * (365 * 24 * 60) -->\n<!-- n30 = 30 * (24 * 60) -->\n<!-- n365 = 365 * (24 * 60) -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- vix = np.sqrt(((v1[\"t\"] * v1[\"v\"] * ((nt2 - n30) / (nt2 - nt1))).values + -->\n<!--                (v2[\"t\"] * v2[\"v\"] * ((n30 - nt1) / (nt2 - nt1))).values) * (n365 / n30)) -->\n<!-- vix -->\n<!-- ``` -->\n\n<!-- ## Implied yield -->\n\n<!-- ```{python} -->\n<!-- def yield_option(type, S, K, r, q, tau, sigma): -->\n\n<!--     d1 = bs_d1(S, K, r, q, tau, sigma) -->\n<!--     d2 = bs_d2(S, K, r, q, tau, sigma) -->\n<!--     value = bs_value(type, S, K, r, q, tau, sigma, d1, d2) -->\n\n<!--     if (type == \"call\"): -->\n<!--         result = (value / S) / tau -->\n<!--     elif (type == \"put\"): -->\n<!--         result = (value / K) / tau -->\n\n<!--     return result -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- sigmas = [x / 100 for x in range(10, 31, 4)] -->\n<!-- taus = [x / 252 for x in range(20, 127, 20)] -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- yield_df = pd.DataFrame([(x, y) for x in sigmas for y in taus]) \\ -->\n<!--     .rename(columns = {0: \"sigma\", 1: \"tau\"}) -->\n<!-- yield_df[\"yield\"] = yield_df.apply(lambda x: yield_option(type, S, K, r, q, x[\"tau\"], x[\"sigma\"]), -->\n<!--                                    axis = 1) -->\n<!-- yield_df[\"sigma\"] = yield_df[\"sigma\"] * 100 -->\n<!-- yield_df[\"tau\"] = yield_df[\"tau\"] * scale[\"periods\"] -->\n<!-- ``` -->\n\n<!-- ```{r, fig.width = 4, fig.height = 3} -->\n<!-- yield_plt <- plot_heatmap(py$yield_df, x = \"tau\", y = \"sigma\", z = \"yield\", -->\n<!--                           title = \"Yield (%)\", xlab = \"Tau\", ylab = \"Sigma\") -->\n<!-- print(yield_plt) -->\n<!-- ``` -->\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}