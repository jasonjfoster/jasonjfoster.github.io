{
  "hash": "221323cffb8fab38a6d2a7ca73200e6c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Risk\"\nauthor: \"[Jason Foster](mailto:jason.j.foster@gmail.com)\"\ndate: last-modified\ncategories:\n  - analysis\n  - finance\n  - python\n---\n\n::: {.cell}\n\n```{.python .cell-code}\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nintercept = True\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n# Regression analysis\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport math\nimport statsmodels.api as sm\n```\n:::\n\n\n## Ordinary least squares\n\n### Coefficients\n\n$$\n\\begin{aligned}\n\\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy\n\\end{aligned}\n$$\n\n-   <https://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef lm_coef(x, y, weights, intercept):\n    \n    if (intercept): x = sm.add_constant(x)\n        \n    result = np.dot(np.linalg.inv(np.dot(x.T, np.multiply(weights, x))),\n                    np.dot(x.T, np.multiply(weights, y)))\n    \n    return np.ravel(result)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_coef(overlap_x_df, overlap_y_df, weights, intercept)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([ 1.63031573e-04,  2.08407851e-01, -1.19139710e-01,  2.90456280e+00,\n        1.10228919e+00])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nif (intercept): overlap_x_df = sm.add_constant(overlap_x_df)\n    \nfit = sm.WLS(overlap_y_df, overlap_x_df, weights = weights).fit()\n\nif (intercept): overlap_x_df = overlap_x_df.iloc[:, 1:]\n\nnp.array(fit.params)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([ 1.63031573e-04,  2.08407851e-01, -1.19139710e-01,  2.90456280e+00,\n        1.10228919e+00])\n```\n\n\n:::\n:::\n\n\n### R-squared\n\n$$\n\\begin{aligned}\nR^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef lm_rsq(x, y, weights, intercept):\n            \n    coef = np.matrix(lm_coef(x, y, weights, intercept))\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n    result = np.dot(coef, np.dot(np.dot(x.T, np.multiply(weights, x)), coef.T)) / \\\n        np.dot(y.T, np.multiply(weights, y))\n    \n    return result.item()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_rsq(overlap_x_df, overlap_y_df, weights, intercept)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.7173255446186038\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfit.rsquared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnp.float64(0.7173255446186044)\n```\n\n\n:::\n:::\n\n\n### Standard errors\n\n$$\n\\begin{aligned}\n\\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\\n&=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\\n\\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right)\n\\end{aligned}\n$$\n\n-   <http://people.duke.edu/~rnau/mathreg.htm>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef lm_se(x, y, weights, intercept):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept):\n        \n        x = sm.add_constant(x)\n        y = y - np.average(y, axis = 0, weights = weights.reshape(-1))\n        \n        df_resid = n_rows - n_cols - 1 \n        \n    else:\n        df_resid = n_rows - n_cols        \n    \n    var_y = np.dot(y.T, np.multiply(weights, y))\n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    result = np.sqrt(var_resid * np.linalg.inv(np.dot(x.T, np.multiply(weights, x))).diagonal())\n    \n    return np.ravel(result)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_se(overlap_x_df, overlap_y_df, weights, intercept)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([5.69616557e-05, 1.96667202e-02, 4.43622346e-02, 2.89175173e-01,\n       2.87849110e-01])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.array(fit.bse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([5.69616557e-05, 1.96667202e-02, 4.43622346e-02, 2.89175173e-01,\n       2.87849110e-01])\n```\n\n\n:::\n:::\n\n\n### Shapley values\n\n$$\nR^{2}_{i}=\\sum_{S\\subseteq N\\setminus\\{i\\}}{\\frac{|S|!\\;(n-|S|-1)!}{n!}}(R^{2}(S\\cup\\{i\\})-R^{2}(S))\n$$\n\n-   <https://real-statistics.com/multiple-regression/shapley-owen-decomposition/>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef lm_shap(x, y, weights, intercept):\n  \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    n_combn = 2 ** n_cols\n    n_vec = np.zeros(n_combn)\n    ix_mat = np.zeros((n_cols, n_combn))\n    rsq = np.zeros(n_combn)\n    result = np.zeros(n_cols)\n    \n    # number of binary combinations\n    for k in range(n_combn):\n      \n        n = 0\n        n_size = k\n        \n        # find the binary combination\n        for j in range(n_cols):\n          \n            if n_size % 2 == 0:\n              \n                n += 1\n                \n                ix_mat[j, k] = j + 1\n                \n            n_size //= 2\n        \n        n_vec[k] = n\n        \n        if n > 0:\n          \n            ix_subset = np.where(ix_mat[:, k] != 0)[0]\n            x_subset = x.iloc[:, ix_subset]\n            \n            rsq[k] = lm_rsq(x_subset, y, weights, intercept)\n\n    # calculate the exact Shapley value for r-squared\n    for j in range(n_cols):\n      \n        ix_pos = np.where(ix_mat[j, :] != 0)[0]\n        ix_neg = np.where(ix_mat[j, :] == 0)[0]\n        ix_n = n_vec[ix_neg]\n        rsq_diff = rsq[ix_pos] - rsq[ix_neg]\n\n        for k in range(int(n_combn / 2)):\n          \n            s = int(ix_n[k])\n            weight = math.factorial(s) * math.factorial(n_cols - s - 1) \\\n                / math.factorial(n_cols)\n            result[j] += weight * rsq_diff[k]\n\n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_shap(overlap_x_df, overlap_y_df, weights, intercept)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([0.30421791, 0.09151911, 0.23433115, 0.08725737])\n```\n\n\n:::\n:::\n\n\n## Principal component regression\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ncomps = 1\n```\n:::\n\n\n### Coefficients\n\n$$\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n$$\n\n-   <https://en.wikipedia.org/wiki/Principal_component_regression>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef pcr_coef(x, y, comps):\n    \n    x = x - np.average(x, axis = 0)\n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    V = V[:, idx]\n    \n    W = np.dot(x, V)\n    gamma = np.dot(np.dot(np.linalg.inv(np.dot(W.T, W)), W.T), y)\n    \n    result = np.dot(V[:, :comps], gamma[:comps])\n    \n    return np.ravel(result)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nscale_x_df = (overlap_x_df - np.average(overlap_x_df, axis = 0)) \\\n    / np.std(overlap_x_df, axis = 0, ddof = 1)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npcr_coef(scale_x_df, overlap_y_df, comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([-1.10386321e-05, -3.55058217e-04,  3.90631611e-04, -1.74394824e-04])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npcr_coef(overlap_x_df, overlap_y_df, comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([ 0.29296845, -0.01373458,  0.00347208,  0.0121573 ])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npca = PCA(n_components = len(factors))\npca_x_df = pca.fit_transform(scale_x_df)\n\nfit = LinearRegression(fit_intercept = False).fit(pca_x_df, overlap_y_df)\n\ngamma = fit.coef_\nnp.dot(pca.components_.T[:, :comps], gamma.T[:comps]).ravel()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([-1.10386321e-05, -3.55058217e-04,  3.90631611e-04, -1.74394824e-04])\n```\n\n\n:::\n:::\n\n\n### R-squared\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef pcr_rsq(x, y, comps):\n    \n    coef = np.matrix(pcr_coef(x, y, comps))\n    \n    x = x - np.average(x, axis = 0)\n    y = y - np.average(y, axis = 0)\n    \n    result = np.dot(np.dot(coef, np.dot(x.T, x)), coef.T) / np.dot(y.T, y)\n    \n    return result.item()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npcr_rsq(scale_x_df, overlap_y_df, comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.19270846187239188\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npcr_rsq(overlap_x_df, overlap_y_df, comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.47041343905230315\n```\n\n\n:::\n:::\n\n\n### Standard errors\n\n$$\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# unable to verify the result\ndef pcr_se(x, y, comps):\n    \n    n_rows = x.shape[0]\n    n_cols = x.shape[1]\n    \n    rsq = pcr_rsq(x, y, comps)\n    \n    y = y - np.average(y, axis = 0)\n    \n    df_resid = n_rows - n_cols - 1\n    \n    var_y = np.dot(y.T, y)   \n    var_resid = (1 - rsq) * var_y / df_resid\n    \n    # uses statsmodels for illustrative purposes\n    pca = sm.multivariate.PCA(x, standardize = False, demean = True)\n    L = pca.eigenvals[:comps]\n    V = pca.eigenvecs.iloc[:, :comps]\n    \n    result = np.sqrt(var_resid * np.dot(V, np.dot(np.diag(1 / L), V.T)).diagonal())\n    \n    return np.ravel(result)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npcr_se(scale_x_df, overlap_y_df, comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([1.43757858e-06, 4.62397949e-05, 5.08725744e-05, 2.27117146e-05])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npcr_se(overlap_x_df, overlap_y_df, comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([0.01977885, 0.00092725, 0.00023441, 0.00082076])\n```\n\n\n:::\n:::\n\n\n## Partial least squares\n\n# Risk decomposition\n\n## Standalone risk\n\n$$\n\\begin{aligned}\n\\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\\n\\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef cov_wt(x, weights, center):\n    \n    sum_w = sum(weights)\n    sumsq_w = sum(np.power(weights, 2))\n    \n    if (center):\n    \n        x = x - np.average(x, axis = 0, weights = weights.reshape(-1))\n    \n    result = np.dot(x.T, np.multiply(weights, x)) / (sum_w - sumsq_w / sum_w)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef lm_sar(x, y, weights, intercept):\n    \n    coef = lm_coef(x, y, weights, intercept)\n    rsq = lm_rsq(x, y, weights, intercept)\n    \n    if (intercept): x = sm.add_constant(x)\n    \n    # sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n    #                aweights = weights.reshape(-1))\n    sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n    sar = np.multiply(np.power(coef, 2).T, sigma[:-1, :-1].diagonal())\n    sar_eps = (1 - rsq) * sigma[-1, -1]\n    \n    result = np.sqrt(np.concatenate((np.matrix(sigma[-1, -1]),\n                                     np.matrix(sar),\n                                     np.matrix(sar_eps)), axis = 1))\n    \n    return np.ravel(result)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_sar(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([0.05754123, 0.        , 0.02796634, 0.00659472, 0.02758507,\n       0.01051104, 0.03059302])\n```\n\n\n:::\n:::\n\n\n## Risk contribution\n\n$$\n\\begin{aligned}\n\\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\\n&=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\\n\\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k}\n\\end{aligned}\n$$\n\n-   <https://bookdown.org/compfinezbook/introcompfinr/Portfolio-risk-reports.html>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef lm_mcr(x, y, weights, intercept):\n    \n    coef = np.matrix(lm_coef(x, y, weights, intercept)).T\n    rsq = lm_rsq(x, y, weights, intercept)\n        \n    if (intercept): x = sm.add_constant(x)\n    \n#     sigma = np.cov(np.concatenate((x, y), axis = 1).T,\n#                    aweights = weights.reshape(-1))\n    sigma = cov_wt(np.concatenate((x, y), axis = 1), weights, intercept)\n    mcr = np.multiply(coef, np.dot(sigma[:-1, :-1], coef)) / np.sqrt(sigma[-1, -1])\n    mcr_eps = np.sqrt(sigma[-1, -1]) - sum(mcr)\n    \n    result = np.concatenate((np.sqrt(np.matrix(sigma[-1, -1])),\n                             np.matrix(mcr).T,\n                             np.matrix(mcr_eps)), axis = 1)\n    \n    return np.ravel(result)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlm_mcr(overlap_x_df, overlap_y_df, weights, intercept) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([0.05754123, 0.        , 0.0189779 , 0.00287695, 0.01584286,\n       0.00357808, 0.01626543])\n```\n\n\n:::\n:::\n\n\n# Scenario analysis\n\n## Implied shocks\n\n$$\n\\begin{aligned}\n\\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef implied_shocks(shocks, x, z, weights):\n\n    beta = np.linalg.lstsq(np.multiply(weights, z), np.multiply(weights, x), rcond = None)[0]\n                     \n    result = np.dot(shocks, beta)\n    \n    return np.ravel(result)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nshocks = np.array([-0.1, 0.1])\noverlap_z_df = overlap_x_df.iloc[:, [0, 1]]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimplied_shocks(shocks, overlap_x_df, overlap_z_df, weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([-0.1       ,  0.1       , -0.01046187, -0.00305239])\n```\n\n\n:::\n:::\n\n\n## Stress P&L\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef pnl_stress(shocks, x, y, z, weights, intercept):\n    \n    coef = lm_coef(x, y, weights, intercept)\n    \n    if (intercept): x = sm.add_constant(x)\n    \n    result = np.multiply(coef.T, implied_shocks(shocks, x, z, weights))\n    \n    return np.ravel(result)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npnl_stress(shocks, overlap_x_df, overlap_y_df, overlap_z_df, weights, intercept)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([-0.00045368, -0.02084079, -0.01191397, -0.03038717, -0.00336462])\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}