{
  "hash": "f974a10d617108237bd61939f1cc2092",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Eigen\"\nauthor: \"[Jason Foster](mailto:jason.j.foster@gmail.com)\"\ndate: last-modified\ncategories:\n  - analysis\n  - finance\n  - python\n---\n\n::: {.cell}\n\n```{.python .cell-code}\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n# Decomposition\n\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (*note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales*) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\n$$\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n$$\n\n-   <https://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef eigen(x):\n  \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    V = V[:, idx]\n    \n    result = {\n        \"values\": L,\n        \"vectors\": V\n    }\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef eigen_decomp(x, comps):\n    \n    LV = eigen(x)\n    L = LV[\"values\"][:comps]\n    V = LV[\"vectors\"][:, :comps]\n    \n    result = np.dot(V, np.multiply(L, V.T))\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ncomps = 1\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\neigen_decomp(overlap_df, comps) * scale[\"periods\"] * scale[\"overlap\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[ 2.98878168e-02, -3.65953252e-03, -1.51030633e-04,\n         2.56359134e-03],\n       [-3.65953252e-03,  4.48081516e-04,  1.84925355e-05,\n        -3.13891976e-04],\n       [-1.51030633e-04,  1.84925355e-05,  7.63195658e-07,\n        -1.29544699e-05],\n       [ 2.56359134e-03, -3.13891976e-04, -1.29544699e-05,\n         2.19888947e-04]])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# np.cov(overlap_df.T) * scale[\"periods\"] * scale[\"overlap\"]\n```\n:::\n\n\n# Variance\n\nWe often look at the proportion of variance explained by the first $i$ principal components as an indication of how many components are needed.\n\n$$\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef variance_explained(x):\n    \n    LV = eigen(x)\n    L = LV[\"values\"]\n    \n    result = L.cumsum() / L.sum()\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nvariance_explained(overlap_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([0.87279922, 0.9921658 , 0.99826915, 1.        ])\n```\n\n\n:::\n:::\n\n\n# Similarity\n\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\n$$\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef roll_eigen1(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evec = eigen(x.iloc[idx])[\"vectors\"][:, comp - 1]\n        result_ls.append(evec)\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df  \n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ncomp = 1\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nraw_df = roll_eigen1(overlap_df, width, comp)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=576}\n:::\n:::\n\n\n-   <https://quant.stackexchange.com/a/3095>\n-   <https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef roll_eigen2(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evecs = eigen(x.iloc[idx])[\"vectors\"]\n        \n        if i > width - 1:\n            \n            similarity = np.dot(evecs.T, result_ls[-1])\n            order = np.argmax(np.abs(similarity))\n            evec = np.multiply(np.sign(similarity[order]), evecs[:, order])\n            \n            result_ls.append(evec)\n            \n        else:\n            result_ls.append(evecs[:, comp - 1])\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nclean_df = roll_eigen2(overlap_df, width, comp)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=576}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}