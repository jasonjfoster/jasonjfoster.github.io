{
  "hash": "b0df8390a66dfa584d5c08acee88b89c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Eigen\"\nauthor: \"[Jason Foster](mailto:jason.j.foster@gmail.com)\"\ndate: last-modified\ncategories:\n  - analysis\n  - finance\n  - python\n---\n\n::: {.cell}\n\n```{.python .cell-code}\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n# Decomposition\n\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (*note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales*) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\n$$\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n$$\n\n-   <https://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef eigen(x):\n  \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    V = V[:, idx]\n    \n    result = {\n        \"values\": L,\n        \"vectors\": V\n    }\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef eigen_decomp(x, comps):\n    \n    LV = eigen(x)\n    L = LV[\"values\"][:comps]\n    V = LV[\"vectors\"][:, :comps]\n    \n    result = np.dot(V, np.multiply(L, V.T))\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ncomps = 1\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\neigen_decomp(overlap_df, comps) * scale[\"periods\"] * scale[\"overlap\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[ 2.98380602e-02, -3.65933302e-03, -1.47878851e-04,\n         2.55557936e-03],\n       [-3.65933302e-03,  4.48779783e-04,  1.81358292e-05,\n        -3.13415681e-04],\n       [-1.47878851e-04,  1.81358292e-05,  7.32894651e-07,\n        -1.26655734e-05],\n       [ 2.55557936e-03, -3.13415681e-04, -1.26655734e-05,\n         2.18881048e-04]])\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# np.cov(overlap_df.T) * scale[\"periods\"] * scale[\"overlap\"]\n```\n:::\n\n\n# Variance\n\nWe often look at the proportion of variance explained by the first $i$ principal components as an indication of how many components are needed.\n\n$$\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef variance_explained(x):\n    \n    LV = eigen(x)\n    L = LV[\"values\"]\n    \n    result = L.cumsum() / L.sum()\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nvariance_explained(overlap_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([0.87364043, 0.99215076, 0.99826523, 1.        ])\n```\n\n\n:::\n:::\n\n\n# Similarity\n\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\n$$\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{v}_{t}\\cdot\\mathbf{v}_{t-1}}{\\|\\mathbf{v}_{t}\\|\\|\\mathbf{v}_{t-1}\\|}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef roll_eigen1(x, width, comp):\n    \n    n_rows = len(x)\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        \n        LV = eigen(x.iloc[idx])\n        V = LV[\"vectors\"]\n        \n        result_ls.append(V[:, comp - 1])\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df  \n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ncomp = 1\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nraw_df = roll_eigen1(overlap_df, width, comp)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=576}\n:::\n:::\n\n\n-   <https://quant.stackexchange.com/a/3095>\n-   <https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4400158>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef roll_eigen2(x, width, comp):\n    \n    n_rows = len(x)\n    V_ls = []\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        \n        LV = eigen(x.iloc[idx])\n        V = LV[\"vectors\"]\n        \n        if i > width - 1:\n            \n            similarity = np.dot(V, V_ls[-1])\n            order = np.argmax(np.abs(similarity), axis = 1)\n            V = np.sign(np.diag(similarity[:, order])) * V[:, order]\n            \n        V_ls.append(V)\n        result_ls.append(V[:, comp - 1])\n    \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nclean_df = roll_eigen2(overlap_df, width, comp)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=576}\n:::\n:::\n\n\n# Implied shocks\n\nProduct of the $n$th eigenvector and square root of the $n$th eigenvalue:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef roll_shocks(x, width, comp):\n    \n    n_rows = len(x)\n    V_ls = []\n    result_ls = []\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        \n        LV = eigen(x.iloc[idx])\n        L = LV[\"values\"]\n        V = LV[\"vectors\"]\n        \n        if len(V_ls) > 1:\n            \n            similarity = np.dot(V.T, V_ls[-1])\n            order = np.argmax(np.abs(similarity), axis = 1)\n            L = L[order]\n            V = np.sign(np.diag(similarity[:, order])) * V[:, order]\n        \n        shocks = np.sqrt(L[comp - 1]) * V[:, comp - 1]\n        V_ls.append(V)\n        result_ls.append(shocks)\n        \n    result_df = pd.DataFrame(result_ls, index = x.index[(width - 1):],\n                             columns = x.columns)\n    \n    return result_df\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nshocks_df = roll_shocks(overlap_df, width, comp) * np.sqrt(scale[\"periods\"] * scale[\"overlap\"])\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=576}\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}