{
  "hash": "f5855c3dd36cdf2f1493cbe8e4ab140b",
  "result": {
    "markdown": "---\ntitle: \"Portfolios\"\nauthor: \"[Jason Foster](mailto:jason.j.foster@gmail.com)\"\ndate: last-modified\ncategories:\n  - analysis\n  - finance\n  - python\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n-   <https://pandas-datareader.readthedocs.io/en/latest/remote_data.html>\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# import datetime\nfrom scipy.optimize import minimize\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\n```\n:::\n\n\n-   Open: <https://github.com/pydata/pandas-datareader/issues/965>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\nprices_df = pdr.get_data_tiingo(tickers, start = \"1900-01-01\", api_key = os.getenv(\"TIINGO_API_KEY\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nC:\\Users\\jason\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas_datareader\\tiingo.py:234: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  return pd.concat(dfs, self._concat_axis)\n```\n:::\n\n```{.python .cell-code}\nprices_df = prices_df.pivot_table(index = \"date\", columns = \"symbol\", values = \"adjClose\") \\\n    .tz_localize(None)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_cols = list(zip([\"returns\"], tickers))\noverlap_cols = list(zip([\"overlap\"], tickers))\nreturns_df[returns_cols] = np.log(prices_df).diff()\nreturns_df[overlap_cols] = returns_df[returns_cols].rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df.sort_index(axis = 1, inplace = True)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# weights = np.array([0.9 ** i for i in range(width - 1, -1, -1)]).reshape((width, 1))\nweights = np.array([1] * width).reshape((width, 1))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors]\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers]\noverlap_x_mat = np.matrix(overlap_x_df[-width:])\noverlap_y_mat = np.matrix(overlap_y_df[-width:])\n```\n:::\n\n\n## Random portfolios\n\nNeed to generate uniformly distributed weights $\\mathbf{w}=(w_{1},w_{2},\\ldots,w_{N})$ such that $\\sum_{j=1}^{N}w_{i}=1$ and $w_{i}\\geq0$:\n\n-   **Approach 1**: tempting to use $w_{i}=\\frac{u_{i}}{\\sum_{j=1}^{N}u_{i}}$ where $u_{i}\\sim U(0,1)$ but the distribution of $\\mathbf{w}$ is not uniform\n\n-   **Approach 2**: instead, generate $\\text{Exp}(1)$ and then normalize\n\nCan also scale random weights by $M$, e.g. if sum of weights must be 10% then multiply weights by 10%.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights1(n_sim, n_assets, lmbda):  \n    \n    rand_exp = np.matrix(np.random.uniform(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = np.divide(rand_exp, rand_exp_sum)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# Methodology: uniform sampling from the simplex (http://mathoverflow.net/a/76258)\n# z ~ U(0, 1) then -ln(z) is an exponential(1) distribution\n# This is also known as generating a random vector from the symmetric Dirichlet distribution\ndef rand_weights2(n_sim, n_assets, lmbda):   \n    \n    rand_exp = np.matrix(-np.log(1 - np.random.uniform(size = (n_sim, n_assets))) / lmbda)\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = np.divide(rand_exp, rand_exp_sum)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# 1. Generate n exponential(1) random variables x_1, x_2, ..., x_n\n# 2. Let y_i = x_i / (sum_{i = 1}^{n} x_i)\ndef rand_weights3(n_sim, n_assets, lmbda):\n    \n    rand_exp = np.matrix(np.random.exponential(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = np.divide(rand_exp, rand_exp_sum)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlmbda = 1\nn_assets = 3\nn_sim = 10000\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach1 = rand_weights1(n_sim, n_assets, lmbda)\napproach2 = rand_weights2(n_sim, n_assets, lmbda)\napproach3 = rand_weights3(n_sim, n_assets, lmbda)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(py$approach1), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(py$approach2), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(py$approach3), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=384}\n:::\n:::\n\n\n### Random turnover\n\nHow to generate random weights between lower bound $a$ and upper bound $b$ that sum to zero?\n\n-   **Approach 1**: tempting to multiply random weights by $M$ and then subtract by $\\frac{M}{N}$ but the distribution is not between $a$ and $b$\n\n-   **Approach 2**: instead, use an iterative approach for random turnover:\n\n    1.  Generate $N-1$ uniformly distributed weights between $a$ and $b$\n    2.  For $u_{N}$ compute sum of values and subtract from $M$\n    3.  If $u_{N}$ is between $a$ and $b$, then keep; otherwise, discard\n\nThen add random turnover to previous period's random weights.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    plug = False\n    \n    while not plug:\n        \n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n        if ((temp <= upper) and (temp >= lower)):\n            plug = True\n        \n    result = np.append(result, temp)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights3(n_sim, n_assets, lmbda) * rng\n    result = result - rng / n_assets\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n    \n    result = np.matrix(rand_iterative(n_assets, lower, upper, target))\n    \n    while result.shape[0] < n_sim:\n    \n        temp = np.matrix(rand_iterative(n_assets, lower, upper, target))\n        result = np.concatenate((result, temp), axis = 0)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlower = -0.05\nupper = 0.05\ntarget = 0\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(py$approach1), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(py$approach2), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-28-1.png){width=384}\n:::\n:::\n\n\n## Mean-variance\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef geometric_mean(x, scale):\n    \n    result = np.prod(1 + x) ** (scale / x.shape[1]) - 1\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_x_df = returns_df.dropna()[\"returns\"][factors] # REMOVE LATER\nreturns_x_mat = np.matrix(returns_x_df) # extended history # REMOVE LATER\nmu = np.apply_along_axis(geometric_mean, 0, returns_x_mat, scale[\"periods\"])\nsigma = np.cov(overlap_x_df.T, ddof = 1) * scale[\"periods\"] * scale[\"overlap\"]\n```\n:::\n\n\n### Maximum return\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 0.06\nstart = np.array([1] * len(factors))\nbnds = [(np.finfo(float).eps, 1) for i in range(len(factors))]\ncons = [{\"type\": \"ineq\", \"fun\": lambda params, sigma, target: max_pnl_cons(params, sigma, target),\n         \"args\": (sigma, target)},\n        {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef max_pnl_cons(params, sigma, target):\n    \n    var = np.matmul(np.transpose(params), np.matmul(sigma, params))\n    \n    result = target ** 2 - var\n    \n    return result\n\ndef max_pnl_obj(params, mu):\n    \n    result = np.matmul(mu, params)\n    \n    return -result\n\ndef max_pnl_optim(params, mu):\n    \n    result = minimize(max_pnl_obj, params, args = (mu), bounds = bnds, constraints = cons)\n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams1 = max_pnl_optim(start, mu)\nparams1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([3.33478228e-01, 5.67596349e-01, 9.89254230e-02, 2.27645187e-16])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.matmul(mu, params1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.032906364465747885\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params1), np.matmul(sigma, params1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.06000003526699834\n```\n:::\n:::\n\n\n### Minimum variance\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min_{x}&\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 0.03\nstart = np.array([1] * len(factors))\ncons = [{\"type\": \"ineq\", \"fun\": lambda params, mu, target: min_risk_cons(params, mu, target),\n         \"args\": (mu, target)},\n        {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef min_risk_cons(params, mu, target):\n    \n    result = np.matmul(mu, params) - target\n    \n    return result\n\ndef min_risk_obj(params, sigma):\n    \n    result = np.matmul(np.transpose(params), np.matmul(sigma, params))\n    \n    return result\n\ndef min_risk_optim(params, sigma):\n    \n    result = minimize(min_risk_obj, params, args = (sigma), bounds = bnds, constraints = cons)\n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams2 = min_risk_optim(start, sigma)\nparams2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([2.97430685e-01, 5.52140503e-01, 1.50428812e-01, 2.49800181e-16])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.matmul(mu, params2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.02999999999411795\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params2), np.matmul(sigma, params2))) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.054626654580512614\n```\n:::\n:::\n\n\n### Maximum ratio\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}-\\frac{1}{2}\\delta(\\mathbf{w}^T\\Sigma\\mathbf{w})\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\nstart = np.array([1] * len(factors))\ncons = [{\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(start)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 1 1 1]\n```\n:::\n\n```{.python .cell-code}\nprint(start.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(4,)\n```\n:::\n\n```{.python .cell-code}\nprint(mu)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 0.0741518   0.01444308 -0.00019696  0.00084524]\n```\n:::\n\n```{.python .cell-code}\nprint(mu.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(4,)\n```\n:::\n\n```{.python .cell-code}\nprint(sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ 3.00635294e-02 -3.11798296e-03 -1.77879306e-04  2.54324229e-03]\n [-3.11798296e-03  4.53805294e-03 -1.23898887e-04 -3.78080435e-04]\n [-1.77879306e-04 -1.23898887e-04  7.18234782e-05 -3.96740334e-05]\n [ 2.54324229e-03 -3.78080435e-04 -3.96740334e-05  4.27709578e-04]]\n```\n:::\n\n```{.python .cell-code}\nprint(sigma.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(4, 4)\n```\n:::\n\n```{.python .cell-code}\nprint(np.matmul(np.transpose(start), np.matmul(sigma, start)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.032512568704157904\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef max_ratio_obj(params, mu, sigma, target):\n    \n    result = np.matmul(mu, params) - 0.5 * target * (np.matmul(np.transpose(params),\n                                                               np.matmul(sigma, params)))\n#     result = np.matmul(mu, params) / np.sqrt(np.matmul(np.transpose(params), np.matmul(sigma, params)))\n    \n    return -result\n\ndef max_ratio_optim(params, mu, sigma, target):\n    \n    result = minimize(max_ratio_obj, params, args = (mu, sigma, target), bounds = bnds,\n                      constraints = cons) \n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams3 = max_ratio_optim(start, mu, sigma, target)\nparams3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([3.55021061e-01, 6.02524365e-01, 4.24545734e-02, 2.78748379e-16])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.matmul(mu, params3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.035019395052128664\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.06396234177264824\n```\n:::\n:::\n\n\n<!-- ## Black-Litterman -->\n\n<!-- ### Prior distribution -->\n\n<!-- $$ -->\n\n<!-- \\begin{aligned} -->\n\n<!-- \\text{Risk aversion: } &\\lambda=\\frac{E(r)-r_{f}}{\\sigma^{2}}=\\frac{IR}{\\sigma}\\\\ -->\n\n<!-- \\text{Implied returns: } &\\Pi=\\lambda\\Sigma w\\\\ -->\n\n<!-- \\text{Distribution: } &N\\sim(\\Pi,\\tau\\Sigma) -->\n\n<!-- \\end{aligned} -->\n\n<!-- $$ -->\n\n<!-- ```{python} -->\n\n<!-- def implied_pnl(params, ir, sigma): -->\n\n<!--     lmbda = ir / np.sqrt(np.matmul(np.transpose(params), np.matmul(sigma, params))) -->\n\n<!--     result = np.matmul(lmbda * sigma, params) -->\n\n<!--     return result -->\n\n<!-- ``` -->\n\n<!-- ```{python} -->\n\n<!-- implied_pnl(params3, ir, sigma) -->\n\n<!-- ``` -->\n\n<!-- ### Conditional distribution -->\n\n<!-- $$ -->\n\n<!-- \\begin{aligned} -->\n\n<!-- \\text{Prior mean variance: } &\\tau\\in(0.01, 0.05)\\approx(0.025)\\\\ -->\n\n<!-- \\text{Asset views: } &\\mathbf{P}={\\begin{bmatrix} -->\n\n<!-- p_{11}&\\cdots&p_{1n}\\\\ -->\n\n<!-- \\vdots&\\ddots&\\vdots\\\\ -->\n\n<!-- p_{k1}&\\cdots&p_{kn} -->\n\n<!-- \\end{bmatrix}}= -->\n\n<!-- {\\begin{bmatrix} -->\n\n<!-- 0&0&0&0&0&0&1&0\\\\ -->\n\n<!-- -1&1&0&0&0&0&0&0\\\\ -->\n\n<!-- 0&0&0.5&-0.5&0.5&-0.5&0&0 -->\n\n<!-- \\end{bmatrix}}\\\\ -->\n\n<!-- \\text{View returns: } &\\mathbf{Q}={\\begin{bmatrix} -->\n\n<!-- q_{1}\\\\ -->\n\n<!-- \\vdots\\\\ -->\n\n<!-- q_{k} -->\n\n<!-- \\end{bmatrix}}= -->\n\n<!-- {\\begin{bmatrix} -->\n\n<!-- 0.0525\\\\ -->\n\n<!-- 0.0025\\\\ -->\n\n<!-- 0.0200 -->\n\n<!-- \\end{bmatrix}}\\\\ -->\n\n<!-- \\text{View confidence: } &\\mathbf{C}={\\begin{bmatrix} -->\n\n<!-- c_{1}\\\\ -->\n\n<!-- \\vdots\\\\ -->\n\n<!-- c_{k} -->\n\n<!-- \\end{bmatrix}}= -->\n\n<!-- {\\begin{bmatrix} -->\n\n<!-- 0.2500\\\\ -->\n\n<!-- 0.5000\\\\ -->\n\n<!-- 0.6500 -->\n\n<!-- \\end{bmatrix}}\\\\ -->\n\n<!-- \\text{View covariance: } &\\mathbf{\\Omega}={\\begin{bmatrix} -->\n\n<!-- \\tau\\left(\\frac{1-c_{1}}{c_{1}}\\right)\\left(p_{1}\\Sigma p_{1}^{T}\\right)&0&0\\\\ -->\n\n<!-- 0&\\ddots&0\\\\ -->\n\n<!-- 0&0&\\tau\\left(\\frac{1-c_{k}}{c_{k}}\\right)\\left(p_{k}\\Sigma p_{k}^{T}\\right) -->\n\n<!-- \\end{bmatrix}}\\\\ -->\n\n<!-- \\text{Distribution: } &N\\sim(\\mathbf{Q}, \\mathbf{\\Omega}) -->\n\n<!-- \\end{aligned} -->\n\n<!-- $$ -->\n\n<!-- ### Posterior distribution -->\n\n<!-- $$ -->\n\n<!-- \\begin{aligned} -->\n\n<!-- \\text{Implied returns: } &\\hat{\\Pi}=\\Pi+\\tau\\Sigma \\mathbf{P}^{T}\\left(\\tau \\mathbf{P}\\Sigma \\mathbf{P}^{T}+\\mathbf{\\Omega}\\right)^{-1}\\left(\\mathbf{Q}-\\mathbf{P}\\Pi^{T}\\right)\\\\ -->\n\n<!-- \\text{Covariance: } &\\hat{\\Sigma}=\\Sigma+\\tau\\left[\\Sigma-\\Sigma\\mathbf{P}^{T}\\left(\\tau\\mathbf{P}\\Sigma\\mathbf{P}^{T}+\\mathbf{\\Omega}\\right)^{-1}\\tau\\mathbf{P}\\Sigma\\right]\\\\ -->\n\n<!-- \\text{Weights: } &\\hat{w}=\\hat{\\Pi}\\left(\\lambda\\Sigma\\right)^{-1}\\\\ -->\n\n<!-- \\text{Distribution: } &N\\sim\\left(\\left[\\left(\\tau\\Sigma\\right)^{-1}+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{P}\\right]^{-1}\\left[\\left(\\tau\\Sigma\\right)^{-1}\\Pi+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{Q}\\right],\\left[\\left(\\tau\\Sigma\\right)^{-1}+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{P}\\right]^{-1}\\right) -->\n\n<!-- \\end{aligned} -->\n\n<!-- $$ -->\n\n<!-- ```{python} -->\n\n<!-- def black_litterman(params, ir, sigma, views): -->\n\n<!--     # prior distribution -->\n\n<!--     weights_prior = params -->\n\n<!--     sigma_prior = sigma -->\n\n<!--     lmbda = ir / np.sqrt(np.matmul(np.transpose(weights_prior), np.matmul(sigma_prior, weights_prior))) -->\n\n<!--     pi_prior = np.transpose(np.matrix(np.matmul(lmbda * sigma_prior, weights_prior))) -->\n\n<!--     # matrix calculations -->\n\n<!--     matmul_left = np.multiply(views[\"tau\"], np.matmul(sigma_prior, views[\"P\"].T)) -->\n\n<!--     matmul_mid = np.multiply(views[\"tau\"], np.matmul(views[\"P\"], np.matmul(sigma_prior, views[\"P\"].T))) -->\n\n<!--     matmul_right = views[\"Q\"] - np.matmul(views[\"P\"], pi_prior) -->\n\n<!--     # conditional distribution -->\n\n<!--     omega = np.diag(np.diag(np.matmul(np.diag([(1 - x) / x for x in views[\"C\"]]), matmul_mid))) -->\n\n<!--     # posterior distribution -->\n\n<!--     pi_posterior = pi_prior + np.matmul(matmul_left, np.matmul(np.linalg.inv(matmul_mid + omega), matmul_right)) -->\n\n<!--     sigma_posterior = sigma_prior + np.multiply(views[\"tau\"], sigma_prior) - \\ -->\n\n<!--         np.matmul(matmul_left, np.matmul(np.linalg.inv(matmul_mid + omega), -->\n\n<!--                                          np.multiply(views[\"tau\"], np.matmul(views[\"P\"], sigma_prior)))) -->\n\n<!--     weights_posterior = np.matmul(pi_posterior.T, np.linalg.inv(lmbda * sigma_prior)) -->\n\n<!--     # implied confidence -->\n\n<!--     pi_posterior_100 = pi_prior + np.matmul(matmul_left, np.matmul(np.linalg.inv(matmul_mid), matmul_right)) -->\n\n<!--     weights_posterior_100 = np.matmul(pi_posterior_100.T, np.linalg.inv(lmbda * sigma_prior)) -->\n\n<!--     implied_confidence = (weights_posterior - weights_prior) / (weights_posterior_100 - weights_prior) -->\n\n<!--     result = {\"implied_confidence\": implied_confidence, -->\n\n<!--               \"weights_prior\": np.matrix(weights_prior), -->\n\n<!--               \"weights_posterior\": weights_posterior, -->\n\n<!--               \"pi_prior\": np.transpose(pi_prior), -->\n\n<!--               \"pi_posterior\": np.transpose(pi_posterior), -->\n\n<!--               \"sigma_prior\": sigma_prior, -->\n\n<!--               \"sigma_posterior\": sigma_posterior} -->\n\n<!--     return result -->\n\n<!-- ``` -->\n\n<!-- ```{python} -->\n\n<!-- tau = 0.025 -->\n\n<!-- P = np.diag([1] * len(factors)) -->\n\n<!-- Q = np.transpose(np.matrix(implied_shocks([0.1], overlap_x_mat, overlap_x_mat[:, 0], 1))) -->\n\n<!-- C = [0.95] * len(factors) -->\n\n<!-- views = {\"tau\": tau, \"P\": P, \"Q\": Q, \"C\": C} -->\n\n<!-- ``` -->\n\n<!-- ```{python} -->\n\n<!-- bl = black_litterman(params3, ir, sigma, views) -->\n\n<!-- bl -->\n\n<!-- ``` -->\n\n<!-- ```{python} -->\n\n<!-- params4 = np.array(bl[\"weights_posterior\"])[0] -->\n\n<!-- params4 = params4 / sum(params4) # no leverage -->\n\n<!-- params4 -->\n\n<!-- ``` -->\n\n<!-- ```{python} -->\n\n<!-- np.matmul(mu, params4) -->\n\n<!-- ``` -->\n\n<!-- ```{python} -->\n\n<!-- np.sqrt(np.matmul(np.transpose(params4), np.matmul(sigma, params4))) -->\n\n<!-- ``` -->\n\n## Risk parity\n\nRisk parity is an approach to portfolio management that focuses on allocation of risk rather than allocation of capital. In a risk parity strategy, the asset allocations are leveraged, or deleveraged, to have equal risk contributions. Suppose that $\\mathbf{R}$ is a $T \\times N$ matrix of asset returns where the return of the $i^{th}$ asset is $R_{i,t}$ at time $t$. Define $\\Sigma$ to be the covariance matrix of $\\mathbf{R}$ and let $\\mathbf{w}=(w_{1},\\dots,w_{N})$ be a vector of asset weights. Then the volatility of the return of the strategy is $\\sigma_{P}=\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}$ and, by Euler's Theorem, satisfies:\n\n$$\n\\begin{aligned}\n\\sigma_{P}&=\\sum_{i=1}^{N}w_{i}\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}\\\\\n&=w_{1}\\frac{\\partial\\sigma_{P}}{\\partial w_{1}}+\\dots+w_{N}\\frac{\\partial\\sigma_{P}}{\\partial w_{N}}\n\\end{aligned}\n$$\n\nwhere each element is the risk contribution of the $i^{th}$ risky asset. The risk parity objective solves for weights such that each asset contributes equal risk using the following nonlinear constrained optimization problem:\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\displaystyle\\sum_{i=1}^{N}\\log(w_{i})\\\\\n\\textrm{s.t.}&\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}&\\leq&\\sigma \n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce a new variable $\\lambda$ that is the Lagrange multiplier and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\sum_{i=1}^{N}\\log(w_{i})-\\lambda(\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}-\\sigma)\n\\end{aligned}\n$$\n\nThen set the partial derivatives of $\\mathcal{L}$ equal to zero for each asset $i$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w_{i}}&=\\frac{1}{w_{i}}-\\lambda\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}=0\n\\Leftrightarrow\nw_{i}\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}=\\frac{1}{\\lambda}\n\\end{aligned}\n$$\n\nNotice that $1/\\lambda$ is the risk contribution of the $i^{th}$ asset. Now use `Python` to maximize the Lagrangian numerically:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# http://faculty.washington.edu/ezivot/econ424/riskbudgetingslides.pdf\n# https://systematicinvestor.wordpress.com/2011/11/16/black-litterman-model/\n# https://cran.r-project.org/web/packages/BLCOP/vignettes/BLCOP.pdf\n# http://math.stackexchange.com/questions/17776/inverse-of-the-sum-of-matrices\ndef risk_parity_obj(params, sigma, target):\n    \n    risk = np.sqrt(np.matmul(np.transpose(params), np.matmul(sigma, params)))\n    risk_contrib = target / len(params)\n    \n    result = -(sum(np.log(params)) - (1 / risk_contrib) * (risk - target))\n    \n    return result\n\ndef risk_parity_optim(params, sigma, target):\n    \n    result = minimize(risk_parity_obj, params, args = (sigma, target)).x\n    result = result / sum(result) # no leverage\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 1\nstart = np.array([1] * len(factors))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams5 = risk_parity_optim(start, sigma, target)\nparams5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0.02197783, 0.09196207, 0.67896   , 0.2071001 ])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nrisk = np.sqrt(np.matmul(np.transpose(params5), np.matmul(sigma, params5)))\nrisk_contrib = np.multiply(params5, np.matmul(sigma, params5)) / risk\nrisk_contrib\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0.00207012, 0.00206996, 0.00207006, 0.00207012])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.matmul(mu, params5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.002999229811662311\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params5), np.matmul(sigma, params5))) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.008280260152428262\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npd.DataFrame.from_dict({\"max_pnl\": params1,\n                        \"min_risk\": params2,\n                        \"max_ratio\": params3,\n                        # \"black_litterman\": params4,\n                        \"risk_parity\": params5})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        max_pnl      min_risk     max_ratio  risk_parity\n0  3.334782e-01  2.974307e-01  3.550211e-01     0.021978\n1  5.675963e-01  5.521405e-01  6.025244e-01     0.091962\n2  9.892542e-02  1.504288e-01  4.245457e-02     0.678960\n3  2.276452e-16  2.498002e-16  2.787484e-16     0.207100\n```\n:::\n:::\n\n\n## Portfolio attribution\n\n### Single-period\n\nThe arithmetic active return is commonly decomposed using the Brinson-Fachler method:\n\n$$\n\\begin{aligned}\n\\text{Allocation: } &r_{a}=\\sum_{k=1}^{n}(w_{p,k}-w_{b,k})(r_{b,k}-r_{b})\\\\\n\\text{Selection: } &r_{s}=\\sum_{k=1}^{n}w_{p,k}(r_{p,k}-r_{b,k})\\\\\n\\end{aligned}\n$$\n\nwhere $k=1,\\ldots,n$ is each sector or factor.\n\n### Multi-period\n\nArithmetic attributes add to the active return of a single period; however, they cannot be summed or compounded to explain the active return over multiple periods. To solve this problem, the original arithmetic attribute is multiplied by a single scaling coefficient for that period. After all single-period original attributes have been transformed, the adjusted attributes sum to the active return over the periods.\n\n$$\n\\begin{aligned}\n\\text{Carino scaling coefficient: } &c_{t}=\\frac{[\\ln(1+r_{p,t})-\\ln(1+r_{b,t})]/(r_{p,t}-r_{b,t})}{[\\ln(1+r_{p})-\\ln(1+r_{b})]/(r_{p}-r_{b})}\n\\end{aligned}\n$$\n\nwhere $t=1,\\ldots,n$ is each period.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# http://www.frongello.com/support/Works/Chap20RiskBook.pdf\n# https://github.com/R-Finance/PortfolioAttribution/blob/master/R/Carino.R\ndef pnl_attrib(params, x):\n    \n    total_i = np.sum(x, axis = 1)\n    total = np.prod(1 + total_i) - 1\n    \n    coef = (np.log(1 + total_i) / total_i) / (np.log(1 + total) / total)\n    \n    result = np.sum(np.multiply(x, coef), axis = 0)\n    \n    return np.ravel(result)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nattrib_mat = np.multiply(params1, np.matrix(returns_x_df)[-width:])\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npnl_attrib(params1, attrib_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([ 6.28488339e-02, -3.29098189e-02,  3.97256516e-04,  2.34689866e-18])\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}