{
  "hash": "b7aa886e85f229bd48e07664ef4d3f1b",
  "result": {
    "markdown": "---\ntitle: \"Portfolios\"\nauthor: \"[Jason Foster](mailto:jason.j.foster@gmail.com)\"\ndate: last-modified\ncategories:\n  - analysis\n  - finance\n  - python\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas_datareader as pdr\nfrom scipy.stats import norm, chi2\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfactors_r = [\"SP500\", \"DTWEXAFEGS\"] # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d = [\"DGS10\", \"BAMLH0A0HYM2\"]\nfactors = factors_r + factors_d\nwidth = 252\nscale = {\"periods\": 252, \"overlap\": 5}\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n-   <https://pandas-datareader.readthedocs.io/en/latest/remote_data.html>\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlevels_df = pdr.get_data_fred(factors, start = \"1900-01-01\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_df = levels_df.apply(lambda x: np.log(x).diff() if x.name in factors_r else -x.diff() / 100)\noverlap_df = returns_df.rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df = pd.concat([returns_df, overlap_df], keys = [\"returns\", \"overlap\"], axis = 1)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# import datetime\nfrom scipy.optimize import minimize\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\n```\n:::\n\n\n-   Open: <https://github.com/pydata/pandas-datareader/issues/965>\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntickers = [\"BAICX\"] # fund inception date is \"2011-11-28\"\nprices_df = pdr.get_data_tiingo(tickers, start = \"1900-01-01\", api_key = os.getenv(\"TIINGO_API_KEY\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nC:\\Users\\jason\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas_datareader\\tiingo.py:234: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  return pd.concat(dfs, self._concat_axis)\n```\n:::\n\n```{.python .cell-code}\nprices_df = prices_df.pivot_table(index = \"date\", columns = \"symbol\", values = \"adjClose\") \\\n    .tz_localize(None)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_cols = list(zip([\"returns\"], tickers))\noverlap_cols = list(zip([\"overlap\"], tickers))\nreturns_df[returns_cols] = np.log(prices_df).diff()\nreturns_df[overlap_cols] = returns_df[returns_cols].rolling(scale[\"overlap\"], min_periods = 1).mean()\nreturns_df.sort_index(axis = 1, inplace = True)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# weights = np.array([0.9 ** i for i in range(width - 1, -1, -1)]).reshape((width, 1))\nweights = np.array([1] * width).reshape((width, 1))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\noverlap_x_df = returns_df.dropna()[\"overlap\"][factors]\noverlap_y_df = returns_df.dropna()[\"overlap\"][tickers]\noverlap_x_mat = np.matrix(overlap_x_df[-width:])\noverlap_y_mat = np.matrix(overlap_y_df[-width:])\n```\n:::\n\n\n## Principal component analysis\n\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (*note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales*) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\n### Eigendecomposition\n\n$$\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# https://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\ndef eigen_decomp(x, comps):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    V = V[:, idx]\n        \n    L = L[:comps]\n    V = V[:, :comps]\n    \n    result = np.matmul(V, np.multiply(L, V.T))\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ncomps = 1\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\neigen_decomp(overlap_x_mat, comps) * scale[\"periods\"] * scale[\"overlap\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[ 2.10177072e-02, -4.77529237e-03,  5.86663972e-04,\n         1.43982775e-03],\n       [-4.77529237e-03,  1.08496217e-03, -1.33291988e-04,\n        -3.27133612e-04],\n       [ 5.86663972e-04, -1.33291988e-04,  1.63754596e-05,\n         4.01896867e-05],\n       [ 1.43982775e-03, -3.27133612e-04,  4.01896867e-05,\n         9.86360661e-05]])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# np.cov(overlap_x_mat.T) * scale[\"periods\"] * scale[\"overlap\"]\n```\n:::\n\n\n### Variance explained\n\nWe often look at the proportion of variance explained by the first $i$ principal components as an indication of how many components are needed.\n\n$$\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef variance_explained(x):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))   \n    idx = L.argsort()[::-1]\n    L = L[idx]\n    \n    result = L.cumsum() / L.sum()\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nvariance_explained(overlap_x_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0.85645941, 0.99017229, 0.99659713, 1.        ])\n```\n:::\n:::\n\n\n### Cosine similarity\n\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\n$$\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef eigen_vals(x):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    L = L[idx]\n    \n    return pd.DataFrame(L)\n\ndef eigen_vecs(x):\n    \n    L, V = np.linalg.eig(np.cov(x.T, ddof = 1))\n    idx = L.argsort()[::-1]\n    V = V[:, idx]\n    \n    return pd.DataFrame(V)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef roll_eigen1(x, width, comp):\n    \n    n_rows = len(x)\n    result = pd.DataFrame()\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evec = eigen_vecs(x.iloc[idx]).iloc[:, comp - 1]\n        result = result.append(evec.transpose())\n    \n    result.index = x.index[(width - 1):]\n    result.columns = x.columns\n    \n    return result  \n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ncomp = 1\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nraw_df = roll_eigen1(overlap_x_df, width, comp)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_mlt <- melt(as.data.table(py$raw_df, keep.rownames = \"index\"), id.vars = \"index\")\nraw_mlt[ , index := as.Date(index)]\nraw_plt <- plot_ts(raw_mlt, title = \"Eigenvector 1Y\")\nprint(raw_plt)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=576}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# https://quant.stackexchange.com/a/3095\ndef roll_eigen2(x, width, comp):\n    \n    n_rows = len(x)\n    result = pd.DataFrame()\n    \n    for i in range(width - 1, n_rows):\n        \n        idx = range(max(i - width + 1, 0), i + 1)\n        evec = eigen_vecs(x.iloc[idx]).iloc[:, comp - 1]\n        \n        if i > width - 1:\n            \n            similarity = np.matmul(np.matrix(evec),\n                                   np.matrix(result.iloc[-1, :]).T)\n            evec = pd.DataFrame(np.multiply(np.sign(similarity), np.matrix(evec))) \n            result = result.append(evec)\n            \n        else:\n            result = result.append(evec.transpose())\n    \n    result.index = x.index[(width - 1):]\n    result.columns = x.columns\n    \n    return result  \n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nclean_df = roll_eigen2(overlap_x_df, width, comp)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_mlt <- melt(as.data.table(py$clean_df, keep.rownames = \"index\"), id.vars = \"index\")\nclean_mlt[ , index := as.Date(index)]\nclean_plt <- plot_ts(clean_mlt, title = \"Eigenvector 1Y\")\nprint(clean_plt)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=576}\n:::\n:::\n\n\n<!-- ### Contour ellipsoid -->\n\n<!-- The contours of a multivariate normal (MVN) distribution are ellipsoids centered at the mean. The directions of the axes are given by the eigenvectors of the covariance matrix and squared lengths are given by the eigenvalues: -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- ({\\mathbf{x}}-{\\boldsymbol{\\mu}})^{\\mathrm{T}}{\\boldsymbol{\\Sigma}}^{-1}({\\mathbf{x}}-{\\boldsymbol{\\mu}})=c^{2} -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- Or, in general parametric form: -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- X(t)&=X_{c}+a\\,\\cos t\\,\\cos \\varphi -b\\,\\sin t\\,\\sin \\varphi\\\\ -->\n<!-- Y(t)&=Y_{c}+a\\,\\cos t\\,\\sin \\varphi +b\\,\\sin t\\,\\cos \\varphi -->\n<!-- \\end{aligned} -->\n<!-- $$ where $t$ varies from $0,\\ldots,2\\pi$. Here $(X_{c},Y_{c})$ is the center of the ellipse and $\\varphi$ is the angle between the x-axis and the major axis of the ellipse. -->\n\n<!-- Specifically: -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- &\\text{Center: }\\boldsymbol{\\mu}=(X_{c},Y_{c})\\\\ -->\n<!-- &\\text{Radius: }c^{2}= \\chi_{\\alpha}^{2}(df)\\\\ -->\n<!-- &\\text{Length: }a=c\\sqrt{\\lambda_{k}}\\\\ -->\n<!-- &\\text{Angle of rotation: }\\varphi=\\text{atan2}\\left(\\frac{V_{k}(2)}{V_{k}(1)}\\right) -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ```{python} -->\n<!-- # https://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/ -->\n<!-- # https://maitra.public.iastate.edu/stat501/lectures/MultivariateNormalDistribution-I.pdf -->\n<!-- # https://en.wikipedia.org/wiki/Multivariate_normal_distribution -->\n<!-- # https://en.wikipedia.org/wiki/Ellipse#General_parametric_form -->\n<!-- def ellipse(n_sim, x, y, sigma): -->\n\n<!--     data = np.concatenate((x, y), axis = 1) -->\n<!--     L, V = np.linalg.eig(np.cov(data.T, ddof = 1)) -->\n<!--     idx = L.argsort()[::-1] -->\n<!--     L = L[idx] -->\n<!--     V = V[:, idx] -->\n\n<!--     c = np.sqrt(chi2.ppf(norm.cdf(sigma), 2)) -->\n<!--     t = np.linspace(0, 2 * np.pi, n_sim) -->\n<!--     phi = np.arctan2(V[1, 0], V[0, 0]) -->\n<!--     a = c * np.sqrt(L[0]) * np.cos(t) -->\n<!--     b = c * np.sqrt(L[1]) * np.sin(t) -->\n<!--     R = np.matrix([[np.cos(phi), np.sin(phi)], [-np.sin(phi), np.cos(phi)]]) -->\n<!--     r = np.matmul(np.matrix([a, b]).T, R) -->\n\n<!--     result = np.add(r, np.mean(data, axis = 0)) # 2D only -->\n\n<!--     return result -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- returns_x_df = returns_df.dropna()[\"returns\"][factors]  -->\n<!-- returns_x_mat = np.matrix(returns_x_df) # extended history -->\n<!-- ellipse_x_mat = ellipse(1000, returns_x_mat[:, [0]], returns_x_mat[:, [2]], 1) -->\n<!-- ``` -->\n\n<!-- ```{r, fig.width = 3, fig.height = 3} -->\n<!-- ellipse_plt <- plot_scatter(data.table(py$returns_x_mat[ , c(1, 3)]), x = \"V1\", y = \"V2\", -->\n<!--                             title = \"Return 1D (%)\", xlab = \"SP500\", ylab = \"DGS10\") + -->\n<!--   geom_point(data = data.table(py$ellipse_x_mat), aes(x = V1 * 100, y = V2 * 100)) -->\n<!-- print(ellipse_plt) -->\n<!-- ``` -->\n\n<!-- ### Marchenko--Pastur distribution -->\n\n<!-- Marchenko--Pastur distribution is the limiting distribution of eigenvalues of Wishart matrices as the matrix dimension $m$ and degrees of freedom $n$ both tend to infinity with ratio $m/n\\,\\to \\,\\lambda\\in(0,+\\infty)$: -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- d\\nu(x)&={\\frac {1}{2\\pi\\sigma ^{2}}}{\\frac{\\sqrt{(\\lambda_{+}-x)(x-\\lambda_{-})}}{\\lambda x}}\\,\\mathbf{1}_{x\\in[\\lambda_{-},\\lambda _{+}]}\\,dx -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- with -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\lambda_{\\pm}&=\\sigma^{2}(1\\pm{\\sqrt{\\lambda }})^{2} -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ```{python} -->\n<!-- # https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution -->\n<!-- # https://faculty.baruch.cuny.edu/jgatheral/RandomMatrixCovariance2008.pdf -->\n<!-- def dmp(x, sigma = 1): -->\n\n<!--     L, V = np.linalg.eig(np.cov(x.T, ddof = 1)) -->\n<!--     idx = L.argsort()[::-1] -->\n<!--     L = L[idx] -->\n\n<!--     lmbda = x.shape[1] / x.shape[0] -->\n<!--     lower = sigma * (1 - np.sqrt(lmbda)) ** 2 -->\n<!--     upper = sigma * (1 + np.sqrt(lmbda)) ** 2 -->\n\n<!--     d = np.where((L <= lower) | (L >= upper), 0, -->\n<!--                  1 / (2 * np.pi * sigma * lmbda * L) * np.sqrt((upper - L) * (L - lower))) -->\n\n<!--     return d -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- n_sim = 5000 -->\n<!-- n_cols = 1000 -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- data_sim = np.random.normal(size = n_sim * n_cols).reshape((n_sim, n_cols)) -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- L, V = np.linalg.eig(np.cov(data_sim.T, ddof = 1)) -->\n<!-- idx = L.argsort()[::-1] -->\n<!-- L = L[idx] -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- dmp_df = pd.DataFrame.from_dict({\"evals\": L, -->\n<!--                                  \"dmp\": dmp(data_sim)}) -->\n<!-- ``` -->\n\n<!-- ```{r, fig.width = 4, fig.height = 3} -->\n<!-- dmp_plt <- plot_density(py$dmp_df, x = \"evals\", y = \"dmp\", -->\n<!--                         title = \"Marchenko-Pastur distribution\", xlab = \"Eigenvalues\", ylab = \"Density\") -->\n<!-- print(dmp_plt) -->\n<!-- ``` -->\n\n## Random portfolios\n\nNeed to generate uniformly distributed weights $\\mathbf{w}=(w_{1},w_{2},\\ldots,w_{N})$ such that $\\sum_{j=1}^{N}w_{i}=1$ and $w_{i}\\geq0$:\n\n-   **Approach 1**: tempting to use $w_{i}=\\frac{u_{i}}{\\sum_{j=1}^{N}u_{i}}$ where $u_{i}\\sim U(0,1)$ but the distribution of $\\mathbf{w}$ is not uniform\n\n-   **Approach 2**: instead, generate $\\text{Exp}(1)$ and then normalize\n\nCan also scale random weights by $M$, e.g. if sum of weights must be 10% then multiply weights by 10%.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_weights1(n_sim, n_assets, lmbda):  \n    \n    rand_exp = np.matrix(np.random.uniform(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = np.divide(rand_exp, rand_exp_sum)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# Methodology: uniform sampling from the simplex (http://mathoverflow.net/a/76258)\n# z ~ U(0, 1) then -ln(z) is an exponential(1) distribution\n# This is also known as generating a random vector from the symmetric Dirichlet distribution\ndef rand_weights2(n_sim, n_assets, lmbda):   \n    \n    rand_exp = np.matrix(-np.log(1 - np.random.uniform(size = (n_sim, n_assets))) / lmbda)\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = np.divide(rand_exp, rand_exp_sum)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# 1. Generate n exponential(1) random variables x_1, x_2, ..., x_n\n# 2. Let y_i = x_i / (sum_{i = 1}^{n} x_i)\ndef rand_weights3(n_sim, n_assets, lmbda):\n    \n    rand_exp = np.matrix(np.random.exponential(size = (n_sim, n_assets)))\n    rand_exp_sum = np.sum(rand_exp, axis = 1)\n    \n    result = np.divide(rand_exp, rand_exp_sum)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlmbda = 1\nn_assets = 3\nn_sim = 10000\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach1 = rand_weights1(n_sim, n_assets, lmbda)\napproach2 = rand_weights2(n_sim, n_assets, lmbda)\napproach3 = rand_weights3(n_sim, n_assets, lmbda)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(py$approach1), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(py$approach2), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-34-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(py$approach3), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-35-1.png){width=384}\n:::\n:::\n\n\n### Random turnover\n\nHow to generate random weights between lower bound $a$ and upper bound $b$ that sum to zero?\n\n-   **Approach 1**: tempting to multiply random weights by $M$ and then subtract by $\\frac{M}{N}$ but the distribution is not between $a$ and $b$\n\n-   **Approach 2**: instead, use an iterative approach for random turnover:\n\n    1.  Generate $N-1$ uniformly distributed weights between $a$ and $b$\n    2.  For $u_{N}$ compute sum of values and subtract from $M$\n    3.  If $u_{N}$ is between $a$ and $b$, then keep; otherwise, discard\n\nThen add random turnover to previous period's random weights.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_iterative(n_assets, lower, upper, target):\n    \n    plug = False\n    \n    while not plug:\n        \n        result = np.random.uniform(low = lower, high = upper, size = n_assets - 1)\n        temp = target - sum(result)\n        \n        if ((temp <= upper) and (temp >= lower)):\n            plug = True\n        \n    result = np.append(result, temp)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_turnover1(n_sim, n_assets, lower, upper, target):\n    \n    rng = upper - lower\n    \n    result = rand_weights3(n_sim, n_assets, lmbda) * rng\n    result = result - rng / n_assets\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef rand_turnover2(n_sim, n_assets, lower, upper, target):\n    \n    result = np.matrix(rand_iterative(n_assets, lower, upper, target))\n    \n    while result.shape[0] < n_sim:\n    \n        temp = np.matrix(rand_iterative(n_assets, lower, upper, target))\n        result = np.concatenate((result, temp), axis = 0)\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlower = -0.05\nupper = 0.05\ntarget = 0\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\napproach1 = rand_turnover1(n_sim, n_assets, lower, upper, target)\napproach2 = rand_turnover2(n_sim, n_assets, lower, upper, target)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(py$approach1), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-41-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(py$approach2), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-42-1.png){width=384}\n:::\n:::\n\n\n## Mean-variance\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef geometric_mean(x, scale):\n    \n    result = np.prod(1 + x) ** (scale / x.shape[1]) - 1\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nreturns_x_df = returns_df.dropna()[\"returns\"][factors] # REMOVE LATER\nreturns_x_mat = np.matrix(returns_x_df) # extended history # REMOVE LATER\nmu = np.apply_along_axis(geometric_mean, 0, returns_x_mat, scale[\"periods\"])\nsigma = np.cov(overlap_x_df.T, ddof = 1) * scale[\"periods\"] * scale[\"overlap\"]\n```\n:::\n\n\n### Maximum return\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 0.06\nstart = np.array([1] * len(factors))\nbnds = [(np.finfo(float).eps, 1) for i in range(len(factors))]\ncons = [{\"type\": \"ineq\", \"fun\": lambda params, sigma, target: max_pnl_cons(params, sigma, target),\n         \"args\": (sigma, target)},\n        {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef max_pnl_cons(params, sigma, target):\n    \n    var = np.matmul(np.transpose(params), np.matmul(sigma, params))\n    \n    result = target ** 2 - var\n    \n    return result\n\ndef max_pnl_obj(params, mu):\n    \n    result = np.matmul(mu, params)\n    \n    return -result\n\ndef max_pnl_optim(params, mu):\n    \n    result = minimize(max_pnl_obj, params, args = (mu), bounds = bnds, constraints = cons)\n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams1 = max_pnl_optim(start, mu)\nparams1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([3.26405507e-01, 5.98307299e-01, 7.52871933e-02, 2.22044605e-16])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.matmul(mu, params1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.03376578562571013\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params1), np.matmul(sigma, params1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.06000001495531018\n```\n:::\n:::\n\n\n### Minimum variance\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min_{x}&\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 0.03\nstart = np.array([1] * len(factors))\ncons = [{\"type\": \"ineq\", \"fun\": lambda params, mu, target: min_risk_cons(params, mu, target),\n         \"args\": (mu, target)},\n        {\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef min_risk_cons(params, mu, target):\n    \n    result = np.matmul(mu, params) - target\n    \n    return result\n\ndef min_risk_obj(params, sigma):\n    \n    result = np.matmul(np.transpose(params), np.matmul(sigma, params))\n    \n    return result\n\ndef min_risk_optim(params, sigma):\n    \n    result = minimize(min_risk_obj, params, args = (sigma), bounds = bnds, constraints = cons)\n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams2 = min_risk_optim(start, sigma)\nparams2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0.28129901, 0.57444227, 0.14268022, 0.0015785 ])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.matmul(mu, params2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.030000000014880894\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params2), np.matmul(sigma, params2))) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.053329800985138204\n```\n:::\n:::\n\n\n### Maximum ratio\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}-\\frac{1}{2}\\delta(\\mathbf{w}^T\\Sigma\\mathbf{w})\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\nir = 0.5\ntarget = ir / 0.06 # ir / std (see Black-Litterman)\nstart = np.array([1] * len(factors))\ncons = [{\"type\": \"eq\", \"fun\": lambda params: np.sum(params) - 1}]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(start)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 1 1 1]\n```\n:::\n\n```{.python .cell-code}\nprint(start.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(4,)\n```\n:::\n\n```{.python .cell-code}\nprint(mu)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 0.07405352  0.01611388 -0.00062083  0.00057442]\n```\n:::\n\n```{.python .cell-code}\nprint(mu.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(4,)\n```\n:::\n\n```{.python .cell-code}\nprint(sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ 3.01221457e-02 -3.09311371e-03 -1.82736546e-04  2.54333924e-03]\n [-3.09311371e-03  4.52148557e-03 -1.23227666e-04 -3.75643354e-04]\n [-1.82736546e-04 -1.23227666e-04  7.13041018e-05 -4.01338204e-05]\n [ 2.54333924e-03 -3.75643354e-04 -4.01338204e-05  4.27710435e-04]]\n```\n:::\n\n```{.python .cell-code}\nprint(sigma.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(4, 4)\n```\n:::\n\n```{.python .cell-code}\nprint(np.matmul(np.transpose(start), np.matmul(sigma, start)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.032599614083650405\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef max_ratio_obj(params, mu, sigma, target):\n    \n    result = np.matmul(mu, params) - 0.5 * target * (np.matmul(np.transpose(params),\n                                                               np.matmul(sigma, params)))\n#     result = np.matmul(mu, params) / np.sqrt(np.matmul(np.transpose(params), np.matmul(sigma, params)))\n    \n    return -result\n\ndef max_ratio_optim(params, mu, sigma, target):\n    \n    result = minimize(max_ratio_obj, params, args = (mu, sigma, target), bounds = bnds,\n                      constraints = cons) \n    \n    return result.x\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams3 = max_ratio_optim(start, mu, sigma, target)\nparams3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([3.56760581e-01, 6.43239419e-01, 2.22044605e-16, 2.63244287e-16])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.matmul(mu, params3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.03678445682847213\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params3), np.matmul(sigma, params3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.06546033283152945\n```\n:::\n:::\n\n\n<!-- ## Black-Litterman -->\n\n<!-- ### Prior distribution -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\text{Risk aversion: } &\\lambda=\\frac{E(r)-r_{f}}{\\sigma^{2}}=\\frac{IR}{\\sigma}\\\\ -->\n<!-- \\text{Implied returns: } &\\Pi=\\lambda\\Sigma w\\\\ -->\n<!-- \\text{Distribution: } &N\\sim(\\Pi,\\tau\\Sigma) -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ```{python} -->\n<!-- def implied_pnl(params, ir, sigma): -->\n\n<!--     lmbda = ir / np.sqrt(np.matmul(np.transpose(params), np.matmul(sigma, params))) -->\n\n<!--     result = np.matmul(lmbda * sigma, params) -->\n\n<!--     return result -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- implied_pnl(params3, ir, sigma) -->\n<!-- ``` -->\n\n<!-- ### Conditional distribution -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\text{Prior mean variance: } &\\tau\\in(0.01, 0.05)\\approx(0.025)\\\\ -->\n<!-- \\text{Asset views: } &\\mathbf{P}={\\begin{bmatrix} -->\n<!-- p_{11}&\\cdots&p_{1n}\\\\ -->\n<!-- \\vdots&\\ddots&\\vdots\\\\ -->\n<!-- p_{k1}&\\cdots&p_{kn} -->\n<!-- \\end{bmatrix}}= -->\n<!-- {\\begin{bmatrix} -->\n<!-- 0&0&0&0&0&0&1&0\\\\ -->\n<!-- -1&1&0&0&0&0&0&0\\\\ -->\n<!-- 0&0&0.5&-0.5&0.5&-0.5&0&0 -->\n<!-- \\end{bmatrix}}\\\\ -->\n<!-- \\text{View returns: } &\\mathbf{Q}={\\begin{bmatrix} -->\n<!-- q_{1}\\\\ -->\n<!-- \\vdots\\\\ -->\n<!-- q_{k} -->\n<!-- \\end{bmatrix}}= -->\n<!-- {\\begin{bmatrix} -->\n<!-- 0.0525\\\\ -->\n<!-- 0.0025\\\\ -->\n<!-- 0.0200 -->\n<!-- \\end{bmatrix}}\\\\ -->\n<!-- \\text{View confidence: } &\\mathbf{C}={\\begin{bmatrix} -->\n<!-- c_{1}\\\\ -->\n<!-- \\vdots\\\\ -->\n<!-- c_{k} -->\n<!-- \\end{bmatrix}}= -->\n<!-- {\\begin{bmatrix} -->\n<!-- 0.2500\\\\ -->\n<!-- 0.5000\\\\ -->\n<!-- 0.6500 -->\n<!-- \\end{bmatrix}}\\\\ -->\n<!-- \\text{View covariance: } &\\mathbf{\\Omega}={\\begin{bmatrix} -->\n<!-- \\tau\\left(\\frac{1-c_{1}}{c_{1}}\\right)\\left(p_{1}\\Sigma p_{1}^{T}\\right)&0&0\\\\ -->\n<!-- 0&\\ddots&0\\\\ -->\n<!-- 0&0&\\tau\\left(\\frac{1-c_{k}}{c_{k}}\\right)\\left(p_{k}\\Sigma p_{k}^{T}\\right) -->\n<!-- \\end{bmatrix}}\\\\ -->\n<!-- \\text{Distribution: } &N\\sim(\\mathbf{Q}, \\mathbf{\\Omega}) -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ### Posterior distribution -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\text{Implied returns: } &\\hat{\\Pi}=\\Pi+\\tau\\Sigma \\mathbf{P}^{T}\\left(\\tau \\mathbf{P}\\Sigma \\mathbf{P}^{T}+\\mathbf{\\Omega}\\right)^{-1}\\left(\\mathbf{Q}-\\mathbf{P}\\Pi^{T}\\right)\\\\ -->\n<!-- \\text{Covariance: } &\\hat{\\Sigma}=\\Sigma+\\tau\\left[\\Sigma-\\Sigma\\mathbf{P}^{T}\\left(\\tau\\mathbf{P}\\Sigma\\mathbf{P}^{T}+\\mathbf{\\Omega}\\right)^{-1}\\tau\\mathbf{P}\\Sigma\\right]\\\\ -->\n<!-- \\text{Weights: } &\\hat{w}=\\hat{\\Pi}\\left(\\lambda\\Sigma\\right)^{-1}\\\\ -->\n<!-- \\text{Distribution: } &N\\sim\\left(\\left[\\left(\\tau\\Sigma\\right)^{-1}+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{P}\\right]^{-1}\\left[\\left(\\tau\\Sigma\\right)^{-1}\\Pi+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{Q}\\right],\\left[\\left(\\tau\\Sigma\\right)^{-1}+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{P}\\right]^{-1}\\right) -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ```{python} -->\n<!-- def black_litterman(params, ir, sigma, views): -->\n\n<!--     # prior distribution -->\n<!--     weights_prior = params -->\n<!--     sigma_prior = sigma -->\n<!--     lmbda = ir / np.sqrt(np.matmul(np.transpose(weights_prior), np.matmul(sigma_prior, weights_prior))) -->\n<!--     pi_prior = np.transpose(np.matrix(np.matmul(lmbda * sigma_prior, weights_prior))) -->\n\n<!--     # matrix calculations -->\n<!--     matmul_left = np.multiply(views[\"tau\"], np.matmul(sigma_prior, views[\"P\"].T)) -->\n<!--     matmul_mid = np.multiply(views[\"tau\"], np.matmul(views[\"P\"], np.matmul(sigma_prior, views[\"P\"].T))) -->\n<!--     matmul_right = views[\"Q\"] - np.matmul(views[\"P\"], pi_prior) -->\n\n<!--     # conditional distribution -->\n<!--     omega = np.diag(np.diag(np.matmul(np.diag([(1 - x) / x for x in views[\"C\"]]), matmul_mid))) -->\n\n<!--     # posterior distribution -->\n<!--     pi_posterior = pi_prior + np.matmul(matmul_left, np.matmul(np.linalg.inv(matmul_mid + omega), matmul_right)) -->\n\n<!--     sigma_posterior = sigma_prior + np.multiply(views[\"tau\"], sigma_prior) - \\ -->\n<!--         np.matmul(matmul_left, np.matmul(np.linalg.inv(matmul_mid + omega), -->\n<!--                                          np.multiply(views[\"tau\"], np.matmul(views[\"P\"], sigma_prior)))) -->\n\n<!--     weights_posterior = np.matmul(pi_posterior.T, np.linalg.inv(lmbda * sigma_prior)) -->\n\n<!--     # implied confidence -->\n<!--     pi_posterior_100 = pi_prior + np.matmul(matmul_left, np.matmul(np.linalg.inv(matmul_mid), matmul_right)) -->\n\n<!--     weights_posterior_100 = np.matmul(pi_posterior_100.T, np.linalg.inv(lmbda * sigma_prior)) -->\n\n<!--     implied_confidence = (weights_posterior - weights_prior) / (weights_posterior_100 - weights_prior) -->\n\n<!--     result = {\"implied_confidence\": implied_confidence, -->\n<!--               \"weights_prior\": np.matrix(weights_prior), -->\n<!--               \"weights_posterior\": weights_posterior, -->\n<!--               \"pi_prior\": np.transpose(pi_prior), -->\n<!--               \"pi_posterior\": np.transpose(pi_posterior), -->\n<!--               \"sigma_prior\": sigma_prior, -->\n<!--               \"sigma_posterior\": sigma_posterior} -->\n\n<!--     return result -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- tau = 0.025 -->\n<!-- P = np.diag([1] * len(factors)) -->\n<!-- Q = np.transpose(np.matrix(implied_shocks([0.1], overlap_x_mat, overlap_x_mat[:, 0], 1))) -->\n<!-- C = [0.95] * len(factors) -->\n<!-- views = {\"tau\": tau, \"P\": P, \"Q\": Q, \"C\": C} -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- bl = black_litterman(params3, ir, sigma, views) -->\n<!-- bl -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- params4 = np.array(bl[\"weights_posterior\"])[0] -->\n<!-- params4 = params4 / sum(params4) # no leverage -->\n<!-- params4 -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- np.matmul(mu, params4) -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- np.sqrt(np.matmul(np.transpose(params4), np.matmul(sigma, params4))) -->\n<!-- ``` -->\n\n## Risk parity\n\nRisk parity is an approach to portfolio management that focuses on allocation of risk rather than allocation of capital. In a risk parity strategy, the asset allocations are leveraged, or deleveraged, to have equal risk contributions. Suppose that $\\mathbf{R}$ is a $T \\times N$ matrix of asset returns where the return of the $i^{th}$ asset is $R_{i,t}$ at time $t$. Define $\\Sigma$ to be the covariance matrix of $\\mathbf{R}$ and let $\\mathbf{w}=(w_{1},\\dots,w_{N})$ be a vector of asset weights. Then the volatility of the return of the strategy is $\\sigma_{P}=\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}$ and, by Euler's Theorem, satisfies:\n\n$$\n\\begin{aligned}\n\\sigma_{P}&=\\sum_{i=1}^{N}w_{i}\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}\\\\\n&=w_{1}\\frac{\\partial\\sigma_{P}}{\\partial w_{1}}+\\dots+w_{N}\\frac{\\partial\\sigma_{P}}{\\partial w_{N}}\n\\end{aligned}\n$$\n\nwhere each element is the risk contribution of the $i^{th}$ risky asset. The risk parity objective solves for weights such that each asset contributes equal risk using the following nonlinear constrained optimization problem:\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\displaystyle\\sum_{i=1}^{N}\\log(w_{i})\\\\\n\\textrm{s.t.}&\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}&\\leq&\\sigma \n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce a new variable $\\lambda$ that is the Lagrange multiplier and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\sum_{i=1}^{N}\\log(w_{i})-\\lambda(\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}-\\sigma)\n\\end{aligned}\n$$\n\nThen set the partial derivatives of $\\mathcal{L}$ equal to zero for each asset $i$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w_{i}}&=\\frac{1}{w_{i}}-\\lambda\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}=0\n\\Leftrightarrow\nw_{i}\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}=\\frac{1}{\\lambda}\n\\end{aligned}\n$$\n\nNotice that $1/\\lambda$ is the risk contribution of the $i^{th}$ asset. Now use `Python` to maximize the Lagrangian numerically:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# http://faculty.washington.edu/ezivot/econ424/riskbudgetingslides.pdf\n# https://systematicinvestor.wordpress.com/2011/11/16/black-litterman-model/\n# https://cran.r-project.org/web/packages/BLCOP/vignettes/BLCOP.pdf\n# http://math.stackexchange.com/questions/17776/inverse-of-the-sum-of-matrices\ndef risk_parity_obj(params, sigma, target):\n    \n    risk = np.sqrt(np.matmul(np.transpose(params), np.matmul(sigma, params)))\n    risk_contrib = target / len(params)\n    \n    result = -(sum(np.log(params)) - (1 / risk_contrib) * (risk - target))\n    \n    return result\n\ndef risk_parity_optim(params, sigma, target):\n    \n    result = minimize(risk_parity_obj, params, args = (sigma, target)).x\n    result = result / sum(result) # no leverage\n    \n    return result\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ntarget = 1\nstart = np.array([1] * len(factors))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparams5 = risk_parity_optim(start, sigma, target)\nparams5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0.02186021, 0.09158876, 0.68015919, 0.20639183])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nrisk = np.sqrt(np.matmul(np.transpose(params5), np.matmul(sigma, params5)))\nrisk_contrib = np.multiply(params5, np.matmul(sigma, params5)) / risk\nrisk_contrib\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0.00205912, 0.00205897, 0.00205906, 0.00205913])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.matmul(mu, params5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.002790967874284264\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.sqrt(np.matmul(np.transpose(params5), np.matmul(sigma, params5))) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.008236287723178887\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npd.DataFrame.from_dict({\"max_pnl\": params1,\n                        \"min_risk\": params2,\n                        \"max_ratio\": params3,\n                        # \"black_litterman\": params4,\n                        \"risk_parity\": params5})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        max_pnl  min_risk     max_ratio  risk_parity\n0  3.264055e-01  0.281299  3.567606e-01     0.021860\n1  5.983073e-01  0.574442  6.432394e-01     0.091589\n2  7.528719e-02  0.142680  2.220446e-16     0.680159\n3  2.220446e-16  0.001578  2.632443e-16     0.206392\n```\n:::\n:::\n\n\n## Portfolio attribution\n\n### Single-period\n\nThe arithmetic active return is commonly decomposed using the Brinson-Fachler method:\n\n$$\n\\begin{aligned}\n\\text{Allocation: } &r_{a}=\\sum_{k=1}^{n}(w_{p,k}-w_{b,k})(r_{b,k}-r_{b})\\\\\n\\text{Selection: } &r_{s}=\\sum_{k=1}^{n}w_{p,k}(r_{p,k}-r_{b,k})\\\\\n\\end{aligned}\n$$\n\nwhere $k=1,\\ldots,n$ is each sector or factor.\n\n### Multi-period\n\nArithmetic attributes add to the active return of a single period; however, they cannot be summed or compounded to explain the active return over multiple periods. To solve this problem, the original arithmetic attribute is multiplied by a single scaling coefficient for that period. After all single-period original attributes have been transformed, the adjusted attributes sum to the active return over the periods.\n\n$$\n\\begin{aligned}\n\\text{Carino scaling coefficient: } &c_{t}=\\frac{[\\ln(1+r_{p,t})-\\ln(1+r_{b,t})]/(r_{p,t}-r_{b,t})}{[\\ln(1+r_{p})-\\ln(1+r_{b})]/(r_{p}-r_{b})}\n\\end{aligned}\n$$\n\nwhere $t=1,\\ldots,n$ is each period.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# http://www.frongello.com/support/Works/Chap20RiskBook.pdf\n# https://github.com/R-Finance/PortfolioAttribution/blob/master/R/Carino.R\ndef pnl_attrib(params, x):\n    \n    total_i = np.sum(x, axis = 1)\n    total = np.prod(1 + total_i) - 1\n    \n    coef = (np.log(1 + total_i) / total_i) / (np.log(1 + total) / total)\n    \n    result = np.sum(np.multiply(x, coef), axis = 0)\n    \n    return np.ravel(result)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nattrib_mat = np.multiply(params1, np.matrix(returns_x_df)[-width:])\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npnl_attrib(params1, attrib_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([ 7.00131842e-02, -4.54433669e-02,  3.23924382e-04,  1.10987945e-18])\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}