{
  "hash": "e736acacdc4b8803e9377d2e5b2f6d22",
  "result": {
    "markdown": "---\ntitle: \"Portfolios\"\nauthor: \"[Jason Foster](mailto:jason.j.foster@gmail.com)\"\ndate: last-modified\ncategories:\n  - analysis\n  - finance\n  - r\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quantmod)\nlibrary(roll)\nlibrary(data.table)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfactors_r <- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d <- c(\"DGS10\", \"BAMLH0A0HYM2\")\nfactors <- c(factors_r, factors_d)\nwidth <- 252\nscale <- list(\"periods\" = 252, \"overlap\" = 5)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngetSymbols(factors, src = \"FRED\")\nlevels_xts <- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreturns_xts <- do.call(merge, lapply(factors, function(i) {\n    if (i %in% factors_r) {\n        diff(log((levels_xts[ , i])))\n    } else if (i %in% factors_d) {\n        -diff(levels_xts[ , i]) / 100\n    }    \n}))\noverlap_xts <- roll_mean(returns_xts, scale[[\"overlap\"]], min_obs = 1, na_restore = TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pls)\nlibrary(CVXR)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntickers <- \"BAICX\" # fund inception date is \"2011-11-28\" \ninvisible(getSymbols(tickers, src = \"tiingo\", api.key = Sys.getenv(\"TIINGO_API_KEY\"), adjust = TRUE))\nprices_xts <- do.call(merge, c(lapply(tickers, function(i) Cl(get(i))), all = TRUE))\ncolnames(prices_xts) <- tickers\nindex(prices_xts) <- as.Date(index(prices_xts))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreturns_xts <- merge(returns_xts, diff(log(prices_xts)))\noverlap_xts <- merge(overlap_xts, roll_mean(returns_xts[ , tickers], scale[[\"overlap\"]], min_obs = 1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# weights <- 0.9 ^ ((width - 1):0)\nweights <- rep(1, width)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# overlap_df <- na.omit(overlap_xts)\noverlap_x_df <- na.omit(overlap_xts)[ , factors]\noverlap_y_df <- na.omit(overlap_xts)[ , tickers]\noverlap_x_xts <- tail(overlap_x_df, width)\noverlap_y_xts <- tail(overlap_y_df, width)\n```\n:::\n\n\n# Random portfolios\n\nNeed to generate uniformly distributed weights $\\mathbf{w}=(w_{1},w_{2},\\ldots,w_{N})$ such that $\\sum_{j=1}^{N}w_{i}=1$ and $w_{i}\\geq0$:\n\n-   **Approach 1**: tempting to use $w_{i}=\\frac{u_{i}}{\\sum_{j=1}^{N}u_{i}}$ where $u_{i}\\sim U(0,1)$ but the distribution of $\\mathbf{w}$ is not uniform\n\n-   **Approach 2**: instead, generate $\\text{Exp}(1)$ and then normalize\n\nCan also scale random weights by $M$, e.g. if sum of weights must be 10% then multiply weights by 10%.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrand_weights1 <- function(n_sim, n_assets, lmbda) {\n    \n    rand_exp <- matrix(runif(n_sim * n_assets), nrow = n_sim, ncol = n_assets)\n    result <- sweep(rand_exp, 1, rowSums(rand_exp), \"/\")\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Methodology: uniform sampling from the simplex (http://mathoverflow.net/a/76258)\n# z ~ U(0, 1) then -ln(z) is an exponential(1) distribution\n# This is also known as generating a random vector from the symmetric Dirichlet distribution\nrand_weights2 <- function(n_sim, n_assets, lmbda) {\n    \n    rand_exp <- matrix(-log(1 - runif(n_sim * n_assets)) / lmbda, nrow = n_sim, ncol = n_assets)\n    result <- sweep(rand_exp, 1, rowSums(rand_exp), \"/\")\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Generate n exponential(1) random variables x_1, x_2, ..., x_n\n# 2. Let y_i = x_i / (sum_{i = 1}^{n} x_i)\nrand_weights3 <- function(n_sim, n_assets, lmbda) {\n    \n    rand_exp <- matrix(rexp(n_sim * n_assets), nrow = n_sim, ncol = n_assets)\n    result <- sweep(rand_exp, 1, rowSums(rand_exp), \"/\")\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlmbda <- 1\nn_assets <- 3\nn_sim <- 10000\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\napproach1 <- rand_weights1(n_sim, n_assets, lmbda)\napproach2 <- rand_weights2(n_sim, n_assets, lmbda)\napproach3 <- rand_weights2(n_sim, n_assets, lmbda)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(approach1), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(approach2), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(approach3), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=384}\n:::\n:::\n\n\n## Random turnover\n\nHow to generate random weights between lower bound $a$ and upper bound $b$ that sum to zero?\n\n-   **Approach 1**: tempting to multiply random weights by $M$ and then subtract by $\\frac{M}{N}$ but the distribution is not between $a$ and $b$\n\n-   **Approach 2**: instead, use an iterative approach for random turnover:\n\n    1.  Generate $N-1$ uniformly distributed weights between $a$ and $b$\n    2.  For $u_{N}$ compute sum of values and subtract from $M$\n    3.  If $u_{N}$ is between $a$ and $b$, then keep; otherwise, discard\n\nThen add random turnover to previous period's random weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrand_iterative <- function(n_assets, lower, upper, target) {\n    \n    plug <- FALSE\n    \n    while (!plug) {\n        \n        result <- as.matrix(runif(n_assets - 1, min = lower, max = upper))\n        temp <- target - sum(result)\n        \n        if ((temp <= upper) && (temp >= lower)) {\n            plug <- TRUE            \n        }\n        \n    }\n    \n    result <- append(result, temp)\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrand_turnover1 <- function(n_sim, n_assets, lower, upper, target) {\n    \n    rng <- upper - lower\n    \n    result <- rand_weights3(n_sim, n_assets, lmbda) * rng\n    result <- result - rng / n_assets\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrand_turnover2 <- function(n_sim, n_assets, lower, upper, target) {\n    \n    result <- matrix(rand_iterative(n_assets, lower, upper, target), nrow = 1, ncol = n_assets)\n    \n    while (nrow(result) < n_sim) {\n        \n        temp <- matrix(rand_iterative(n_assets, lower, upper, target), nrow = 1, ncol = n_assets)\n        result <- rbind(result, temp)\n        \n    }\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlower <- -0.05\nupper <- 0.05\ntarget <- 0\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\napproach1 <- rand_turnover1(n_sim, n_assets, lower, upper, target)\napproach2 <- rand_turnover2(n_sim, n_assets, lower, upper, target)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(approach1), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(approach2), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=384}\n:::\n:::\n\n\n# Mean-variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngeometric_mean <- function(x, scale) {\n    \n    result <- prod(1 + x) ^ (scale / length(x)) - 1\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreturns_x_xts <- na.omit(returns_xts)[ , factors] # extended history # REMOVE LATER\nmu <- apply(returns_x_xts, 2, geometric_mean, scale = scale[[\"periods\"]])\nsigma <- cov(overlap_x_xts) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n```\n:::\n\n\n## Maximum return\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget <- 0.06\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://palomar.home.ece.ust.hk/MAFS6010R_lectures/slides_robust_portfolio.html\nmax_pnl_optim <- function(mu, sigma, target) {\n    \n    params <- Variable(length(mu))\n    \n    cons <- list(params >= 0, sum(params) == 1,\n                 quad_form(params, sigma) <= target ^ 2)\n    \n    obj <- Maximize(t(params) %*% mu)\n        \n    result <- solve(Problem(obj, cons))$getValue(params)\n    \n    return(result)\n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparams1 <- max_pnl_optim(mu, sigma, target)\nparams1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]\n[1,] 4.570765e-01\n[2,] 5.429235e-01\n[3,] 3.283939e-08\n[4,] 1.879820e-08\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmu %*% params1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] 0.04173453\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(t(params1) %*% sigma %*% params1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,] 0.06\n```\n:::\n:::\n\n\n## Minimum variance\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min_{x}&\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget <- 0.03\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmin_risk_optim <- function(mu, sigma, target) {\n    \n    params <- Variable(length(mu))\n    \n    cons <- list(params >= 0, sum(params) == 1,\n                 sum(mu * params) >= target)\n    \n    obj <- Minimize(quad_form(params, sigma))\n        \n    result <- solve(Problem(obj, cons))$getValue(params)\n    \n    return(result)\n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparams2 <- min_risk_optim(mu, sigma, target)\nparams2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]\n[1,] 3.068265e-01\n[2,] 5.044244e-01\n[3,] 1.887491e-01\n[4,] 2.130983e-21\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmu %*% params2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,] 0.03\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(t(params2) %*% sigma %*% params2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] 0.04220845\n```\n:::\n:::\n\n\n## Maximum ratio\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}-\\frac{1}{2}\\delta(\\mathbf{w}^T\\Sigma\\mathbf{w})\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nir <- 0.5\ntarget <- ir / 0.06 # ir / std (see Black-Litterman)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmax_ratio_optim <- function(mu, sigma, target) {\n    \n    params <- Variable(length(mu))\n    \n    cons <- list(params >= 0, sum(params) == 1)\n    \n    obj <- Maximize(t(mu) %*% params - 0.5 * target * quad_form(params, sigma))\n        \n    result <- solve(Problem(obj, cons))$getValue(params)\n    \n    return(result)\n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparams3 <- max_ratio_optim(mu, sigma, target)\nparams3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              [,1]\n[1,]  4.747901e-01\n[2,]  5.252099e-01\n[3,] -6.324392e-24\n[4,] -5.022383e-23\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmu %*% params3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] 0.04279219\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(t(params3) %*% sigma %*% params3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] 0.06200391\n```\n:::\n:::\n\n\n<!-- # Black-Litterman -->\n\n<!-- ## Prior distribution -->\n\n<!-- $$ -->\n\n<!-- \\begin{aligned} -->\n\n<!-- \\text{Risk aversion: } &\\lambda=\\frac{E(r)-r_{f}}{\\sigma^{2}}=\\frac{IR}{\\sigma}\\\\ -->\n\n<!-- \\text{Implied returns: } &\\Pi=\\lambda\\Sigma w\\\\ -->\n\n<!-- \\text{Distribution: } &N\\sim(\\Pi,\\tau\\Sigma) -->\n\n<!-- \\end{aligned} -->\n\n<!-- $$ -->\n\n<!-- ```{r} -->\n\n<!-- implied_pnl <- function(params, ir, sigma) { -->\n\n<!--     lmbda <- as.numeric(ir / sqrt(t(params) %*% sigma %*% params)) -->\n\n<!--     result <- lmbda * sigma %*% params -->\n\n<!--     return(result)     -->\n\n<!-- } -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n\n<!-- implied_pnl(params3, ir, sigma) -->\n\n<!-- ``` -->\n\n<!-- ## Conditional distribution -->\n\n<!-- $$ -->\n\n<!-- \\begin{aligned} -->\n\n<!-- \\text{Prior mean variance: } &\\tau\\in(0.01, 0.05)\\approx(0.025)\\\\ -->\n\n<!-- \\text{Asset views: } &\\mathbf{P}={\\begin{bmatrix} -->\n\n<!-- p_{11}&\\cdots&p_{1n}\\\\ -->\n\n<!-- \\vdots&\\ddots&\\vdots\\\\ -->\n\n<!-- p_{k1}&\\cdots&p_{kn} -->\n\n<!-- \\end{bmatrix}}= -->\n\n<!-- {\\begin{bmatrix} -->\n\n<!-- 0&0&0&0&0&0&1&0\\\\ -->\n\n<!-- -1&1&0&0&0&0&0&0\\\\ -->\n\n<!-- 0&0&0.5&-0.5&0.5&-0.5&0&0 -->\n\n<!-- \\end{bmatrix}}\\\\ -->\n\n<!-- \\text{View returns: } &\\mathbf{Q}={\\begin{bmatrix} -->\n\n<!-- q_{1}\\\\ -->\n\n<!-- \\vdots\\\\ -->\n\n<!-- q_{k} -->\n\n<!-- \\end{bmatrix}}= -->\n\n<!-- {\\begin{bmatrix} -->\n\n<!-- 0.0525\\\\ -->\n\n<!-- 0.0025\\\\ -->\n\n<!-- 0.0200 -->\n\n<!-- \\end{bmatrix}}\\\\ -->\n\n<!-- \\text{View confidence: } &\\mathbf{C}={\\begin{bmatrix} -->\n\n<!-- c_{1}\\\\ -->\n\n<!-- \\vdots\\\\ -->\n\n<!-- c_{k} -->\n\n<!-- \\end{bmatrix}}= -->\n\n<!-- {\\begin{bmatrix} -->\n\n<!-- 0.2500\\\\ -->\n\n<!-- 0.5000\\\\ -->\n\n<!-- 0.6500 -->\n\n<!-- \\end{bmatrix}}\\\\ -->\n\n<!-- \\text{View covariance: } &\\mathbf{\\Omega}={\\begin{bmatrix} -->\n\n<!-- \\tau\\left(\\frac{1-c_{1}}{c_{1}}\\right)\\left(p_{1}\\Sigma p_{1}^{T}\\right)&0&0\\\\ -->\n\n<!-- 0&\\ddots&0\\\\ -->\n\n<!-- 0&0&\\tau\\left(\\frac{1-c_{k}}{c_{k}}\\right)\\left(p_{k}\\Sigma p_{k}^{T}\\right) -->\n\n<!-- \\end{bmatrix}}\\\\ -->\n\n<!-- \\text{Distribution: } &N\\sim(\\mathbf{Q}, \\mathbf{\\Omega}) -->\n\n<!-- \\end{aligned} -->\n\n<!-- $$ -->\n\n<!-- ## Posterior distribution -->\n\n<!-- $$ -->\n\n<!-- \\begin{aligned} -->\n\n<!-- \\text{Implied returns: } &\\hat{\\Pi}=\\Pi+\\tau\\Sigma \\mathbf{P}^{T}\\left(\\tau \\mathbf{P}\\Sigma \\mathbf{P}^{T}+\\mathbf{\\Omega}\\right)^{-1}\\left(\\mathbf{Q}-\\mathbf{P}\\Pi^{T}\\right)\\\\ -->\n\n<!-- \\text{Covariance: } &\\hat{\\Sigma}=\\Sigma+\\tau\\left[\\Sigma-\\Sigma\\mathbf{P}^{T}\\left(\\tau\\mathbf{P}\\Sigma\\mathbf{P}^{T}+\\mathbf{\\Omega}\\right)^{-1}\\tau\\mathbf{P}\\Sigma\\right]\\\\ -->\n\n<!-- \\text{Weights: } &\\hat{w}=\\hat{\\Pi}\\left(\\lambda\\Sigma\\right)^{-1}\\\\ -->\n\n<!-- \\text{Distribution: } &N\\sim\\left(\\left[\\left(\\tau\\Sigma\\right)^{-1}+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{P}\\right]^{-1}\\left[\\left(\\tau\\Sigma\\right)^{-1}\\Pi+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{Q}\\right],\\left[\\left(\\tau\\Sigma\\right)^{-1}+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{P}\\right]^{-1}\\right) -->\n\n<!-- \\end{aligned} -->\n\n<!-- $$ -->\n\n<!-- ```{r} -->\n\n<!-- black_litterman <- function(params, ir, sigma, views) { -->\n\n<!--     # prior distribution -->\n\n<!--     weights_prior <- params -->\n\n<!--     sigma_prior <- sigma     -->\n\n<!--     lmbda <- as.numeric(ir / sqrt(t(weights_prior) %*% sigma %*% weights_prior)) -->\n\n<!--     pi_prior <- lmbda * sigma_prior %*% weights_prior -->\n\n<!--     # matrix calculations -->\n\n<!--     matmul_left <- views[[\"tau\"]] * sigma_prior %*% t(views[[\"P\"]]) -->\n\n<!--     matmul_mid <- views[[\"tau\"]] * views[[\"P\"]] %*% sigma_prior %*% t(views[[\"P\"]]) -->\n\n<!--     matmul_right <- views[[\"Q\"]] - views[[\"P\"]] %*% pi_prior -->\n\n<!--     # conditional distribution -->\n\n<!--     omega <- diag(diag(diag((1 - views[[\"C\"]]) / views[[\"C\"]]) %*% matmul_mid)) -->\n\n<!--     # posterior distribution -->\n\n<!--     pi_posterior <- pi_prior + matmul_left %*% solve(matmul_mid + omega) %*% matmul_right -->\n\n<!--     sigma_posterior <- sigma_prior +  views[[\"tau\"]] * sigma_prior - -->\n\n<!--         matmul_left %*% solve(matmul_mid + omega) %*% (tau * views[[\"P\"]] %*% sigma_prior) -->\n\n<!--     weights_posterior <- t(pi_posterior) %*% solve(lmbda * sigma_prior) -->\n\n<!--     # implied confidence -->\n\n<!--     pi_posterior_100 <- pi_prior + matmul_left %*% solve(matmul_mid) %*% matmul_right -->\n\n<!--     weights_posterior_100 <- t(pi_posterior_100) %*% solve(lmbda * sigma_prior) -->\n\n<!--     implied_confidence <- (weights_posterior - weights_prior) / (weights_posterior_100 - weights_prior) -->\n\n<!--     result <- list(\"implied_confidence\" = implied_confidence, -->\n\n<!--                    \"weights_prior\" = t(as.matrix(weights_prior)), -->\n\n<!--                    \"weights_posterior\" = weights_posterior, -->\n\n<!--                    \"pi_prior\" = t(pi_prior), -->\n\n<!--                    \"pi_posterior\" = t(pi_posterior), -->\n\n<!--                    \"sigma_prior\" = sigma_prior, -->\n\n<!--                    \"sigma_posterior\" = sigma_posterior) -->\n\n<!--     return(result)    -->\n\n<!-- } -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n\n<!-- tau <- 0.025 -->\n\n<!-- P <- diag(length(factors)) -->\n\n<!-- Q <- t(implied_shocks(0.1, overlap_x_xts, overlap_x_xts[ , 1], 1)) -->\n\n<!-- C <- rep(0.95, length(factors)) -->\n\n<!-- views <- list(\"tau\" = tau, \"P\" = P, \"Q\" = Q, \"C\" = C) -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n\n<!-- bl <- black_litterman(as.vector(params3), ir, sigma, views) -->\n\n<!-- bl -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n\n<!-- params4 <- as.vector(bl[[\"weights_posterior\"]]) -->\n\n<!-- params4 <- params4 / sum(params4) # no leverage -->\n\n<!-- params4 -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n\n<!-- mu %*% params4 -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n\n<!-- sqrt(t(params4) %*% sigma %*% params4) -->\n\n<!-- ``` -->\n\n# Risk parity\n\nRisk parity is an approach to portfolio management that focuses on allocation of risk rather than allocation of capital. In a risk parity strategy, the asset allocations are leveraged, or deleveraged, to have equal risk contributions. Suppose that $\\mathbf{R}$ is a $T \\times N$ matrix of asset returns where the return of the $i^{th}$ asset is $R_{i,t}$ at time $t$. Define $\\Sigma$ to be the covariance matrix of $\\mathbf{R}$ and let $\\mathbf{w}=(w_{1},\\dots,w_{N})$ be a vector of asset weights. Then the volatility of the return of the strategy is $\\sigma_{P}=\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}$ and, by Euler's Theorem, satisfies:\n\n$$\n\\begin{aligned}\n\\sigma_{P}&=\\sum_{i=1}^{N}w_{i}\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}\\\\\n&=w_{1}\\frac{\\partial\\sigma_{P}}{\\partial w_{1}}+\\dots+w_{N}\\frac{\\partial\\sigma_{P}}{\\partial w_{N}}\n\\end{aligned}\n$$\n\nwhere each element is the risk contribution of the $i^{th}$ risky asset. The risk parity objective solves for weights such that each asset contributes equal risk using the following nonlinear constrained optimization problem:\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\displaystyle\\sum_{i=1}^{N}\\log(w_{i})\\\\\n\\textrm{s.t.}&\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}&\\leq&\\sigma \n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce a new variable $\\lambda$ that is the Lagrange multiplier and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\sum_{i=1}^{N}\\log(w_{i})-\\lambda(\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}-\\sigma)\n\\end{aligned}\n$$\n\nThen set the partial derivatives of $\\mathcal{L}$ equal to zero for each asset $i$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w_{i}}&=\\frac{1}{w_{i}}-\\lambda\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}=0\n\\Leftrightarrow\nw_{i}\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}=\\frac{1}{\\lambda}\n\\end{aligned}\n$$\n\nNotice that $1/\\lambda$ is the risk contribution of the $i^{th}$ asset. Now use `R` to maximize the Lagrangian numerically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# http://faculty.washington.edu/ezivot/econ424/riskbudgetingslides.pdf\n# https://systematicinvestor.wordpress.com/2011/11/16/black-litterman-model/\n# https://cran.r-project.org/web/packages/BLCOP/vignettes/BLCOP.pdf\n# http://math.stackexchange.com/questions/17776/inverse-of-the-sum-of-matrices\nrisk_parity_optim <- function(sigma, target) {\n    \n    params <- Variable(nrow(sigma))\n    \n    risk <- quad_form(params, sigma)\n    risk_contrib <- target ^ 2 / nrow(sigma)\n            \n    obj <- Maximize((sum(log(params)) - (1 / risk_contrib) * (risk - target ^ 2)))\n        \n    result <- solve(Problem(obj))$getValue(params)\n    result <- result / sum(result) # no leverage\n    \n    return(result)\n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget <- 1\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparams5 <- risk_parity_optim(sigma, target)\nparams5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] 0.02912723\n[2,] 0.13019225\n[3,] 0.57848972\n[4,] 0.26219081\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrisk <- as.numeric(sqrt(t(params5) %*% sigma %*% params5))\nrisk_contrib <- params5 * sigma %*% params5 / risk\nrisk_contrib\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    [,1]\nSP500        0.002298729\nDTWEXAFEGS   0.002298724\nDGS10        0.002298727\nBAMLH0A0HYM2 0.002298724\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmu %*% params5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n[1,] 0.004147885\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(t(params5) %*% sigma %*% params5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n[1,] 0.009194905\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\"max_pnl\" = params1,\n           \"min_risk\" = params2,\n           \"max_ratio\" = params3,\n           # \"black_litterman\" = params4,\n           \"risk_parity\" = params5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       max_pnl     min_risk     max_ratio risk_parity\n1 4.570765e-01 3.068265e-01  4.747901e-01  0.02912723\n2 5.429235e-01 5.044244e-01  5.252099e-01  0.13019225\n3 3.283939e-08 1.887491e-01 -6.324392e-24  0.57848972\n4 1.879820e-08 2.130983e-21 -5.022383e-23  0.26219081\n```\n:::\n:::\n\n\n# Portfolio attribution\n\n## Single-period\n\nThe arithmetic active return is commonly decomposed using the Brinson-Fachler method:\n\n$$\n\\begin{aligned}\n\\text{Allocation: } &r_{a}=\\sum_{k=1}^{n}(w_{p,k}-w_{b,k})(r_{b,k}-r_{b})\\\\\n\\text{Selection: } &r_{s}=\\sum_{k=1}^{n}w_{p,k}(r_{p,k}-r_{b,k})\\\\\n\\end{aligned}\n$$\n\nwhere $k=1,\\ldots,n$ is each sector or factor.\n\n## Multi-period\n\nArithmetic attributes add to the active return of a single period; however, they cannot be summed or compounded to explain the active return over multiple periods. To solve this problem, the original arithmetic attribute is multiplied by a single scaling coefficient for that period. After all single-period original attributes have been transformed, the adjusted attributes sum to the active return over the periods.\n\n$$\n\\begin{aligned}\n\\text{Carino scaling coefficient: } &c_{t}=\\frac{[\\ln(1+r_{p,t})-\\ln(1+r_{b,t})]/(r_{p,t}-r_{b,t})}{[\\ln(1+r_{p})-\\ln(1+r_{b})]/(r_{p}-r_{b})}\n\\end{aligned}\n$$\n\nwhere $t=1,\\ldots,n$ is each period.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# http://www.frongello.com/support/Works/Chap20RiskBook.pdf\n# https://github.com/R-Finance/PortfolioAttribution/blob/master/R/Carino.R\npnl_attrib <- function(params, x) {\n    \n    total_i <- rowSums(x)\n    total <- prod(1 + total_i) - 1\n    \n    coef <- (log(1 + total_i) / total_i) / (log(1 + total) / total)\n    \n    result <- colSums(x * coef)\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nattrib_mat <- sweep(tail(na.omit(returns_xts)[ , factors], width), 2, params1, \"*\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npnl_attrib(params1, attrib_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 8.657379e-02 -3.172667e-02  1.332956e-10  1.944644e-10 \n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}