{
  "hash": "375e5542d0d7b9ea5020773c1cef60df",
  "result": {
    "markdown": "---\ntitle: \"Portfolios\"\nauthor: \"[Jason Foster](mailto:jason.j.foster@gmail.com)\"\ndate: last-modified\ncategories:\n  - analysis\n  - finance\n  - r\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quantmod)\nlibrary(roll)\nlibrary(data.table)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfactors_r <- c(\"SP500\", \"DTWEXAFEGS\") # \"SP500\" does not contain dividends; note: \"DTWEXM\" discontinued as of Jan 2020\nfactors_d <- c(\"DGS10\", \"BAMLH0A0HYM2\")\nfactors <- c(factors_r, factors_d)\nwidth <- 252\nscale <- list(\"periods\" = 252, \"overlap\" = 5)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngetSymbols(factors, src = \"FRED\")\nlevels_xts <- do.call(merge, c(lapply(factors, function(i) get(i)), all = TRUE))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreturns_xts <- do.call(merge, lapply(factors, function(i) {\n    if (i %in% factors_r) {\n        diff(log((levels_xts[ , i])))\n    } else if (i %in% factors_d) {\n        -diff(levels_xts[ , i]) / 100\n    }    \n}))\noverlap_xts <- roll_mean(returns_xts, scale[[\"overlap\"]], min_obs = 1, na_restore = TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pls)\nlibrary(CVXR)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntickers <- \"BAICX\" # fund inception date is \"2011-11-28\" \ninvisible(getSymbols(tickers, src = \"tiingo\", api.key = Sys.getenv(\"TIINGO_API_KEY\"), adjust = TRUE))\nprices_xts <- do.call(merge, c(lapply(tickers, function(i) Cl(get(i))), all = TRUE))\ncolnames(prices_xts) <- tickers\nindex(prices_xts) <- as.Date(index(prices_xts))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreturns_xts <- merge(returns_xts, diff(log(prices_xts)))\noverlap_xts <- merge(overlap_xts, roll_mean(returns_xts[ , tickers], scale[[\"overlap\"]], min_obs = 1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# weights <- 0.9 ^ ((width - 1):0)\nweights <- rep(1, width)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# overlap_df <- na.omit(overlap_xts)\noverlap_x_df <- na.omit(overlap_xts)[ , factors]\noverlap_y_df <- na.omit(overlap_xts)[ , tickers]\noverlap_x_xts <- tail(overlap_x_df, width)\noverlap_y_xts <- tail(overlap_y_df, width)\n```\n:::\n\n\n<!-- # Factor models -->\n\n<!-- ## Ordinary least squares -->\n\n<!-- ### Coefficients -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\hat{\\beta}=(X^\\mathrm{T}WX)^{-1}X^\\mathrm{T}Wy -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- -   <https://faculty.washington.edu/ezivot/research/factormodellecture_handout.pdf> -->\n\n<!-- ```{r} -->\n<!-- lm_coef <- function(x, y, weights, intercept) { -->\n\n<!--     if (intercept) x <- model.matrix(~ x) -->\n\n<!--     # result <- solve(t(x) %*% sweep(x, 1, weights, \"*\")) %*% t(x) %*% sweep(y, 1, weights, \"*\") -->\n<!--     result <- solve(crossprod(x, diag(weights)) %*% x) %*% crossprod(x, diag(weights) %*% y) -->\n\n<!--     return(result) -->\n\n<!-- } -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- intercept <- TRUE -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- lm_coef(overlap_x_xts, overlap_y_xts, weights, intercept) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- # START HERE -->\n<!-- if (intercept) { -->\n<!--     fit <- lm(overlap_y_xts ~ overlap_x_xts, weights = weights) -->\n<!-- } else { -->\n<!--     fit <- lm(overlap_y_xts ~ overlap_x_xts - 1, weights = weights) -->\n<!-- } -->\n\n<!-- coef(fit) -->\n<!-- ``` -->\n\n<!-- ### R-squared -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- R^{2}=\\frac{\\hat{\\beta}^\\mathrm{T}(X^\\mathrm{T}WX)\\hat{\\beta}}{y^\\mathrm{T}Wy} -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ```{r} -->\n<!-- lm_rsq <- function(x, y, weights, intercept) { -->\n\n<!--     coef <- lm_coef(x, y, weights, intercept) -->\n\n<!--     if (intercept) { -->\n\n<!--         x <- model.matrix(~ x) -->\n<!--         x <- sweep(x, 2, apply(x, 2, weighted.mean, w = weights), \"-\") -->\n<!--         y <- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\") -->\n\n<!--     } -->\n\n<!--     result <- (t(coef) %*% (t(x) %*% sweep(x, 1, weights, \"*\")) %*% coef) / (t(y) %*% sweep(y, 1, weights, \"*\")) -->\n\n<!--     return(result) -->\n\n<!-- } -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- lm_rsq(overlap_x_xts, overlap_y_xts, weights, intercept) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- summary(fit)$r.squared -->\n<!-- ``` -->\n\n<!-- ### Standard errors -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\sigma_{\\hat{\\beta}}^{2}&=\\sigma_{\\varepsilon}^{2}(X^\\mathrm{T}WX)^{-1}\\\\ -->\n<!-- &=\\frac{(1-R^{2})}{n-p}(X^\\mathrm{T}WX)^{-1}\\\\ -->\n<!-- &=\\frac{SSE}{df_{E}}(X^\\mathrm{T}WX)^{-1}\\\\ -->\n<!-- \\sigma_{\\hat{\\alpha}}^{2}&=\\sigma_{\\varepsilon}^{2}\\left(\\frac{1}{n}+\\mu^\\mathrm{T}(X^\\mathrm{T}WX)^{-1}\\mu\\right) -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ```{r} -->\n<!-- # http://people.duke.edu/~rnau/mathreg.htm -->\n<!-- lm_se <- function(x, y, weights, intercept) { -->\n\n<!--     n_rows <- nrow(x) -->\n<!--     n_cols <- ncol(x) -->\n\n<!--     rsq <- lm_rsq(x, y, weights, intercept) -->\n\n<!--     if (intercept) { -->\n\n<!--         x <- model.matrix(~ x) -->\n<!--         y <- sweep(y, 2, apply(y, 2, weighted.mean, w = weights), \"-\") -->\n\n<!--         df_resid <- n_rows - n_cols - 1 -->\n\n<!--     } else { -->\n<!--         df_resid <- n_rows - n_cols -->\n<!--     } -->\n\n<!--     var_y <- t(y) %*% sweep(y, 1, weights, \"*\") -->\n<!--     var_resid <- as.vector((1 - rsq) * var_y / df_resid) -->\n\n<!--     result <- sqrt(var_resid * diag(solve(t(x) %*% sweep(x, 1, weights, \"*\")))) -->\n\n<!--     return(result) -->\n<!-- } -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- lm_se(overlap_x_xts, overlap_y_xts, weights, intercept) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- coef(summary(fit))[ , \"Std. Error\"] -->\n<!-- ``` -->\n\n<!-- ## Standalone risk -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\text{SAR}_{k}&=\\sqrt{w_{k}^{2}\\sigma_{k}^{2}}\\\\ -->\n<!-- \\text{SAR}_{\\varepsilon}&=\\sqrt{(1-R^{2})\\sigma_{y}^{2}} -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ```{r} -->\n<!-- lm_sar <- function(x, y, weights, intercept) { -->\n\n<!--     coef <- lm_coef(x, y, weights, intercept) -->\n<!--     rsq <- lm_rsq(x, y, weights, intercept) -->\n\n<!--     if (intercept) x <- model.matrix(~ x) -->\n\n<!--     sigma <- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov -->\n<!--     sar <- coef ^ 2 * diag(sigma[-ncol(sigma), -ncol(sigma)]) -->\n<!--     sar_eps <- (1 - rsq) * sigma[ncol(sigma), ncol(sigma)] -->\n\n<!--     result <- sqrt(c(sigma[ncol(sigma), ncol(sigma)], -->\n<!--                      sar, -->\n<!--                      sar_eps)) -->\n\n<!--     return(result) -->\n\n<!-- } -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- lm_sar(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]]) * sqrt(scale[[\"overlap\"]]) -->\n<!-- ``` -->\n\n<!-- ## Risk contribution -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\text{MCR}&=w^\\mathrm{T}\\frac{\\partial\\sigma_{y}}{\\partial w}\\\\ -->\n<!-- &=w^\\mathrm{T}\\frac{\\Sigma w}{\\sigma_{y}}\\\\ -->\n<!-- \\text{MCR}_{\\varepsilon}&=\\sigma_{y}-\\sum_{k=1}^{n}\\text{MCR}_{k} -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ```{r} -->\n<!-- # http://faculty.washington.edu/ezivot/research/factormodelrisklecture_handout.pdf -->\n<!-- lm_mcr <- function(x, y, weights, intercept) { -->\n\n<!--     coef <- lm_coef(x, y, weights, intercept) -->\n<!--     rsq <- lm_rsq(x, y, weights, intercept) -->\n\n<!--     if (intercept) x <- model.matrix(~ x) -->\n\n<!--     sigma <- cov.wt(cbind(x, y), wt = weights, center = intercept)$cov -->\n<!--     mcr <- coef * sigma[-ncol(sigma), -ncol(sigma)] %*% coef / sqrt(sigma[ncol(sigma), ncol(sigma)]) -->\n<!--     mcr_eps <- sqrt(sigma[ncol(sigma), ncol(sigma)]) - sum(mcr) -->\n\n<!--     result <- c(sqrt(sigma[ncol(sigma), ncol(sigma)]), -->\n<!--                 mcr, -->\n<!--                 mcr_eps) -->\n\n<!--     return(result) -->\n\n<!-- } -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- lm_mcr(overlap_x_xts, overlap_y_xts, weights, intercept) * sqrt(scale[[\"periods\"]]) * sqrt(scale[[\"overlap\"]]) -->\n<!-- ``` -->\n\n<!-- ## Implied shocks -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\hat{\\beta}&=(Z^\\mathrm{T}WZ)^{-1}Z^\\mathrm{T}WX -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ```{r} -->\n<!-- implied_shocks <- function(shocks, x, z, weights) { -->\n\n<!--     beta <- solve(t(z) %*% sweep(z, 1, weights, \"*\")) %*% t(z) %*% sweep(x, 1, weights, \"*\") -->\n\n<!--     result <- shocks %*% beta -->\n\n<!--     return(result) -->\n\n<!-- } -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- shocks <- c(-0.1, 0.1) -->\n<!-- overlap_z_xts <- overlap_x_xts[ , 1:2] -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- implied_shocks(shocks, overlap_x_xts, overlap_z_xts, weights) -->\n<!-- ``` -->\n\n<!-- ## Stress P&L -->\n\n<!-- ```{r} -->\n<!-- pnl_stress <- function(shocks, x, y, z, weights, intercept) { -->\n\n<!--     coef <- lm_coef(x, y, weights, intercept) -->\n\n<!--     if (intercept) x <- model.matrix(~ x) -->\n\n<!--     result <- t(coef) * implied_shocks(shocks, x, z, weights) -->\n\n<!--     return(result)     -->\n\n<!-- } -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- pnl_stress(shocks, overlap_x_xts, overlap_y_xts, overlap_z_xts, weights, intercept) -->\n<!-- ``` -->\n\n# Principal component analysis\n\nUnderlying returns are structural bets that can be analyzed through dimension reduction techniques such as principal components analysis (PCA). Most empirical studies apply PCA to a covariance matrix (*note: for multi-asset portfolios, use the correlation matrix because asset-class variances are on different scales*) of equity returns (yield changes) and find that movements in the equity markets (yield curve) can be explained by a subset of principal components. For example, the yield curve can be decomposed in terms of shift, twist, and butterfly, respectively.\n\n## Eigendecomposition\n\n$$\n\\begin{aligned}\n\\boldsymbol{\\Sigma}&=\\lambda_{1}\\mathbf{v}_{1}\\mathbf{v}_{1}^\\mathrm{T}+\\lambda_{2}\\mathbf{v}_{2}\\mathbf{v}_{2}^\\mathrm{T}+\\cdots+\\lambda_{k}\\mathbf{v}_{k}\\mathbf{v}_{k}^\\mathrm{T}\\\\\n&=V\\Lambda V^{\\mathrm{T}}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://www.r-bloggers.com/fixing-non-positive-definite-correlation-matrices-using-r-2/\neigen_decomp <- function(x, comps) {\n    \n    LV <- eigen(cov(x))\n    L <- LV$values[1:comps]\n    V <- LV$vectors[ , 1:comps]\n    \n    result <- V %*% sweep(t(V), 1, L, \"*\")\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncomps <- 1\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\neigen_decomp(overlap_x_xts, comps) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]          [,2]          [,3]          [,4]\n[1,]  0.021017707 -0.0047752924  5.866640e-04  1.439828e-03\n[2,] -0.004775292  0.0010849622 -1.332920e-04 -3.271336e-04\n[3,]  0.000586664 -0.0001332920  1.637546e-05  4.018969e-05\n[4,]  0.001439828 -0.0003271336  4.018969e-05  9.863607e-05\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# cov(overlap_x_xts) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n```\n:::\n\n\n## Variance explained\n\nWe often look at the proportion of variance explained by the first $i$ principal components as an indication of how many components are needed.\n\n$$\n\\begin{aligned}\n\\frac{\\sum_{j=1}^{i}{\\lambda_{j}}}{\\sum_{j=1}^{k}{\\lambda_{j}}}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvariance_explained <- function(x) {\n    \n    LV <- eigen(cov(x))\n    L <- LV$values\n    \n    result <- cumsum(L) / sum(L)\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvariance_explained(overlap_x_xts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8564594 0.9901723 0.9965971 1.0000000\n```\n:::\n:::\n\n\n## Cosine similarity\n\nAlso, a challenge of rolling PCA is to try to match the eigenvectors: may need to change the sign and order.\n\n$$\n\\begin{aligned}\n\\text{similarity}=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\neigen_vals <- function(x) {\n    \n    LV <- eigen(cov(x))\n    L <- LV$values\n    \n    return(L)    \n}\n\neigen_vecs <- function(x) {\n    \n    LV <- eigen(cov(x))\n    V <- LV$vectors\n    \n    return(V)    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nroll_eigen1 <- function(x, width, comp) {\n    \n    n_rows <- nrow(x)\n    result_ls <- list()\n    \n    for (i in width:n_rows) {\n        \n        idx <- max(i - width + 1, 1):i\n        evec <- eigen_vecs(x[idx, ])[ , comp]\n        result_ls <- append(result_ls, list(evec))\n                \n    }\n    \n    result <- do.call(rbind, result_ls)\n    result <- xts(result, index(x)[width:n_rows])\n    colnames(result) <- colnames(x)\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncomp <- 1\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_df <- roll_eigen1(overlap_x_df, width, comp)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_mlt <- melt(as.data.table(raw_df), id.vars = \"index\")\nraw_plt <- plot_ts(raw_mlt, title = \"Eigenvector 1Y\")\nprint(raw_plt)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=576}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://quant.stackexchange.com/a/3095\nroll_eigen2 <- function(x, width, comp) {\n    \n    n_rows <- nrow(x)\n    result_ls <- list()\n    \n    for (i in width:n_rows) {\n        \n        idx <- max(i - width + 1, 1):i\n        evec <- eigen_vecs(x[idx, ])[ , comp]\n                \n        if (i > width) {\n            \n            similarity <- evec %*% result_ls[[length(result_ls)]]\n            evec <- as.vector(sign(similarity)) * evec\n            result_ls <- append(result_ls, list(evec))\n            \n        } else {\n            result_ls <- append(result_ls, list(evec))\n        }\n                \n    }\n    \n    result <- do.call(rbind, result_ls)\n    result <- xts(result, index(x)[width:n_rows])\n    colnames(result) <- colnames(x)\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_df <- roll_eigen2(overlap_x_df, width, comp)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_mlt <- melt(as.data.table(clean_df), id.vars = \"index\")\nclean_plt <- plot_ts(clean_mlt, title = \"Eigenvector 1Y\")\nprint(clean_plt)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){width=576}\n:::\n:::\n\n\n## Contour ellipsoid\n\nThe contours of a multivariate normal (MVN) distribution are ellipsoids centered at the mean. The directions of the axes are given by the eigenvectors of the covariance matrix and squared lengths are given by the eigenvalues:\n\n$$\n\\begin{aligned}\n({\\mathbf{x}}-{\\boldsymbol{\\mu}})^{\\mathrm{T}}{\\boldsymbol{\\Sigma}}^{-1}({\\mathbf{x}}-{\\boldsymbol{\\mu}})=c^{2}\n\\end{aligned}\n$$\n\nOr, in general parametric form:\n\n$$\n\\begin{aligned}\nX(t)&=X_{c}+a\\,\\cos t\\,\\cos \\varphi -b\\,\\sin t\\,\\sin \\varphi\\\\\nY(t)&=Y_{c}+a\\,\\cos t\\,\\sin \\varphi +b\\,\\sin t\\,\\cos \\varphi\n\\end{aligned}\n$$ where $t$ varies from $0,\\ldots,2\\pi$. Here $(X_{c},Y_{c})$ is the center of the ellipse and $\\varphi$ is the angle between the x-axis and the major axis of the ellipse.\n\nSpecifically:\n\n$$\n\\begin{aligned}\n&\\text{Center: }\\boldsymbol{\\mu}=(X_{c},Y_{c})\\\\\n&\\text{Radius: }c^{2}= \\chi_{\\alpha}^{2}(df)\\\\\n&\\text{Length: }a=c\\sqrt{\\lambda_{k}}\\\\\n&\\text{Angle of rotation: }\\varphi=\\text{atan2}\\left(\\frac{V_{k}(2)}{V_{k}(1)}\\right)\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/\n# https://maitra.public.iastate.edu/stat501/lectures/MultivariateNormalDistribution-I.pdf\n# https://en.wikipedia.org/wiki/Multivariate_normal_distribution\n# https://en.wikipedia.org/wiki/Ellipse#General_parametric_form\nellipse <- function(n_sim, x, y, sigma) {\n    \n    data <- cbind(x, y)\n    LV <- eigen(cov(data))\n    L <- LV$values\n    V <- LV$vectors\n    \n    c <- sqrt(qchisq(pnorm(sigma), 2))\n    t <- seq(0, 2 * pi, len = n_sim)\n    phi <- atan2(V[2, 1], V[1, 1])\n    a <- c * sqrt(L[1]) * cos(t)\n    b <- c * sqrt(L[2]) * sin(t)\n    R <- matrix(c(cos(phi), -sin(phi), sin(phi), cos(phi)), nrow = 2, ncol = 2)\n    r <- t(rbind(a, b)) %*% R\n    \n    result <- sweep(r, 2, colMeans(data), \"+\") # 2D only\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreturns_x_xts <- na.omit(returns_xts)[ , factors] # extended history\nellipse_x_xts <- ellipse(1000, returns_x_xts[ , 1], returns_x_xts[ , 3], 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nellipse_plt <- plot_scatter(data.table(returns_x_xts[ , c(1, 3)]), x = \"SP500\", y = \"DGS10\",\n                            title = \"Return 1D (%)\") +\n  geom_point(data = data.table(ellipse_x_xts), aes(x = V1 * 100, y = V2 * 100))\nprint(ellipse_plt)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-1.png){width=288}\n:::\n:::\n\n\n## Principal component regression\n\n### Coefficients\n\n$$\n\\begin{aligned}\nW_{k}&=\\mathbf{X}V_{k}=[\\mathbf{X}\\mathbf{v}_{1},\\ldots,\\mathbf{X}\\mathbf{v}_{k}]\\\\\n{\\widehat{\\gamma}}_{k}&=\\left(W_{k}^\\mathrm{T}W_{k}\\right)^{-1}W_{k}^\\mathrm{T}\\mathbf{Y}\\\\\n{\\widehat{\\boldsymbol{\\beta}}}_{k}&=V_{k}{\\widehat{\\gamma}}_{k}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://en.wikipedia.org/wiki/Principal_component_regression\npcr_coef <- function(x, y, comps) {\n    \n    x <- sweep(x, 2, colMeans(x), \"-\")\n    LV <- eigen(cov(x))\n    V <- LV$vectors\n    \n    W <- x %*% V\n    gamma <- solve(t(W) %*% W) %*% (t(W) %*% y)\n    \n    result <- V[ , 1:comps] %*% as.matrix(gamma[1:comps])\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nscale_x_xts <- scale(overlap_x_xts)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npcr_coef(scale_x_xts, overlap_y_xts, comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              [,1]\n[1,]  0.0008109848\n[2,] -0.0007132832\n[3,]  0.0005759142\n[4,]  0.0005549691\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npcr_coef(overlap_x_xts, overlap_y_xts, comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n[1,]  0.40426830\n[2,] -0.09185109\n[3,]  0.01128428\n[4,]  0.02769459\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- pcr(reformulate(termlabels = \".\", response = tickers), \n           data = merge(scale_x_xts, overlap_y_xts), ncomp = comps)\ncoef(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n, , 1 comps\n\n                     BAICX\nSP500         0.0008109848\nDTWEXAFEGS   -0.0007132832\nDGS10         0.0005759142\nBAMLH0A0HYM2  0.0005549691\n```\n:::\n:::\n\n\n### R-squared\n\n\n::: {.cell}\n\n```{.r .cell-code}\npcr_rsq <- function(x, y, comps) {\n    \n    coef <- pcr_coef(x, y, comps)\n    \n    x <- sweep(x, 2, colMeans(x), \"-\")\n    y <- sweep(y, 2, colMeans(y), \"-\")\n    \n    result <- (t(coef) %*% (t(x) %*% x) %*% coef) / (t(y) %*% y)\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npcr_rsq(scale_x_xts, overlap_y_xts, comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          BAICX\nBAICX 0.8245769\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npcr_rsq(overlap_x_xts, overlap_y_xts, comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          BAICX\nBAICX 0.6934975\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nR2(fit)$val[comps + 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8245769\n```\n:::\n:::\n\n\n### Standard errors\n\n$$\n\\begin{aligned}\n\\text{Var}({\\widehat{\\boldsymbol{\\beta}}}_{k})&=\\sigma^{2}V_{k}(W_{k}^\\mathrm{T}W_{k})^{-1}V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}V_{k}\\text{diag}\\left(\\lambda_{1}^{-1},\\ldots,\\lambda_{k}^{-1}\\right)V_{k}^\\mathrm{T}\\\\\n&=\\sigma^{2}\\sum_{j=1}^{k}{\\frac{\\mathbf{v}_{j}\\mathbf{v}_{j}^\\mathrm{T}}{\\lambda_{j}}}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# unable to verify the result\npcr_se <- function(x, y, comps) {\n    \n    n_rows <- nrow(x)\n    n_cols <- ncol(x)\n    \n    rsq <- pcr_rsq(x, y, comps)\n    \n    y <- sweep(y, 2, colMeans(y), \"-\")\n    \n    df_resid <- n_rows - n_cols - 1\n    \n    var_y <- t(y) %*% y\n    var_resid <- as.vector((1 - rsq) * var_y / df_resid)\n    \n    LV <- eigen(cov(x))\n    L <- LV$values[1:comps] * (n_rows - 1)\n    V <- LV$vectors[ , 1:comps]\n    \n    result <- sqrt(var_resid * diag(V %*% sweep(t(V), 1, 1 / L, \"*\")))\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npcr_se(scale_x_xts, overlap_y_xts, comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.380083e-05 2.093347e-05 1.690196e-05 1.628726e-05\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npcr_se(overlap_x_xts, overlap_y_xts, comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0171007759 0.0038853527 0.0004773313 0.0011714966\n```\n:::\n:::\n\n\n## Marchenko--Pastur distribution\n\nMarchenko--Pastur distribution is the limiting distribution of eigenvalues of Wishart matrices as the matrix dimension $m$ and degrees of freedom $n$ both tend to infinity with ratio $m/n\\,\\to \\,\\lambda\\in(0,+\\infty)$:\n\n$$\n\\begin{aligned}\nd\\nu(x)&={\\frac {1}{2\\pi\\sigma ^{2}}}{\\frac{\\sqrt{(\\lambda_{+}-x)(x-\\lambda_{-})}}{\\lambda x}}\\,\\mathbf{1}_{x\\in[\\lambda_{-},\\lambda _{+}]}\\,dx\n\\end{aligned}\n$$\n\nwith\n\n$$\n\\begin{aligned}\n\\lambda_{\\pm}&=\\sigma^{2}(1\\pm{\\sqrt{\\lambda }})^{2}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution\n# https://faculty.baruch.cuny.edu/jgatheral/RandomMatrixCovariance2008.pdf\ndmp <- function(x, sigma = 1) {\n  \n  LV <- eigen(cov(x))\n  L <- LV$values\n  \n  lmbda <- ncol(x) / nrow(x)\n  lower <- sigma * (1 - sqrt(lmbda)) ^ 2\n  upper <- sigma * (1 + sqrt(lmbda)) ^ 2\n  \n  d <- ifelse((L <= lower) | (L >= upper), 0,\n              1 / (2 * pi * sigma * lmbda * L) * sqrt((upper - L) * (L - lower)))\n  \n  return(d)\n  \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nn_sim <- 5000\nn_cols <- 1000\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_sim <- matrix(rnorm(n_sim * n_cols), nrow = n_sim, ncol = n_cols)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndmp_dt <- data.table(evals = eigen(cov(data_sim))$values,\n                     dmp = dmp(data_sim))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndmp_plt <- plot_density(dmp_dt, x = \"evals\", y = \"dmp\",\n                        title = \"Marchenko-Pastur distribution\", xlab = \"Eigenvalues\", ylab = \"Density\")\nprint(dmp_plt)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `after_stat(density)` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-46-1.png){width=384}\n:::\n:::\n\n\n# Random portfolios\n\nNeed to generate uniformly distributed weights $\\mathbf{w}=(w_{1},w_{2},\\ldots,w_{N})$ such that $\\sum_{j=1}^{N}w_{i}=1$ and $w_{i}\\geq0$:\n\n-   **Approach 1**: tempting to use $w_{i}=\\frac{u_{i}}{\\sum_{j=1}^{N}u_{i}}$ where $u_{i}\\sim U(0,1)$ but the distribution of $\\mathbf{w}$ is not uniform\n\n-   **Approach 2**: instead, generate $\\text{Exp}(1)$ and then normalize\n\nCan also scale random weights by $M$, e.g. if sum of weights must be 10% then multiply weights by 10%.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrand_weights1 <- function(n_sim, n_assets, lmbda) {\n    \n    rand_exp <- matrix(runif(n_sim * n_assets), nrow = n_sim, ncol = n_assets)\n    result <- sweep(rand_exp, 1, rowSums(rand_exp), \"/\")\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Methodology: uniform sampling from the simplex (http://mathoverflow.net/a/76258)\n# z ~ U(0, 1) then -ln(z) is an exponential(1) distribution\n# This is also known as generating a random vector from the symmetric Dirichlet distribution\nrand_weights2 <- function(n_sim, n_assets, lmbda) {\n    \n    rand_exp <- matrix(-log(1 - runif(n_sim * n_assets)) / lmbda, nrow = n_sim, ncol = n_assets)\n    result <- sweep(rand_exp, 1, rowSums(rand_exp), \"/\")\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Generate n exponential(1) random variables x_1, x_2, ..., x_n\n# 2. Let y_i = x_i / (sum_{i = 1}^{n} x_i)\nrand_weights3 <- function(n_sim, n_assets, lmbda) {\n    \n    rand_exp <- matrix(rexp(n_sim * n_assets), nrow = n_sim, ncol = n_assets)\n    result <- sweep(rand_exp, 1, rowSums(rand_exp), \"/\")\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlmbda <- 1\nn_assets <- 3\nn_sim <- 10000\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\napproach1 <- rand_weights1(n_sim, n_assets, lmbda)\napproach2 <- rand_weights2(n_sim, n_assets, lmbda)\napproach3 <- rand_weights2(n_sim, n_assets, lmbda)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(approach1), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-52-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(approach2), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-53-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(approach3), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-54-1.png){width=384}\n:::\n:::\n\n\n## Random turnover\n\nHow to generate random weights between lower bound $a$ and upper bound $b$ that sum to zero?\n\n-   **Approach 1**: tempting to multiply random weights by $M$ and then subtract by $\\frac{M}{N}$ but the distribution is not between $a$ and $b$\n\n-   **Approach 2**: instead, use an iterative approach for random turnover:\n\n    1.  Generate $N-1$ uniformly distributed weights between $a$ and $b$\n    2.  For $u_{N}$ compute sum of values and subtract from $M$\n    3.  If $u_{N}$ is between $a$ and $b$, then keep; otherwise, discard\n\nThen add random turnover to previous period's random weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrand_iterative <- function(n_assets, lower, upper, target) {\n    \n    plug <- FALSE\n    \n    while (!plug) {\n        \n        result <- as.matrix(runif(n_assets - 1, min = lower, max = upper))\n        temp <- target - sum(result)\n        \n        if ((temp <= upper) && (temp >= lower)) {\n            plug <- TRUE            \n        }\n        \n    }\n    \n    result <- append(result, temp)\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrand_turnover1 <- function(n_sim, n_assets, lower, upper, target) {\n    \n    rng <- upper - lower\n    \n    result <- rand_weights3(n_sim, n_assets, lmbda) * rng\n    result <- result - rng / n_assets\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrand_turnover2 <- function(n_sim, n_assets, lower, upper, target) {\n    \n    result <- matrix(rand_iterative(n_assets, lower, upper, target), nrow = 1, ncol = n_assets)\n    \n    while (nrow(result) < n_sim) {\n        \n        temp <- matrix(rand_iterative(n_assets, lower, upper, target), nrow = 1, ncol = n_assets)\n        result <- rbind(result, temp)\n        \n    }\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlower <- -0.05\nupper <- 0.05\ntarget <- 0\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\napproach1 <- rand_turnover1(n_sim, n_assets, lower, upper, target)\napproach2 <- rand_turnover2(n_sim, n_assets, lower, upper, target)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(approach1), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-60-1.png){width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_pairs(as.data.table(approach2), title = \"Weight (%)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-61-1.png){width=384}\n:::\n:::\n\n\n# Mean-variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngeometric_mean <- function(x, scale) {\n    \n    result <- prod(1 + x) ^ (scale / length(x)) - 1\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- apply(returns_x_xts, 2, geometric_mean, scale = scale[[\"periods\"]])\nsigma <- cov(overlap_x_xts) * scale[[\"periods\"]] * scale[[\"overlap\"]]\n```\n:::\n\n\n## Maximum return\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mathbf{w}^T\\Sigma\\mathbf{w}&\\leq&\\sigma^{2}\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget <- 0.06\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://palomar.home.ece.ust.hk/MAFS6010R_lectures/slides_robust_portfolio.html\nmax_pnl_optim <- function(mu, sigma, target) {\n    \n    params <- Variable(length(mu))\n    \n    cons <- list(params >= 0, sum(params) == 1,\n                 quad_form(params, sigma) <= target ^ 2)\n    \n    obj <- Maximize(t(params) %*% mu)\n        \n    result <- solve(Problem(obj, cons))$getValue(params)\n    \n    return(result)\n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparams1 <- max_pnl_optim(mu, sigma, target)\nparams1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]\n[1,] 4.496723e-01\n[2,] 5.503276e-01\n[3,] 7.078720e-08\n[4,] 4.321928e-08\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmu %*% params1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] 0.04216773\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(t(params1) %*% sigma %*% params1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,] 0.06\n```\n:::\n:::\n\n\n## Minimum variance\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\min_{x}&\\mathbf{w}^T\\Sigma\\mathbf{w}\\\\\n\\textrm{s.t.}&\\mu^{T}\\mathbf{w}&\\geq&M\\\\\n&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget <- 0.03\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmin_risk_optim <- function(mu, sigma, target) {\n    \n    params <- Variable(length(mu))\n    \n    cons <- list(params >= 0, sum(params) == 1,\n                 sum(mu * params) >= target)\n    \n    obj <- Minimize(quad_form(params, sigma))\n        \n    result <- solve(Problem(obj, cons))$getValue(params)\n    \n    return(result)\n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparams2 <- min_risk_optim(mu, sigma, target)\nparams2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]\n[1,] 2.958446e-01\n[2,] 5.096489e-01\n[3,] 1.945065e-01\n[4,] 1.254106e-21\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmu %*% params2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,] 0.03\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(t(params2) %*% sigma %*% params2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] 0.04174501\n```\n:::\n:::\n\n\n## Maximum ratio\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\mu^{T}\\mathbf{w}-\\frac{1}{2}\\delta(\\mathbf{w}^T\\Sigma\\mathbf{w})\\\\\n\\textrm{s.t.}&e^T\\mathbf{w}&=&1\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nir <- 0.5\ntarget <- ir / 0.06 # ir / std (see Black-Litterman)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmax_ratio_optim <- function(mu, sigma, target) {\n    \n    params <- Variable(length(mu))\n    \n    cons <- list(params >= 0, sum(params) == 1)\n    \n    obj <- Maximize(t(mu) %*% params - 0.5 * target * quad_form(params, sigma))\n        \n    result <- solve(Problem(obj, cons))$getValue(params)\n    \n    return(result)\n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparams3 <- max_ratio_optim(mu, sigma, target)\nparams3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              [,1]\n[1,]  4.563530e-01\n[2,]  5.436470e-01\n[3,] -6.071938e-23\n[4,] -1.009534e-22\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmu %*% params3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n[1,] 0.0425548\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(t(params3) %*% sigma %*% params3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] 0.06075687\n```\n:::\n:::\n\n\n<!-- # Black-Litterman -->\n\n<!-- ## Prior distribution -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\text{Risk aversion: } &\\lambda=\\frac{E(r)-r_{f}}{\\sigma^{2}}=\\frac{IR}{\\sigma}\\\\ -->\n<!-- \\text{Implied returns: } &\\Pi=\\lambda\\Sigma w\\\\ -->\n<!-- \\text{Distribution: } &N\\sim(\\Pi,\\tau\\Sigma) -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ```{r} -->\n<!-- implied_pnl <- function(params, ir, sigma) { -->\n\n<!--     lmbda <- as.numeric(ir / sqrt(t(params) %*% sigma %*% params)) -->\n\n<!--     result <- lmbda * sigma %*% params -->\n\n<!--     return(result)     -->\n\n<!-- } -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- implied_pnl(params3, ir, sigma) -->\n<!-- ``` -->\n\n<!-- ## Conditional distribution -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\text{Prior mean variance: } &\\tau\\in(0.01, 0.05)\\approx(0.025)\\\\ -->\n<!-- \\text{Asset views: } &\\mathbf{P}={\\begin{bmatrix} -->\n<!-- p_{11}&\\cdots&p_{1n}\\\\ -->\n<!-- \\vdots&\\ddots&\\vdots\\\\ -->\n<!-- p_{k1}&\\cdots&p_{kn} -->\n<!-- \\end{bmatrix}}= -->\n<!-- {\\begin{bmatrix} -->\n<!-- 0&0&0&0&0&0&1&0\\\\ -->\n<!-- -1&1&0&0&0&0&0&0\\\\ -->\n<!-- 0&0&0.5&-0.5&0.5&-0.5&0&0 -->\n<!-- \\end{bmatrix}}\\\\ -->\n<!-- \\text{View returns: } &\\mathbf{Q}={\\begin{bmatrix} -->\n<!-- q_{1}\\\\ -->\n<!-- \\vdots\\\\ -->\n<!-- q_{k} -->\n<!-- \\end{bmatrix}}= -->\n<!-- {\\begin{bmatrix} -->\n<!-- 0.0525\\\\ -->\n<!-- 0.0025\\\\ -->\n<!-- 0.0200 -->\n<!-- \\end{bmatrix}}\\\\ -->\n<!-- \\text{View confidence: } &\\mathbf{C}={\\begin{bmatrix} -->\n<!-- c_{1}\\\\ -->\n<!-- \\vdots\\\\ -->\n<!-- c_{k} -->\n<!-- \\end{bmatrix}}= -->\n<!-- {\\begin{bmatrix} -->\n<!-- 0.2500\\\\ -->\n<!-- 0.5000\\\\ -->\n<!-- 0.6500 -->\n<!-- \\end{bmatrix}}\\\\ -->\n<!-- \\text{View covariance: } &\\mathbf{\\Omega}={\\begin{bmatrix} -->\n<!-- \\tau\\left(\\frac{1-c_{1}}{c_{1}}\\right)\\left(p_{1}\\Sigma p_{1}^{T}\\right)&0&0\\\\ -->\n<!-- 0&\\ddots&0\\\\ -->\n<!-- 0&0&\\tau\\left(\\frac{1-c_{k}}{c_{k}}\\right)\\left(p_{k}\\Sigma p_{k}^{T}\\right) -->\n<!-- \\end{bmatrix}}\\\\ -->\n<!-- \\text{Distribution: } &N\\sim(\\mathbf{Q}, \\mathbf{\\Omega}) -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ## Posterior distribution -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\text{Implied returns: } &\\hat{\\Pi}=\\Pi+\\tau\\Sigma \\mathbf{P}^{T}\\left(\\tau \\mathbf{P}\\Sigma \\mathbf{P}^{T}+\\mathbf{\\Omega}\\right)^{-1}\\left(\\mathbf{Q}-\\mathbf{P}\\Pi^{T}\\right)\\\\ -->\n<!-- \\text{Covariance: } &\\hat{\\Sigma}=\\Sigma+\\tau\\left[\\Sigma-\\Sigma\\mathbf{P}^{T}\\left(\\tau\\mathbf{P}\\Sigma\\mathbf{P}^{T}+\\mathbf{\\Omega}\\right)^{-1}\\tau\\mathbf{P}\\Sigma\\right]\\\\ -->\n<!-- \\text{Weights: } &\\hat{w}=\\hat{\\Pi}\\left(\\lambda\\Sigma\\right)^{-1}\\\\ -->\n<!-- \\text{Distribution: } &N\\sim\\left(\\left[\\left(\\tau\\Sigma\\right)^{-1}+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{P}\\right]^{-1}\\left[\\left(\\tau\\Sigma\\right)^{-1}\\Pi+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{Q}\\right],\\left[\\left(\\tau\\Sigma\\right)^{-1}+\\mathbf{P}^{T}\\Omega^{-1}\\mathbf{P}\\right]^{-1}\\right) -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ```{r} -->\n<!-- black_litterman <- function(params, ir, sigma, views) { -->\n\n<!--     # prior distribution -->\n<!--     weights_prior <- params -->\n<!--     sigma_prior <- sigma     -->\n<!--     lmbda <- as.numeric(ir / sqrt(t(weights_prior) %*% sigma %*% weights_prior)) -->\n<!--     pi_prior <- lmbda * sigma_prior %*% weights_prior -->\n\n<!--     # matrix calculations -->\n<!--     matmul_left <- views[[\"tau\"]] * sigma_prior %*% t(views[[\"P\"]]) -->\n<!--     matmul_mid <- views[[\"tau\"]] * views[[\"P\"]] %*% sigma_prior %*% t(views[[\"P\"]]) -->\n<!--     matmul_right <- views[[\"Q\"]] - views[[\"P\"]] %*% pi_prior -->\n\n<!--     # conditional distribution -->\n<!--     omega <- diag(diag(diag((1 - views[[\"C\"]]) / views[[\"C\"]]) %*% matmul_mid)) -->\n\n<!--     # posterior distribution -->\n<!--     pi_posterior <- pi_prior + matmul_left %*% solve(matmul_mid + omega) %*% matmul_right -->\n\n<!--     sigma_posterior <- sigma_prior +  views[[\"tau\"]] * sigma_prior - -->\n<!--         matmul_left %*% solve(matmul_mid + omega) %*% (tau * views[[\"P\"]] %*% sigma_prior) -->\n\n<!--     weights_posterior <- t(pi_posterior) %*% solve(lmbda * sigma_prior) -->\n\n<!--     # implied confidence -->\n<!--     pi_posterior_100 <- pi_prior + matmul_left %*% solve(matmul_mid) %*% matmul_right -->\n\n<!--     weights_posterior_100 <- t(pi_posterior_100) %*% solve(lmbda * sigma_prior) -->\n\n<!--     implied_confidence <- (weights_posterior - weights_prior) / (weights_posterior_100 - weights_prior) -->\n\n<!--     result <- list(\"implied_confidence\" = implied_confidence, -->\n<!--                    \"weights_prior\" = t(as.matrix(weights_prior)), -->\n<!--                    \"weights_posterior\" = weights_posterior, -->\n<!--                    \"pi_prior\" = t(pi_prior), -->\n<!--                    \"pi_posterior\" = t(pi_posterior), -->\n<!--                    \"sigma_prior\" = sigma_prior, -->\n<!--                    \"sigma_posterior\" = sigma_posterior) -->\n\n<!--     return(result)    -->\n\n<!-- } -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- tau <- 0.025 -->\n<!-- P <- diag(length(factors)) -->\n<!-- Q <- t(implied_shocks(0.1, overlap_x_xts, overlap_x_xts[ , 1], 1)) -->\n<!-- C <- rep(0.95, length(factors)) -->\n<!-- views <- list(\"tau\" = tau, \"P\" = P, \"Q\" = Q, \"C\" = C) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- bl <- black_litterman(as.vector(params3), ir, sigma, views) -->\n<!-- bl -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- params4 <- as.vector(bl[[\"weights_posterior\"]]) -->\n<!-- params4 <- params4 / sum(params4) # no leverage -->\n<!-- params4 -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- mu %*% params4 -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- sqrt(t(params4) %*% sigma %*% params4) -->\n<!-- ``` -->\n\n# Risk parity\n\nRisk parity is an approach to portfolio management that focuses on allocation of risk rather than allocation of capital. In a risk parity strategy, the asset allocations are leveraged, or deleveraged, to have equal risk contributions. Suppose that $\\mathbf{R}$ is a $T \\times N$ matrix of asset returns where the return of the $i^{th}$ asset is $R_{i,t}$ at time $t$. Define $\\Sigma$ to be the covariance matrix of $\\mathbf{R}$ and let $\\mathbf{w}=(w_{1},\\dots,w_{N})$ be a vector of asset weights. Then the volatility of the return of the strategy is $\\sigma_{P}=\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}$ and, by Euler's Theorem, satisfies:\n\n$$\n\\begin{aligned}\n\\sigma_{P}&=\\sum_{i=1}^{N}w_{i}\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}\\\\\n&=w_{1}\\frac{\\partial\\sigma_{P}}{\\partial w_{1}}+\\dots+w_{N}\\frac{\\partial\\sigma_{P}}{\\partial w_{N}}\n\\end{aligned}\n$$\n\nwhere each element is the risk contribution of the $i^{th}$ risky asset. The risk parity objective solves for weights such that each asset contributes equal risk using the following nonlinear constrained optimization problem:\n\n$$\n\\begin{aligned}\n\\begin{array}{rrcl}\n\\displaystyle\\max_{x}&\\displaystyle\\sum_{i=1}^{N}\\log(w_{i})\\\\\n\\textrm{s.t.}&\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}&\\leq&\\sigma \n\\end{array}\n\\end{aligned}\n$$\n\nTo incorporate these conditions into one equation, introduce a new variable $\\lambda$ that is the Lagrange multiplier and define a new function $\\mathcal{L}$ as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w},\\lambda)&=\\sum_{i=1}^{N}\\log(w_{i})-\\lambda(\\sqrt{\\mathbf{w}^T\\Sigma\\mathbf{w}}-\\sigma)\n\\end{aligned}\n$$\n\nThen set the partial derivatives of $\\mathcal{L}$ equal to zero for each asset $i$:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\mathcal{L}(\\mathbf{w},\\lambda)}{\\partial w_{i}}&=\\frac{1}{w_{i}}-\\lambda\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}=0\n\\Leftrightarrow\nw_{i}\\frac{\\partial\\sigma_{P}}{\\partial w_{i}}=\\frac{1}{\\lambda}\n\\end{aligned}\n$$\n\nNotice that $1/\\lambda$ is the risk contribution of the $i^{th}$ asset. Now use `R` to maximize the Lagrangian numerically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# http://faculty.washington.edu/ezivot/econ424/riskbudgetingslides.pdf\n# https://systematicinvestor.wordpress.com/2011/11/16/black-litterman-model/\n# https://cran.r-project.org/web/packages/BLCOP/vignettes/BLCOP.pdf\n# http://math.stackexchange.com/questions/17776/inverse-of-the-sum-of-matrices\nrisk_parity_optim <- function(sigma, target) {\n    \n    params <- Variable(nrow(sigma))\n    \n    risk <- quad_form(params, sigma)\n    risk_contrib <- target ^ 2 / nrow(sigma)\n            \n    obj <- Maximize((sum(log(params)) - (1 / risk_contrib) * (risk - target ^ 2)))\n        \n    result <- solve(Problem(obj))$getValue(params)\n    result <- result / sum(result) # no leverage\n    \n    return(result)\n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget <- 1\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparams5 <- risk_parity_optim(sigma, target)\nparams5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] 0.02997364\n[2,] 0.13366437\n[3,] 0.57124484\n[4,] 0.26511714\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrisk <- as.numeric(sqrt(t(params5) %*% sigma %*% params5))\nrisk_contrib <- params5 * sigma %*% params5 / risk\nrisk_contrib\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    [,1]\nSP500        0.002420125\nDTWEXAFEGS   0.002420123\nDGS10        0.002420123\nBAMLH0A0HYM2 0.002420124\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmu %*% params5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n[1,] 0.004171147\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(t(params5) %*% sigma %*% params5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n[1,] 0.009680495\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\"max_pnl\" = params1,\n           \"min_risk\" = params2,\n           \"max_ratio\" = params3,\n           # \"black_litterman\" = params4,\n           \"risk_parity\" = params5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       max_pnl     min_risk     max_ratio risk_parity\n1 4.496723e-01 2.958446e-01  4.563530e-01  0.02997364\n2 5.503276e-01 5.096489e-01  5.436470e-01  0.13366437\n3 7.078720e-08 1.945065e-01 -6.071938e-23  0.57124484\n4 4.321928e-08 1.254106e-21 -1.009534e-22  0.26511714\n```\n:::\n:::\n\n\n# Portfolio attribution\n\n## Single-period\n\nThe arithmetic active return is commonly decomposed using the Brinson-Fachler method:\n\n$$\n\\begin{aligned}\n\\text{Allocation: } &r_{a}=\\sum_{k=1}^{n}(w_{p,k}-w_{b,k})(r_{b,k}-r_{b})\\\\\n\\text{Selection: } &r_{s}=\\sum_{k=1}^{n}w_{p,k}(r_{p,k}-r_{b,k})\\\\\n\\end{aligned}\n$$\n\nwhere $k=1,\\ldots,n$ is each sector or factor.\n\n## Multi-period\n\nArithmetic attributes add to the active return of a single period; however, they cannot be summed or compounded to explain the active return over multiple periods. To solve this problem, the original arithmetic attribute is multiplied by a single scaling coefficient for that period. After all single-period original attributes have been transformed, the adjusted attributes sum to the active return over the periods.\n\n$$\n\\begin{aligned}\n\\text{Carino scaling coefficient: } &c_{t}=\\frac{[\\ln(1+r_{p,t})-\\ln(1+r_{b,t})]/(r_{p,t}-r_{b,t})}{[\\ln(1+r_{p})-\\ln(1+r_{b})]/(r_{p}-r_{b})}\n\\end{aligned}\n$$\n\nwhere $t=1,\\ldots,n$ is each period.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# http://www.frongello.com/support/Works/Chap20RiskBook.pdf\n# https://github.com/R-Finance/PortfolioAttribution/blob/master/R/Carino.R\npnl_attrib <- function(params, x) {\n    \n    total_i <- rowSums(x)\n    total <- prod(1 + total_i) - 1\n    \n    coef <- (log(1 + total_i) / total_i) / (log(1 + total) / total)\n    \n    result <- colSums(x * coef)\n    \n    return(result)\n    \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nattrib_mat <- sweep(tail(na.omit(returns_xts)[ , factors], width), 2, params1, \"*\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npnl_attrib(params1, attrib_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        SP500    DTWEXAFEGS         DGS10  BAMLH0A0HYM2 \n 9.715323e-02 -4.220013e-02  3.077217e-10  2.148297e-10 \n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}